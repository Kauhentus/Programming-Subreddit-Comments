Windows Rustacean here, I will do my part because being a Windows Rustacean is suffering. 
&gt; And also there is the issue that if you use the system malloc, code will start relying on the ability to interchange malloc/free and Rust boxes, and it won't be possible to ever change this choice. That's not necessarily true; we can still declare that the behavior is undefined if we want to. We have to always reserve the right to change undefined behavior, because in the limit we couldn't change anything at all if we didn't.
Relevant issue: https://github.com/rust-lang/rust/issues/16456
I meant we should be able to ditch full-blown comparators in this case for simple Eq&lt;K&gt; impls (when that's supported). Unlike Sorted Maps, I don't think you need quite such a robust comparison mechanism.
And I just thought his phrasing was weird…
I think we should start posting to /r/playrust as retaliation :-)
"I'm having lifetime problems ... I keep dying"
Worked on Windows 8.1 64-bit. Two minor nitpicks: * Use HTTPS when fetching 7-zip. * Don't hardcode C:\TEMP as the temporary directory. I think it's better to use TEMP or TMP environment variables if they have been set. Powershell might also provide an even better way for this.
It's about 2m in my machine. I guess this may be undesirable if you have a lot of small rust scripts (for instance, unit testing. If you have hundreds of small testing scripts under test/, you would need gigabytes to compile your code. In this case you would be better off by using dynamic linking).
You can wrap a function taking a function pointer and a data pointer into a function taking a closure by passing a pointer to the closure as a data and using a shim to cast back and call it.
Rust's tasks aren't any more isolated than native threads. Exiting or aborting on one task will bring down the whole process, and various forms of shared memory are permitted in safe code along with transferring memory allocations.
No, providing a safe M:N threading abstraction is unrealistic. It's *already* unsound and adding support for real TLS would make it much more blatant. It's only safe in isolation from very basic primitives like TLS and SIMD.
Thanks for doing this. As /u/tomaka17 pointed out I have a bug on my plate to make Cargo and Rust install together on all plattforms. That issue in particular does not involve creating a rustup script for Windows, but that is something I'm interested in doing if it's desirable. I'm curious about what you think the best solution for Windows is. If Cargo and Rust were both [in the same installer](https://github.com/rust-lang/rust/issues/16456) would there still be any desire for a script-based solution on Windows? Is powershell installed by default on Windows these days and the preferred way to run Windows scripts (vs. cmd.exe)? If script-based installation *is* still desirable once #16456 is fixed would it be preferable not to invoke a GUI at all?
The amount of allocated memory has very little impact on the performance of jemalloc. It *never* performs any operations that are `O(n)` where `n` is the amount of allocated memory. It does perform `O(log n)` work via red-black tree manipulations but that work is performed in batches via thread caches and *soon* arena caches underlying those and only when the caches need to be flushed or filled.
&gt; per thread and therefor heap Rust has never had "per thread heaps" and it's unclear what they would accomplish beyond increased memory usage.
The string interpolation itself is pure macros. It just parses a string and produces an AST, no even need for an implicit. The thing you pointed to refers to exactly String macros, and their exact sugar.
PowerShell definitely ships with windows now. At least since 7, not sure about earlier than that. Not familiar with the scripting scene on windows. There's a lot of people who still just use `cmd` out of sheer momentum/comfort, though. A GUI for installation on windows is still overwhelmingly the convention for almost everything. It might be nice to have a "update rust" command installed in the path with rustc and cargo, though.
Thanks for the confirmation on 8.1 x64 and the suggestions, they will be implemented.
Works well for me. - Ran as administrator, else copying binaries failed - Would love to prevent the rust installer gui from showing (some sort of -y flag)
The fact that Cargo and Rust don't install together was the main reason I wrote this script. If the Rust installer included Cargo by default, that would be wonderful. Is there anything else that rustup.sh does that would be useful to add to this script? As for Powershell, the only annoyance is the ExecutionPolicy detail. Other than that, it should be just as supported as a Batch scripts on modern Windows.
Cargo will not auto-update things. It will build the same versions of dependencies that are in your Cargo.lock, as long as you have it. The reason your stuff broke is that you updated the compiler, not the dependencies. Basically, just don't update until you're ready to deal with some possible breakage. I personally recompile Rust every Monday morning, and go from there. ;)
You currently can't build a closure and return it from a function, because the variables it captures are by-reference and thus die with the function. That requires unboxed closures, which are still incomplete and will have a slightly different syntax.
I *guess* you'd need to restore both the files and their timings in a lot of different places (/root/.cargo, the working directory) to prevent the Cargo from git-updating the packages after a Rust rollback. You could use https://www.docker.com/ or some other system with snapshots (BTRFS, ZFS, LVM) to do that in a systematic way. I'm using Docker: A Dockerfile script installs a fresh Rust and tries to build the project with it. If everything builds and passes the tests then I have a new container image to send to the servers and the team. If something fails then we simply keep using the previous image until things are fixed. Inside the container you can experiment with upgrading, then if something fails you simply restart the container returning to a known good state.
It is bundled since Vista, but it is already at version 4. So care needs to be taken about which commands to use, if to remain portable across versions.
Was your Rust install directory into Program Files? That would explain why you needed admin privileges in order to copy binaries. Were you running 32 bit or 64? I can't test 32 bit so I'm waiting for someone to confirm it works.
AFAIU, something like `Vec&lt;Uninit&lt;T&gt;&gt;` can be used instead of reimplementation of vector-like type with realloc, capacity, etc. `Uninit&lt;T&gt;` is something like this: https://gist.github.com/stepancheg/218d34510917e2b4f63b
As a frequent rust observer (not yet contributor) it seems like most libraries have to eventually include *some* unsafe code to get good perf. Is it the case that the most performant implementations can't be only safe code, on the whole?
Often, yes, in library code. The idea is that you can write a safe implementation first, and then tweak those parts of it that aren't performing well enough using unsafe code. However, it should be noted that this doesn't necessarily have to extend into regular programs--most of Rust's submissions on the benchmark game don't have any unsafe in them, since it is all safely abstracted behind existing libraries. As Rust's type system improves and the ecosystem matures, that will be more and more true. It should also be noted that while the libraries use quite a bit of unsafe code, it is dwarfed by the amount of safe code--making it much easier to pinpoint problems and know where to look for potential badness (the first thing I do when I check out any Rust project is scan for unsafe).
100% safety in the face of panics is always going to be a bit wasteful (option dance), but Collections are a special case IMO. They're effectively computational primitives, and efficient computational primitives are almost certainly going to need unsafe code. Once they're written all the perf is free to use in safe code for users, though. The biggest challenge in writing the standard libraries is to make fast, usable, and safe abstractions so others don't have to.
I don't think that works. How do you Drop such a Ringbuf?
`Drop` needs to be implemented same same way it is implemented in patch. But functions like `capacity` and `reserve` can be reused from `Vec`.
The `regex` crate doesn't use *any* `unsafe` code. (The `regex!` macro does use it once.) (It's also arguably the slowest Rust benchmark, so maybe this isn't a good example, but I contend that it is slow primarily because of the algorithm used.)
Longterm it would be leaving perf on the table, though. The `len` field of the Vec would be wasted space. Also, this isn't implemented in the patch (future work), but by manually taking over reserve we can make it more efficient by manually copying the data when realloc fails to run in-place (common). This is something Vec could never do for us. I agree the duplicated logic between the two is unfortunate, but a lot of this should be resolved by pnkfelix's plans for the future allocator APIs.
The golden standard of installers for Windows applications are [Windows Installer](http://en.wikipedia.org/wiki/Windows_Installer) packages. They can be scripted for unattended install among other things. I could help with creating one for Rust+Cargo, if the story with building the artifacts to be packaged were a bit more clear. The workflow proposed in [#16456](https://github.com/rust-lang/rust/issues/16456) requires setting up multiple build bots and servers for intermediate tarballs, which only Rust core developers can do. IMO, it'd be much simpler if 'make dist' in Rust repo would download nightly cargo and incorporate it into the platform package it creates. Alternatively, cargo could be an optional submodule of the Rust repo (kinda like clang is an optional sub-project of LLVM). Of course, building from source takes longer, but #16456 proposes building Rust from source anyways, and it's by far the longer build of the two.
For the point you are trying to make, D is a pretty poor example. First, D's CTFE is already very complex (and powerful) even tho it is still limited compared to a full VM. Second, you can already use D as a scripting language. So whatever you want to do with a compiler VM and/or CTFE you can already do by writing a D script that runs itself and modifies some source file that then gets compiled. It is not a matter of "have fun compiling big project". It is a matter of, if you are going to allow it anyways, make it at least easy to do. D's CTFE "VM" is optimized, but could be better. The same would apply for a "full" VM for D.
I don't bother with GUI installers if there's a chocolatey package.
That's great! I noticed the `Vec&lt;Option&lt;T&gt;&gt;`-based implementation of `RingBuf` just a couple of days ago when I implemented a double-ended priority using `RingBuf`. But that was before I learned how cool [interval heaps](https://github.com/sellibitze/morerustcollections/blob/master/src/interval_heap.rs) are. ;)
Technically, you *could*, it would just be a bit ungainly. From the [RFC]: trait IterableOwned { type A; type I: Iterator&lt;A&gt;; fn iter_owned(self) -&gt; I; } trait Iterable { fn iter&lt;'a&gt;(&amp;'a self) -&gt; &lt;&amp;'a Self&gt;::I where &amp;'a Self: IterableOwned { IterableOwned::iter_owned(self) } } I was under the impression that the question of implementing higher-kinded types was being deferred until after 1.0. [RFC]: https://github.com/rust-lang/rfcs/blob/master/text/0195-associated-items.md#encoding-higher-kinded-types
It would be a "logical" heap backed by the same actual heap, with perhaps some minor tweaks to improve multithreaded performance (you *know* each allocation is only ever modified by one thread). The separation is more to avoid stopping all threads for stop-the-world GC, so conceptually each thread has its own heap with its own GC, even though all those GCs are backed by the same allocator. 
Again, I'm not saying they would be implemented as actually separate heaps, just that they're logically separated so that GC can collect parts of the heap without affecting others. You'd need some kind of internal flagging to indicate which memory goes where, but presumably there's already some per thread arenas (cache) or whatever in your allocator to avoid contention so there wouldn't be any real extra overhead.
I really loved static-by-default in Go, and how easy it was to set up cross compilation toolchains. Would love to see that in Rust too so I could develop tools for RPi and Android easily.
N00b question: would you use a Box in this case on the variable, then? As I understand it Boxes are there to explicitly pass ownership, so passing ownership to the closure would make it share the closure's scope?
did I read rightly the D creator giving the rationale: being able to do absolutely anything at compile time is just too dangerous?
Did forget that I may have better posted this question at stackoverflow.
Rust would really need a GUI library designed around its' ideas (enums are really good for messages, state machines..). I think people in the piston community might be experimenting in this direction ? Seems like GUI frameworks are heavily language dependant. apple-verse: objC. Microsoft C(win32) -&gt; C++(MFC) -&gt; C#. Android - Java. web: javascript/DOM. In these examples, new languages (e.g. objC-&gt;swift, C#-&gt;F#, java-&gt;scala) use their platforms existing GUI's by virtue of being designed around the underlying system of the language the GUI was designed for. Rust being "native code" is closest to C++ (use of which is sadly an afterthought on android), but it also declares a lot of what C++ does to be misfeatures hence can have have trouble interfacing with it (not just unsafe, but language philosophy). eg a while back they tried to port Qt... and those API's use overloading. Is the situation any different now? (multiparam traits might help?)
yes, porting a native code GL game between platforms is easier than porting a platform dependant GUI
Ow, my brain. Dammit Rust.
If you're using btrfs you could easily use subvolumes which is effectively just a snapshot of a partition or directory. There are some nice guides on how to do this, and with the assistance of tools like `snapper` you can easily take a snapshot, update, and then IF anything broke just role back. 
Before or after the `debug_assert!` change? It seemed to me like when they were introduced unfortunately performance got back to where it was.
&gt; Is it the case that the most performant implementations can't be only safe code, on the whole? - C++: 100% unsafe - Rust: X% unsafe Being `unsafe` free would be nice, but it's impossible to be less safe than C++ or C anyway so I'll take even 50% unsafe if I can! Furthermore, as you did notice, it's mostly used where performance matters. Applying the typical 80/20 rule, we thus get that in writing in Rust you end up with (X / 5)% of the code being unsafe, so that even in the extraordinary case where you need 100% unsafe code for your performance hogs, it's still only 20% of the overall program.
Yes, the two are equivalent. I don’t see how that makes `if` as an expression redundant in the slightest. What exactly do you mean?
This case can be expressed conveniently in both forms, since it's just matching against constants. How about a more complicated expression, like: fn main() { let a = 1i; let b = match a { _ if a % 2 == 0 =&gt; 2i, _ =&gt; 3 }; let c = if a % 2 == 0 { 2i } else { 3 }; } In this case the match needs a guard, takes longer to type, and is less readable.
One can definitely regard `if` as syntactic sugar for `match`: *any* `if cond { stuff } else { other_stuff }` can can be written as match cond { true =&gt; { stuff } false =&gt; { other_stuff } // (or `=&gt; {}` if there is no else) } or match () { _ if cond =&gt; { stuff }, _ =&gt; { other_stuff } } (The latter lends itself to longer `if`-`else` chains more nicely.) However, IMO, it would be rather annoying to have to write a full `match` for every single `if` expression, and a mechanical translation obscures meaning, so having the option to use an `if` is nice. Sure, for plain equality with primitive types it's not so bad and often better (because there's nice pattern matching on the values themselves), but any non-trivial condition is going to have to do the `true`/`false` match like the above. (Making `if` *not* an expression would be a strange special case: at the moment the only non-expressions are declaring names, e.g. `let ...` and `fn foo() {}`, for which being an expression would obscure scoping rules.)
OK, I see the rationale, thnx.
Yes. Yes. C, Java etc. are the same with "if" vs "switch". The reason "if" exists is that it is shorter and more closely matches the programmer's mental model in some case. Almost everyone generally thinks "if this, do that else do that other thing" rather than "let's act differently depending the truth of this: in case it equals true, let's do this and in case it equals false, let's do that other thing". 
&gt; First, when you import something (use something;), this equivalent to the from something import *, which is considered harmful This is not the case. `use something;` just brings the `something` name into scope, and any use of functionality from the module has to be qualified like `something::foo()`, just like the safe `import something` in Python. (Don't know/have an opinion on the rest.)
if without else works fine.
You're right ! I don't know what made me think that, sorry for the noise, I'll strike that part.
(and is an expression that returns `()`, similar to `for` or `while`.)
Is there any difference in the optimizations? Large matches apparently have some hashtable thingy, would that work for the equivalent if?
I'm no expert here, but: 1. Rust imports *are* explicit by default (though there is ::* if you need it); 2. In a systems language like Rust it really makes no sense IMO to have first-class types and modules which can be stored in arrays; 3. The module syntax is nothing like C++ include syntax; 4. Not to be rude, but some of the other points are a little too incoherent to respond to (e.g. "types have no type in rust")
&gt; In a systems language like Rust it really makes no sense IMO to have first-class types and modules which can be stored in arrays; ``` use commands; for command in commands::all() { if command::name() == name { command::fire() } } ``` &gt; The module syntax is nothing like C++ include syntax I was talking about the ```::``` syntax, why have ```something::useful()``` instead of ```something.useful()``` ? &gt; Not to be rude, but some of the other points are a little too incoherent to respond to (e.g. "types have no type in rust") Well, what's the type of a type in Rust ? Why can't I do ```rust let shape_types: ??? = [Cube, Circle, ...] fn create_shape(name: String) -&gt; box&lt;Shape&gt; { for shape_type in shape_types { if shape_type.name == name { return box shape_type() } } } ``` 
llvm should treat both constructs identically
You can do that with traits. Modules are just for encapsulation and source code organization.
Well, since you plan to call each `shape_type` as a function, you could just store `fn() -&gt; Box&lt;Shape&gt;` values (which are function pointers), alongside name strings. But that really doesn't describe the types at all, just a way to create values of those types, as opaque trait objects. If you're after a form of reflection, you might be interested in the `Encodable`/`Decodable` serialization support.
Coming from a Haskell background, match is much more powerful than if; in Haskell, `case` expressions (like `match` in Rust) allows refining types. An example: -- like enum X&lt;T&gt; in Rust, but each enum element is tagged with a type. data X a where I :: X Int B :: X Bool -- fn f&lt;A&gt;(X&lt;A&gt; x) -&gt; A {...} f :: X a -&gt; a f x = case x of I -&gt; 1 B -&gt; False On the right-hand-side of the `-&gt;` in the case expression, the type variable `a` is constrained by the declaration of the type `X`, which allows returning *different types* from each branch, since the return type we want is now statically known.
&gt; use commands; for command in commands::all() { if command::name() == name { command::fire() } } I take it `::all()` is meant to return an iterator over all the functions in the module `commands` (and similarly `command::name()` is meant to get the name of the specific function `command`?). What happens with: mod foo { fn bar() {} fn baz(_: int) -&gt; int {} } for func in foo.funcs() { func.fire() } `bar` and `baz` have different type signatures, so how can `fire` work?
You're right, i've done that, I was just wondering why static polymorphism is not available for modules: ```fn fire_command&lt;T&gt;(command: T);``` cannot possibly accept a modules. Even if it where the implementation syntax wouldn't be compatible between ```module::function()``` call and ```instance.function()```. 
In python, that's done dynamically, in Rust you'll have to store all those as an array of function pointer with the same signature. What I don't understand is why modules can't be passed to a function: fn fire_baz&lt;T&gt;(something: T) -&gt; result_of(something.baz()) { something.baz() } My question is really just about "passing" modules around, especially when it's statically realistic.
Cargo also has no unsafe code.
Thanks for pointing out the serialization support, which is mostly about data than methods. &gt; Well, since you plan to call each shape_type as a function, you could just store fn() -&gt; Box&lt;Shape&gt; values (which are function pointers), alongside name strings. The design I've come up with is roughly that: trait Command { fn name(&amp;self) -&gt; String { use std::intrinsics::get_tydesc; use std::ascii::AsciiStr; let name = unsafe { (*get_tydesc::&lt;Self&gt;()).name.to_ascii().to_lowercase() }; return String::from_str(name.as_str_ascii()); } } struct Init; impl Command for Init { } struct Lol; impl Command for Lol {} fn all&lt;'a&gt;() -&gt; Vec&lt;Box&lt;Command + 'a&gt;&gt; { return vec![ box Init as Box&lt;Command&gt;, box Lol as Box&lt;Command&gt;, ]; } fn main() { println!("Commands:") for cmd in all().iter() { println!("\t- {}", cmd.name(),); } } You can see the problem here, I have to instantiate a command to get its name. Do you have a better way to make a factory ?
To call that `baz` method in Rust, you'd need a trait bound on `T` (bounded polymorphism, unlike duck-typing, e.g. C++ or Python), and an implementation of that trait for the module (but the module isn't a type/value, so how do you make that work?).
As noted in the thread, the debug_assert stuff should get stripped away in release builds when that's A Thing. However even with them, perf is still *much* better (can't recall/find the precise numbers).
MSI, Chocolatey.
&gt; In a systems language like Rust it really makes no sense IMO to have first-class types and modules which can be stored in arrays; I disagree here about first class types. A type list is so useful in a systems programming language like C++ that before C++11 there were lots of libraries that emulated them (Loki, Boost.MPL and Boost.Fusion among others), and in C++11 they are built into the language in the form of parameter packs. There is currently a discussion about standardizing a minimal metaprogramming library in C++17 that provides algorithms and data-structures for type manipulation (Eric Niebler has a blogpost about this).
I'm not a regular windows user, but I have had to do some packaging for Windows based on customer demand, and my impression is that a Windows Installer package (aka MSI) that includes all of the necessary components to get have a usable system is the gold standard for deploying windows software. As someone who normally does packaging on Unix like platforms, I must say that the process of building such packages is quite frustrating (we use [wix](http://wixtoolset.org/) and found the documentation to be a bit lacking), but once they are built they will work with most of the tools that people commonly use for deploying packages on Windows. I've also used Inno Setup, and while I found Inno Setup much nicer to use, it doesn't produce a Windows Installer package which is what a variety of administrative tools prefer to consume. It looks like Chocolatey is the preferred repository of script-based wrappers around such packages to allow you to conveniently download and install such packages by name from the command line without having to click through the UI of the installer. Once Rust and Cargo were available in such a form, adding the wrapper script to Chocolatey would likely be fairly easy, and would avoid the annoying details of having to fiddle with trust settings to execute PowerShell scripts that rustup.ps1 has.
I don't understand what you're suggesting, Python has a strongly typed system. However, what you can do with types in Python must be done at compile time in a statically typed system like Rust (or C++). I'm wondering why namespaces can't be used as arguments to function, and why the syntax has to be different from the one used to access methods. Maybe I'm not as educated as you are, but it seems to me that types can be also values, of type `type` (in Python). And it can be /proven/ at compile time whether or not a method belong to it.
If all of the types have a ::new() constructor, you can make a factory factory with a macro. Unfortunately I'm on mobile and tying code is hard, but its relatively easy to write a macro `create_factory!(shape_factory, Shape, [Circle, Square, Triangle])` that expands into a `fn shape_factory(name: &amp;str) -&gt; Box&lt;Shape&gt;`. The macro itself is quite small, and reusable!
&gt; What do you think ? What prevented such thing to happen ? Because time is precious and limited. Having modules as a thing you can inspect and pass around is probably one of the least demanded feature. Adding this in a concise and performant way takes time which is better spend on more important issues because there are a **lot** of them (trait inheritance, deriving of user defined types, GC, green threads, etc. pp.). In short: Won't happen even in the mid future.
Example: trait Test { fn test(self); } struct A { b: int } impl Test for A { fn test(self) { println!("{}", self.b) } } fn main() { let a = box A { b: 42 } as Box&lt;Test&gt;; }
&gt;Python has a strongly typed system. The theoretically-minded like to argue about whether Python has a typesystem *at all*. Python values come attached with tags, and those tags determine the semantics of how these values can be used. When they are misused, a "TypeError" gets thrown, but it's up for debate whether this has anything to do with types. In theory, types are a syntactic restriction on programs; by definition, the way the program runs can *not* be influenced by its type, or lack thereof; types are only used to reject invalid programs *at compile-time*. Some languages extended their type systems with run-time tags; those are used for reflection, `instanceof`, et cetera. Those have serious implications for performance, which is why high-performance languages such as C, C++, etc. tend to shun reflection. Other languages have no type system to speak of, and rely purely on run-time tags; this is the path of Python, Ruby, etc. &gt;I'm wondering why namespaces can't be used as arguments to function, and why the syntax has to be different from the one used to access methods. What you're asking for can easily be done using structs instead of namespaces. Namespaces are meant to be used for program organization, not metaprogramming.
if is just easier to read and write in some cases, match in others. depends on the nesting etc.. They've further added 'if let' which is like sugar for more of what match can do, if i've understood correctly. if the language didn't have either, people would be making macros. the language engine can do these things more elegantly
I'd go with match since there's a lot less typing and a lot more pattern matching. Take advantage of the pattern matching, man. 
Thanks for your answer :)
Thanks ! I'll look into that !
citation needed
If is sugar for match. Match is sugar for X which is sugar for Y which is sugar for Z which is sugar for assembly, etc. Is match redundant? Couldn't we just write assembly?
&gt; C++: 100% unsafe C++ code usually doesn't use a lot of raw pointers. So you are in a similar situation as in Rust: You have X% unsafe code in some libraries and your main program just uses that. &gt; Furthermore, as you did notice, it's mostly used where performance matters. Since the language is intended to be used where performance matters, that's really the most important use case. If people end up writing lots of unsafe code when they want best performance, that is a problem. It means that Rust's safety features are not quite as good and flexible yet as they'll need to be in the long run.
I agree with you, but I think you are giving C++ a little too much credit here. The C++ stdlib tries hard not to expose unsafe interfaces, but it still does--in the form of iterators and so on. And you can easily cause problems by just using references, statics, or globals, you don't need raw pointers to be unsafe. You can already write a lot more high performance code in safe Rust than you can in "safe C++", aka the C++ stdlib without raw pointers, references, iterators, string_view, statics, globals, and so on (or even if you are allowed to use the full stdlib).
if and switch are not the same in C, as far as I know. At least, I don't think if statements are ever optimized to a jump table.
If you can't use filesystem-level snapshots, there's [rsnapshot](http://www.rsnapshot.org/). I install rustc into its own directory instead of /usr(/local) to be able to swap versions easily if needed. In your project don't forget to check Cargo.lock into version control!
Pardon my ignorance, but I've seen that acronym thrown around a lot in the things I've been reading. What does it stand for?
Ah, so `if` actually always has an implicit `else` clause that returns `()`?
That's not what I meant by redundant, in this case.
It stands for uniform call syntax. This is the corresponding [rfc](https://github.com/rust-lang/rust/issues/16293).
This is correct, at least with GCC.
`if let` is for refutable pattern matching, whereas a `match` expression needs to be irrefutable.
LLVM uses phi nodes: declare double @foo() declare double @bar() define double @baz(double %x) { entry: %ifcond = fcmp one double %x, 0.000000e+00 br i1 %ifcond, label %then, label %else then: ; preds = %entry %calltmp = call double @foo() br label %ifcont else: ; preds = %entry %calltmp1 = call double @bar() br label %ifcont ifcont: ; preds = %else, %then %iftmp = phi double [ %calltmp, %then ], [ %calltmp1, %else ] ret double %iftmp }
**U**niform **f**unction **c**all **s**yntax
 &lt;Foo as Trait&gt;::bar() I think. Double check the RFC linked below.
NB. there is no *guarantee* that they are the same, but they are often optimised to the same thing, especially for simple integer equality.
whats the precise difference there: i had thought if let ..=x {} is basically like match x{ ...=&gt;{}, _=&gt;{} }; does 'irrefutable' refer to the entire body of the match - or is there another difference in what can go in the pattern (i haven't used this feature myself yet)
I think you need to output some more flags, specifically `-l` (lower-case L), e.g. [libgit2-sys's build script](https://github.com/alexcrichton/git2-rs/blob/a3f9a99fe64934814d70e0a6797699a89eb7b031/libgit2-sys/build.rs#L61-L69).
There is no synchronization for the thread caches in jemalloc.
It's not possible to implement the full API of the jemalloc-specific functions on top of the libc allocator. &gt; rust_alloc a weak symbol that uses libc malloc unless it is overrides by a strong rust_alloc defined by the executable which uses jemalloc. This would add two levels of indirection to all allocation calls, which would be awful for performance. There would be the wrapper for weak linking and then an adaptor function to convert from the exposed API to the jemalloc API.
This isn't about micro-benchmarks. The glibc allocator compares very poorly to jemalloc in terms of concurrency / scalability, memory usage and data locality. It also has pathological fragmentation issues rather than tight bounds like jemalloc.
&gt; Don't already modern allocators do something similar with thread-local caches? Yes, modern allocators already have thread caches where no synchronization is required. They need to balance the size of the thread caches against the fact that caching in each thread is external fragmentation. Entirely independent thread-local allocators would lead to pathological increases in memory usage and wouldn't perform significantly better than a very optimistic cache. Everyone wants to chime in on this issue despite having a poor grasp of it, and that definitely applies to the core developers too. I don't think this post does a good job presenting the facts. It's tainted by the fact that it places equal weight in the dubious counter-arguments that were used to argue against a change without negative consequences. The caching in jemalloc is quite conservative (for good reason) but there's nothing to gain from per-thread allocators that's not obtainable by making the existing thread caches more greedy. &gt; Any ideas on what (if any) optimizations the might enable? It wouldn't enable any optimizations that I'm aware of.
Having Rust use jemalloc is better than using a mediocre system allocator (glibc, Windows) even if it means having multiple general purpose allocators in the same process. FreeBSD and Android Lollipop use jemalloc already, so Rust could just call into the system allocator there via the same non-standard API.
Hm, that's a good point about the accessibility of the combined installer build. I think you are right about MSIs too. I assume the reason we went with inno setup is because it's open source, but it seems like msi's would be the more versatile option. I'll try to put some work into the combined install this week. I'm still not sure I want to include cargo in the rust build but I'll put some thought to it.
Well, I would love to see a nice long, explanatory blog post from your side, then. From what I've read on the PR, I couldn't quite follow...
Short version: you have to use a reference to a trait that all of the concrete types implement. Long version: the match arms are *not* returning `LineBufferedWriter`s. `LineBufferedWriter` is *not* a type; it's a *generic* type. If you change the parameters to a generic type, you end up with a different concrete type. What you want to do is use a trait that all of those concrete types implement. But you can't return a trait directly (since traits are unsized); you need to return some kind of pointer to that trait. So, your choices (in general) are things like `&amp;Writer` or `Box&lt;Writer&gt;`. `&amp;Writer` won't really work here, since you'd be returning a reference that outlives the thing it's pointing at. So that means taking each concrete value, `box`ing it, then casting it to a `Box&lt;Writer&gt;`. **Edit**: Example: use std::io::stdio; fn main() { let mut output = { // Long: /* let lbw = stdio::stdout(); let boxed_lbw = box lbw; let boxed_w = boxed_lbw as Box&lt;Writer&gt;; boxed_w */ // Short: box stdio::stdout() as Box&lt;Writer&gt; }; let _ = output.write_str("Hi.\n"); } 
I believe there is no language reason that `if` cannot be optimised to a switch/jump table, it is just an implementation detail of GCC, e.g. LLVM can do it for Rust (and so I assume clang does it for C/C++ but I cannot test right now).
`if let` and `match` both do refutable pattern matching, and they are actually essentially equivalent. The following are the same: if let pattern = expr { body } match expr { pattern =&gt; { body } _ =&gt; {} } (I believe the former is actually desugared to the latter inside the compiler.) Irrefutable patterns are those for which the type system can guarantee they will always match, e.g. a pattern `(a,b)` will always work with tuples `(T,U)`, and refutable ones are those for which the type system cannot make this guarantee, e.g. the pattern `Some(_)` can fail to match because an `Option` can be `None`. Places like `let` statements and function arguments are restricted to irrefutable patterns.
Shouldn't the Long version be: use std::io::stdio; fn main() { let mut output = { // Long: let lbw = stdio::stdout(); let boxed_lbw = box lbw; let boxed_w = boxed_lbw as Box&lt;Writer&gt;; boxed_w // Short: //box stdio::stdout() as Box&lt;Writer&gt; }; let _ = output.write_str("Hi.\n"); }
But it *is* like that. What are you talking about? *whistles innocently*
Maybe I meant that `match` expressions are exhaustive, but I thought (perhaps wrongly) that they were synonymous. In the `match` expression of your example, you still need to specify every case, so it is exhaustive, a catch-all still makes it exhaustive. The `if let` expression does not have to be exhaustive, however, it can specify any amount of patterns &lt;= n. Also, the [`if let` rfc](https://github.com/rust-lang/rfcs/blob/master/text/0160-if-let.md#summary) specifically states that an `if let` expression is refutable.
`Reader` doesn't have `read_line` because it's impossible to write a completely correct `read_line` *without* access to a buffer. It's all Mac OS's fault. That said, if you don't care about compatibility with old-school Mac OS newlines, you can steal this [`read_line` alternative](https://github.com/DanielKeep/rust-scan-util/blob/master/src/io.rs) which doesn't require buffering. Next, when you're asking for help in solving errors, *please* provide a compilable example. Preferably, using the [Rust playpen](http://play.rust-lang.org/) so people can just click and link and see the problem. If you can't, at least include the *full* output of the compiler. Otherwise, people have to either try and reverse-engineer the problematic code or guess. So I'll guess. You can't have a `BufferedReader&lt;Reader&gt;`. The problem is that, as I said before, traits are unsized. That is, the compiler literally has *no way* of knowing how big a trait value is going to be at compile time. Generally speaking, aside from certain specialised types, you can't use an unsized type as a parameter in a generic. Consider that `BufferedReader` probably wants to *store* the `Reader` you pass it... but it doesn't know the size, so it has no idea how big it's supposed to be, and it all falls apart. If you look at the [docs for BufferedReader](http://doc.rust-lang.org/std/io/struct.BufferedReader.html), you'll see that it implements the `Buffer` trait, which has the `read_line` method. So what you want is a `Box&lt;Buffer&gt;`.
&gt;What do you think ? That you are a Python programmer and don't want people to like things you don't like?
Hopefully we'll get something like `fn return_unboxed() -&gt; impl Fn(int) -&gt; int` where you can return an unboxed closure by value. That would also be nice for returning giant chains of iterators.
Indeed.
There are a ton of C libraries that have rust wrappers! * Alex Crichton has bindings to [many popular libraries](https://github.com/alexcrichton?tab=repositories) including libgit, libssh, and libbzip2. * [Rustpy](https://github.com/lukemetz/rustpy) binds to the Python C API. * [jit.rs](https://github.com/TomBebbington/jit.rs) is a library that binds to libjit. There are tons more, I see people writing wrappers all the time! C will exist for at least a very very long time more, and Rust has a zero cost FFI, making C interoperation an easy task for Rust. You are also right that a lot of people like to make their own implementations of libraries in Rust, instead of just using C bindings. [Servo](https://github.com/servo/servo) at one point used a lot of C libraries, but they have slowly been moving to a pure rust ecosystem. If you find a C library that you want to use, binding it is really simple in rust once you have read the [ffi guide](http://doc.rust-lang.org/guide-ffi.html). If you want to create a Rust version of a C library in pure Rust, do that! To answer a few of your questions: &gt; Is there a list of C libraries for which Rust wrappers have been written? Or if you could just share the link to your project repository? There is no official list, but there are a ton of wrappers, and you can ask around IRC if you have a question if one exists. &gt;How comprehensive is Rust documentation for writing such interfaces? The FFI guide talks a lot about what goes into making a binding, so I would say its in depth enough. &gt;What were the corner cases or pain points in writing such interfaces? There will be a lot of `unsafe` for the wrapper, so make sure you double check all of those blocks! A lot of people make a safe wrapper around the unsafe C functions to avoid having to use `unsafe` a lot in client code. &gt;Are there any platform specific issues? Like: do people have to do special stuff while working in Windows? Funny that you mention that, to get Win32 COM bindings to work with Rust you need to execute the following code snippet at the top of `main`: unsafe { ::std::rt::stack::record_sp_limit(0); } Sorry for the wall of text :) hopefully people will correct me if I am wrong about anything.
Keyword arguments: https://github.com/siegelord/kwarg
Thanks for the link and info. It does seem to not be possible yet. I found this post: http://stackoverflow.com/q/25262551/69671 which says that you can have a `Rc&lt;RefCell&lt;Box&lt;Trait&gt;&gt;&gt;`, but I guess my problem is I need to have a `Rc&lt;RefCell&lt;Box&lt;Struct&gt;&gt;&gt;` and `Rc&lt;RefCell&lt;Box&lt;Trait&gt;&gt;&gt;`of the same object, and I can't coerce from one to another. I wonder if it would make sense to somehow have a struct's methods be usable as an implicit trait so you could have `Rc&lt;RefCell&lt;Box&lt;Control + Label&gt;&gt;&gt;` and be able to coerce that by dropping one of the bounds to `Rc&lt;RefCell&lt;Box&lt;Control&gt;&gt;&gt;`. That doesn't work now either.
&gt; ... to get Win32 COM bindings to work with Rust you need to ... Actually, that's no longer true. At least, it's not true for my test case that *did* previously fail.
Just a quick remark: I wouldn't work with strings. There are too many windows... encodings out there that would break. Since Rust introduced byte string literals some point it is actually [quite handy](https://github.com/nwin/chatIRC/blob/master/src/msg/raw.rs) to work with them.
Thank you. I think I'm mostly understanding lifetimes, except not completely when using them with traits. Using an enum for controls would partially work, except I really want the collection of controls to be extensible. Almost every program I've worked on has at least one custom control, so one of my requirements is that custom controls are on equal footing with controls that are part of the GUI library. It would also be nice to allow loading new controls from a dynamic library at runtime like many programs with plugins can do. I'll keep trying to find a way to implement the control tree. Maybe I can use `Any` and downcast, but downcasting is ugly.
Wow, interesting library :) &gt; It is not possible to export these generated macros, so instead you should provide a macro that re-generates them and export it instead: We're in the middle of a [discussion](https://github.com/rust-lang/rfcs/pull/453#issuecomment-62784895) about allowing macro-generating macros to export the macros they generate. So it's interesting to hear that you've already run into this. I think in my proposed system you could support syntax like this: extern kwargs! foo { a: 1, b: None, c: Some(6) } pub fn foo(a: int, b: Option&lt;int&gt;, c: Option&lt;int&gt;) -&gt; (int, Option&lt;int&gt;, Option&lt;int&gt;)
I've created a Rust wrapper for libcpuid and wrote about it here: http://siciarz.net/ffi-rust-writing-bindings-libcpuid/ And yes, I think about porting the library to pure Rust, if only I had time :-)
Yeah, that is interesting! I skimmed the code, am I correct in thinking that you only have procedural macros which generate macro_rules macros? (As opposed to procedural or rules macros which generate other procedural/rules macros).
You mean "type(type) == type"? Are we really talking about the real definition of type safety which is about progress and preservation? A proof (or disproof) of type safety would require a formal semantic of Python, which nobody has done afaik.
There is nothing about soundness of safety in this article. It rants about the complexity.
Indeed `if` is redundant, like many language features it is added to make code clearer and more concise, not for being more expressive. But just a note, `match` can work with types that don't implement `PartialEq` (the trait that has the `==` operator). That is, this don't work: struct A(int); fn main() { let p1 = A(1); if (p1 == A(2)) { println!("they are equal"); } else { println!("not equal"); } } But this works: struct A(int); fn main() { let p1 = A(1); match p1 { A(2) =&gt; println!("they are equal"), _ =&gt; println!("not equal") } } To make the first example work, you need to add `#[deriving(PartialEq)]` before the declaration of `A` (your example works because primitive types implement `PartialEq`)
These things are at present implemented manually for `[T, ..N]` for values of `N` up to 32, in `src/libcore/array.rs`. The implementation is simply `self[] == other[]`; `a1[] == a2[]` will work fine for you (with `#![feature(slicing_syntax)]` on the crate, or else replace `[]` with `.as_slice()`).
[Actually yes!](https://github.com/lifthrasiir/rust-encoding) Supported encodings [here.](https://github.com/lifthrasiir/rust-encoding/blob/master/src/all.rs)
One thing to add: the thing that makes the tracebacks not as nice currently is that the error location info does not contain the function name. This is a regression from python and could easily be fixed by introducing a `function_name!` macro that returns the function name as option.
&gt; You can't have these features you're asking for and also have a strong type system that forbids runtime type errors. Unless, of course, it's made in a way that the compiler refuses to compile if there's a type error (since `commands` is known at compiler time, iterating over it could also be done in compile time, eg. with a macro)
No, that makes perfect sense—`&amp;[T]` has a `PartialEq` implementation so long as `T` does too; `[T, ..N]` does for values of `N` up to 32, provided `T` does. Thus, while `[int, ..32]` does, `[int, ..33]` doesn’t, and so while `&amp;[[int, ..32]]` does, `&amp;[[int, ..33]]` doesn’t.
Since you are creating `libjit.so`, you should print `jit:dylib` instead of `jib:static` (I'm not sure that it's "dylib", but rustc should tell you the available options if it's wrong)
How disruptive would this be if incorporated in the language?
In the current form that would be a terrible idea due to the way the error is actually laid out in memory and how it chains. But it could be done much better. The main issue with this is that you break the current `try!` macro and it would only work with `Error` any more which is not a trait. The current `try!` macro works on any result. I do think that it would be quite tricky to support a non error try under the same name, but maybe it would be possible.
Macros generate code, they can't (cleanly) iterate over data of arbitrary length. That amounts to having the compiler run arbitrary code, which is awesome but also a whole 'nother can of worms.
I think that relying on dynamic typing for error handling is very likely to be a mistake, for the same reasons why it's usually a mistake elsewhere ([see also my earlier comment here](https://github.com/rust-lang/rfcs/pull/201#issuecomment-57922021)). Could you elaborate on why you think dynamic typing is the right solution?
&gt; whatever you want to do with a compiler VM and/or CTFE you can already do by writing a D script that runs itself and modifies some source file that then gets compiled You can do that with any language. D doesn't have a VM for running D 'scripts'. Don't you think CTFE which allows all side-effects is going to be slower than a pure subset?
pain points: C libraries are sometimes built around very fuzzy coercions. OpenGL has some situations where you have a void* parameter that might be a struct offset instead (yes thats' crazy), and with rusts *extra* casting requirements thats' *extra* painful. I had wondered if rust- whilst remaining type safe itself - could recover C's void* as automatically coercing anything, because thats' explicitely what void* is for; And obviously you wouldn't use void* within a pure rust ecosystem (maybe restrict void* to extern "C" declarations). I don't see cross-langauge issues going away, ever. C++, Rust,D,.. etc are too complex to be universal and people will always disagree how things should work (how should polymorphism work, etc). So I actually hope C lives on, always serving a purpose as a simpler level around which higher level languages can interoperate. I'm not condoning the use of void* - just suggesting something that could make interfacing legacy &amp; cross-langauge code easier. I don't see the world agreeing on 'one type system to rule them all' so we'll always have this kind of issue. 
A common-ish solution for this in C#/VB is to have a base class B that defines the tests, and has a field of interface type IWhatever. Then, for each implementer of IWhatever that you want to test, make a new subclass of B that sets the field. I remember there were a lot of proposals about inheritance in Rust a few months ago, but I don't know what if anything was ultimately decided.
Could this be addressed by making the formatting macros (`format!`, `println!`, etc.) accept syntax which *looks* like it is a string literal, but isn't actually?
Mitsuhiko, have you considered making an RFC for this? https://github.com/rust-lang/rfcs
Need feedback first, RFC will follow :)
Maybe i don't understand this correctly, but my impression was that `unsafe` code should effectively be safe (i.e. there should still be no conditions which will crash it) but the compiler can simply not make that guarantee because it's not as smart as the programmer...
Are non-type generic parameters (e.g. `template&lt;size_t N&gt;`) coming soon?
Erick, I've sent you an e-mail couple of days ago about rust-serialization-benchmark being unfair to rust-protobuf (go version reuses struct, while rust version always allocates new). Have you received my e-mail?
It is not gone, you can still use it via cargo...
If you do, please be very careful about being robust to problems in codecs; cmus is broken in ubuntu 14.04 LTS because of instability caused by the libavcodec vs. ffmpeg fork. I can build cmus from source but I don't get any sound (aftermath from the alsa to pulseaudio transition?). The ubuntu-standard music player (rhythmbox these days) performs terribly (it's written in python, and I have a lot of mp3's). I have had to fall back to calling mplayer on the command like and it isn't fun. Linux is nice because there at least ~is a stable set of software that always works, but it is shockingly small. Sorry for the rant.
That's just GADT-s, an Haskell feature that allows enum variants to have type bounds. which is unrelated to match vs. case vs. if (except that bool isn't a GADT). A "Rust syntax" version of GADT-s (also with equality bounds) would be something like ` enum Expr&lt;T&gt; { Const(T), Add(Box&lt;Expr&lt;uint&gt;&gt;, Box&lt;Expr&lt;uint&gt;&gt;) where T=uint, Mul(Box&lt;Expr&lt;uint&gt;&gt;, Box&lt;Expr&lt;uint&gt;&gt;) where T=uint, Eq(Box&lt;Expr&lt;uint&gt;&gt;, Box&lt;Expr&lt;uint&gt;&gt;) where T=bool } ` 
&gt; All libs propagate a single error type for the whole crate out, which might even wrap other errors. So that strong typing is already not very strong. This is a compelling critique of current practice, but - and you knew this was coming - what about trying to improve it, instead of abandoning it as a lost cause? (I suspect that two complicating factors are: * People tend to find having to deal with error cases annoying, and hence to minimize how much they have to do it. Finer-grained error reporting is even more annoying than coarse-grained, hence the trend towards coarse-grained reporting everywhere, in Rust, and in this thread. * Rust APIs often wrap, expose, or are built on existing components (including native OS facilities). Due to the above these *also* tend to report errors in a coarse-grained way, typically using error codes. It is difficult (and annoying!) to recover finer-grained static distinctions from these (e.g. you have to rely on documentation, which may itself be inaccurate). I wonder why it is that the error handling path tends to be so much more annoying and complex than "normal" code, and whether there are any missing abstractions or language features which could change that, or if all of the complexity is somehow [inherent](https://twitter.com/glaebhoerl/status/534389585519063041).)
&gt; what about trying to improve it, instead of abandoning it as a lost cause? I'm not giving up. That's why `rust-welder` is a separate package. I'm trying both, but definitely enjoying `rust-eh` more currently. &gt; I suspect that two complicating factors The only complicating factor I have is that errors need meta information to be useful (error debugging info). Where does that go, if everybody makes their own errors? That's why I changed it so that the error itself is the same everywhere but the error data that is carried is library or even function specific.
With respect to complicating factors, I was referring to things which make it harder to try to change the fact that: &gt; All libs propagate a single error type for the whole crate out, which might even wrap other errors. So that strong typing is already not very strong. - &gt; The only complicating factor I have is that errors need meta information to be useful (error debugging info). Where does that go, if everybody makes their own errors? I suspect this could be done with some type magic by making a generic "error transformer" type which takes an existing error type, adds the additional information, and (fully applied) is itself an error type. The trick is making it compose well when errors are nested and/or propagated. (I haven't thought it through...)
That is correct.
&gt; I suspect this could be done with some type magic by making a generic "error transformer" type which takes an existing error type, adds the additional information, and (fully applied) is itself an error type. The trick is making it compose well when errors are nested and/or propagated. (I haven't thought it through...) I completely failed with that approach, mainly because my mindset is in two extremes: type wrapping like in rust-welder and error chaining like in rust-eh. They are completely opposing concepts and do not overlap unfortunately.
not having time in stdlib sounds like a very, um, unorthodox move. are there any plans for it or an alternative to come back?
Just one quick thing: isn't removing something from a collection you're iterating over widely considered to be a Bad Thing? I guess Ruby lets you do it, but you get weird behavior...
Not any time soon. Eventually, probably.
Well it's currently moved into a separate package, I expect they'll merge a non-crappy datetime library if one arises. Though that may not happen for some time, mapping datetimes to computers is not exactly trivial. All in all I'd rather no datetime library be included than Rust repeat Java's mistakes (or even Python's, though `datetime` was only added a good 10 years into the language's history, it still has issues)
Our general strategy for libraries at the moment is to pare down the standard distribution to something small, high-quality and maintainable, punting as much as possible to the cargo ecosystem where crates can evolve and stabilize independently, and battle it out for supremacy. Cargo is intended to be convenient and pervasive enough that whether the crate comes installed by default or is fetched from the network *shouldn't* be a major issue. Over time, the best cargo crates may become 'blessed' and re-enter the standard distribution, but exactly how is yet to be determined. 
Hum... I see how that might have been interpreted. By thread-local pool I did not mean completely independent allocator; I meant to say that (de)allocators would be completely unsynchronised *unless* they happen to exchange a page with the global arena.
It's somewhat hacky, but you could put all tests into a special file and include it in your tests via `include!("test.incl.rs")`. #[cfg(test)] mod test_foo { use super::FooLogger; use logger::Logs; fn instance() -&gt; Box&lt;Logs + Send&gt; { return box FooLogger::new() as Box&lt;Logs + Send&gt;; } include!("test.incl.rs") } #[cfg(test)] mod test_bar { use super::BarLogger; use logger::Logs; fn instance() -&gt; Box&lt;Logs + Send&gt; { return box BarLogger::new() as Box&lt;Logs + Send&gt;; } include!("test.incl.rs") } and in `test.incl.rs`: #[test] fn test_logs_empty_value() { let mut f = instance(); f.log(&amp;String::new()); } // Other tests...
Oh, do you know how it manages to allocate in a thread and deallocate in another without synchronizing the two actions? In any case, does it mean that there would be no optimization possible from having a strictly-local thread-cache?
Space complexity, not time. I did store the items to delete in a vector, this is where the O(n) space comes from. It also means the algorithm involves memory allocation/deallocation traffic, whereas before it would not have had any.
Yeah, I misred it as time, that's why I deleted the comment. It's not a big deal though complexity-wise, because to delete items you need to have exclusive access to the tree so only one algorithm instance runs at once per tree, and the tree already takes O(n) space, so at worst it multiplies overall memory usage at any given time by a fixed constant. It would be useful to have though. 
FWIW I agree that this is a good strategy. But it would be great if you could integrate the ex-`std` or `extra` modules into the rust integration tests. Maybe not all of them but some of these modules I still see as "core functionality" although they are maybe not mature enough to be in `std`. For example, `num` had not been kept on track with the Rust nightlies for a few days which cause some breakage.
That makes sense. :)
Yup. It requires type system machinery we don't have yet. Not full dependent types, but still, more than we've got.
We could have a RodaTime (like NodaTime and JodaTime) library :)
Rust's lifetime system is actually perfect for always making deletion-during-iteration totally safe. Except that the way that we designed iterators is completely counter to allowing it. :/
I'm not familiar with jemalloc internals, but it seems like it should be enough to push the freed segment onto a thread-local free-list. As long as you don't try to give it back to the "global pool" there shouldn't be any need for synchronization as far as I can see. As a the free()-er I don't think I need to care about where the memory came from originally.
As /u/chris-morgan pointed out, that's because you called `as_slice()` on the outer arrays. `[T, ..33].as_slice()` has type `&amp;[T]`, which always impls `PartialEq` if `T` does as well. It would have failed if you hadn't sliced them before comparing. See [this playpen][pp] [pp]: http://play.rust-lang.org/?code=fn%20main()%20%7B%0A%20%20%20%20let%20i%20%3D%20%5B%5B0i%2C%20..32%5D%2C%20..33%5D%3B%0A%20%20%20%20let%20j%20%3D%20%5B%5B0i%2C%20..32%5D%2C%20..33%5D%3B%0A%20%20%20%20%2F%2Fprintln!(%22%7B%7D%22%2C%20i%20%3D%3D%20j)%3B%20%2F%2F%20doesn%27t%20work%2C%20because%20the%20array%20size%20is%20%3E%2032%0A%20%20%20%20%0A%20%20%20%20println!(%22%7B%7D%22%2C%20i%5B%5D%20%3D%3D%20j%5B%5D)%3B%20%2F%2F%20does%20work%2C%20because%20it%20was%20sliced%0A%20%20%20%20%0A%20%20%20%20let%20i%20%3D%20%5B%5B0i%2C%20..32%5D%2C%20..32%5D%3B%0A%20%20%20%20let%20j%20%3D%20%5B%5B0i%2C%20..32%5D%2C%20..32%5D%3B%0A%20%20%20%20println!(%22%7B%7D%22%2C%20i%20%3D%3D%20j)%3B%20%2F%2F%20does%20work%2C%20because%20the%20array%20size%20is%20%3C%3D%2032%0A%7D
Striving to understand why JSR-310[0] is how it is would certainly be a good idea, but blindly copying it would give a Java library in Rust, I'm not certain that would make for a non-crappy datetime library. [0] rather than Joda, since JSR-310 is Stephen Colebourne's second baby… after him having learnt from Joda
Internally represented as an 256 bit integer in units of the Planck time? Should be enough for the foreseeable future…
I'm really glad you keep plugging away at this problem! Keep up the great work!
I tried to write a interface to a gc library. I used rust bindgen to generate most of the extern "C" interface. Overall it worked relativly well however on the very lowest level I run into problems but thats nobody thought really. The documentation is ok, but not fantastic, you can find most things but a you will have to do some trying around until it works. The std::mem is usful to write unsafe code. Overall I was suprised how easy it was.
Do you think this is a mistake in the design of iterators, or are both approaches useful in different situations?
&gt; Is there a list of C libraries for which Rust wrappers have been written? Or if you could just share the link to your project repository? [I'm working on wrapping Lua 5.3.](https://github.com/jcmoyer/rust-lua53) &gt; How comprehensive is Rust documentation for writing such interfaces? [The FFI guide](http://doc.rust-lang.org/guide-ffi.html) is good enough. I found sections 5 and 11 to be especially useful. &gt; What were the corner cases or pain points in writing such interfaces? 1.) Wrapping C macros, particularly those defined conditionally, such as: #if defined(LUA_REAL_FLOAT) #define LUA_NUMBER float #elif defined(LUA_REAL_LONGDOUBLE) #define LUA_NUMBER long double #elif defined(LUA_REAL_DOUBLE) #define LUA_NUMBER double #else #error "numeric real type not defined" #endif In my case, I opted to create a [C program](https://github.com/jcmoyer/rust-lua53/blob/master/src/glue/glue.c) that generates a Rust module exporting the appropriate values. This way I can leverage a C preprocessor to extract the bits that have multiple possible values or are likely to change in the future. 2.) You can't wrap macros like these in pure Rust without CTFE (`size_of` is a function in Rust): #define LUA_EXTRASPACE (sizeof(void *)) 3.) C developers like to write macros that take an array as a parameter and use `sizeof` to get the length of the array: #define luaL_newlibtable(L,l) \ lua_createtable(L, 0, sizeof(l)/sizeof((l)[0]) - 1) Technically you can use a `&amp;[T]` here (to get `.len()`), but then your function signature becomes incompatible with the original C macro. It gets really problematic when you look at this function and macro: LUALIB_API void (luaL_setfuncs) (lua_State *L, const luaL_Reg *l, int nup); #define luaL_newlib(L,l) \ (luaL_checkversion(L), luaL_newlibtable(L,l), luaL_setfuncs(L,l,0)) `luaL_setfuncs` takes `*const luaL_Reg`, therefore `l` in `luaL_newlib` must also be `*const luaL_Reg`. This pointer type is incompatible with `&amp;[T]`, so you couldn't use a slice reference in the definition of `luaL_newlibtable` anyways. Maybe there's a better way to do this.
Well, I would actually favor a `Date` type in the datetime library; you probably want the same functionality between both, apart from the precision.
Both are useful. The current design has two great advantages: * Iterator&lt;T&gt;, Iterator&lt;&amp;T&gt; and Iterator&lt;&amp;mut T&gt; all impl the same trait, because all the iterator trait says is "I return something" * You can safely obtain *multiple* results simultaneously. This allows you to have iterators-of-iterators-of-iterators all running in unison without stepping on each others toes. Also just getting the next two elements. But it has the following disadvantages: * Mutation of the underlying collection is very delicate * It *requires* unsafe code to be written for collections (in the reference case) * You can't freely seek, only move forward (in the owning and mutable case). Cursors completely invert the advantages and disadvantages columns. They perfectly complement Iterators, in this regard. For disadvantages, if you were to write a Cursors trait, you would need Cursor and MutCursor for &amp;T and &amp;mut T. Unless... maybe if you genericized with Deref and DerefMut (this only recently became possible)? Supporting by-value is certainly a non-starter. Also you definitely can't peek at multiple elements at once. Only one at a time. But it solves all the other problems of Iterators trivially. Edit: cleaned up some stuff.
Well, not dependent types at all even, since C++ does not have anything like dependent types. On the other hand, even though it is useful to have non-type template parameters, the fact that they are limited to a blessed set of types (not comprising `std::string`) in C++ can be very painful... ... so I have hope that Rust would jump straight ahead and accept any type for which CTFE works.
Agreed. Date handling has a lot of edge cases, and all of these can be found in one place with these libraries. Plus we might pick up a thing or two about design.
Check out the bindings section on [rust-ci](http://www.rust-ci.org/projects/) for a nice list.
Hopefully rust-ci will fill this role. It will be pretty easy to set up Travis (or whatever) to build with allow-fail against nightly and beta channels, as well as the regular build against stable.
&gt;Afair everything except the last parameter is guaranteed to be ascii by spec but I'm not sure about that. The spec says that the user part of a prefix (i.e. between the `!` and `@` characters) can be &gt;any 8bit code except SPACE (0x20), NUL (0x0), CR (0xd), and LF (0xa). Also, the *entire* param list can contain &gt;any \*non-empty\* sequence of octets not including SPACE or NUL or CR or LF, the first of which may not be ':' except for the trailing part which can contain &gt;Any, possibly \*empty\*, sequence of octets not including NUL or CR or LF. The other problem is that nobody *really* conforms to the specs, RFC 1459 is *mostly* complied with, some IRCds/clients conform to RFCs 2810-2813, but that series is mostly ignored. Case in point: Most IRCds allow the use of UTF-8 and other non-8-bit codes, despite both 1459 and 2812 stating that IRC is an 8-bit protocol. The IRC protocol is a complete mess where specification and standardisation is concerned. I did once try and start creating a consolidated I-D folding in the IRCv3 extensions but that was difficult, tedious and I've since lost the source files for that :( Edit: What you say about being able to safely split on `SPACE` and `:` is true, though - and I guess `@` for message tags. I thought I was also using `String::from_utf8_lossy()` but apparently not - that must have been in an earlier iteration before I refactored message parsing.
I wrote out a fairly long reply, but it looks like Reddit ate it &amp;#3232;_&amp;#3232;. Suffice to say, the assumption that everything except the last parameter is ASCII is incorrect. See sections [2.2 (Character codes)](https://tools.ietf.org/html/rfc1459#section-2.2) and [2.3.1 (Message format in 'pseudo' BNF)](https://tools.ietf.org/html/rfc1459#section-2.3.1) of RFC 1459 - the user part of the prefix and *almost all* of the parameter list can be made up of arbitrary octets, with the exception of the control characters `\r`, `n`, `\0`, `SPACE` and `:`. It's tricky.
Oh thank you, they finally did this. It makes much more sense to have enum variants be namespaced under the enum itself, instead of standalone. They can even avoid any spam people would have to do for Option::None by re-exporting None standalone. Very good change.
[Here](https://gist.github.com/dradtke/627801ca5da80aa5ab4c)'s the approach I would take. It's similar to yours in that it uses enums for `Value` and `Suit`, but simply defines `Card` as `(Value, Suit)`, and `Deck` as `Vec&lt;Card&gt;`. It's also pretty easy to shuffle a deck using a task-local random number generator.
One small error I found: You have both One and Ace defined in the Value enum. ;)
That's simple enough, thanks! Looks like this problem doesn't require anything that's unique to rust really. Pattern matching will definitely make analyzing cards easy but that's pretty much it. Guess I'll need to find myself a tougher project after this one, I still need to get my head around lifetimes!
Oh, here we go... No, but seriously. I have actually been looking forwards to this. Time to rename some awkward enum variants.
actually I have question that's not in the scope to my original one. Let's say you were making a game of texas hold 'em, how would you define what a, let's say, royal flush is (which for whoever doesn't know is a 10, J, Q, K, A of the same suit)? This is more just out of curiosity. My imperative mind makes me think just loop through the set of cards asserting their suit and checking it has the necessary values. Right off the bat though that's terrible cause you would also want to see what other kinds of hands you may have within that set which means you would need a method for each possible hand.
The problem with this, I think, is that the onus to fix bugs falls on the people maintaining the libs, rather than the people making the changes. Since the latter group is much larger than the former, this doesn't scale very well. There is almost certainly some benefit for us to gate landing to the Rust repo on not breaking some core packages. Although then you often get horrible synchronisation problems.
Yeah, I'm excited for this. No more weird variant names, like prefixing the enum name on the front of the variant itself.
Stable will work because it won't make these changes. Nightly may break because it's nightly. That's part of the deal with using a nightly. :) Putting the onus on compiler writers to fix tons of packages isn't scalable either, really.
One can just import the names though. use std::option::Option::{Some, None};
https://twitter.com/horse_rust/status/525390980518658048 (This account is pure gold, by the way.)
I'm surprised how little attention this feature gets from the developers, given a regular need to parameterize things on sizes, alignments, dimensions etc. Maybe the reason is that it isn't widely needed in code typical to rustc or servo and people tend to serve their own needs first? Or maybe it's just complex to implement? I don't know.
You could even use a `match` on it, in order to get correct precedence, as well. I don't know if `match` can pattern match across the contents of vectors, however.
You probably want to use something similar to a trie. Since poker hands follow well-defined rules it shouldn't be too hard to dynamically build a list of possible next cards from a base card, rather than build a static trie, then pass the next card into that, ultimately resolving in a bunch of hands or a bunch of nothings. Though you could probably enumerate all possible hands upfront and build a static trie from *that*.
Great! Hope Rust will have union types one day: struct Ipv4Address { ... } struct Ipv6Address { ... } type IpAddress = Ipv4Address | Ipv6Address;
&gt; I don't like my solution because first I have to do the effort of making sure each card is unique when creating a standard 52 card deck. Also shuffling the cards would require a non-trivial method, not sure if that's avoidable though. You probably want something like this: http://is.gd/ISCHI2 I haven't been following the latest Rust developments so maybe there is an even nicer way by now. For shuffling see: http://doc.rust-lang.org/std/rand/trait.Rng.html#tymethod.shuffle You probably also want to wrap the "deck" in a struct to allow the user to manipulate it only in appropriate ways. You could also write your own Show implementation for Card to print "Ace of Hearts" etc.
It depends a lot on what you want to do with computers. Rust is particularly good for low-latency, high performance code -- games, signal processing, operating systems, that sort of thing. It's also suited to embedded systems (e.g. robots) and other resource-constrained scenarios. If you start with Rust, you'll have to learn certain programming concepts (such as explicit ownership of data) that come with the territory, but you'll get the chance to learn them in an exceptionally pleasant and modern setting. If you want to make something like a webapp, or helper scripts for your "day job", a higher level language such as Python will allow you to ignore more of these system-level details. I think Rust can be good for these domains too, but it's not as much of a clear win. There is a notion that systems programming, whatever that means exactly, is harder than applications programming. There's some truth to this, but it's also a gap that Rust is trying to close. Plenty of programmers start out writing numerical software in C, or hacking on robots. If that's the kind of thing that interests you then it's a great idea to start with Rust, as it's vastly more beginner-friendly than C or C++.
I believe you need to have at least some experience with C/C++ to be able to appreciate and truly understand Rust.
Seems like that would introduce a lot of ambiguity that would make proving code paths harder?
No. Rust is still in development. It's less likely to change very much on you now than it was before. However, there will be bugs in the implementation, bugs that you won't be able to necessarily identify as bugs in the implementation, given your lack of experience in programming. I recommend picking up Python or Java. The documentation for these languages is plentiful (plenty of 'for dummies' tutorials). The communities are large and helpful. You will be able to write lots of useful code in them. Later, after you've solved a bunch of problems in whatever common language you decide to learn, come back and check out Rust.
Could you please explain what you mean?
How is that... not... enums?
Are there any concerns about fragmentation? If there are two popular but incompatible libraries for something, that could split the ecosystem. It's happening in Haskell with streaming libraries to such an extent that other libraries are avoiding them, and trying to be neutral: http://www.yesodweb.com/blog/2014/04/disemboweling-wai
In my opinion, search a good programming book, then pick up whatever language at low level and try to implement those algorithm, I say low level because you end up learning a lot of stuff of how your box works, how it handle processes, memory, and such.. Keep an hacker attitude and you will have no regret. Possibly pick a single resource that teaches you from a point to an end something in particular about what you are trying to learn, and keep a manual of your choosed language at your side. And don't trust people that tells you that a language is too difficult for a beginner, I was able to teach to a 10 year old a sorting algorithm (mergesort, albeit using [these](http://it.ewrite.us/foto/thumbs/phpThumb.php?src=../1300712070.jpg&amp;w=200&amp;h=200&amp;zc=1&amp;q=100)), surely you could do that too.
&gt; But you're encouraged to use SIMD in the benchmarks game, because C and C++ do. You are neither encouraged to copy the approach others have taken, nor discouraged from doing so. You are perfectly free to contribute programs that implement *the "simple"/"obvious" algorithm* -- so why don't you?
Interesting to see this now. The same proposal was closed as "maybe post-1.0" a bit under a year ago: https://github.com/rust-lang/rust/issues/10090
Not related to the question at hand, but is there any reason `log` is declared as fn log(&amp;mut self, msg: &amp;String); instead of fn log(&amp;mut self, msg: &amp;str); ?
The data structure &amp; iterator have to be designed to support it (i.e. making it not-a-bad-thing).
I imagine it's meant to be an anonymous/structural version of enums (like tuples are an anonymous/structural version of structs).
The substance of the proposal is the same as before, but I completely reorganized the RFC to (hopefully!) make it easier to follow. In the new organization it's more clear that I'm proposing two semantic changes (one of which is already implemented on a branch) and five or so syntactic cleanups. One question where I'd like to have more input from the wider community is: To what extent do you want these changes to come all at once? Should we introduce the new syntax along with the semantic changes? Or should we ship Rust 1.0 with macro syntax closer to the current syntax, and improve it later in backwards-compatible ways? (There is also the possibility of multiple rounds of changes before 1.0, but the timeline is really tight even with a single round.)
I don't know, every language (D, C++,...) keeps allowing more and more in CTFE every coucple of years. Maybe it will stop at some point, but right now, C++14 has added a lot of CTFE, and C++17 will add more CTFE too (constexpr lambdas are under discussion). D seems to expand its CTFE every year too. Both languages allow a lot of side-effects already, I don't know how much slower a full VM could be, but it is just a language feature, you don't have to use it if you don't want too, but people have been working around the limitations around CTFE for years in mainstream languages. How much slower are these workaround vs "full CTFE" in "development time"?
Imagine you have a DOM, where each Node type has children of a selection of other node types. Those selections overlap. How would you store this if not with anonymous enums containing options they don't “own”.
Hi, Josh! &gt; It felt very weird that static trait methods are called as Trait::method(). Just wanted to say I agree that this feel like a wart. I don't know whether any changes to it have been considered/rejected. &gt; I ended up requiring std::fmt::Show only because I was using assert_eq! for debugging checks. Is there a cleaner way of doing this so the main, non-debug code doesn't need to refer to std::fmt::Show? Note that `assert_eq!` in Rust is **not** debug-only. You might want `debug_assert_eq!`. In some cases you can separate out debug-only code using #[cfg(not(ndebug))], but I don't see a simple way to do that in your use case. Maybe you could factor out the assertions into a method in a separate trait, with a default implementation that doesn't require Show, and a separate implementation that does?
Don't pick up Java. Programming in Java will make you think programming is tedious and annoying. It took me years to recover from.
I have no experience with C/C++ and I can grok Rust just fine. You do need to understand the stack and the heap, but just a conceptual understanding is enough to be able to tell where you do heap allocation and where you do stack allocation in Rust.
Well to be perfectly honest, you can't answer this question without dealing with a much bigger one: What's a good way to learn programming? Personally, I don't think you should start people out on systems languages (ones where resource management is visible to the user). I think initial teaching should allow someone to structure and specify a solution to a problem in enough detail for a computer to deal with. Things like resource management or strict type systems add an additional burden that are incidental to many problems. I want to start people off in the simplest possible environment (Python or Logo), and then add in more complicated topics later, like resource management or type systems. By this metric, Rust is a poor first language--it is both a systems language, and one with a strict type system. I think Rust is a fantastic language for using, and those features are the reason why, but for a beginner they are learning to dive before you can swim. But, I'm not a teacher and I'm not well-versed in modern pedagogy, so I don't know what's actually best. Quite a few people think you should start close to the metal and work your way up to managed languages, in which case Rust may be a good place to start. I think it also depends on your background; for someone with a strong math background, I might start them off on a Lisp or ML dialect. When someone asks me how to learn programming, my go-to resource I point them to is [Learn Python the Hard Way](http://learnpythonthehardway.org/) by Zed Shaw.
~~Sorry, there is no `include!` macro that I know of. `include_bin!` and `include_str!` expand to `&amp;'static [u8]` and `&amp;'static str`, respectively.~~ ~~I'm guessing this was omitted on purpose to encourage use of the strongly typed import system, instead of having a mess of `include!` invocations *a-la* the C preprocessor.~~ A syntax extension would have to separately tokenize the included file and append it to the syntax tree. There's no way to mutate the raw source AFAIK, since it's already been parsed and tokenized by the time any compiler plugin is invoked. ~~This could potentially be implemented as a [Cargo build script][1] running a very simple find-and-replace on source files, but at that point you might as well just hardcode it to append the tests where it sees a specific token.~~ [1]: http://doc.crates.io/build-script.html
That's basically how it works already. The synchronization with an underlying arena (which is fine-grained locking, `n_cores * 4` arenas by default) only occurs when the thread cache needs to be filled or flushed.
Unify quantum mechanics and general relativity [A-an-interesting-project] [E-hard]
Preface: I have not used Julia, so I'm making most of these comparisons based on secondary research. They both use LLVM, so in an ultimate performance comparison, there's probably not going to be a huge difference. Julia is JITed while Rust is compiled ahead of time, and each have their own merits and tradeoffs. Julia has a REPL while Rust doesn't ([yet!][1]), so it's probably more convenient for quick-and-dirty calculations or small one-off programs. Both have convenient C interop, though I think Rust makes it easier to build safe abstractions since it emphasizes the difference between safe and unsafe code. Julia is dynamically typed, Rust is statically typed. Both have type inference and take strong cues from functional programming, albeit Julia much more so than Rust. Julia focuses on coroutines (green threading) while Rust is now entirely focused on 1:1 threading. Both have a package manager, and while Rust's PM is less mature than Julia's, it is quickly catching up and will soon be on-par feature-wise. I'd say Rust has a more robust type system, but I might be biased as I've been bitten in the ass quite a bit in the past by dynamic typing. Julia is GC'ed while Rust's memory management is based on lifetimes determined at compile time, with very limited dynamic memory management implemented in the standard library by reference-counted containers. This means for equivalent programs, Julia will have (probably marginally, but not negligible) worse and less consistent memory usage than Rust, which may not be suitable for very constrained environments like OS kernels or embedded systems, where Rust coincidentally shines. However, Rust requires the developer to sort out lifetime and type errors before it successfully compiles, so programmer effort is front-loaded. Due to its tight focus on scientific computing, Julia's ecosystem also focuses on scientific computing. A lot of the community discussions I could find about Julia's ecosystem lament its lack of libraries for GUIs, databases, networking, etc. I haven't done much searching, but I'm guessing Rust doesn't have the strongest scientific computing ecosystem. However, it has a game engine, several abstractions for graphics APIs with different approaches, a native GUI toolkit plus bindings for several popular ones written in C/C++, several HTTP libraries encompassing both client and server, bindings to pretty much every popular SQL database flavor, and cooperates tightly with Mozilla's Servo team developing a faster browser rendering engine. That's just the tip of the iceberg. Rust and Julia are designed for very different purposes. Julia *might* work in systems or applications programming with dedicated effort, and Rust can and will probably get some strong scientific computing libraries in the near future, if it doesn't have them already. However, I'm a proponent of choosing the right tools for the job, instead of trying to shoehorn the ones I have experience with into applications they're not meant for. Rust: systems, games or applications programming, web servers, anywhere performance and memory usage need to be consistently good, and stability is crucial. Applications where you might use C or C++ otherwise. Julia: better for one-off programs focusing on calculations or data analysis, where errors and crashes don't necessarily mean the end of the world, and the overhead of garbage collection isn't a dealbreaker. Applications where you might use Python or MATLAB otherwise. TL;DR: It doesn't make sense to compare the relative merits of hammers to those of screwdrivers. [1]: https://github.com/rust-lang/rust/issues/9898
&amp;String is a better abstraction. &amp;String allows you pass a log message to multiple targets without using .clone() (1) &amp;String -&gt; &amp;str is 0 cost, the inverse is not true, and it's arbitrary as to if the logger will use a String or &amp;str. &amp;str to String for static messages is easily wrapped using format!: foo.log(&amp;format!("{}", 100)); Arguably it should take String, not &amp;String, but in my particular use it's important to be able to dispatch the log message to multiple targets. (ie. (1)) Broadly speaking my view is that api's should use &amp;String, not &amp;str. &amp;str is a convenience hack purely because otherwise you can't easily have &amp;'static str objects and I don't approve of it... ...but that's just my opinion. Other people disagree... :) (You can argue that &amp;'static str is zero cost, where String always involves some kind of allocation, which is true, but I still maintain: either have c style `* const c_char` strings, OR have real strings. *not both* /rant :)
Nope, include! is a thing. Not sure what you mean. test.rs: mod t1 { fn factory() -&gt; int { return 1; } include!("tests.rs") } mod t2 { fn factory() -&gt; int { return -1; } include!("tests.rs") } tests.rs: #[test] fn test_it() { let value = factory(); assert!(value &gt; 0); } output: $ rustc test.rs --test $ ./test running 2 tests test t1::test_it ... ok test t2::test_it ... FAILED failures: ---- t2::test_it stdout ---- task 't2::test_it' panicked at 'assertion failed: value &gt; 0', tests.rs:4 failures: t2::test_it test result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured task '&lt;main&gt;' panicked at 'Some tests failed', /Users/rustbuild/src/rust-buildbot/slave/nightly-mac/build/src/libtest/lib.rs:241 
That's probably the best suggestion I've seen. Thanks!
You can do `use std::fmt::Show` so it's just `Show` in the trait bound. That's basically moving the noise from the type definition to the imports section of your source file. All the primitive types implement `Show` so you're not really adding any restrictions there unless the user wants to use it with their own type implementing `Int`. In that case they should probably implement `Show` anyways. If you still want to remove that restriction, you can move `assert_ranges()` to a separate `impl` block: impl&lt;T: Int&gt; IntSet&lt;T&gt; { // All `IntSet` methods that don't require `Show` here } impl&lt;T: Int + Show&gt; IntSet&lt;T&gt; { fn assert_ranges(...) { ... } } In the future, you will be able to use a `where` clause to put it in the same `impl` block: impl&lt;T: Int&gt; IntSet&lt;T&gt; { // All `IntSet` methods that don't require `Show` here fn assert_ranges(...) where T: Show { ... } } The former is usually preferred for the sake of organization and to emphasize the different trait bounds, but the latter is definitely much cleaner.
Interesting. It doesn't come up when I search the documentation even though all the other compiler built-ins appear. But you're right, it is available. I see it [used occasionally][1] in the Rust source. Probably because it doesn't appear to be defined as a normal compiler plugin. That invalidates basically my entire previous comment. Sometimes it's nice to be wrong. [1]: https://github.com/rust-lang/rust/search?l=rust&amp;p=1&amp;q=include!&amp;utf8=%E2%9C%93
What about the most basic functions, like `precise_time_ns()` needed by many programs that don't necessarily care about the higher-level concepts of time and just want it represented by a single number?
&gt; Haskell is a *lot* more awesome if you can afford GC It's not just about GC. Predicting performance in Haskell is incredibly hard. While Haskell semantics avoid global mutable state, *implementations* use it pervasively to provide lazy evaluation. The result is that none of the nice functional reasoning works as soon as you care about (even asymptotic) performance. Lazy evaluation enables some pretty cool techniques in datastructures and API design. But I've never been convinced that these benefits outweigh the costs in reasoning about performance.
Well, you could manually loop through the outer array to slice all the inner arrays to get the type `&amp;[&amp;[int]]`. I'd be very interested to know if there's a way to do this in time O(1).
I think the point being made here is that if you have: fn(foo: Box&lt;Foo&gt;) -&gt; Foo { ... } let local = foo(heaped); This moves the value off of the heap and onto the stack. ie. This is a copy operation to copy from the heap to the stack. However, if you have: let other = box foo(heaped); Rust optimizes this so it just moves a pointer on the heap, so it doesn't copy it from the heap to stack and then back again.
That's not true for all iterables in the STL, though.
Anonymous enums don't enable any new behavior. They're just a specific case of normal enums where variants have positional names (i.e. first variant, second variant, etc.) instead of actual names. So either your DOM example is possible in the current system or it isn't possible either way.
Thanks for clarifying the point the Pointer Guide was expressing. What happens if you introduce borrows? If `fn foo` borrowed an immutable or mutable reference (`&amp;Box&lt;Foo&gt;`) instead, would the Rust compiler still optimise the return so that the `let other = box foo(heaped)` call wouldn't allocate a value on the stack at any point? edit: With the original example in the guide I was specifically wanting to know if there's two heap allocations occurring or one. The text I hightlighted made it sound like the value passed into the function was just moved to y without a separate heap allocation. But my gut feeling is that x would be deallocated when foo returns, and y would be stored in a separate heap allocation.
&gt; fn assert_ranges(...) where T: Show { ... } I don't think this actually works properly yet. (It would be rarely nice if it did!)
After making ~700 lines of changes across several libraries that I use, or that the libraries I use depend on, I can finally sit back and appreciate that this was finally done. Still was a waste of my morning though. :)
And Java has JetBrains Idea.
I actually thought you'd make the new deck more like this: fn new_deck() -&gt; Deck { let mut deck = Vec::new(); for suit in Suit { for val in Value { deck.push( (val, suit)); } } deck } Or like this: fn new_deck() -&gt; Deck { let mut deck = Vec::new(); for (val, suit) in (Value, Suit) { deck.push( (val, suit)); } deck } But alas, neither works. I'm not sure how, if it's possible to loop over the enums like that anyway.
The only problem I see with this is that it can be difficult for a predominant library to emerge. I think Node.js suffers from this. There are 10 different packages for everything, and it can be very time consuming to weed through them all to find the best.
&gt; I think in the section I've bolded above it should be y instead of x? I think the *as x* in your bolded text **passes a pointer to that memory into foo as x** simply says it's passing `x` to function `foo`, where `x` is a pointer to that allocated heap memory. BTW, in the Pointer Guide, is \`box intended as \`box\`, missing a closing tick in the markdown syntax? 
Isn't it pretty important for a language to have a standard DateTime object? It's a pretty common thing to need, and libraries won't be able to interface with each other without it.
Right that makes sense, the language just doesn't make it very clear exactly what *that* in the statement refers to. Maybe it could be clarified to: &gt; room for the `box x` , passes a pointer to that memory into foo as `z`, and then foo writes the value straight into the `y` pointer. This writes the return value directly into the allocated box for `y`. Where `z` is used to label the value `x` inside the `foo()` function, instead of using the same label both inside the function and out of it, confusing readers about which `x` is being referenced.
Read This Next on your blog seems a bit arbitrary: http://i.imgur.com/7xaCeK9.png
Right, this was what seemed logical to me, and was the source of my confusion over the documentation. If I get a chance tonight I'll try and submit a bug and/or PR against the documentation for it.
Aww, damn. But it should work in the future, right?
You probably need to use sets instead of enums, to iterate over.
By the way, if anyone has improvements to the CSS to suggest, please do so! I'm planning on reskinning the site, but I don't have the time to do it myself at the moment.
This is a fairly good list of comparisons. I believe Rust can be used for scientific or data analysis and Julia can be used for lower-level stuffs, but the initial community and use cases of a new programming language largely influence its ecosystem evolvement, which is equally (or more) important to the vanilla language itself. As a dynamic language, Julia is good for quick prototyping/experimentation, which is at the opposite of Rust. With Rust, you pay the cost upfront in exchange of future expenses. Depending on how critical your code is, this can be a pro or con. For one-off task or one-man workshop, Rust could be overkill. But once the task or operation complexity goes up, Rust pays off. That's my perception. 
Thanks DroidLogician, for the detailed explanation of both the languages. Appreciated
Is it possible though to have a library that doesn't have its regrets after 10 years? I'm not sure it's a worthwhile goal.
I find that, for a compiled language, Rust still lends itself to minimal or one-off programs, especially compared to something more verbose like Java. For very simple tasks that don't lean too much on lifetimes or ownership, Rust feels very lightweight. The stdlib is comprehensive enough to cover most use cases for scripting, and for small programs, `rustc` spits out a binary in just a few seconds and it's ready to go. But then there's a sudden jolt where you add something that might work just fine in a managed language, but Rust is having you add lifetimes everywhere and is complaining about multiple mutable borrows and ownership and moves and your code starts to gain complexity at a frightening speed. I think that transition is where beginners most often get confused and frustrated. It happened to me but I was ready for it, having taught myself several languages already. I think we could really improve adoption especially by people used to managed languages if we did what we could to make this transition easier.
&gt; &gt; It felt very weird that static trait methods are called as Trait::method(). &gt; Just wanted to say I agree that this feel like a wart. I don't know whether any changes to it have been considered/rejected. But if you think about it, the other way doesn't make much sense. Because `T` can implement multiple traits with `method`, you'd still need to specify which trait's method you're referring to. So now you're specifying the trait, and given that inference can handle the type pretty well, we can leave that off. Now we're back to the current situation.
I thought you could switch using the original type names.
&amp;str would work for (1) but because of the line below it &amp;str is not suitable. (consider the logger that must consumer a String due to the api it uses; now you have to allocate a new String for the &amp;str)
This is the same mechanism that powers copy elision (RVO, NRVO, etc) in C++. A pointer to the memory location where the value would eventually end up is passed in to the function and the compiler constructs it directly in place rather than copying/moving it.
I'm pretty sure you missed the point.
&gt; Isn't it pretty important for a language to have a standard DateTime object? It is certainly useful, but between being stuck with a terrible datetime object (javascript, the vast majority of java's history) and not having one, I'd just as well not have one. &gt; It's a pretty common thing to need, and libraries won't be able to interface with each other without it. Do they need to though? They'll generally be glued together by the user's code which can transition between representations as long as there is a common exchange format (generally unix timestamps or iso-8601)
&gt; I find that, for a compiled language, Rust still lends itself to minimal or one-off programs, especially compared to something more verbose like Java. Agree it's much better than Java for one-off tasks. Among compiled languages, Rust is a very promising one. I've been writing Java and C++ for years for work. Java is very verbose, and a pain to traverse in its class hierarchy labyrinth, but its excellent IDE support does help a lot. C++ is like a beast which is often unsafe, so I only used it for critical parts of the system. (Disclaimer: my C++ experience was around early 2000's and things might have changed a lot with modern C++.) That said, I still don't think Rust can compete with interpreted languages in one-off or batch-oriented tasks. For example, when distilling a bunch of text files, I often wrote Perl one-liners, hacky and ugly but worked. Python scripts are also quick to write as long as I don't care about long-term stability or refactoring/maintenance needs. Compiled languages, no matter how minimal they are, incur upfront development costs compared to dynamic languages. A van and a sportscar are simply different. That's my view. 
Like [EnumSet](http://doc.rust-lang.org/std/collections/enum_set/struct.EnumSet.html)? It looks like it might work but it definitely needs some examples.
yes, but there‘s two parts to a node: 1. fields, stored like you said 2. allowed child node types if i have union types, i could do: enum NodeType { Doc { children: Vec&lt;Box&lt;Section | Header&gt;&gt; }, Header { text: String }, Section { children: Vec&lt;Box&lt;...&gt;&gt; }, ... } match doc_node.first_child() { sec: NodeType::Section =&gt; ... NodeType::Header { text: header_text } =&gt; ... } without, i’d have to create an enum, and do deeper levels of deconstruction: enum NodeType { Doc { children: Vec&lt;Box&lt;DocChild&gt;&gt; }, Header { text: String }, Section { children: Vec&lt;Box&lt;...&gt;&gt; }, ... } enum DocChild { Section(NodeType::Section), Header(NodeType::Header) } match doc_node.first_child() { DocChild::Section(section) =&gt; ... //well, that works OK, but: DocChild::Header(NodeType::Header { text: header_text }) =&gt; ... }
Whatever happened to the RFC changing macros to use `@`? `vec![1u, 2, 3]` vs `@vec[1u, 2, 3]` edit: and is it an orthogonal discussion? 
Before the language stabilizes and some good Rust programming books (not just blog posts) emerge, I don't think it's a good idea to learn Rust as your first programming language. You'll likely spend much time on the language itself rather than programming. Let's hope Rust 1.0 beta gets out of the door on time. That will allow a lot of things to happen. 
What do you get from using enum for Value? I'd make it as a wrapper around int. This will save you time definining operations for easier card manipulation, so you can test for flush easier and then test if it is flush royal or not. I don't think pattern matching would help for defining such operations, but integer arithmetic definitely would.
the first and second are not solved by “anonymous” type unions, as they get the nondescript names “Left” and “Right” instead of retaining their expressive names. and your workaround of using a custom enum isn’t a workaround at all, but simply using the original solution that has the original problems 1. and 2.! i want to match on the types without having to wrap them. let x: Vec&lt;A|B&gt; = ... already has enough information for the compiler to allow code like for &amp;ab in x.iter() { match ab { a: A =&gt; b: B =&gt; } } no named enums needed.
I thought of any container that contains unique values.
Like someone already mentioned in the last thread, it's disappointing that the semantic of `pub` and `extern` are different from other language items. `pub` should be the keyword for reexport and to define a crate escaping macro. For crate local macro we should use another keyword like `crate`
Enum cases are not types, they're values within a type.
All at once, if possible.
I asked the same [question](http://stackoverflow.com/questions/25840109/whats-going-on-when-returning-pointers-in-rust) in stackoverflow several weeks ago.
It never enjoyed any community support, and after some reflection even the theoretical semantic cleanup it was supposed to enable was not as big of a win as thought. Today I expect to submit a new RFC that proposes the good bits of that PR without the associated baggage of changing the macro syntax.
Can you wrap it in an int with a constraint? (Highest value = 13) Also depending on the game you use the deck for the ace could be either 1 or 13
I dont remember seeing set in the guide but using an EnumSet would allow it to be iterable while still giving the same properties of an enum? Couldn't I also just give each enum an iterator method (which I haven't ever done yet)?
I didn't say that they were. 
I think they're right to focus on the core language.. if people are motivated by the need for Rusts' differentiating features (a safe/sane language without GC) - the libraries will follow
&gt; TL;DR: We need to get the standard library right! This is exactly why there is no http module in the stdlib yet. A missing http module is causing only a mild annoyance (everybody will understand that a 1.0 language wont come with a fully developed stdlib). A bad http module on the other hand will cause the bad blog post you are afraid of. If we would wait for everything to finish we would never reach 1.0. You have to make a compromise.
Yes, high performance Java often pre-allocates things. Minecraft is an example of a game written in Java. GC has given them some trouble as of the latest patch, though...
IMO the syntactic changes should not be included. I think Rust needs to study other macro systems and learn from their designs, namely Nemerle which is very close in spirit to Rust. My ultimate Rust Macro system (after Rust 1.0) would borrow the following design ideas from Nemerle: * multi-level compilation - this is a more general mode and is an easier abstraction to understand and implement. * syntax for splicing token trees - their &lt;| |&gt; syntax. This means in practice: * Macro definitions go into a separate compilation unit and compiled as a SO/DLL. In rust - separate crate for macros. This allows for better separation of concerns e.g. how to handle cross compilation. Also, macros that use other macros are trivial - you just need to compile your crate of macros with the specified macro dependencies exactly as in the regular case, see next point. * Macros are compiler plugins. Already the case in Rust (procedural macros). In Nemerle they are loaded by an additional command line argument to the compiler but in Rust land that would probably translate to an attribute and cargo option * Macros are hygienic - Need to implement this for Rust procedural macros. * In Nemerle all macros are procedural and you can specify at which compilation stage they are run and has an optional syntax clause. In rust terms, that would unify lints and macros (just specify via attribute the stage) and that means that the macro_rules will be subsumed by the regular procedural macros. I'd like to keep the syntax available for such a futuristic macro system. Note that the above removes the need for most of the additional syntax and semantics and removes lots of unnecessary complexity from the language.
&gt; Go requires a big standard library because built-in types are privledged in a way that user-created types aren't. Your premise is true of course, but I'm not sure if your conclusion is. There is a [builtin](http://golang.org/pkg/builtin/) package which provides (documentation) for privileged things (including polymorphic functions!), but the rest of the standard library is Just Plain Ol' Code as far as I know. I'm not sure exactly why Go's stdlib is big (but small compared, to say, Python), but I bet if you asked, the Go people would indeed say that it is *too* big. :-) There's just a lot of *stuff* in it. Things I can think of that aren't in Rust's currently: Parsers for XML, CSV, a binary format (gob)... Implementations of various compression algorithms. `tar` archive support. HTTP (+ cgi + fastcgi), basic crypto things, image manipulation, HTML templates. But there's a lot of overlap too: os stuff, io, string handling, network, sorting, string conversion and so on... To their credit, after the 1.0 release, a lot of attention shifted toward providing quasi-official libraries (outside of the stdlib but maintained by Google folks). e.g., [text](https://godoc.org/golang.org/x/text), [tools](https://godoc.org/golang.org/x/tools), [crypto](https://godoc.org/golang.org/x/crypto) (!!) and [net](https://godoc.org/golang.org/x/net). I'm sure there are others... But yeah, agree with everything else. :-)
I hesitated before typing that, but yes, I was referring to the `builtin` part, not the entire standard library. A lot of it doesn't, but some of it must. TL;DR: maybe I implied more than I meant to :)
Yesssss. Die explicit generic lifetimes, die!
&gt; Standard libraries are where code goes to die. That really depends, Python's standard library generally great and removes an important barrier for getting started, as well as for deciding what to use - no hunting for packages to do X,Y,Z, just use the stdlib unless there's a good reason not to. More generally though there's a special dose of awesome to be had every time you realize you can just DO something with python without installing a package. That said, even python took ages to get http right - urllib, urllib2, httplib, httplib2, urllib3, and finally requests.py which is what you should actually use most of the time... So while getting the stdlib right is important, perhaps the specific example of http is something better left to crates. Personally though I like the idea of "blessed" libraries - they don't have to be in the std lib but they should be closely aligned with the core project and the documentation makes clear "when in doubt, use this!"
Thanks for all the help. That was a frustrating few hours on this project [=
Just one line? Neat! Has the plan for that been written up somewhere?
&gt; And the worst part is, in today's world it only takes a single negative blog post after 1.0 to hit the front page of Hacker News and we have a problem. Ignoring the fact that every language has its haters, I think Rust is bound to attract some negative blog posts after 1.0, for two reasons: 1. Some people will expect a rich standard library and ecosystem, no matter how silly an expectation this is. There are always people that expect too much, and this can't be helped. 2. Rust is simply too opinionated. Safety and correctness above all else isn't for everyone, but there will be a few people who heard Rust is The Next Big Thing^(^TM), and they'll be upset when they find that it doesn't mesh well with their coding style. A bit of negative press is inevitable, but we shouldn't worry too much. Someone with a serious interest in the language won't be put off by a single negative blog post, especially if it's based on inflated expectations.
In general I agree with "Standard libraries are where code goes to die". IMHO the great value of a stdlib is enhancing interoperability between non standard libraries. So maybe, better than a comprehensive stdlib, we need better standard `Traits` in the stdlib.
The standard library should only contain those key features almost any piece of code would need, like containers, convenience functions and traits etc. I think it's better to let all the other features be like the old extra crate, not in the standard library, but easily available outside it. This way you only have to focus on the most important parts to get right. No one would use a language with inefficient containers and iterators, even if the standard library had http, image processing, sound processing, etc. Plus, the langauge needs to be backwards compatible for good libraries to be written! 
I'm not convinced with the idea of letting different libraries fight out to see which one is the best will work well. I think what might end up happening is there will be a few good ones and most people will use one of those. That may be ok most of the time but one of your dependencies may use x and the other one may use y - which is double the amount you would normally have to download and build. What if one library uses datetime-x and another uses datetime-y? It'd get pretty annoying if I had to convert to a unix timestamp and then parse it so I could talk to both libraries. 
yes, see http://doc.crates.io/crates-io.html When you're using stuff from git, it currently looks like this: [dependencies.dining_philosophers] git = "https://github.com/steveklabnik/dining_philosophers.git" when you're using something from Crates.io, it will look like this: [dependencies] dining_philosophers="1.0.0" I'm not counting the first line because when you add a second, you only add the line for it itself :wink:
This is my attempt, which one may argue is even worse fn attenuate&lt;T: FloatMath + FromPrimitive&gt;(channel: T) -&gt; u8 { channel.min(FromPrimitive::from_f64(255.0).unwrap()).max(FromPrimitive::from_f64(0.0).unwrap()).to_u8().unwrap() } fn main() { assert_eq!(attenuate ( 0.0f32 ) , 0u8); assert_eq!(attenuate ( 255.0f64 ) , 255u8); assert_eq!(attenuate ( 256.0f32 ) , 255u8); assert_eq!(attenuate ( - 1.0f64 ) , 0u8); }
What you want is called "polymorphic variant" in OCaml. It's .... a bit more complicated to type and I'm not sure you really want to mix them with normal variants directly. It's not a trivial addition, at least.
Heh, literally my only experience with Python has been switching a script that was way too slow away from the standard http library to requests.
Sure, I get the http module, but removing things like time from the stdlib without having a proper replacement for it isn't a step in the right direction, I think. 
But we already deal with this problem for non-static trait methods; why have a solution that only works for static methods?
Found a solution that seems like it could work pretty well. The idea is to use a macro that defines the enum and implements a `variants()` method on it that iterates over its values: macro_rules! iterable_enum( ($name:ident =&gt; $($val:ident),*) =&gt; { #[deriving(PartialEq)] enum $name { $($val),* } impl $name { fn variants() -&gt; Map&lt;'static, &amp;'static $name, $name, Items&lt;'static, $name&gt;&gt; { static VARIANTS: &amp;'static [$name] = &amp;[$($val),*]; VARIANTS.iter().map(|x| *x) } fn num_variants() -&gt; uint { [$($val),*].len() } } } ) iterable_enum! { Value =&gt; Two, Three, Four, Five, Six, Seven, Eight, Nine, Ten, Jack, Queen, King, Ace } iterable_enum! { Suit =&gt; Clubs, Diamonds, Hearts, Spades } Then `new_deck()` becomes dead simple to write (remember that `Card = (Value, Suit)` and `Deck = Vec&lt;Card&gt;`): fn new_deck() -&gt; Deck { let mut deck = Vec::with_capacity(Value::num_variants() * Suit::num_variants()); for value in Value::variants() { for suit in Suit::variants() { deck.push((value, suit)); } } deck } Props to [dbaupp](http://www.reddit.com/r/rust/comments/29sk5d/iterating_over_static_enum_values/cio3dos) for doing the hard work of actually coming up with this solution. There's probably an optimization available here to provide a `num_variants() -&gt; uint` method and then initializing `deck` with the number of values * the number of suits, but that should be pretty easy to tack on to this. EDIT: updated with aforementioned `num_variants()` optimization.
As an example, the `Net::HTTP` module in Ruby's standard library is pretty bad, and often a source of many problems, which is why HTTP in Ruby is a mess of libraries and middlewares and wrappers and interfaces (to people wondering: Use Faraday).
Your version compiles to much better code as min and max calls in my version are not inlined (even in -O3). This is an even more generic version, http://is.gd/qKO0vc
&gt; but removing things like time from the stdlib *without having a proper replacement* for it (emphasis mine) The replacement is [rust-lang/time](https://github.com/rust-lang/time), which is pretty much the same code just moved out of the rust-lang/rust repo. The [breaking-change](https://github.com/rust-lang/rust/pull/18858#issue-48326815) message has information on how to migrate to it.
Rust 1.0 will be feature complete. But like every 1.0 of anything ever happened it will have less features then a 1.x. I have no reason to doubt that the further plans with Rust and the stdlib will be communicated alongside with the release.
&gt; yes, see http://doc.crates.io/crates-io.html O_O I take it the public registry is coming soon? :D
How would that be much better than std::datetime being crappy and 20% using datetime-x, 20% datetime-y and 60% using std::datetime?
I believe there is a strong intent to have blessed/promoted cargo packages.
If you like iterators you can do this: use std::num::{FloatMath}; fn attenuate&lt;T: FloatMath&gt;(channel: T) -&gt; u8 { NumCast::from(255u8) .into_iter() .zip(NumCast::from(0u8).into_iter()) .map(|(hi,lo)| channel.min(hi).max(lo)) .next() .and_then(|t|t.to_u8()) .unwrap_or(0) } Having said that, for the builtin `f64` and `f32` a simple `channel as u8` appears to do what you want.
I believe the long-term plan is indeed to bless crates that receive substantial community backing. This would effectively uplift them into the rust-lang organization on Github where they can receive the same level of scrutiny and support as the rest of the official crates.
Well, in the case of a producer-consumer setup, the threads have a highly asymmetric memory pattern: specifically the consumer thread `free` much more than it ever `malloc`. Furthermore, in order to free pages you need to know that all the memory ever allocated in that page was freed, and for that whoever holds the page need be notified. So... not too sure about your strategy.
Thanks for your reply!
depending on who you ask - or where you work - an HTTP server is something that almost any piece of code needs... so that's a very dangerous and unclear definition.
&gt; so that someone coming into the language doesn't have to navigate and assess the ecosystem just to figure out how to send/respond-to a web request/parse a csv/etc. This would be a real win, as far as I'm concerned. I can't count the number of times I've started learning a new language, only to find out I'm using some obscure library only four people are using, or the super popular library that was replaced four years ago by the next super popular library that everyone else is using now. When you are new to a language it can be hard to evaluate libraries, as you have no reference. Also, that might be one of the things that drew me to rust - *everyone* is new at rust ;)
Is the fill/flush performed during `malloc`/`free`? If so it would seem to me an optimization of throughput at the expense of latency, and I would wonder if the jitter so created can be detrimental. But then, I guess that when latency really matters at this extent, one has to invest in its own pooling scheme.
I've got good news for you! Run your program with RUST_BACKTRACE=1 i.e. RUST_BACKTRACE=1 ./my_rust_binary
That's what I would have thought, but then why does this work then http://www.reddit.com/r/rust/comments/2ma9xp/struct_with_reference_to_other_value_in_struct/cm32q7o ?
I've uploaded the suggestions here to https://github.com/brookst/rust-cast-shootout My timings are: &gt; ./target/rust-cast-shootout-132f7401037e4cf7 --bench running 20 tests test test_generic ... ignored test test_if ... ignored test test_iter ... ignored test test_minmax ... ignored test bench_generic_max ... bench: 77 ns/iter (+/- 1) test bench_generic_over ... bench: 45 ns/iter (+/- 0) test bench_generic_under ... bench: 70 ns/iter (+/- 2) test bench_generic_zero ... bench: 78 ns/iter (+/- 2) test bench_if_max ... bench: 78 ns/iter (+/- 2) test bench_if_over ... bench: 45 ns/iter (+/- 2) test bench_if_under ... bench: 72 ns/iter (+/- 3) test bench_if_zero ... bench: 79 ns/iter (+/- 8) test bench_iter_max ... bench: 285 ns/iter (+/- 91) test bench_iter_over ... bench: 284 ns/iter (+/- 11) test bench_iter_under ... bench: 284 ns/iter (+/- 13) test bench_iter_zero ... bench: 277 ns/iter (+/- 8) test bench_minmax_max ... bench: 108 ns/iter (+/- 1) test bench_minmax_over ... bench: 108 ns/iter (+/- 2) test bench_minmax_under ... bench: 107 ns/iter (+/- 5) test bench_minmax_zero ... bench: 108 ns/iter (+/- 3) test result: ok. 0 passed; 0 failed; 4 ignored; 16 measured &gt; cargo bench running 20 tests test test_generic ... ignored test test_if ... ignored test test_iter ... ignored test test_minmax ... ignored test bench_generic_max ... bench: 2 ns/iter (+/- 0) test bench_generic_over ... bench: 2 ns/iter (+/- 0) test bench_generic_under ... bench: 2 ns/iter (+/- 0) test bench_generic_zero ... bench: 2 ns/iter (+/- 0) test bench_if_max ... bench: 2 ns/iter (+/- 0) test bench_if_over ... bench: 2 ns/iter (+/- 0) test bench_if_under ... bench: 2 ns/iter (+/- 0) test bench_if_zero ... bench: 2 ns/iter (+/- 0) test bench_iter_max ... bench: 9 ns/iter (+/- 0) test bench_iter_over ... bench: 9 ns/iter (+/- 0) test bench_iter_under ... bench: 9 ns/iter (+/- 0) test bench_iter_zero ... bench: 9 ns/iter (+/- 0) test bench_minmax_max ... bench: 8 ns/iter (+/- 0) test bench_minmax_over ... bench: 8 ns/iter (+/- 0) test bench_minmax_under ... bench: 8 ns/iter (+/- 0) test bench_minmax_zero ... bench: 8 ns/iter (+/- 0) test result: ok. 0 passed; 0 failed; 4 ignored; 16 measured Not sure why `cargo bench` is so much faster. It's surely using the same binary.
`channel as u8` works as long as `channel` is a littoral value. Once it's a variable, you have a non-scalar cast, which is an error in Rust: fn conv&lt;T: Float&gt;(x: T) -&gt; u8 { let y = x as u8; y } fn main() { let x = 256.0f32 as u8; // Works println!("x: {}", x); let y = conv(256.0f32); // Doesn't work println!("y: {}", y); } [Playpen](http://play.rust-lang.org/?code=fn%20conv%3CT%3A%20Float%3E%28x%3A%20T%29%20-%3E%20u8%20%7B%0A%20%20%20%20let%20y%20%3D%20x%20as%20u8%3B%0A%20%20%20%20y%0A%7D%0A%0Afn%20main%28%29%20%7B%0A%20%20%20%20let%20x%20%3D%20256.0f32%20as%20u8%3B%20%2F%2F%20Works%0A%20%20%20%20println%21%28%22x%3A%20%7B%7D%22%2C%20x%29%3B%0A%20%20%20%20let%20y%20%3D%20conv%28256.0f32%29%3B%20%2F%2F%20Doesn%27t%20work%0A%20%20%20%20println%21%28%22y%3A%20%7B%7D%22%2C%20y%29%3B%0A%7D%0A) I've not learnt much about iterators, but can confirm: that is one way to do it. It's in my shootout: https://github.com/brookst/rust-cast-shootout/blob/master/src/main.rs#L28
Could the compiler not check that `i` and `j` are not being changed, and perform this optimization automatically? Or would that give concurrency problems? Do you know if we at least can skip half the bound checks in something like `a[i] = 2*a[i]`?
Nice. Is any support of signatures planned (like in apt-get)?
`cargo bench` may not compile the binary at the same optimization level as build or test. I'm pretty sure bench passes in `-O`.
&gt;&gt; Standard libraries are where code goes to die. &gt; That really depends, Python's standard library generally great Not sure if you are aware, but I think the phrase "The standard library is where modules go to die" was coined in a Python talk. http://www.leancrew.com/all-this/2012/04/where-modules-go-to-die/
This is interesting and pretty cryptic. I'm surprised the std library doesn't provide something for this. It definitely does what I had asked about.
This. Anything shipped in 1.0 is binding. Make a mistake once, suffer from it for life.
Jonathan Blow, author of "Braid." He says Rust is a 'big picture' language, and he likes '85%' languages instead. So he's working on his own language for game development. I'm really pumped to see how it works out!
Actually [strcat was ranting on similar subject on IRC on the other day](https://gist.github.com/anonymous/d51ce84826ea40dd3445). &lt; strcat&gt; there's this very myopic rush towards 1.0 ... &lt; strcat&gt; how about making a good language instead of walking straight into the PR blunder of a disappointing, unusable 1.0 &lt; strcat&gt; why waste all the time making everything into a mediocre "stable" API &lt; strcat&gt; all to release an unusable stable 
[Issue #19076 on GitHub](https://github.com/rust-lang/rust/issues/19067)
&gt; TL;DR: We need to get the standard library right! What is the Standard Library that you are talking about? - Does it matter to you that stuff sits in a `std::` crate? - Or are you, instead, looking for a blessed library for HTTP because you would rather not spend hours on the net trying to divine which of dozens of library that pretend to be the best HTTP library out there to use? Personally, I care less about the first point and much more about the second. What I want is: - an authoritative answer (or as close as possible) to: "which of those should I use?" - an easy access to said library, automated as much as possible - a guarantee that said library has been reasonably designed, and correctly matches its domain - a guarantee that said library will be maintained for the next months/years - a clear deprecation path if it is ever to be replaced, such that I have time to switch This does require some *trust* in the library provider (individual or company or community), and as such I would favor an engagement from a "Rust community managers team". However, writing a HTTP library or writing a datetime library has little to do with writing a compiler; specifically the domain model requires a very different area of expertise, and while a compiler writer can obviously also be an expert in another area, it seems ludicrous to expect them to be expert in ALL the domains at once. Therefore, in order to get the best libraries possible, it seems clear we need the engagement of domain experts, and that those will probably be out of the Rust core team more often than not. Of course, in order to get both community consensus and easy access, it seems that a set of blessed libraries overseen by a "Rust community managers team" could be extremely useful. This does not necessarily imply a blessed repository, it could work equally well with a list of urls to various repositories. 
Last time I checked cargo didn't support binary libraries. Is it still the case? 
Jonathan Blow?
I'm not convinced that Rust is a *great* fit for gamedev, myself. They do some pretty gnarly things at very high development speeds, without a huge concern for safety. Regardless, Blow concedes (I think in his first or second video?) that if Rust becomes the dominant language of gamedev, he thinks it would probably be a big improvement. He just thinks that something better can be done for the specific domain. I think that's a great attitude to have! Always be questioning design decisions.
Ah, that's right. It's a shame that bench doesn't give timings to a higher precision, but I suppose it's not designed for such small tests. I might add a test that iterates over many values to get a better comparison. Thanks!
Yes.
Absolutely.
"Feature complete" is an odd term - do you think people will expect that nobody will improve Rust further after 1.0?
This will be true for the foreseeable future. Rust itself needs things like a stable ABI before it can happen.
There are two heap allocations thoug: One for the data, and one for the pointer to the data
I disagree. I'm talking about every piece of code written from Project Euler submissions, to embedded code, to web servers, to massive physics simulations. 
On the other hand, that criticism has been very useful! I'm feeling quite optimistic about Rust as language for game development :)
Not possible because Rust can't even do this yet. If you both used the exact same hash of the compiler and all the versions on the same platform, it would.
I've been following his work on his new language somewhat. He's of the mindset that it's okay to let certain bugs that the Rust compiler would have caught slip to run time, in order to make development quicker. Personally, I much prefer Rust's approach; I would much rather spend a little more time upfront, rather than debugging segfaults or other bugs that would have been caught at compile time. Several times while I was watching his livecast, I thought to myself, oh Rust would have caught that, while he goes back into debugging mode trying to figure out what happened to cause that particular error. Still, it a somewhat interesting language. It's kinda looking like D without GC (and generics, yet).
Since when can't you improve something when it's "feature complete?"
No, but I doubt people will expect 1.0 is being released with a todo like this: https://github.com/rust-lang/rfcs/issues?q=label%3Apostponed+ Compare rust 1.0 to Ansi C, we are going to see more change in rust in the first year after 1.0 then we have in C's entire lifetime... Improvements I would expect if I hadn't been following the development of the language are optimizations of the compiler, and maybe improvements in the stdlib, not entirely new features in the language. (Yes, I know postponed RFCs aren't technically todo).
Right, but I'm just saying that because Rust can't do this, it's not planned for Cargo anytime soon. The current way would be to just check them in.
i guess it's a matter of opinion. i'm a fan of python's batteries included approach, where i can do whole lot in a very small amount of code thanks to the breadth of its stdlib. logging, config files, option parsing, etc. save loads of time.
Very true! But I think the goal is that Cargo should cover the batteries for Rust.
Feature complete, as in there isn't **a plan** to add entirely new features into the language. Perhaps new features in the compiler options, but not (for example) adding in a keyword to enable tail-call optimization. To be clear, I'm not advocating not releasing 1.0, just being careful to make it clear that 'Rust' isn't really finished, just stable. &gt; Every other language I can think of has received new features post-1.0. I haven't followed any languages previously right after there release, but my feeling is Rust is planning to change more then most. I know it's planning to change more then I would have expected before learning about rust. It's also planning to introduce new features *without* bumping the version number to 2.0, is this really common (I haven't noticed it, but again, I'm new to language building)?
This library provides a great base for building libraries that need to manage their own buffers. Thanks Gankro!
You're gonna want to read http://doc.crates.io/build-script.html Even though you won't be building the dependency, making it feel like a package that you are is the way to go.
&gt; Feature complete, as in there isn't a plan to add entirely new features into the language. Today is RubyConf, and Matz gave a talk about adding gradual typing to Ruby... is Ruby not feature-complete, then? &gt; To be clear, I'm not advocating not releasing 1.0, just being careful to make it clear that 'Rust' isn't really finished, just stable. It is absolutely important that the messaging is right, yes. :) &gt; (I haven't noticed it, but again, I'm new to language building)? Yes, adding features is very common. Languages actually tend to be very poor with versioning, but in the equivalent of SemVer point releases, adding new things is very usual.
I don't think Rust will ever become dominant: If C++ is ever "replaced", I hope/believe we will fragment and specialize (for the better). I agree with what Jonathan Blow says about rust r.e. games -I like it more than he does, but I totally see why he's started his own. I also wholeheartedly agree with his criticisms of C++ and C++ zealots. One thing people miss when looking at gamedev is: the runtime of games don't necessarily have the same requirements as general purpose programming. A game runtime dynamic array doesn't need to do everything every dynamic array in history has ever done. good game runtime engines avoid dynamic allocation &amp; resizing, and *tools* preprocess assets offline to facilitate this. (consistent framerate means consistent load, means consistent predictable memory layouts..) Perhaps this is why he can make rapid progress - I think he will have something genuinely useful for his intended use-cases very quickly. Perhaps complexity will explode once it gets generics etc: but maybe if he hard-codes support for a few simple types he can dodge all that, like Go has .... imagine something about as simple as go, but with a float4 similar to shader languages and obviously no GC. but I know he's said he's planning to do generics. The difficult bit seems to be getting consensus. One thing he mentions is many people already make their own "compile to C" tools. He rejects Rust &amp; D... whilst others may prefer to persevere with them and adapt them (I personally would like to see Rust forked or adapted.. but such a huge source base is intimidating and carries momentum.) and plenty of people accept the "C++ way".
I was thinking about doing that. Would need somewhere reliable to host the dependencies. They're really just mingw compatible libraries for linking to the DirectX DLLs generated by dlltool. Unfortunately I can't rely on dlltool being something the consumer of the library has installed. So I'm planning to build them ahead of time for both 32-bit and 64-bit and include them in some fashion as a part of the package, so when everything is built, DirectX can be linked properly.
&gt; They'll generally be glued together by the user's code which can transition between representations as long as there is a common exchange format Certainly do-able, but a bit tedious. Rust doesn't let you write implicit conversions either, does it?
for gamedev I think he's 100% right; my belief is - you still need to write tests for other reasons - so something that lets you work quickly, adhoc is preferable. Your program doesn't have to be 100% correct/efficient at every step... you experiment, then refine. Rusts' approach would probably pay more dividends for distributed development of a huge project across the internet (as its designed for) games also want a split Debug/Release build, where the release build doesn't have the same runtime checks that Rust has (i.e. bounds-checked arrays).. but the debug build can have *more* checks. It's the job of empirical verification of your design in a debug-build to ensure all indices are valid; a correctly designed game doesn't have runtime failures.
One thing I'd mention is you might want to create a -sys package too http://doc.crates.io/build-script.html#*-sys-packages , and then build this package off of the -sys one. That's going to be the Way of the Future.
I honestly don't think that an HTTP client should EVER be in the standard library. Too many languages have huge expansive standard libraries, and honestly it is more of a hinderance to the standard library and language development. In my perfect world for the standard library, I would ask the question "Is this thing useful for most applications". If the answer is no, it shouldn't go into the standard library. This pretty much only allows for an extensive collections and general algorithms (sorting, etc) in the standard library. It doesn't allow things like xml parsers, http clients, and yaml serializers. It is a PITA to take something out of a standard library, so rough that it almost never happens. So the onus is on the library writer to get everything right the first time. Well, screw that, nobody gets things 100% perfect the first time. So why not push things into libraries that aren't associated with the language? Why not allow those libraries to grow, and polish. It is great for the users because they have control over when they go up to a newer version of the library not tied to a new version of the language. It is great for the library writers because making a breaking change to the library isn't some sort of unspeakable taboo, people can simply refuse to go up to that version of your library. Just look at java and their standard library. They include everything under the sun. And guess what kind of cruft they have? They have classes that have literally been deprecated since the 1.1 version of the language (java.util.Date). They bend over backwards to never make a breaking change to the standard library, and they got a lot of things wrong.
That looks really interesting, and I'd love to make sure that my [heroku-buildpack-rust](https://github.com/emk/heroku-buildpack-rust) plays nicely with *-sys libraries. (One wrinkle is that I don't _think_ Heroku has CMake on its build systems, and it might be worth installing if lots of libraries use it.) Are the any build dependencies that help me compile *-sys libraries yet? Or do I need to roll all that logic from the ground up?
Check https://github.com/alexcrichton/ who has a bunch of them built so far. There aren't a ton yet because the new build script system only landed a week or two ago.
Why bother with all this? I personally really dislike the thought of anything centralized except the required metadata. This is annoying when using apt-get. The ideal flow for me is where a group of developers can create a library and publish it via any number of repositories (including self hosting a repository). They then register their library and repository on the dedicated Rust packages portal and the end-user can find the "best" by using stats such as most-downloaded/most popular/etc.. TL;DR: The central repository only stores the metadata and NOT the packages themselves. 
Time is something a lot of programs need and letting libraries compete may mean you end up with having to deal with several different time types because one dependency used this time library and another one used that time library.
You'll be able to run your own copy of crates.io if you want to, and get packages from any number of them. I can't speak to the difficulty of hosting, but using a mirror is one line in your config. Also, "why bother with this" seems backward to me: running a whole bunch of repository servers seems like a lot more bother than just using the centralized one. It _can_ absolutely be valuable, but it seems like a significant bother. :)
Well, by definition, you can't add new features. By that definition, no language I can think of is feature complete - and nor is any but the most trivial of software.
Ah ok, I thought you were talking from a user perspective. That clears it up.
You may be misunderstanding something. http://crates.io will be exactly the registry that you describe. "Blessing" a library is essentially nothing more than assuring the community that it is a quality, well-documented, stable library that can be relied upon to the same extent as the standard library. It's adding it to the stdlib in all but name. And this plan may even change! Rust is a fickle mistress.
&gt; Anything shipped in 1.0 is binding. Anything shipped in any version is binding, which is worse. You can't really backtrack on your standard library.
For those reading along, here are some links I found: **Several recent examples** https://github.com/alexcrichton/git2-rs/tree/master/libgit2-sys (great CMake example) https://github.com/alexcrichton/bzip2-rs/tree/master/bzip2-sys https://github.com/alexcrichton/flate2-rs/tree/master/miniz-sys https://github.com/sfackler/rust-openssl/tree/master/openssl-sys (uses pkg-config-rs, nested in parent library) https://github.com/alexcrichton/ssh2-rs/tree/master/libssh2-sys (lots of stuff going on here) **More links** https://github.com/alexcrichton/pkg-config-rs (quite a bit of code here already) https://github.com/alexcrichton/cmake-rs (just a stub) https://github.com/alexcrichton/libz-sys (uses pkg-config-rs) https://github.com/alexcrichton/libssh2-static-sys (use a shell-script to build) I think I can more-or-less see how to make this work with rust-uchardet, but I'm eagerly looking forward to some mature examples and docs. :-) Many thanks for your feedback!
I don't think the real issue is it being missing from the standard library, but that there's no great HTTP lib available.
well, i do get to skip creating an explicit child enum for every node type, and to deconstruct that enum that’s something! also i think it’s duplicate information: the `A|B` syntax would allow simple, on the fly, anonymous enums that are defined by the set of types (i.e. have no identity, and can be “created” in multiple places and still be the same) deconstructing or matching them would be done using the types used to create them, which don’t have to be in the same enum! struct A(u64); struct B; let x: A|B = ... match x { A(a) =&gt; ..., B =&gt; }
Tell that to most AAA games that have come out in the past 3 years and have day 1 patches, and continue to be silly buggy for weeks/months after they come out
bench_generic and bench_if compile to pretty much the same instructions. generic should be preferred though as it can work with all numeric types, not only f32 and f64.
Thats just market pressure forcing release ahead of rivals/inline with marketing campaigns etc. does rust save time? I'm not so sure. it just forces you to do more of the work upfront. if you have to make it 100% correct at every stage of experimentation, it will just take longer to experiment. Game development is a fluid, creative process full of u-turns. rust is motivated by a different use case - a web browser is a lot larger and a ubiquitous target for attacks,so the safety and *huge source base* issues are at the forefront. 
crates.io does store the packages: it is forced to, to try to minimise outside factors that would stop reproducible builds (e.g. if git repositories disappear, or if someone decides to edit public history, in a fit of madness).
Should the standard library just be an (implied) cargo dependency. As newer (forward compatible) styling versions are released users can just pick it up. Unless they pin themselves to a specific release? 
True, that is an issue. Maybe the rust devs should "sponsor" an http library. I just don't want the "there is no good implementation so this should go into the standard lib" sort of mentality. Though, for a 1.0 language, I don't think it is a terrible thing to not have a bunch of polished libraries that do everything you want. (or even have many polished libraries at all). Those will come with time.
How would you handle other straights? That is, those from [A, 2, 3, 4, 5] to [10, J, Q, K, A]? Just curious.
The standard library must include a type to denote an absolute time to use with I/O or synchronization API with timeout. `std::duration::Duration` isn't enough because it is a relative time . See [POSIX `pthread_cond_timedwait` &gt; Timed Wait Semantics](http://pubs.opengroup.org/onlinepubs/9699919799/functions/pthread_cond_timedwait.html) for the rationale.
&gt; The standard library must include a type to denote an absolute time to use with I/O or synchronization API with timeout. I recently saw that @alexcrichton is working on a new [sync](http://alexcrichton.com/sync-rs/sync/index.html) library which may end up in the stdlib. I highly encourage you to discuss your concerns with him, now it's a good time (before the std::sync module gets marked as stable) to bring up these issues to the core team.
You've... obviously never switched from C89 to C99. Very, very different.
I 100% agree with that. I would love the stdlib to be bare bones but have some sort of "blessed" crates from the Rust devs at least until things mature.
One can have precise version dependencies on external packages, and upgrade them independently of the core language, e.g. one could have some hypothetical HTTP package have versions 1.0 and 2.0 both working with Rust 1.1, where the 2.0 has made breaking changes. People can then upgrade Rust without being forced into upgrading the HTTP lib and vice versa. The delete-a-repo problem is fixed by the crates.io registry, which stores tarballs of each published package. This gives us maximum flexibility: we have both 100% reproducible builds using registry crates, and cutting-edge functionality using git dependencies (which are reproducible by default, but cargo cannot protect against the repo disappearing, as you say).
More the reason to not stabilize the time library/module yet, which I suspect it's the reason why time has been moved out of tree. If the rust team ends up stabilizing a poor design or a design that has not been thoroughly tested, it's likely we'll end up with the situation you are describing: several competing time libraries because the rust-lang's one is not good enough. Also note that the crates under the rust-lang are "blessed", so it's likely they'll be shipped with the rust distribution at some point (probably not for 1.0, but when each crate has been marked stable).
This thread has been linked to from elsewhere on reddit. - [/r/programmingcirclejerk] [Rust is doomed to fail because its standard library is not webscale.](http://np.reddit.com/r/programmingcirclejerk/comments/2mq66n/rust_is_doomed_to_fail_because_its_standard/) *^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)* 
That's a good point, I'd forgotten about that. I personally like describing it by the trait though, it feels more descriptive than mentioning it by type, as you can get .ore behavioral info from the trait. I'm guessing that's the Haskell in me talking though, I've got much more experience with that.
My huge fear when people say, "Hopefully someone will write a lib to complement the standard library on `X` feature..." is that you often end up with multiple competing libs which implement the same thing in vastly different ways, quality standards, and levels of support and longevity. While this isn't horrible (and competition is one of the staples of open source), it's very confusing and time consuming to the end user on picking a lib. I'm not saying we should have a lib for every niche area under the sun, but some of the sections being taken out aren't niche at all. I fully understand not wanting to present a poor implementation, which I agree would send an equally horrible message, but I think a feature complete *basic* level implementation of common libraries is a must for 1.0. Just my $0.02 I *MUCH* prefer how some languages like Go have gone about the subject (even if by accident). Create a *basic* lib on feature `X` and let the community come up with extras, more niche implementations, etc. This gives me piece of mind, that at least to a basic level I don't need to worry about whether or not feature `X` is officially supported or compatible. TL;DR, finding libs in the community on feature `X` when not provided by the standard library (not the big ticket items) can be time consuming and frustrating as sin. You'll go through varying degrees of support, quality, etc. It's so much easier when it's included in the standard library, at least to a *basic* level. And as far away as we can stay from the Phobos/Tango fiasco that D went through the better. 
Sorry, I'm not sure what you mean. Which language? With respect to what?
I really like this and I think we need more stuff like this - collections of extended behavior for common types that fill certain niches but not everyone needs.
OK, after a fair bit of scratching my head and wrestling with build scripts, rust-uchardet [can now use either the system copy of uchardet or build its own](https://github.com/emk/rust-uchardet/blob/master/uchardet-sys/src/build.rs). This process is still noticeably more arcane than writing the FFI wrappers themselves, but I imagine it will get better once a few more support libraries have been made available. Thank you for your feedback!
&gt; If so it would seem to me an optimization of throughput at the expense of latency The thread cache is there to improve throughput and reduce lock contention. It doesn't hurt the worst-case latency in a significant way.
&gt; true, but you then need to know which packages to use. My point exactly. A pet peeve of mine is going back and forth between different community provided libraries spending valuable time and energy on them only find they won't work for one reason or another. Typically when something is included in the standard lib it at least works to a basic level, which at times is all I need!
While this is awesome, and I'm super excited about this feature it still doesn't remove the finding of the "blessed" library for which ever feature you're searching. I.e. this HTTP lib is kind of a bad example because it's one of the more complex ones. But let's just assume we're talking about something simple for example; typically the more simple the task the more choices their are on library support. So while once we've found exactly which lib we want, adding and using it a one/two line fix...it's the finding that concerns me.
I'm not entirely sure what you're getting at. Your example is entirely different from what is in the guide. With full type annotations you get: let foo: Box&lt;T&gt; = box f; let bar: Box&lt;Box&lt;T&gt;&gt; = box foo; Indeed this only copies/moves a pointer into a new heap allocation. However, the guide example is much more similar to: let foo: Box&lt;T&gt; = box f; let bar: Box&lt;T&gt; = box *foo; It seems sensible that this copies the contents of `foo`'s heap allocation into `bar`'s heap allocation.
sage
&gt;Map#get takes an Object even though it should take the generic key (why doesn't it? legacy) Not sure this is legacy. I agree it's a bad thing, but it is possible for two completely unrelated type to have "compatible" equals/hashCode. So by having Map#get take the generic key, you would restrict some "useful" patterns. Since nobody cares about those, what lose in checking for silly mistakes still makes it a bad choice.
Ha!
Hm, I assumed it would merely mirror them for backups while deferring to a canonical repo.
&gt; without a huge concern for safety And this is why your games crash. Seriously, I've actually worked on a game engine for a AAA title. Shit's mad fast, but safety is just not a priority. The attitude is ship it and patch it. And it shows. Which is why I like waiting for games, because usually you get a better product, all around.
Please let me know of any tools which are not on the list and of course comments on the forwards-looking parts of the doc.
I was thinking it would be more useful for game engines.
My fear is that they will start working on a 2.0 only few years after 1.0 because they can't restrain from breaking things again.
&gt;perf - apparently this works, we have some kind of make file perf.mk in the repo, no idea what it does Perf is one of the highest quality profiling tools around, I haven't tried it with rust but I don't see why it *wouldn't* work, it works just fine with e.g, D.
I look forward to the point where the language and libraries have stabilized enough to allow IDE work going forward.
I've been using callgrind in a limited capacity to profile [img-dup][1] and it's worked pretty well, though the name-mangling can make it hard to identify the functions (not horribly, they're still recognizable for the most part, even in release builds) and the pervasive use of `#[inline]` in the stdlib can make it hard to identify hotspots. Getting annotated source is something that doesn't work currently and I haven't really tried to make it work. [1]: https://github.com/cybergeek94/img-dup
I think this is a really great idea - similar to what golang does. 
I was replying to the part that implied that packages will move under the rust orginization on github. If that is indeed not the case as you describe above than it is indeed exactly what I want :)
I'm surprised that none of the property based testing libs are mentioned under testing. https://github.com/search?utf8=%E2%9C%93&amp;q=rust+quickcheck
&gt; Rust can and will probably get some strong scientific computing libraries in the near future, if it doesn't have them already Interested bystander waiting on the sidelines to see whether that'll actually happen. Problem is I don't see enough unique compelling features in Rust that solve real challenges scientific computing library authors have, to overcome the substantial gap in available functionality. The problems I face are algorithmic correctness, performance (particularly scaling to distributed-memory MPI settings), and ease of development and usage. Struggling with the compiler to get memory-safety is not usually very high up on my list of priorities. To develop a solid, usable scientific computing library ecosystem, you need domain experts creating really attractive tools to start with. To even get started doing things like numerical optimization libraries that I work on (see www.juliaopt.org for examples of what I mean), I would need solid blas and lapack bindings, and good high-level handling of sparse matrices. (Even Julia hasn't gotten that last bit completely right yet.) I don't want to use a thrown-together experimental Rust library written by someone who doesn't really know linear algebra very well when what I actually need is something approaching the quality and usability of NumPy. That's a huge gulf to cross when the community of users who truly have the skill set to pull it off properly is miniscule. This application area appears to hardly even be on the radar of the core language developers and most of its user base - understandably, since browsers, systems programming, and game development are entirely different worlds to scientific computing.
[`Float` is a trait](http://doc.rust-lang.org/std/num/trait.Float.html), so it can't just be cast to a numeric type. It can also be implemented for types where casts doesn't really make sense.
You can just take a reference to the object; that will work well: pub fn get(&amp;self) -&gt; &amp;T { unsafe { &amp;(*self.current).data } } Or, for simple types like `int`, copy the value bitwise: impl&lt;T&gt; List&lt;T&gt; where T: Copy { pub fn get(&amp;self) -&gt; T { unsafe { (*self.current).data } } } Or as a generalisation of that technique clone the value: impl&lt;T&gt; List&lt;T&gt; where T: Clone { pub fn get(&amp;self) -&gt; T { unsafe { (*self.current).data.clone() } } }
&gt; …because the train needs to run on. :D
See, Rust isn't even stable yet. I'm sure once 1.0 hits, we'll see at least an honest effort to develop these kinds of tools. A surprising number of people are waiting for Rust to stabilize; when that happens, I expect explosive growth for the Rust ecosystem. But there's a fundamental issue that I addressed in my conclusion, and you touched on in your reply: is Rust even the right tool for this job? Maybe it would be better to mainly use Julia, because it's designed for this purpose, and then write extensions in Rust that you might otherwise write in C, if you had the need or desire to do so. A lot of people in the Rust community, including myself, have a strong desire to develop a diverse and comprehensive pure-Rust ecosystem, but we also accept the fact that some things just don't work well in Rust, or otherwise it's just not worth reinventing the wheel. And I think that's perfectly fine. Fortunately, because of Rust's emphasis on transparent C interop, it's not at all necessary to have a pure-Rust ecosystem, *or* reinvent the wheel. Rust can call into the same C extensions built for Julia, or NumPy or any other scientific computing library, though those extensions are probably designed for interop with their respective languages and are likely littered with the C representations of their higher-level types. If you want Rust to have better support for scientific computing, I don't think it'll happen if you just stand around and wait. Libraries and tools happen when people are interested in a language but see something missing, and have the gumption to take it upon themselves to fix that. You clearly have the interest and the knowledge, why not take a crack at it? Sure, it might not stack up to NumPy for a while, but keep in mind, NumPy had to start somewhere too.
'on schedule, high quality, within budget - pick 2'
sl is a common mistype for Linux users wanting to list directories with ls. Someone decided to make a little "reward" for each of those mistypes by compiling sl, which displays a train scrolling across your terminal.
Worth noting is that the [classic sl implementation](https://github.com/mtoyoda/sl) is uninterruptible during the animation which adds to the fun :)
You could have types other than f32 and f64 implement Float. It doesn't make sense to have "x as u8" work for some arbitrary type Foo.
What stops Rust from being a great fit for gamedev?
You're awesome!
[quickcheck](https://github.com/BurntSushi/quickcheck) for testing. :-)
I know no programming languages shipped with a better collections library than Java's.
`geany` is another editor with Rust support.
Clear now, thanks.
D purity allows local side-effects and memory allocation via `new`, but the developers don't plan to support I/O. The compiler would have to spawn a process; currently it doesn't need to.
It segfaults *if you're lucky*.
Github's [atom](https://github.com/atom/atom) editor has both [syntax highlighting](https://atom.io/packages/language-rust) and support for [code completion via racer](https://github.com/edubkendo/atom-racer).
I can confirm that callgrind works very nicely, especially with the kcachegrind GUI. I've used this a bunch. But yeah, inlining makes things harder.
You can use the new slicing syntax and then you have let b = vector_f64([3.0, 6.0][]); which is nicer. There's always a tension between explicit and implicit. Rust tends towards explicit.
Fortunately you don't need to use Java to use the JVM.
So tried it. it says the syntax is experimental. I need to add `#![feature(slicing_syntax)]` to the crate. Should I really use it? It might break down in next build. 
This works. Thanks. 
It's true that it's experimental, but it (or something very similar but with a different implementation) is going to be in for the long haul. And things always might break in the next build until 1.0...
that almost looks like a multidimensional array
That is a great idea.
That's a great point.
So disclaimer: I've never done any really low-level gamedev. So I'm not an expert here. All my projects have been Flash/Javascript/Java. Still, my general understanding is that gamedev loves the sort of patterns that Rust hates. Tons of globals, lots of shared mutable state. Crazy tricks abound. It's also probably the most reasonable field to just say "yeah I don't care if my program is safe". It's a game. If it crashes every few hours, leaks a bit of memory, or does bad stuff with user input... it's not the end of the world. It sucks, and I'm sure they'd love if that wasn't a problem, but I don't get the impression its high on their priorities.
Shouldn't we call it ForwardIterator or something similar and make Iterator require it and others?
they'd also hate to have rusts verbosity within unsafe code where its needed, and bounds checked arrays by default. *const *mut, double casts- there is no benefit to this. Plus , C++'s template/adhoc overloading system is still more pleasant for maths work, IMO. the point is engines don't have lots of dynamic allocation going on(once they're well optimised). They're not the same as web-browsers. *tools* preprocess assets such that the runtime game engine doesn't have to work so hard. If you watched blows' video - he mentions concatenated allocations, he doesn't consider "struct {... vec&lt;A&gt;,.. vec&lt;B&gt; }" to be 'production quality code'. I've worked on projects that don't use *any* dynamic allocations, everything preprocessed blobs and dynamic stuff (particles etc) in predefined buffers that can never overlap, and that are throttled. (if you want consistent framerate, you need consistent load, which means predictable sizes &amp; counts)
1st priority is that we need performance, so we don't really have a choice, we need something like C++. And its' cursed with headers (and other syntactic problems). thats why Blow is making a new language. Rust is an improvement in other directions, but doesn't address malleability. Its' not just compiling quickly, its the level of fluidity in the code. In rust's module system+traits I have almost as many problems as with headers if I want to move something (micromanagement of dependancies &amp; 'which hierarchical unit is this function in'). infact adhoc overloading is actually more open than rust traits (what initially drew me to Rust was, splitting classes into struct/impl - its one of my wishes for C++) If you're working on something where the solution is well-known.. well, how boring. Interesting problems haven't been solved yet, and you'd be lying if you claimed you know in advance how everything should be arranged. Really header files are my main gripe; the next is the asymmetry between functions and methods.(its' worsened *by* headers,because of how classes&amp;headers interact). There is a ray of hope in that there's proposals for both (modules &amp; UFCS). I've never been frustrated with segfaults- I think I spend as much time debugging other logic anyway. I know a segfault is a hazard caused by having the control I need, or another logical error. I want to be able to write tests and visualisations of intermediate states to track things down. Header files on the other hand are just completely unecasery. Most of the real problems are outside of what the type system knows about,but I realise advancing type systems to express more is a good idea. But take array indices for example - we need unchecked array indexing, and rust doesn't offer anything new there. You can try to say a bit more by making custom index types at least, but we can do that in C++ already. Another thing on my wishlist might be called "intersection types"(I'm not sure of all the jargon) . I suspect blow is going to add these - the ability to refactor a data structure to &amp; from separate components without having to change code that uses it (basically auto delegation of component methods). But Rust doesn't like this idea, for a philosophy of big-sourcebase maintenance as its' main priority. Inheritance can fake a bit of it in a half broken way. (It might be logical to make tuples do this automatically, and you could make it an error to access a shadowed field?) (again .. I don't think C++'s problem isnt' complexity, its' features that do half of what you want, and you have to stretch them too far to get what you really wanted). anyway I do like rust more than he does. I really like the expression based syntax, and the way match/enum works.. my ideal language would definitely have those
True, however on the other side there is Carmack who would probably like Rust. He pretty much stated that C++ needs more static checks, and gave Haskell as a sort of example of static checks done right. * http://web.archive.org/web/20140713032309/http://www.altdev.co/2011/12/24/static-code-analysis/ * https://www.youtube.com/watch?v=1PhArSujR_A&amp;t=2m20s John is a high profile individual, but hardly the only one. Admittedly, Carmack also says that the benefits of strong typing are more with something that's reused over time, instead of a one time project, which makes sense given his background on Quake Engine. 
I think at the very least, Rust has a lot of promise as a language to build game libraries in. A good C API would allow user to pick and choose how to build their engine. 
Author here: sl(1) is a very traditional unix program that reminds you that you've been hasty. It "rewards" by showing a steam locomotive (sl) running from the right to the left. The traditional options, matching the most popular ones of ls: -l (logo train, also "long", because it has wagons) -a (accident, people sitting in the windows, shouting for help) -F (flying train) -c (show a C51 instead of a D51 Steam locomotive) One of the biggest outstanding bugs of sl even after more then 20 years is that if you mistype it, it shows a file listing instead. Both locomotives are japanese models, I'm not sure about the logo train. Traditional SL traps SIGINT and ignores it. This is important for keeping the (probably stressed) programmer calm. I am, for several reasons that shouldn't concern you (yes, really, I'm fine ;) ), a fan of sl, its history and the role in the software world. As such, it pains me that a certain Linux distribution (debian) ships with non-standard patches that allow passing `-e`, allowing the program to be interrupted. Also, a certain Linux distribution (debian) ships old versions. I am pretty sure that someone will write an sl replacement as a systemd module. Quite frankly, this was written quickly for a Rust talk in Berlin yesterday, to prove the point that Rust is not only for serious programming. So, code quality is questionable in a "it works" sense. Also, curses is the most appropriately named library in the software world. For those that mistype git a lot, there is `gti`. It shows a Golf GTI going from left to right. I am very opposed to this program, because of philosophical differences: it runs git on exit, with the same command line parameters. Also, left to right. http://r-wos.org/hacks/gti
Ah, globals. I'm doing some experiments with a new library "piston-current" which seems to solve this pretty well https://github.com/pistondevelopers/current With the things I've learned about Rust the past few weeks, I'm rather optimistic about the path ahead, because the design issues seems to be solvable. :)
Wrong subredit! Here we are talking about the Rust programming language, not the game The game subreddit is r/playrust
How does this `where` syntax actually used? I've seen it various time but it is neither mentioned in the guide nor in the reference.
120 % agreed
Already saw this. Having line numbers there would be great, though.
Can you elaborate? What do you think should be ForwardIterator, and what should be Iterator?
Here's the RFC regarding `where` clauses: https://github.com/rust-lang/rfcs/blob/master/text/0135-where.md
I can confirm that callgrind/kcachegrind works very well for profiling
Definitely. I hope to get some time savings with a good IDE. Right now, I keep spending a significant amount of time browsing the API docs for how methods are named, whether something is in the prelude, in which submodule I can find something etc. Would be great to have some editor assistance in this regard ...
Great. Last time I tried that there was no coercion from &amp;[T,...N] to &amp;[T]. Apparently, we got it now. :)
Now that's a pretty intense debugging session! Thanks for taking the time to write it down, it was a very interesting read.
&gt; you can flip a switch, declare it ready, and nothing will break for ... long enough for a library ecosystem to suddenly start maturing? That's actually what the goal is, as far as I understand it. 1.0 will mean a commitment to backwards-compatibility, so programs written at release will still compile six months or a year down the road. I guarantee you nothing but the most trivial code written six months ago will compile on the most recent nightly. Stability might not mean too much in your field, but it's a highly desired feature for systems and games and web programmers with massive applications that they don't want to constantly have to be patching to keep up with changes to the compiler. The Rust team is taking this time to break everything they need to break so they can have a cemented design for release, like a doctor re-breaking a bone that didn't set right so it can be fixed. Just a couple days ago, they changed how enum variants are namespaced and it broke probably 90% of libraries. Old: enum MyEnum { Variant1, Variant2, Variant3, } fn get_enum(idx: int) -&gt; MyEnum { match idx { 1 =&gt; Variant1, 2 =&gt; Variant2, 3 =&gt; Variant3, } } New: enum MyEnum { Variant1, Variant2, Variant3, } fn get_enum(idx: int) -&gt; MyEnum { match idx { 1 =&gt; MyEnum::Variant1, 2 =&gt; MyEnum::Variant2, 3 =&gt; MyEnum::Variant3, } } This is great since you only have to import the enum to use all the variants, when previously you had to import each variant you intended to use. It needed to happen, but when it did, all the libraries that depended on the old behavior broke in a massive way. Breaking changes like this used to happen on a weekly basis. A lot of people got tired of rewriting all their Rust code and decided to just wait for release. I'm one of the crazies who decided to try and keep up, though I came in late enough that the breaking changes were more on a monthly basis than weekly. 1.0 is basically a promise to stop doing that. It's saying, "We're happy with how Rust looks and behaves now, so you can feel safe to write your massive, mission-critical applications with it because they should still compile a long ways down the road."
That is a pretty tricky problem, and I'm sure there's a better solution than mine. The first thing to do is make sure that `Value` is marked as `#[deriving(PartialOrd, Ord)]` so that it's easily sortable: fn get_values(hand: &amp;[Card]) -&gt; Box&lt;[Value]&gt; { let mut values = hand.iter().map(|&amp;(ref value, _)| *value).collect::&lt;Vec&lt;Value&gt;&gt;().into_boxed_slice(); values.sort(); // Ace is low here values } Then you need to create a way for each one to identify the next in the sequence. The only good approach I've found for this so far is to just do it manually: impl Value { fn next(&amp;self) -&gt; Value { match *self { Ace =&gt; Two, Two =&gt; Three, Three =&gt; Four, Four =&gt; Five, Five =&gt; Six, Six =&gt; Seven, Seven =&gt; Eight, Eight =&gt; Nine, Nine =&gt; Ten, Ten =&gt; Jack, Jack =&gt; Queen, Queen =&gt; King, King =&gt; Ace, } } } Lastly, these can be used to tell if the cards in a hand all form a sequence: fn is_straight(hand: &amp;[Card]) -&gt; bool { let values = get_values(hand); if *values == &amp;[Ace, Ten, Jack, Queen, King] { // since Ace is low return true; } let mut i = 0u; while i &lt; (values.len() - 1) { ensure!(values[i].next() == values[i+1]); i += 1; } true } Since sorting the values always puts Ace at the bottom, the first thing is to check for a [10, J, Q, K, A] straight; if it's not that, then we know that Ace, if included in the straight, is low. Then each card is checked against the one that comes after it and compared to what we know the next card *should* be. Some other possibilities to consider are: * Conditionally sort the values using an `ace_is_high` parameter to determine where the Ace should be placed. * Find some way to define `Value.next()` as part of the `iterable_enum!` macro.
Heh, yeah. It's a bit extreme. I was mostly imagining finding it in the core/standard libs (which are probably the largest users of `unsafe` you'll depend on).
Oh, I see! I thought `Ord` depended on implementing `next()`. Thank you so much!
`Left` and `Right` are perfectly descriptive, but you're just looking in the wrong place. The type is `Either&lt;A,B&gt;`. `Left` means `A` because it's the left type. `Right` means `B` because it's the right type. It is just as descriptive as `first` and `second` are for accessing the fields of `(A,B)`. The entire point around custom enums is that *making custom enums is not a problem*. With enums-as-namespaces and constructors-as-functions the syntactic overhead is basically identical to that of type unions. For readability, you'll probably want to make a type synonym for your type union `A|B` (especially if it grows to `A|B|C|D...`). So now you'll have `type name = A|B|C|...`. Compare this with `enum name { A(A), B(B), C(C), ... }`. Sure the `enum` has some repetition, but it's otherwise identical (and repetition can easily be fixed with a macro). Creating values of the union type will almost certainly require a cast, so you'll have `x as A|B` versus `A(x)` or `B(x)`. The custom enum version does require you to know which variant to use, but you'll always know which that is. Matching requires specifying the type, which is effectively identical to specifying the constructor. match x { a: A =&gt; b: B =&gt; } versus match x { A(a) =&gt; B(b) =&gt; } That was the point of my post. All of the benefits of anonymous union types can be obtained through better, more general improvements that don't open up all kinds of tricky questions (e.g. anon unions can't have repeated types, but this means they either cannot be generic or that generics can circumvent that ban, so what do we do?). There is no reason to add anonymous unions because they do not add anything to the language.
what they add is creating multiple type units on the same level, not one (or more) top level/primary enums, and secondary ones wrapping the primary level’s enums. fully namespaced enums are very useful for ony one level of enums (those primary ones), but not for overlapping unions. type foo = A|B; type bar = A|C; type baz = B|C; has no hierarchy in which members of the unions are namespaced, and exactly that is useful in some cases.
Does anyone know which PR this got broken/changed in? Usually I can find these things, but now I couldn't.
Few notes on the tooling from the Visual Rust perspective. Debugging with Visual Studio currently kinda works (with cv2pdb, no Visual Rust needed), though it is [not very pretty](http://i.imgur.com/LDu0piv.png). If someone wants to improve the state of debugging of Rust on Windows I'd suggest adding a rust mode to cv2pdb (or working on integration with Visual Rust). One big problem is the lack of error recovery in the parser. In case of an error libsyntax will just panic!() and that's it. This kills auto-completion/tooltips/navigation/live-linting. On a side note it's mildly annoying to fix a list of errors, just to be reward with another one. Though to be fair rustc often will catch errors of the same class and present them together. There's racer, but it builds completion database from correct code. During development, 99% of the time code doesn't compile.
I'm just working through the guide and wrote a simple filter function to try out closures when I ran into this change after updating. To be honest I prefer the explicit approach as I didn't appreciate the implicit conversion before
Nevermind, I guess I was still sleeping. I had the C++ iterator in mind at the time.
&gt; In this document I want to layout what Rust tools exist and where to find them, highlight opportunities for tool developement in the short and long term, and start a discussion about where to focus our time and energy to have maximum impact. Any thoughts on where to focus to have maximum impact? What is most important? Where is the momentum? 
&gt;C-Reduce is a tool that takes a large C or C++ program that has a property of interest (such as triggering a compiler bug) and automatically produces a much smaller C/C++ program that has the same property. It is intended for use by people who discover and report bugs in compilers and other tools that process C/C++ code. C-Reduce is relased under a BSD-like license. I had never heard of C-Reduce but it is certainly very interesting. I only skimmed the rest of your post quickly since I have not yet gotten around to writing anything in Rust myself and therefore find the syntax of Rust cryptic. No fault of yours, just wanting to explain why I cannot give feedback on the blog post itself.
OOM is hard to handle. Let's say you want to return a Result... oops, you don't have any memory! IIRC, Python pre-allocates an instance of an exception object for this case, so it's already allocated. ;) The following should be prefaced with a _huge_ "this is my understanding, I could be wrong", but: On modern systems, handling OOM just isn't going to happen in your application. Virtual memory means that your call to `malloc` is still going to be succeeding even once the physical memory is exhausted. The OOM killer (at least on Linux) is going to kill you far before you'll see a `null` come back fom `malloc`.
You should check out https://air.mozilla.org/rust-meetup-may-2014/ , it's super good.
This is tricky, and I don't know rust internals, but lets consider situation where you are creating a task that tries to allocate large amount of memory. Such task could fail and provide some info about why it failed (I see that panic macro takes a parameter, so I assume something can look at it and provide info for user about the reason of failure). However the act of creating a task itself may need to allocate memory. Same for displaying any message for user. So the problem with an attempt to handle allocation error in any way is that it can happen in any place, and comes to "what to do when doing almost anything requires memory". Typically all you can try is to have some low-level code that has preallocated memory, and will try to log useful message and then kill the app. Doing anything else may fail, and dying is clear signal to user or something that is monitoring the application.
https://github.com/rust-lang/rust/commit/85914df05a95b1d14e483b69d9c4f0eba44b5d25 `grep`ping the `git log` for `[breaking-change]` can help a lot here.
&gt; Virtual memory means that your call to malloc is still going to be succeeding even once the physical memory is exhausted. On a 32-bit bit system, running out of virtual memory should be quite doable.
You can turn overcommit off on linux. However its also not uncommon to have a supervisor that will watch for memory usage and kill the app above defined treshold.
&gt; And just like with bounds-checking, there's no way for the compiler to know it (those indices could be determined by user input!). So in some sense the compiler is right to prevent this But `v.get_mut(2)` clearly has a constant index, not a variable. Couldn't a more sophisticated compiler allow this behavior when the indices are (compile time) constants?
Languages like Haskell receive multiple new features per release. I don't see why Rust would be any different. One important decision they made was to allow the gating of experimental features which will allow new features to ship without breaking the language. Haskell has added many features in this style and I think its worked tremendously well for them. 
Yea, similar to how functions can accept generic arguments: monomorphization. The compiler would likely be able to figure out the return type, and create concrete versions of the functions.
In plenty of languages things in the "stdlib" are broken, and they are there for new and experienced developers to trip up on. I can easily think of things in Ruby, Python, C++, Scala, and Haskell that are only half-baked. The community approach does introduce the problems you mentioned, but I don't know if globbing a bunch of things into standard library is a better approach. For example the Scala community ended up slicing out tons of the code they had baked into the standard library. One of the big limitations they ran into is updates. Community libraries can be updated at any moment making it much easier to provide bug fixes, security updates. While if they are included with language the only time people receive updates are with a new version of the compiler. In an ideal world this wouldn't be a problem, but saying "just upgrade your compiler" isn't a viable strategy when you are shipping a large software product with many dependencies. 
`Result`s are usually stack-allocated.
Out of memory is still out of memory. And you'd need to persist it upward, meaning it can't be stack allocated, no?
I recently started up a crate for us to fool around with more exotic collections and ideas. Submit a PR, I'll probably accept it! https://github.com/Gankro/collect-rs As for new-typing the Iterator, that's something that we haven't really settled on the conventions for. Some new-type, some just make an alias. Most of our sets just make a type alias for the parent Map's iterator. That way at least you can change it out later.
But you said the compiler is preventing us from doing this. I'm confused. Is the `get_mut` allowed or disallowed in your example?
I think that the tone of "it's hard so we can't (or shouldn't) do it" is a bit short-sighted. Maybe requesting the very last byte of memory is pushing it to the point where it's impractical to handle it, but I would expect at some point to be able to * check for system memory usage (I understand we don't run in isolation, but it's a start) * check for process memory usage * check if a large allocation could be made (or maybe reserve a chunk of memory optionally) * have data structures that are automatically freed when memory pressure happens (for caches etc). I confess I don't know how this could be made safely, though I do have some rough ideas. Having said that, I don't think any of it needs compiler-level support, so those libraries can come at any time in the future.
Oh derp. I skimmed my own quote! Only saw the bounds-checking part and assumed that's what it was about. :S My problem with the "allow the constants" argument is that it's kind of magic, and not clear when it will fail. What if I move the last index to a different branch? A different function? A different module? How far will the compiler's analysis flow? We generally like the borrow-checker's analysis to be grokable like that. We don't want programs to randomly compile or not compile because of compiler implementation details. Also, this would require the safety analysis part of the compiler to have *any* idea what array indexing is, and how it works. I'd rather not have that. Perfectly fine to have the optimizer know and care, though.
[Done](https://github.com/Gankro/collect-rs/pull/1)
Not if it's a non-boxed return type, right? Because it gets copied into the current stack frame from the one that's returning it. A plain `Result` never touches the heap. You can still work with the stack even if you're OOM on the heap, unless the OS's memory management is really naiively designed. I'm not an expert in the low-level stuff, but that's how I see it.
'Persist' meaning return, if it was tied to the stack frame, it would get deallocated... But also, to be honest, when I thought of the Python example, it's going to be heap allocated because everything is. In Rust, it would be different. Wires crossed :)
Yup, Just some simple confusion on my part. Totally.
Maybe answering my own confusion, I tried a mutable version, and that indeed fails: http://is.gd/LwardS So I guess the `unsafe` is only necessary for `MutItems`, because you basically have to convince the compiler that none of the mutable references from `next()` will alias. Edit: here's a mutable version with a minimal amount of `unsafe` just to `transmute` the resulting lifetime: http://is.gd/R5X4a4 (also fixed my bogus `as_slice` calls to actually be `as_mut_slice`)
What I got from this: impl Writer for Vec&lt;u8&gt; impl Reader for &amp;[u8] Now we need this: impl Buffer for Ringbuf&lt;u8&gt; impl Stream for Ringbuf&lt;u8&gt; A *true* in-memory buffer, readable from one end and writeable from the other, would be an awesome thing to have. It would be trivial to implement on top of `RingBuf` if there was a way to push or pop multiple elements at once, preferably without allocation. Maybe something like: RingBuf&lt;T&gt;::push_all(&amp;mut self, &amp;[T]); RingBuf&lt;T&gt;::pop_fill(&amp;mut self, &amp;mut[T]) -&gt; uint; RingBuf&lt;T&gt;::pop_multi(&amp;mut self, count: uint) -&gt; Vec&lt;T&gt;; I think these methods would be *insanely* useful on their own as well. A PR is brewing in my head.
I am *very* interested in what the #[no-std] crowd thinks about OOM handling. We've been discussing it on IRC and in the [allocators RFC](https://github.com/rust-lang/rfcs/pull/244). Some of the core-devs seem pretty happy with the std lib just panicking (or aborting) on OOM, since all the Result juggling would be pretty nasty otherwise. I've even proposed just adding an allocator wrapper to do this automatically, since it's tedious to check (currently our allocator API just returns null on OOM). I think that's a pretty decent solution for most, but do what do all you kernel/embedded hackers expect to do?
In my experience, you usually get OOMs while trying to allocate large-ish arrays. At that point there's still plenty of memory for allocation of exception objects. This, btw, is a scenario currently not catered to by the Rust allocator API. I think we need one that returns Option&lt;T&gt;.
It'd be better if the application listened to the OS' Low Memory notifications, instead of waiting for an OOM. Then the application can take application-specific actions to reduce memory use before the system runs out of memory, such as dropping caches, reducing number of concurrent worker tasks, etc. Some Linux-specific proposals: * [Enhanced_Out-Of-Memory_handling](http://elinux.org/Memory_Management#Enhanced_Out-Of-Memory_handling) * [Taming the OOM killer](http://lwn.net/Articles/317814/) 
&gt; On modern systems, handling OOM just isn't going to happen in your application. Unless you're using cgroups (http://lwn.net/Articles/432224/) or other such limits.
While out-of-bounds checking in general is very hard, we could avoid bounds-checking if we allowed types such as `u10`, because if the array size is statically known to be 1024 (i.e. 2^10), then any access to it with an index of such a type is obviously safe. 
Then you traded the cost of bounds checking for the cost of using weird number types. Not clear that there's a big win there?
maybe i wouldn't mind compulsory trait bounds so much if an IDE was navigating for me; ... I guess then I'd start seeing them as posative as they'd allow accurate autocomplete in generic code.
You would be surprised at how many arrays are powers of two. It's obviously not super-critical, but it is a worthwhile optimization. For this to be quite useful, we need integer constants as types in the type system though (like C++: `template&lt;size_t sz&gt; ...`). 
Interestingly, earlier no coercion from &amp;[T,...N] to &amp;[T] as mentioned above by sellibitze above. Now there is. So, here we are doing things implicitly right? 
Maybe you or someone else can help me understand, but I thought everything was demand-paged. i.e. when you allocated memory via mmap or sbrk the OS filled in your page table but the pages themselves weren't made present until the CPU actually tried to access them. So if that understanding is true, then you should be able to allocate large arrays but it's only when you start touching the data that a page shortage will trigger OOM. Can anyone clarify, maybe just for Linux's behavior?
No I mean like... `u10` isn't a thing in CPUs as far as I know. If you add two `u10`'s, and it "overflows" the 10 bits, what do you expect to happen? And then what happens when you try to index with such a number?
Games just don't hit oom. You make sure you stay in your budget and if you go over you warn. If you go over more you crash.
Windows is more honest here - if it can't allocate it reports immediately. At least it was like that when I tried the last time, but maybe I tried with 32bit process. (There are probably a lot of nuances, I'm not a specialist in this matter)
The impl of Clone for `Vec&lt;T&gt;` is for `T: Clone`, so if `T` is not Clone you will get the impl of Clone for `&amp;[T]`, which is the impl of Clone for `&amp;T`, which is just a copy of the reference, meaning you get an `&amp;[T]` out.
You are right, and D also compiles very fast, even with CTFE. AFAIK one could spawn a couple of VM processes at the beginning of the compilation and reuse them to minimize the cost. Still, as CTFE gets more complex, compilation slows down, both because of CTFE being slower, and because of users doing more things at compile-time.
Oh, so if i implement Clone for T, it should work? But wait, in my example it is a Vec&lt;Entity&gt; where Entity is a trait, i cant implement a trait on a trait can i? Sorry if i sound like a newb right now
I would expect the same behavior as `u16` and `u32` (wrap-around). In terms of implementation, a `u10` would map to a `u16` as carrying type, but any read of the value would automatically get `u10_val &amp; 0x3ff` applied to it.
`Some` is just an enum of the form: enum Option&lt;T&gt; { Some(T), None } Could you provide some context to the code? Chances are you're moving `sth` out of a struct or something, or moving it out of `Some` in a pattern match (`Some(ref sth)` should fix it) You can find a draft of the ownership guide [here](https://github.com/rust-lang/rust/blob/941ca299490a1ff78ba3f1d388f426ca9415a746/src/doc/guide-ownership.md) which might explain the concepts a bit better.
You can't have a `Vec&lt;Trait&gt;` (I'm fairly sure). You can however have a `Vec&lt;Box&lt;Trait&gt;&gt;`, in which case implementing `Clone` for `T` will be sufficient, as `Box&lt;T&gt;` implements `Clone` for all `T` that implement clone. See: http://doc.rust-lang.org/std/boxed/struct.Box.html
well technically its a Vec&lt;&amp;Trait&gt;
And this is one of consequences of implementing `Deref` on `Vec`. What prevented implementing slice traits on `Vec`? Only code duplication inside of the standard library implementation, which shouldn't influence design of library's external interfaces, despite of the wishes of implementers. But no, we need more autodereference for the god of autodereference just to spare some modest amount of copypaste and delegating code.
SO, ive nailed the problem down to Vec&lt;T&gt; is only Clone-able if T is Clone-able. in my case im use a Vec&lt;&amp;mut T&gt; where T is a trait (therefore has to be a reference) so i cant implement Clone on T. I though i could do Vec&lt;&amp;mut T + Clone&gt; (bounds) but when i try that, i get a new error message: only builtin traits can be used as closure or object bounds So, unless there is something im missing, or something i can do. I think Cloning is not going to work for me, and i have to figure out a different way :/
&gt;i cant implement a trait on a trait can i? Actually you can :) http://is.gd/dZBt1X Trait here is used as an unsized type. But you still can't keep naked traits in `Vec`, as gnusouth said.
I'd say, for kernel/embedded stuff, giving back an Result&lt;T, OOMError&gt; / Option&lt;T&gt; is probably the best: * If the client wants to panic on OOM, that's as easy as calling unwrap(). * If the client wants to return the OOM error up one level, that's as easy as try!(). Bonus points if either: * the OOMError can optimise itself down to a null pointer, just like None can, or * we can have a try!() equivalent for option, or make try!() magically work on both Results and Options. In the Linux kernel, you see much of the latter type of error handling: e g, suppose a driver when it loads, it needs to allocate memory in 100 different places, all of these contain code that propagate the error down to the "driver loader" component - which can just tell userspace that the driver loading failed, while leaving the kernel in the functional state it was in before the loading started.
I agree, when `Deref` was introduced it was intended for smart pointers, but suddenly it's on `Vec` and `String`.
You have to work with a generic function, like in fn foo&lt;T: Clone&gt;() { let v: Vec&lt;T&gt; = Vec::new(); let m = v.clone(); }
Hmm, no, I don't think touching a data page can cause an OOM on Linux. (It can cause swapping, but that's a different story.) Linux should make sure that whatever variable data it might need to keep does not exceed physical memory + swap, and do so at allocation time.
If exceptions/unwinding are acceptable in the environment, OOM error should just result in a crash, there's no point in dealing with it. In environments where unwinding is wasteful (nearly everywhere except huge applications) a low level std library api that returns options and a higher level wrapper that discards them would be a possible solution. If the higher level wrapper discards the error the process crashes, if you want to be able to recover from OOM then you need to use the low level API and deal with every allocation being wrapped in an option. 
The mechanism the rust team have decided to go with is to panic on OOM, in rust panics are essentially C++ exceptions. When oom happens the call panics and unwinds up to the task boundary freeing things as it goes. The standard library and runtime don't cater to use cases where you don't want unwinding, you have to get rid of them with #[no-std]. There are a quite a few people in the community who aren't happy with the unwinding requirement, so you've stumbled on a bit of a hornets nest here. This issue hasn't been addressed very well by the rust core team. As you suggest, to deal with oom you have to discard the runtime and standard libraries and reimplement them all returning options everywhere an allocation happens. 
`Vec` and `String` are essentially smart pointers around `[T]` and `str`, just like `Box&lt;[T]&gt;` and `Box&lt;str&gt;` with one extra word of metadata (the capacity).
A related question: what other error conditions are there which are "similar to" OOM, in that they can occur "just about anywhere", and/or are catastrophic, and/or dealing with them and propagating them correctly (as opposed to just aborting) would be really difficult and/or annoying? The only other similar condition I can think of is stack overflow. I only ask because if there are other similar things, then we'd want some kind of general solution for them, while if OOM and stack overflow are the only errors in this category which can *conceivably exist*, and we are fairly sure that there aren't and won't be any others, then even adding special-case features to the language to handle them wouldn't necessarily be such a crazy idea. (Before you ask, I don't have anything in particular in mind.) I don't think that e.g. dividing by 0 qualifies, because, while it can theoretically "happen anywhere", unlike OOM and stack overflow it can (and should be) reasoned about and protected against locally, and it can only ever happen in practice if there's a logic error in the program. (As an aside, one other which would be fairly similar is Haskell's asynchronous exceptions - they can occur "almost anywhere", and they're actually quite similar to OOM in that they're also delivered during memory allocations. Which works reasonably well in an allocation-happy language like Haskell, but is not really applicable to Rust.)
But I want my Vec of Entities to work with many different implementations of Entity at the same time. SO it could contain a Cow, Zombie, Unicorn and Dragon (Different Structs all implementing Entity), afaik your method would restrict it to one implementation, ie it can contain only Cows or Only Zombies
Woah, i didnt know that. So, what exactly does the line `trait MyTrait1 for Sized` mean?
&gt; Alternatively, you could have T inherit Clone. wait, inherit? rust has inheritance? also, arent Box pointer immutable? I need it to be mutable because the .update() method take a mutable borrow of self (obviously because it has to update it self) and, `Vec&lt;Box&lt;T + Clone&gt;&gt;` doesnt work in a struct, as i said i get the message "Only the builtin traits can be used as object bounds"
I think that code compiles because `struct A;` is unit struct (is that the name) and so the compiler infers that it can be Cloned
Not an expert, but I've worked on C++ applications where we attempt to allocate huge vectors in advance, and report any problems sensibly to the user. In these cases we could be virtually certain there was enough space to carry on running the app, just not enough space to perform a particular very expensive operation. Any other OOM issues apart from these were just crashes. The ability to either just panic or return a Result seems quite useful for this.
&gt; * check for system memory usage Wont tell you anything about what you app is allowed to allocate &gt;* check for process memory usage Won't work. There several places where such limit can be defined (setrlimit, cgroups, dozens of others) and no clear way of obtaining them, or figuring out which ones are in use. &gt; * check if a large allocation could be made That's the only doable thing, though its limited for this special case, its not a generic solution. As said, memory allocation is spread all over the code, not just places controlled by app developer. &gt; * have data structures that are automatically freed when memory pressure happens Doable perhaps, but adds a lot of complexity with no clear benefit. 
I Fixed It, im so proud of myself. Thanks, all you guys, for your help! Its an amazing feeling when you finally get your code past the compiler! If you want to know how i fixed it then here: 1. Change block to a struct from trait(because there were no actual methods defined, only getters) this allowed me to easily implement Clone on it so that part works 2. Change from `Vec&lt;&amp;mut Entity&gt;` to `Vec&lt;Box&lt;Entity&gt;&gt;` much more sensible and easier to deal with 3. Added a get_copy() method on Entity (basically the same as the clone() method in Clone) then implement Clone on Box&lt;Entity&gt;, forwarding any call to clone() to the get_copy() method 4. this let me easily implement Clone on GameWorld, but then i got an error but simultaneous borrows when i did this: `self.world.player.react(self.world.clone()...);` 5. to fix this i added another struct called GameWorldCopy purely for passing copies of GameWorld to Entity.update() method calls, with a constructor which builds from a GameWorld. 6. In the constructor i took it by move, because borrowing would cause simultaneous borrows, but i simply cloned the GameWorld that i wanted to construct from into the method, so it technically takes by copy 7. Now, the GameWorldCopy is instantiated so I can pass it around as immutable references as much as i want. No multiple borrows, lifetime problems or anything, VICTORY! This is probably a very ugly workaround but it works, and its safe so YAY! Any suggestions for clean up or better ways of doing it would be very welcome
I have tried it with a nontrivial struct in another program that I've been writing and it worked fine. ~~Also, I appear to be wrong about how trait inheritance works, so ignore what I said about that -- it exists, but I seem to be able to have trait `T` inherit trait `S` and create a struct `A` that implements `T` but not `S` so I guess I don't understand what the inheritance does.~~ Edit: Ignore that, what I said initially about inheritance is correct.
Well thanks anyway, i managed to fix it, using a whole bunch of workaround, i have a comment detailing it if you want to see
`for Sized?` part means that trait `MyTrait1` can be implemented for unsized types, you have to explicitly opt-in for it. Unfortunately, it isn't reflected in any documentation and has to be learned from the sources. Unsized types are types like `str`, `[T]` or trait object types, for which size is not known at compile time.
Doesn't try! spawn a thread? Seems inefficient for this scenario, unless you can associate it to a task pool,idk
You are thinking of the `task::try` function. The `try!(expr)` macro does something different, it exands to match expr { Ok(v) =&gt; v, Err(e) =&gt; return Err(e) } That is, it unwraps or returns the error from the function.
&gt;That's the only doable thing, though its limited for this special case, its not a generic solution. As said, memory allocation is spread all over the code, not just places controlled by app developer. That's a very important thing for kernels etc. which should be able to be written in Rust. &gt;Doable perhaps, but adds a lot of complexity with no clear benefit. "No clear benefit" is downplaying. It has a benefit.
You just copy it upward like any Rust library (that I know of) does. `fn foobar() -&gt; Result&lt;(),()&gt;` does not in itself result in a heap allocation.
I would be nice to compare performance of such a wrap-around with bounds checking. Besides, this wrap-around turns out-of-bound access from panic to silent wrong behaviour, which is probably not a good thing to do.
Your problem is the replace. It allocates a new String that does not live long enough, which is also what the compiler is telling you. Consider just cutting http:// out by slicing.
No it just restricts it to cloneable things. This is what generics are good for. You may need to add more bounds depending on what you want to do. Edit: in case you stumbled over T. Well just use &amp;T instead. 
 fn push_back_node(&amp;mut self, mut new_tail: Box&lt;Node&lt;T&gt;&gt;) { match self.tail.resolve() { None =&gt; { self.tail = RawLink::some(&amp;mut *new_tail); // #1 self.head = Some(new_tail); // if switch this line with #1, i will get the error } Some(tail) =&gt; { tail.next = link_pre(new_tail, RawLink::some(tail)); } } self.length += 1; }
How or where is `resolve()` defined? And on what line exactly do you get the error?
 &gt; "No clear benefit" is downplaying. It has a benefit. I don't see any. Memory allocation may happen anywhere, including some low-level critical places that should not touch anything else and deal with complex foreign datastructures, so you are trying to introduce something VERY complex, that will be used in very few cases, and you can't rely on it, so you still need to design everything same way as if this mechanism wasn't there. The memory you'll be trying to release is referenced, possibly read and written by several threads, so you'd need to track that somehow, ensure its locked while released and everything that may use it will know its not available anymore, so the complexity cost is huge. What should happen for example when some code is iterating over items in a cache that suddenly is gone? How to ensure its safely handled? What to do when there several threads are just about to read the memory you suddenly want to release or reuse for different purpose?
Yes, the two key insights here are: 1. A `&amp;str`is a pointer to memory that's owned by somebody else, and if that underlying memory goes away, Rust won't let you keep the `&amp;str`. 2. A variable of type `String` only lives as long as whatever contains it. If you have a local variable of type `String`, it goes away when the function ends. If you have a `String` in a `struct`, it goes away when the `struct` does. And so on. So here, `urlstring3` is ultimately pointing at memory owned by `urlstring`, which goes away at the end of the function. As diwic suggests, your two options are either return new `String`s that have their own memory, or to convert `urlsring` to a `&amp;str` somehow, thus allowing everybody to point into whatever memory ultimately underlies `url`. I hope this helps!
Thanks for the detailed answer, that seemed to work. Before, I have never really had to care about these problems, as I was working with Python, C# and Java before, so all of this is kind of new for me. So as far as I understood, I'm actually passing the reference around and not the contents with &amp;str? And with a String I'm creating a new reference with a new lifetime?
Based on the other answers, it could be as simple as fn split_url(url: &amp;str) -&gt; Vec&lt;&amp;str&gt; { url[7..].split_str("/").collect() }
Maybe late, but I just wanted to chime in and say: A `&amp;mut T` is never clonable, as that would mean you could duplicated a `&amp;mut T`, which would be unsafe: You are only allowed to have one `&amp;mut T` to the same memory at a time.
So once `new_tail` is moved into `self.head`, you can't use it anymore. However the above should still give an error about using a mutably borrowed value.
That's true. But an important difference is that most of those are required for the program to interact with the outside world in particular ways. Stack and (to a slightly lesser extent) heap are required for the program to be able to *compute*, for its own execution, and are much more existential matters. Not every program uses the disk or network. All of them use the stack, and almost all of them the heap. (Likewise individual functions, which are just smaller programs.) EDIT: Another perspective is that "pure" (in the Haskell sense) functions use stack and heap, but not the other things. &gt; the general case is usually solved by deleting-and-letting-root-sort-them-out Could you elaborate?
It's something the JVM can also handle, you can handle it similarily in Rust. Add a callback for freeing memory during memory pressure or so. 
Of course, the difference is in the *need* – if my program wants to beep and has no audio, it may well be able to cope. If my program needs memory to store the results of a specific calculation, it *may* be able to do without with worse performance, but usually most programs will just balk and exit (barring extreme cases, e.g. programs running bomb disposal machinery or mars rovers).
I hope they do it even sooner. I for one want to see some bits stablilise that probably won't do so in time for 1.0. Macros and compiler plugins in particular are key areas I want stable asap.
I dont want to mutable pointers to the same memory, I want 2 separate mutable pointers to 2 separate locations, that are the same at the time of creation
Oh, i was under the wrong impression it seems, thanks :)
&gt; The impl of Clone for Vec&lt;T&gt; is for T: Clone, so if T is not Clone you will get the impl of Clone for &amp;[T], which is the impl of Clone for &amp;T, which is just a copy of the reference, meaning you get an &amp;[T] out. This is because of `Deref` right? So why doesn't this compile? use std::ops::Deref; struct Foo; struct Smaht&lt;T&gt;(T); impl&lt;T&gt; Deref&lt;T&gt; for Smaht&lt;T&gt; { fn deref(&amp;self) -&gt; &amp;T { let Smaht(ref t) = *self; t } } fn main() { let vec = vec![Foo]; let x: &amp;_ = vec.clone(); // works, _ is [T], goes through Deref&lt;[T]&gt; on Vec&lt;T&gt;, and Clone impl on &amp;T let p = Smaht(Foo); let x: &amp;_ = p.clone(); // doesn't work, type `Smaht&lt;Foo&gt;` does not implement any method in scope named `clone` }
Embedded systems typically have all of their memory pre-allocated, so they don't *need* to exit. Also, programs need to be prepared for disk/network/whatever disconnection for other reasons.
True that (although some systems counting as embedded these days may well use a heap).
This has created a lot more error :( All says undefined reference
that's wrong, because if the URL doesn't start with `http://` you're hacking off the first 7 characters anyway
Nope, it's explicit. The second `[]` converts to a slice.
Wow thanks! When I read the guide, it only had a very short into about String and &amp;str. This will come in handy.
Awesome. Please open issues against anything that could be improved there. :)
This is better, but kind of hackish. What happens when URL is something like `//ajax.googleapis.com/`? I also don't like that you have to manually count 8 or 7 characters in the two cases. If you add more cases you could accidentally miscount. Maybe there should be an array of common URL prefixes you'd like to remove like `http://`, `https://`, `ftp://`, `//`, etc. that you slice on based on the length of the string (if the prefix matches) so maybe it will even be an ordered list of prefixes you remove so `remove_prefix("https:", "ftp:", "http:", "//")`
Panic requires memory allocation and can't be caught in the current task. It's not a viable way of handling a runtime error, and it's even less appropriate for handling out-of-memory.
Yes, that's exactly correct!
&gt; Hmm, no, I don't think touching a data page can cause an OOM on Linux. (It can cause swapping, but that's a different story.) Yes it can, that's what overcommit is. &gt; Linux should make sure that whatever variable data it might need to keep does not exceed physical memory + swap, and do so at allocation time. It has the ability to do this (virtual memory accounting) but it greatly reduces scalability and isn't used by default.
Rust worries a lot about memory management (managing how data is stored in RAM when your program is run). In particular, if you need to do manual management in systems languages like C or C++ (or Rust), there's a concept of "ownership" - the piece of code that is responsible to release a certain resource, such as memory, when it's not needed anymore, because failing to do so results in a leak. Rust helps you with that, by managing the ownership of variables, preventing bugs like "use after free" (accessing a variable after you released its memory). But for a beginner programmer, it's more useful to dedicate yourself to make your computer perform an useful task. Memory management is a nice topic when you already master the "make computers do stuff" part, but not before. A language like Python is necessarily simpler because it lets the language itself manage the memory: the only piece of code in a Python program that "owns" memory is the garbage collector, which is provided by the language. It operates in the shadows, releasing memory automatically when you don't need it anymore. In this sense, the extra concepts of Rust may get in your way to learn programming.
&gt; Windows is more honest here - if it can't allocate it reports immediately. Windows isn't any more honest. The option of using struct virtual memory accounting exists on Linux, but usually isn't desired. Memory is a global resource and yet most processes cannot handle out-of-memory better than simply cleaning up and exiting. Giving out-of-memory errors to every process due to a single runaway application would not be a good default. Every Rust application would currently immediately abort with no attempt to clean up if a single process consumed the available memory. Linux has memory control groups which are the closest thing to a sane way of dealing with these issues.
The low-level allocator API returns a null pointer on out-of-memory. The problems are high up in the stack. For example, `push` on `Vec&lt;T&gt;` would need to return `Option&lt;T&gt;` or `Result&lt;T, ()&gt;` and callers would need to check for failure.
&gt; Maybe you or someone else can help me understand, but I thought everything was demand-paged. i.e. when you allocated memory via mmap or sbrk the OS filled in your page table but the pages themselves weren't made present until the CPU actually tried to access them. That's how things work on all major operating systems, but it doesn't imply the ability to overcommit memory. &gt; So if that understanding is true, then you should be able to allocate large arrays but it's only when you start touching the data that a page shortage will trigger OOM. Nope, it doesn't imply that. It does enable the possibility of having it work that way. &gt; Can anyone clarify, maybe just for Linux's behavior? From the kernel documentation: overcommit_memory: This value contains a flag that enables memory overcommitment. When this flag is 0, the kernel attempts to estimate the amount of free memory left when userspace requests more memory. When this flag is 1, the kernel pretends there is always enough memory until it actually runs out. When this flag is 2, the kernel uses a "never overcommit" policy that attempts to prevent any overcommit of memory. Note that user_reserve_kbytes affects this policy. This feature can be very useful because there are a lot of programs that malloc() huge amounts of memory "just-in-case" and don't use much of it. The default value is 0. See Documentation/vm/overcommit-accounting and security/commoncap.c::cap_vm_enough_memory() for more information. ============================================================== overcommit_ratio: When overcommit_memory is set to 2, the committed address space is not permitted to exceed swap plus this percentage of physical RAM. See above. Note that it's possible to create memory control groups managing groups of processes with a memory limit and nested out-of-memory handling. See the [memory control group](https://www.kernel.org/doc/Documentation/cgroups/memory.txt) documentation for the state of the art, which is currently in flux.
&gt; The OOM killer (at least on Linux) is going to kill you far before you'll see a null come back fom malloc. Linux has optional support for virtual memory accounting. The default is neither full overcommit or accounted virtual memory. It's overcommit with heuristics to catch some pathological cases.
Neat!
You can convert a `&amp;mut [T]` to a `&amp;[T]` using `as_slice()`. [Example](http://is.gd/Vok9jF)
There were other ways of getting rid of the code duplication.
Thank you so much. Although I was taking to heart all the advice in this thread I still was lost at what made Rust so different and revered. Your post cleared that up, definitely will be jumping into Python and the ilk before coming back, Thanks again!
Python is nice, but for some reason I think Javascript is very charming. You already have a full featured Javascript development environment - your browser. If you press ctrl+shift+i on Chrome and click in the "console" tab you are presented with a Javascript prompt running on the current page - for example, here on reddit the code `$('#header-bottom-left')[0].innerHTML = 1 + 1` sets the header (with the links to the subreddit, etc) to `2`. I think that's so so cool. (the `#id` of the header or something else in the page is found by clicking on the search button on the left, and then on the element - but not everything has an id) But Python is great too. And also Ruby. edit: setting the header to the sequence of 8th, 9th, 10th, 11th Fibonacci numbers: `$('#header-bottom-left')[0].innerHTML = [8, 9, 10, 11].map(function fib(n) { if (n &lt;= 1) return n; else return fib(n-1) + fib(n-2) })`
It's not wrong, and it's actually wanted, but nobody has officially approved or started work on implementing coercions from traits to one of their supertraits, so it's likely to not end up in 1.0.
Thanks. That worked and allowed me to avoid that additional indirection. Interestingly, if I don't end `state`'s scope before returning, compiler complains that it's still borrowed: pub fn encrypt(data: &amp;[u8], key: &amp;[u8], iv: &amp;[u8]) -&gt; Vec&lt;u8&gt; { let mut res = Vec::from_elem(data.len(), 0); { let mut state = iv; let crypt = AesSafe128Encryptor::new(key); for (src, dst) in data.chunks(16).zip(res.chunks_mut(16)) { crypt.encrypt_block(slice_xor(src, state)[], dst); state = dst[]; } } res } This works, but without the inner block I get ``error: cannot move out of `res` because it is borrowed``. Why `state` moving out of scope at the same time doesn't satisfy the compiler?
No, this is procedural macros generating other procedural macros.
It is a function that is a thin wrapper over a `match` statement: https://github.com/rust-lang/rust/blob/09e2ad13d0aa01143bcb20dece3ff6c5a7e34ea3/src/libcore/option.rs#L329-L359 Semantically, it means "While this may or may not have a value, I'm saying it does. If it doesn't, just crash, I don't want to handle the error." `unwrap` is the most primitive way of handling this kind of thing. `expect` at least lets you give your own error message.
Because it is implemented like this. Eventually this will be changed at some point (see [this issue](https://github.com/rust-lang/rust/issues/6393)).
Thank you for writing this up! It's going to sad when the macros in all my dependencies like [rust-peg](https://github.com/kevinmehall/rust-peg/) and [docopt.rs](https://github.com/docopt/docopt.rs) are broken by the deactivation of compiler plugins. These libraries are absolutely delightful to use in their current forms. The build.rs infrastructure could presumably handle some of this, though in a less convenient fashion. Still, there's a lot of great tooling using the plugin infrastructure right now, and I'm going to miss it. On the other hand, it will be nice to stop seeing my morning pile of Travis CI emails informing me that a dependency broke. So there's that. :-)
To expand, you really, really shouldn't call `unwrap()` unless you've verified that the option `is_ok()` or you really can't do anything but crash the application to the ground.
(to be clear, brson wrote this, I just submitted it) The issue with stabilzing compiler plugins is that they would require stabilizing the compiler internals, and we're far, far away from that. But! In the shorter term, there will be a way to provide similar features: build scripts, as you mention. http://doc.crates.io/build-script.html#case-study:-code-generation is one of the case studies, with compile-time code generation. It does require re-writing these kinds of things, but it gets stability until we can provide real plugins.
Or you're writing scrap code, and you don't care if it crashes or not. (Although then, it will inevitably end up crashing. )
May one should rename it to `assert` after all. I was skeptical towards this proposal it might make people more careful to use it.
Posting a rust game server in /r/rust instead of /r/playrust. &gt; Just bot things. 
This is the case for `u16` and `u32` right now. 0xffffu16 + 1 == 0 See: http://is.gd/GZG8nI 
oh sorry didnt see the rust section sorry man if u play it then come join :D
Eh, it's pretty common programming practice to crash when you detect a programmer error. Since they're using OpenGL and SDL, it's likely they're doing game dev, and in games it's even more common for crashing to be the most reasonable thing to do, since you also want to detect errors made by non-programmer developers. Often it's much more reasonable to crash early and let the developers know that their resource is broken than to obfuscate the problem by trying to make the engine resilient against broken resources.
... I think so too. Now I'm just confused. &gt; earlier no coerction There was?
I'm also in favor of `assert`, even though I thought it was weird at first.
It's kinda like dereferencing a possibly NULL pointer without checking in C or `fromJust` in Haskell.
In context, it is not as dismissive of the general idea: &gt; &gt; - have data structures that are automatically freed when memory pressure happens &gt; Doable perhaps, but adds a lot of complexity with no clear benefit. 
awesome! one feature that should be there from day 1 is a link to the package's home page... unless it's supported already and projects have some manifest updating to do. as it is, i have no idea what most of those packages are for (random clicks revealed no/super concise descriptions, too).
http://doc.crates.io/crates-io.html (the heading is a bit odd right now, I agree.)
Neat. It's strange that "homepage", "documentation", and "repository" links are hidden all the way at the bottom of a crate's description page. I would consider those among the most important parts of the page, as reading the documentation and looking at the code are obviously crucial things to do when using libraries.
Nice! I'm looking forward to seeing how this develops. UI knit: The Homepage/Documentation/Repository links should be more visible on crate pages (they should at least be above the "Downloads over the last 90 days" section which I don't find very important). Also, are there any plans to include a larger description (like a Readme)?
Totally. Lots of room for improvement here.
There is support for that, authors just need to include the appropriate metadata in their `Cargo.toml` files for it to appear.
You can indeed do this! See [the documentation](http://doc.crates.io/manifest.html#package-metadata) for a full list of keys you can set in your manifest.
Awesome
If there's an error when, say, creating an OpenGL context, then that should be a Result&lt;T, E&gt;, so that I can dump said E to a log file, instead of an Option&lt;T&gt;. Option should not be used to represent error conditions, but to represent what would be possibly null values in other languages.
To be clear, I did almost none of the work for this, just wrote some docs. Alex has been _killing it_ on this for the past few months.
With the restrictions on version numbering and the inherent nature of developing rust programs before rust 1.0, I could see several of my projects hitting v20.0.0 in the next few weeks just trying to keep up with compiler versions. 
Yes, you probably don't want to release a 1.0.0 until Rust itself is a 1.0.0.
I'm a noob to Rust, so correct me if I'm wrong, but doesn't Result also have an `unwrap()`?
 [dependencies.hexfloat] git = "https://github.com/rust-lang/hexfloat.git" [dependnecies] semver = "*" It works like that. You're right that it's not directly spelled out, but they use different keys.
You're completely right, sorry. I was thinking about options the whole time.
Sorry, I'm probably being thick here. I have this in my Cargo.toml [dependencies.rust-xml] git = "https://github.com/netvl/rust-xml.git" [dependencies] flate2 = "~0.0.3" And when I ``cargo publish``: `` all dependencies must come from the same source. dependency `rust-xml` comes from https://github.com/netvl/rust-xml.git instead ``
`T` needs to extend `Eq` and `std::hash::Hash` and your `T` isn't specified thus. Also `contains_key` takes a borrow (an `&amp;T`). Are you using a recent nightly of Rust? I get much more extensive and clear error messages: test.rs:9:36: 9:40 error: mismatched types: expected `&amp;T`, found `T` (expected &amp;-ptr, found type parameter) test.rs:9 if ! self.map.contains_key(node) { ^~~~ test.rs:9:14: 9:41 error: the trait `core::cmp::Eq` is not implemented for the type `T` test.rs:9 if ! self.map.contains_key(node) { ^~~~~~~~~~~~~~~~~~~~~~~~~~~ test.rs:9:14: 9:41 error: the trait `collections::hash::Hash` is not implemented for the type `T` test.rs:9 if ! self.map.contains_key(node) { ^~~~~~~~~~~~~~~~~~~~~~~~~~~ test.rs:10:13: 10:50 error: the trait `core::cmp::Eq` is not implemented for the type `T` test.rs:10 self.map.insert(node, HashSet::new()); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ test.rs:10:13: 10:50 error: the trait `collections::hash::Hash` is not implemented for the type `T` test.rs:10 self.map.insert(node, HashSet::new()); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ test.rs:10:35: 10:47 error: the trait `core::cmp::Eq` is not implemented for the type `T` test.rs:10 self.map.insert(node, HashSet::new()); ^~~~~~~~~~~~ test.rs:10:35: 10:47 error: the trait `collections::hash::Hash` is not implemented for the type `T` test.rs:10 self.map.insert(node, HashSet::new()); ^~~~~~~~~~~~ fixing these impl&lt;T&gt; Graph&lt;T&gt; where T: Eq + Hash { pub fn add_node(&amp;mut self, node: T) { if ! self.map.contains_key(&amp;node) { self.map.insert(node, HashSet::new()); } } } makes the code compile.
So, check it out: &gt; [dependnecies] And if I look in my terminal... steve@warmachine:~/src/foo$ cargo build unused manifest key: dependnecies.semver Updating git repository `https://github.com/rust-lang/hexfloat.git` Compiling hexfloat v0.0.1 (https://github.com/rust-lang/hexfloat.git#4695e33f) Compiling foo v0.0.1 (file:///home/steve/src/foo) So, uh, I thought it worked, but it did not. :( Mis-spell + silent failure for the win! But, secondarily: &gt; when I cargo publish Yes, you cannot _publish_ a package with git deps. You can build one locally though.
Im concerned about it too. I guess I will just count the last letter up: 0.2.0-alpha.x 
&gt;&gt; when I cargo publish &gt; &gt; Yes, you cannot publish a package with git deps. You can build one locally though. Ok. Thanks
it knows more about move semantics, and it has a more elegant syntax, no header files. It enforces more safety for you. C++ is great, but is cursed with headers and mis-purposed syntax - designed primarily for low level unsafe code with raw pointer arithmetic, e.g. the way the [] characters are used, ":" is for bitfields (hardly ever used) whereas in Rust it signifies types(used everywhere), helping it ditch the horrible ambiguity of &lt;template parameters&gt;. IMO The expression oriented syntax also helps writing safely.. it's easier to create everything initialised. Match with tuples and ADTs are amazingly elegant. this is my favourite aspect of it. I fundamentally like everything C++ does, its just header files ruin it ( e.g. include order can change template behaviour, and the way class definition/declaration splits key information repeated slightly differently seems like someone is deliberately trying to annoy you.. the standards committee must be genuinely vindictive to resist adding something like "impl" or other fixes) Beyond that I know what you're saying - Rust is basically restrictions which could be retrofitted with lints in C++; I do believe you could create templated reference smart pointers to annotate lifetimes, which another clang-based tool processes , something could check iterator/move invalidation ... In C++ .. I basically always want to drop back to "C with classes &amp; templates". Rust has a chance to make something close to the 'modern C++ style' more comfortable to use. (I preferred it when it had sigils for 'unique_ptr' because it made them look &amp; feel completely natural, on the same 'level' as *,&amp;, whereas 'unique_ptr&lt;&gt;' is some bolt on; and i hope they return eventually :) ) 
Not directly. You can host them somewhere else and point your `documentation` key in your `Cargo.toml` at somewhere else, and it'll put the link on the page though. Some external service will be here eventually. I think cmr owns rustdoc.org...
Also: how is this going to work with the "release train" model? For example, if someone is using a beta (or nightly) version of rust to package their code, and it depends on some of those features, couldn't that be a problem? Or is that just a big no-no? Will "beta" and "nightly" versions of rust warn about this?
Your code would fail to compile because it doesn't know what those features are.
First come first serve. I'd push up an empty project for now. We'll have a more comprehensive policy in place eventually.
Cool, thanks! I just bought a domain name for the project and I'd hate for it to go to waste!
Nice! Is it a `.rs`? I wish there were more places to register them...
&gt; Is it a `.rs`? Nah, its a `.graphics` :D
bahahha, even better!
That's pretty Crate :P
Yeah, it was totally unbelievable. I was going to buy `lux-graphics.org` when I noticed that `.graphics` was a new TLD. So yeah, now I own `lux.graphics`. Basically the best luck I've ever had when buying domain names.
I have the feeling your server just got reddited. It feels a but slow right now. ;)
It's doing okay at proggit but also is at the top of Hacker News... time to spin up some more servers!
Wow, that looks really slick! I've already uploaded my `uchardet` and `uchardet-sys` crates just to try it out. But I ran into a problem with [`rust-cld2`](https://github.com/emk/rust-cld2), a wrapper around Google's ["compact language detector"](https://code.google.com/p/cld2/): $ du -sh target/package/cld2-sys-0.0.1.crate 35M target/package/cld2-sys-0.0.1.crate The C source code for this library contains statistical models of many human languages, and I'm pretty sure there's no way to shrink it below the 10MB crate limit. And since cld2 is not packaged for most systems, I don't want to ask the user to install it. Would it be possible to politely petition for an exemption to the 10MB limit at some point in the future? Thank you very much for your advice, as always.
Thanks! It compiles now. I'm using the most recent nightly. $ rustc --version rustc 0.13.0-nightly (399ff259e 2014-11-20 00:27:07 +0000)
Thanks!
This is a valid use case, but will make space baloon fast. There will be some sort of solution eventually.
Note that `unwrap` is reserved for the case that may panic (`Option`, `Result`) whereas `into_inner` is used only when you're guaranteed *not* to panic. The idea is to make the distinction/guarantee very clear.
One of the biggest features IMO is that Rust's macros are semantically-aware and based on pattern matching. Creating macros in Rust feels correct and normal, while, in my opinion, macros in C++ feels like a gigantic hack.
Mmm indeed after updating to the latest nightly I only get the messages you had instead of the significantly more extensive ones I quoted.
these modern times are so boring… ;)
This is a very thin `*-sys` wrapper around an upstream source tree, and I don't expect it to change very often. Would it be better to archive a tarball somewhere and download it at build time? I'd really prefer to keep all the build dependencies in the package itself, if at all possible, because that would help make builds reproducible. But maybe that's just not feasible yet.
That'd probably be better if the upstream isn't going to change. Otherwise, you'll be including the same 35MB each time... You could make it download to a location and check if it's already there first? Yay pioneering new things! :)
That's true, but github will also host your package; the nice thing about `crates.io` is that it makes the whole thing that much easier, with a single `cargo publish` command doing it all at once. It would be nice if there was a `cargo publish --docs` that did the same sort of thing: compile and upload all at once, so you don't even have to worry about it, and also maintain versions that correspond to your `crates.io` versions. That's asking a lot, but that's that, I guess...
Nice! A few nitpicky frontend notes (mobile, android default browser): it took a few seconds for it to display, sat at an empty green page for long enough I thought it was broken. Switching into a project does not reset the scroll position, which means I have to scroll back up to see some of the details.
It's getting pretty hammered right now, what with Reddit and HN and Twitter :)
I can understand that. Also for some reason the browser's back button causes it to turn all black and stop working somehow, so that seems fun. Overall great job on it though.
C macros are little more than textual replacement by the preprocessor. C++ does not have its own macros, it only has C macros with the same limitations of strictly being textual replacement. "Proper" C++ codegen uses templates, which are semantically-aware, safer and significantly more powerful. However templates have limitations in the kind of things they can generate.
That seems, uh, very bad, and something Ember should never do. Does it happen consistently, or intermittently? And where? (and great jobs go to Alex, not me :) )
I have https://github.com/rust-lang/cargo/issues/841 open which would end up triggering the rust-ci integration upon publishing too...
I've gone ahead and [opened an issue to discuss this subject further](https://github.com/rust-lang/crates.io/issues/40), explaining the use case and a couple of possible ideas for handling it. Thank you!
Oh okay! I just see your name popping up with the posts, sorry! Well thanks for the docs then :D and Alex you rock
Woot! Thank _you_!
 alias c=cargo c clean &amp;&amp; c update &amp;&amp; c build (And if `clean` is necessary, something’s badly wrong.)
yea, but that takes all the fun out of it :)
&gt; Yes, you cannot publish a package with git deps. You can build one locally though. this doesn't feel right. first, how i'm supposed to publish a package that depends on a package that isn't on crates.io? upload it in author's stead and figure out how to transfer ownership later? nah, sounds bad. second, you can point to an exact git commit sha1 and you'll have pretty strong guarantees on immutability, too. for added convenience, have cargo upload git dependencies in the package itself.
&gt; how i'm supposed to publish a package that depends on a package that isn't on crates.io? If you did publish such a package, others wouldn't be able to use it if your git repository goes away. If you depend on a crates.io package, you _know_ that you'll always be able to build, no matter what. (Well, unless _it_ goes away...)
Yeah. I'd have prefered to keep `unwrap()` for the "make an owning/wrapper/RAII/... type give up its claim on its content" operation, and then either put up with `unwrap()` sometimes panicking and sometimes not or using a worse name like `unwrap_assert()` or `unwrap_or_panic()` for the panicking flavors. Alas!
My thoughts exactly. To reproduce: 1. Open reddit is fun on android 2. Open crates.io 3. Go to any package page 4. Hit the device's back button It might have something to do with how the webview stuff works inside the app, not sure though.
Ahhh yes, that might be it. Bug filed anyway: https://github.com/rust-lang/cargo/issues/926
No null pointers! As a Java vet, this was a big sell to me. The type system makes you explicitly handle cases where a value might not be present, i.e. `Option` and `Result`.
My only concern is that we will exhaust crate names fairly quickly. Perhaps it would be better to do as GitHub does: http://crates.io/crates/ **[username]** / **[crate]** 
Maybe I am too lazy, but can someone point me to API documentations? Or should I hack on the source?
There are a _lot_ of names available. :) Ruby/Python/JavaScript haven't hit Peak Package Name yet. That scheme would be feasable, but would also complicate importing. What happens when we both have package foo?
https://github.com/rust-lang/crates.io is the source code. Unsure if the API is considered public...
Yes please.
I though the API is public and encouraged to use because there's this text in the front page "... Use the API to interact and find out more information about available crates..." Maybe just not ready yet. 
Then it must be! I just said I didn't know, not that it wasn't. :)
I didn't realize this was a common, working convention. My line of reasoning was that it's very unlikely to have two conflicts in packages you were importing, but I see your point.
&gt; If you're perfectly careful (and so are your coworkers), I'm guessing Rust has no advantage over modern C++. I think traits and generalized enums are also pretty strong advantages of Rust.
Note however that at the moment templates in C++ are significantly more powerful (and significantly less safe) than in Rust.
As others have said, Rust statically prevents bugs like iterator invalidation and data races that are still common in C++. I won't belabor that point, here are some other advantages though: Rust's enums and pattern matching are really great. I wrote an HTML5 parser and used nested `match` for the whole thing. I've used Boost.Variant before and C++ just can't compare. In Rust, moves are *always* a memcpy (at most), whereas C++ allows overloading the move constructor. One consequence is that Rust's `Vec&lt;T&gt;` can resize with `realloc`, whereas a `std::vector&lt;T&gt;` resize has to manually move the elements -- the ctors can even throw! So Rust's `Vec&lt;T&gt;` is a lot faster than `std::vector&lt;T&gt;` (or so I'm told -- I don't have the numbers handy). On the other hand, Rust will have a harder time supporting intrusive data structures. Rust also has more optimization potential due to stronger pointer aliasing rules. We don't expose them all to LLVM yet, though. Rust has traits, whereas C++ polymorphism is duck-typed. In Rust your generic stuff is typechecked before it's instantiated. I guess C++ is adding Concepts Lite eventually, but it's not part of "modern C++" as used today. It's also nice to write a 10,000 line generic library without putting the entire thing in header files. Rust has a much nicer macro system than C. It can't do everything that C++ templates can, but I find it far more pleasant in the common cases. Rust also supports compiler plugins for totally new syntax, but that's an unstable and implementation-dependent feature so I'm not sure you want to count it. Philosophically, you can't make a nice language by adding stuff to C for 40 years. At some point you have to start from a clean slate.
Can you elaborate a little bit more? I've only used Cabal a few times, and I'm not _exactly_ sure what you mean.
Please no. assert has a consistently different meaning in other languages - so in the name of least surprise don't make it something else.
Abstract data types with pattern matching is pretty great. Also bounded polymorphism via traits feels better and is more well documented than C++'s ad hoc polymorphism.
&gt; Neither package can be updated since metadata is immutable, so cabal thinks the old ones are valid forever. Hmmm. So, the package can't put out a release with `foo &lt; 1.7`? Seems bad :/ In this situation, I would expect that either a 1.7.1 comes out fixing their deal, or a new version of package gets released with 'foo &lt; 1.7'. But! Does Cabal have the lockfile abstracton? Putting out foo-1.7 won't just break your build: you'd keep using foo-1.6 until you explicitly update.
&gt; It's kinda like dereferencing a possibly NULL pointer without checking in C Nope. `unwrap` is guaranteed to crash if it's null, null pointers aren't.
But is currently broken: https://github.com/hansjorg/rust-ci/issues/22
Just published a package and really enjoyed it! Good work guys!
:O
I found it is built with Ember, and all the APIs seem public transparently. For example, curl https://crates.io/api/v1/crates Provides JSON output, and data schema can be found from the crates.io source files. Thanks /u/steveklabnik1 for linking to source code.
Right now the "kind" traits (`Send`, `Sync`, `Copy`, `Sized`) are opt-out. There is a proposal to make them opt-in though. The [docs for `Sync`](http://doc.rust-lang.org/std/kinds/trait.Sync.html) explain it better than I could.
Some stuff [here](http://www.reddit.com/r/rust/comments/2epym3/looking_for_rust_projects_to_contribute_to/) Come to Servo! We're an experimental browser engine in Rust. Tips on getting started [here](https://github.com/servo/servo/blob/master/CONTRIBUTING.md) -- also a video on getting involved [here](https://air.mozilla.org/bay-area-rust-meetup-november-2014/) (the last talk of the four), feel free to ping me or someone on IRC (#servo) anytime! If you want, I can mentor you with a couple easy issues to get you started :)
And while we're at, I managed to break it again. :-) [Another ticket with a Cargo panic](https://github.com/rust-lang/cargo/issues/937).
Because I do so love throwing spanners: doesn't work on Windows in PowerShell: it doesn't support `&amp;&amp;` (no, seriously).
Congrats! Looking great. :-) I'm a bit curious about naming conventions and enforcement. Is there any guidance on crate names? I've already noticed `crates-named-like-this` and `crates_named_like_this`. Which is preferred? What about crates using generic or ambiguous names like `cargo`, `rustc`, or `memory`? Are these allowed? (It was already mentioned in this thread, but I feel like a `[user]/[package]` scheme could help disambiguate.) Does anything go, or should users expect to conform to some rules?
Convention is _, like other identifiers. There is the -sys convention though... We'll have stricter policy about certain things up soon.
I imagine if you really want one, you can write a `c-for` macro that transforms it into an equivalent while-loop.
The declaration would be extern { fn c_function(out: *mut *mut the_struct, out_num *mut c_int); } and it would be called like use std::ptr; let mut structs = ptr::mut_null(); let mut number = 0; unsafe { c_function(&amp;mut structs, &amp;mut number) } In particular, `*mut T` is the type that corresponds to `T*` in C, and a `&amp;mut T` will automatically become a `*mut T` if necessary.
Awesome! Thanks, all! This is a great milestone for the community. Just went and tried a test app, and it worked beautifully. One thing I noticed was that the [Getting Started](http://doc.crates.io/guide.html) docs linked on the home page only seem to describe getting started with Cargo in general, and only using the external dependencies checked out from git. It doesn't appear to actually describe how to start using crates downloaded from the repo (which is what one might expect). I only found this described in the [manifest format documentation](http://doc.crates.io/manifest.html). It would be great to have a simple example of using `crates.io` on the main page or prominently in the getting started docs. In the individual crate pages, the 'usage' info shows: Depend num = "~0.0.3" The "Depend" part is a little confusing, as it isn't actually valid syntax for the manifest. How about making it look just like how you would use it so you can copy/paste: [dependencies] num = "~0.0.3" 
Just found a small issue - on the [`image` crate's page](https://crates.io/crates/image), it shows the dependencies line should be: image = "~0.2.0-alpha.1" but adding this to `Cargo.toml` and running `cargo build` results in the error: Cargo.toml is not a valid manifest The given version requirement is invalid. Shortening this line to: image = "~0.2.0" fixed the problem. (Is there a better way to report feedback such as this? Thanks!) 
thank you
&gt; there's no automatic library download. Isn't Apt or Yum enough? I think the only advantage of pip, npm, gem and Cargo is that they can update software modules at a faster pace than the whole OS distro (probably at the cost of stability.)
I miss 90's web design. they looked so good in text-only web browsers, and you can easily back them up in case the server goes down (wget example.com/foo.html)
That seems unrealistically ambitious to me. Haven't we been through this before with D and Go (warn me if I'm treading too close to rule 4 here)? Rust has a different target with no GC and a more interesting type system, but that's not absolutely mandatory in scientific computing unless it will allow some new unprecedented functionality. Maybe it will, but observe the lack of takeup of D, Go, Haskell, etc in scientific computing despite the fact that they've been stable for some time. If "Rust For Scientific Computing" is ever going to be a thing, it needs to be an organized effort with serious academic takeup from influential people writing "killer app" libraries. I don't see how the core language's development model switching from breaking changes to backwards compatible improvements would be a prerequisite for this to show signs of coming together already.
http://i.imgur.com/usJ0mpu.png
25% of the commits are from non-employees (of either Mozilla or Samsung), and our top committer is also not an employee: http://www.joshmatthews.net/servo-stats.html Though not having volunteers shouldn't deter you, every Mozilla project I've come across till date is welcoming to volunteers even if they have very few. The employees don't bite ;) [I'm a volunteer too]
Hmm. [Playpen](http://play.rust-lang.org/) doesn't seem to have that error: f1 = 1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e-24 f2 = 1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e-24 same? true Maybe it's due to architecture? What version of Rust you are using? What OS? What processor? This seems platform or version specific. From what I've read in source, the powi just compiles into a intrinsic LLVM instruction. 
To add to the confusion: [running your example on the playpen](http://play.rust-lang.org/?code=fn%20main%28%29%20{%0A%20%20%20%20let%20f1%3A%20f64%20%3D%201e-24%3B%0A%20%20%20%20let%20f2%3A%20f64%20%3D%2010f64.powi%28-24%29%3B%0A%20%20%20%20println!%28%22f1%20%3D%20{%3A.200e}\nf2%20%3D%20{%3A.200e}\nsame%3F%20{}%22%2C%20f1%2C%20f2%2C%20f1%3D%3Df2%29%3B%0A%20%20%20%20println!%28%22%20%20%20%20%20f2%20%3D%20{%3A.200e}\nf1%20%2B%20f2%20%3D%20{%3A.200e}%22%2C%20f2%2C%20f1%2Bf2%29%3B%0A}%0A) yields the following output: f1 = 1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e-24 f2 = 1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e-24 same? true f2 = 1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e-24 f1 + f2 = 2.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e-24
In many (if not most) cases `a^b` with a non-integer result is calculated via `exp(log(a)*b)`. (Even in Python, it [falls back](https://hg.python.org/cpython/file/c9b4dc1ab7ae/Objects/intobject.c#l735) to the floating point code when b is a negative integer.) That means its result really depends on the accuracy and implementation quirks of transcendental functions. Some implementations have *cosmetic* routines to make their results *look* correct (as [W. Kahan said](http://www.cs.berkeley.edu/~wkahan/LOG10HAF.TXT)), but you really should not depend on them. EDIT: I've accidentally swapped `a` and `b` :p Also, W. Kahan's essay is worth reading.
Huh, it seems to be the difference between the -O0 flag and the -O1 flag. You can switch between them and see here http://is.gd/Znt15L It's odd that Rust is apparently able to store this number exactly. I am really curious as to what it's doing.
Hope that policy includes a convention about naming nested/hierarchical crates, like `main_crate-additional_crate` or `rusty_robot-battery_pack`, as in RubyGems.
I believe that Chris Morgan had some ideas for a Django like Web Framework in Rust, but decided a better HTTP library had to be built first. http://chrismorgan.info/blog/introducing-teepee.html
Which C++ compiler are you using for comparison? How does clang behave?
That was with g++, but I just tried with clang and got the same results. It should be noted that I also used the pow() function from math.h, which doesn't treat integer powers specially.
I'm aware of Iron, and while it's an interesting project, it seems to have pretty different goals than something Django-like
I have a blank green page with `SecurityError: The operation is insecure.` in the console. =/
I'm confused by Rust's printing. `1e-24` cannot be represented exactly as an IEEE-754 float, but printing that many zeros implies that it is being represented exactly. E.g. Python gives &gt;&gt;&gt; print '%.30g' % 1e-24 9.99999999999999923700499551703e-25 &gt;&gt;&gt; print '%.30g' % numpy.nextafter(1e-24, 1000) 1.00000000000000010737149186769e-24 for the closest `f64`s on either side of `1e-24`. This is a bug similar to [#7030](https://github.com/rust-lang/rust/issues/7030), [#7648](https://github.com/rust-lang/rust/issues/7648) or [#18038](https://github.com/rust-lang/rust/issues/18038) 
There's still too much to do as far as completing the necessary dependencies.
Looks weird in Opera/Linux: * http://i.imgur.com/8MaxSpJ.png * http://i.imgur.com/aD7Y9et.png
This is exactly what I wanted Oxidize to be like, but I've started school again and there's no time to work on it right now. In the future I'm planning on resurrecting it again and working on making it more like flask. I've been going back and forth between wanting to make a Django like framework and a Flask like framework but I think I'm going to settle on flask simply because of its plug in play style. I've been using Django for quite some time now, but I'm trying out flask and I'm loving it. I find it funny how I end up coding in Flask as if I'm coding in Django, but its super neat that I don't have to if I don't want to. If I don't want to have a complicated app, I can just throw it all in one file and run it. One idea I have for development is I'm planning on pretty much making a copy of Flask in rust just to see the speed difference. I want to start by requiring Flask and then having Rust-python bindings for all of flask and slowly rewrite sections in pure rust. That way I can test my framework with existing flask sites that I convert to rust and can use it to monitor functionality. I'm kinda not expecting much out of it, but my thought is there are already really mature and amazing web frameworks in other languages so why would I want to reinvent the wheel? Of course there has to be advantages to using my framework. Speed of the language is a big factor I'm hoping to have additionally I want to try to make it type safe as in the request GET POST and such will all be typed. But anyway, its all just a dream right now until I finally graduate. Last year I was lucky and turned oxidize into a school project and there is a very slim chance I'll be able to pull that off again :/
I am actively working to implement something like Symfony2 in PHP world. I am especially interested in bundle-like project structure and proper modularity with dependency management. My philosophy would be to reuse and integrate other modules and not reinvent the wheel. But it was only 3 weeks so far, so I did not get far :)
Oh, I didn't even realise we could do that!
added (to 'Compiler built-in tools'), thanks for the reminder!
Great! I added both links
added it, thanks!
Do you know if there'll be a fallback to static html? Pypi, Hackage and such work fine with no JS at all, requiring JS seems like a regression, especially if you're remoted into a machine somewhere and only have lynx/links or you're trying to bring up a page in emacs.
could you tell me a bit more about what fluid assertions are? I couldn't find much from casual googling
Great, thanks!
Awesome, I've added the info to the doc, thanks!
Are you on Windows? It turns out the download page is pointing to an ancient (early this month) version. If you are on Windows, take the URL, and change the triple to `i686-pc-windows-gnu` for 32-bit. If you really want 64-bit cargo, maybe try `x86_64-pc-windows-gnu`, but I haven't verified that. *Also* be aware that we've just found a bug in Cargo, in that it completely ignores your user-wide `.cargo/config` file. If it's acting like `cargo login` doesn't stick, you need to copy your user `.cargo` directory so that it's in the working directory heirarchy somewhere.
I haven't thought too much about these questions, but they are important questions to ask and answer :-) I've added them to the doc, and will spend some time thinking about them. Thanks!
&gt;&gt;"One consequence is that Rust's Vec&lt;T&gt; can resize with realloc, whereas a std::vector&lt;T&gt; resize has to manually move the elements -- the ctors can even throw! " you can of course get around that with extra unsafe non-idiomatic "C/C++" style code, but C++ zealots would have a fit. I've never really been happy with the C++ standard library. There are all these cases of things you take for granted being able to do in plain C that don't quite fit into C++'s abstractions.
~~I think you should open an issue on the GitHub project (if you haven't already)~~ EDIT: I just read Quxxy's answer, his anwer makes more sense.
C++ does have non-nullable pointers: references. Sadly it does not force handling the nullable case of raw pointers.
I've run into the same issue on Linux while trying to build cargo on Gentoo. The ebuild is very simple, it just fetches from cargo git repository, then everything is built by default (./configure; make). But Cargo.lock in the repo points to old revisions of some deps which no longer compile, and if I remove the lockfile, cargo successfully fetches all deps and then dies with the 'example.com' error.
That's awesome! I've been pretty intimidated by the idea of contributing to a project such as this in the past. Graduation should be just around the corner for me so I'm thinking I'm going to give this a real shot as I should have a little more free time. Thanks again!
Even that depends on your definition of "non-nullable". The following is a thing in C++: int *x = nullptr; // ... int &amp;y = *x; Though rather fortunately it rarely happens by accident (or so I'm told).
Yeah, that is what confuses me the most -- and it seems that for code run with optimization level 1 or greater, this is purely a printing bug. For example, if you do let (x, exp) = 1e-24.frexp(); and then paste the resuts into python, then it will tell you that x*2**exp == 1e-24 is `True`.
Perhaps with optimization it's all computed at compile time, which may use a different function?
You're right about the 64-bit version. Just for reference: - 32-bit download: https://static.rust-lang.org/cargo-dist/cargo-nightly-i686-pc-windows-gnu.tar.gz - 64-bit download: https://static.rust-lang.org/cargo-dist/cargo-nightly-x86_64-pc-windows-gnu.tar.gz 
I'm still partial to `.or_bust()`. `.assert()` is still weird. Edit: What has convinced you that it isn't weird anymore?
Yes! Want! And actually something better for the parts where Rust shines over Python, by which I mean runtime performance and safety. Where do we meet?
I'm not aware of any.
Remember that this site documents both Cargo itself as well as Crates.io, so that's why you're seeing some split. That said, there could be better cross-linking between the two. I agree with you about the crate page thing.
It makes the site unusable for people using the tor browser too, so there probably needs to be something or we'll be excluding those users entirely.
Great, thanks for the advice!
It works out of the box! It has syntax highlighting, and commands for building, although at the moment those commands are `rustc "{FILE}"` and `make`, with cargo not built-in, although they are easy enough to change. No auto-completion, but Geany doesn't really do that for any language...
the data on crates.io should be a wget away, I shouldn't have to evaluate client code to get what should be pure data.
Apart from the core features other people have mentioned, let's add the modern development ecosystem: No ugly header files and text preprocessor, cargo build and package management, document generation from day 0.
So perhaps `cargo` should have a way of marking a package as dead? (e.g. `cargo unpublish`)
This is great. Good to see a place where people can post the existence of libs. How is this different from rust-ci? The information density seems rather low. It might be more useful if metadata is extractable via a rest interface. Dep graphs, native deps, test coverage, code indexing are vastly more useful than having what could be replaced by a subreddit. If there was a more regular project description it could be pulled in directly. Take a README.md that contains a microfomat in code literal block, along with Cargo. Some projects don't have repos for the lib. How is this possible? I can't read the source? No user visible checksums? Am I downloading binaries? Am I just getting cargo dep lines? Why can't I copy and paste lines like? [dependencies.civet] git = "https://github.com/wycats/rust-civet" 
&gt; of either Mozilla or Samsung woah, is samsung partnered with mozilla for rust or something? thats really cool, because if samsung is backing rust that could be really good
&gt; How is this different from rust-ci? Rust-ci does not host packages, for one. &gt; It might be more useful if metadata is extractable via a rest interface. It all should be, given that the website is an Ember app. :) &gt; what could be replaced by a subreddit. Not everyone likes or uses reddit. If I could, I would never come back to Reddit ever again. &gt; If there was a more regular project description it could be pulled in directly. There is a metadata key in Cargo.toml for a description, and it will show on the site if the author gives one. Recent Cargos will warn upon `cargo publish` if you leave it out. &gt; Some projects don't have repos for the lib. The author did not fill out the 'website' metdata :) &gt; I can't read the source? Cargo is source-only distribution, so you could check out the package directly. That said, it's nice when authors link to the libraries, you're right. &gt; No user visible checksums? There are plans for package signing, etc. They're just not there yet. &gt; Am I downloading binaries? Nope, source only. &gt; Why can't I copy and paste lines Agreed, though the format would be [dependencies] civet = "*" or whatever versions you wanted to match against.
It is. Try: $ curl https://crates.io/api/v1/crates/sha1 Etc.
Well, in ruby, `require` just requires a file. So extension gems have those names so that you can end up `require "rusty_robot/battery_pack"`. Rust has an actual module system, and so you can't do the same kind of thing. (This doens't mean we shouldn't have a convention, just that what exactly it means is less obvious)
What dependencies are left to be completed?
https://blog.mozilla.org/blog/2013/04/03/mozilla-and-samsung-collaborate-on-next-generation-web-browser-engine/
Yeah, but that still means that they _use_ Rust, and steve@warmachine:~/src/rust$ git shortlog -s -n -e | grep samsung 75 Young-il Choi &lt;duddlf.choi@samsung.com&gt; 20 Bruno de Oliveira Abinader &lt;bruno.d@partner.samsung.com&gt; 11 Sangeun Kim &lt;sammy.kim@samsung.com&gt; 10 Youngmin Yoo &lt;youngmin.yoo@samsung.com&gt; 5 Seonghyun Kim &lt;sh8281.kim@samsung.com&gt; 3 Junyoung Cho &lt;june0.cho@samsung.com&gt; 1 Jaemin Moon &lt;jaemin.moon@samsung.com&gt; 1 Kyeongwoon Lee &lt;kyeongwoon.lee@samsung.com&gt; 1 Pawel Olzacki &lt;p.olzacki2@samsung.com&gt; 1 aydin.kim &lt;aydin.kim@samsung.com&gt; So there is that. :)
I think it's really important that we pay attention to the floating-point spec, and not just compare to other languages. I'm not an FP wizard, but none of the behavior you've described strikes me as being exactly *wrong* except for 1-^-24 being exactly representable (which may be constant-folding to a string at compile time). Approximations up to a point are permitted, so the fact that languages do things in different ways doesn't mean that either of them are bugged. Floating Point is explicitly *not* bit-for-bit exact. It does mean that some of these other languages have some nicities about working with floating point that make them behave a bit more like programmers expect numbers to behave. It's worth investigating if some of those safeguards should be incorporated, or if Rust should choose similar implementation strategies for compatibility's sake. &gt; What's more, the expression (f1 + f2) - f1 == f2 evaluates to false (although (f2 - f1) + f1) == f2 evaluates to true). That's just floats. Floating point math is not associative in general.
Excellent!
Quick tip: If you name your README as README.md, then GitHub will automatically render it as markdown.
you can also kind of use an if-ladder `if (auto p=dynamic_cast&lt;Foo*&gt;(x)){...do stuff with p..}else if (auto p=dynamic_cast&lt;Bar*&gt;(x)){..` in a manner thats' a little in the spirit of tagged union use cases (like the vtable ptr is a tag), thats got a bit more safety to it. ( i.e. polymorphism with 'sort by function' instead of 'sort by type'). Its' just , like everything else in C++, clunky verbose syntax.. taking a feature that does half of what you wanted, and stretching it to do what you *really* wanted,only it still has flaws, so you'll bounce between the alternatives. There is one advantage of this over rust-enums: the 'variants' are individually sized. Imagine C++ getting something like this:- switch (v:expr){ //analogous to for (auto x:collection...) Foo: //...do stuff with Foo* v=dynamic_cast&lt;Foo*&gt;(expr)... Bar: //.. do stuff with Bar* v=... ... } .. and similarly imagine some sugar to roll a bunch of classes derived from a given base. (i)C manually tagged union : - for: compiler can figure out more efficient dispatch; and you can choose handle any tag type. - against: not type safe , you'd have to use C macros to stop it being verbose (ii)dynamic-cast if ladder: - for: typesafe. - against: tag has to be pointer-sized, and dispatch is inefficient for larger N. (iii)boost variant - for: typesafe, efficient dispatch? - against: verbosity 3 methods.. the existence of each would have its advocates resist additional compiler complexity for a dedicated feature, yet a dedicated feature is the only way to combine the advantages of each so you don't have waste time bouncing between 'least bad' compromises. People say C++'s problem is "complexity" - I disagree - its missing features or half broken features that interact badly. This problem could be fixed by adding more features (improving the template system till you really can implement variants that work smoothly? or a 'switch on vtable ptr' ? etc)
My understanding is that before we had a coercion from `[T, ..N]` to `&amp;[T]` and now we have a coercion from `&amp;[T, ..N]` to `&amp;[T]`. It's a little change really, but very sweet on ergonomics.
I know, but I am an old man and get nostalgic for the days when you didn't have to add `.md` to everything :) Thanks though, sorry to answer your helpfulness with crabbiness :)
Or use `boost::variant`, for compile-time safety and the ability to store non-PODs even in C++03.
I am concerned about the fact that you might end up with a dangling reference pretty easily: you can always shy away from pointers, or give them extra scrutiny, but detecting all cases leading to dangling references is a nigh impossible task.
Having the same issue; dazed and confused.
Note about concurrency support: the foolhardy uses comments, a serious developer uses the Proxy pattern. class Data { friend class Proxy; friend class MutableProxy; std::shared_mutex mutex; int a; // ... }; // class Data class Proxy { friend class MutableProxy; public: explicit Proxy(Data const&amp; d): lock(d.mutex), data(d) {} int read() const { return data.a; } private: struct WriteLock {}; Proxy(Data&amp; d, WriteLock): data(d) {} std::shared_lock&lt;std::shared_mutex&gt; lock; Data&amp; data; }; // class Proxy class MutableProxy: public Proxy { public: explicit MutableProxy(Data&amp; d): Proxy(data, WriteLock{}), lock(d.mutex) {} void increment() { ++data.a; } private: std::lock_guard&lt;std::shared_mutex&gt; lock; }; // class MutableProxy This does not save you from deadlocks though...
Looks amazing, thanks for the summary. Something I couldn’t find, though, when browsing Cargo’s sources is an approach to packaging the build for distribution. Or how installing is handled in general. I’m not asking for a fits-all solution, rather I’d be interested in whether Cargo can be used in some way to derive the required information, e. g. which targets are libaries and should go into ``/*/lib``, which are executables for ``/*/bin``, what parts are data, what resources are required for developers (like header packages in most common distros), etc. Furthermore, a description in ``pkg-config`` format would come in handy once libraries written in Rust become widespread. Which is, hopefully, very soon ``=)``. Are there any plans to extend Cargo in that direction? I certainly do not intend to shift of work from the packager to the developer. However, Cargo already appears to understand a project quite intimately and from my outsider perspective leveraging that knowledge for common integration tasks appear to be the obvious next step. In any case, thanks to everybody involved with the project. Despite being in an early stage, Cargo is already a pleasure to use. 
&gt; There isn't a clear way to 'generate a .deb from a Cargo package' yet, no. But having that would be pretty cool, and is generally desired. No problem. I’d never ask for a direct-to-deb packager. I rarely use that format anyways ``:p``. Just for the build engine to spit out its internals in a simple format, so whatever packager can extract what they need in order to ready a component for distribution -- that’d be very convenient. 
I think the sigils disappeared because they made the smart pointers part of the language rather than part of the standard library. The only way I see them coming back is if Rust introduces Haskell-style operator overloading.
What is Conduit supposed to be? Because nothing in the repo really describes it that well besides being... an API for web programming?
That's what we are trying to figure out :)
As a fellow C++ programmer and someone who has been programming in Rust in the spare time for about half a year, I can say, I llike Rust for a lot of reasons. * Syntactically, Rust is less of an imposition. Compare this to `typename some_allocator_type::template rebind&lt;T&gt;::type` where the keywords `typename` and `template` are just there for disambiguation. C++11 helps a bit, though. It offers "templated typedefs" which reduces the need for many `typename` uses. This improves the use of type traits. But I also don't like the redundancy between cpp and header files. Eventually, C++ will have a better module system. But right now, it's not available. * It leaves much less room for shooting yourself in the foot. Yes, I know modern C++. And I tend not to run into memory safety or data race issues because I'm careful and use certain design patterns that are less error-prone. But it happens once in a while and I like the fact that as a library author I can express things like lifetimes as part of the interface that helps the compiler helping the user not screw up. Rust tends to force users to use a "good style". And I think there is a lot of value in this. And the fact that lifetimes are statically checked makes writing functions that return references into things as opposed to copying stuff less worrisome because you *know* that the compiler will catch any misuse errors. This includes thread-safety issues. The compiler is *aware* of which types are safely sharable and/or sendable across thread boundaries. Very nice! * Rust is simpler in some respects. I like how Rust has implemented ownership and move semantics. The chance of finding C++ code that is broken in this respect is just higher. You have to know things like "the rule of three". I've seen too many C++ classes already that seem to be authored by people who don't know this rule of thumb and the pitfalls around it. C++ compilers still don't warn about rule-of-three violations even though C++11 declared some of the compiler-generated special member functions as "deprecated". And moves in Rust are simple! It's just copying the raw bits. Isn't it nice to have the certainty that every type is cheaply movable without possibly throwing an exception? I think it is. In C++ move constructors can do complicated things and even fail (`std::pair&lt;std::string, some_legacy_type&gt;` where `some_legacy_type` has no move ctor but a possibly throwing copy ctor). In C++ these kinds of things can get complicated and necessitate helper functions like `std::move_if_no_except` that falls back on copying if there is no other way of making the "strong exception guarantee" or you have to overload a vector's `push_back` to make things efficient where in Rust you simply use pass-by-value. I understand the intricacies of C++ rvalue references. But it's nice to know that Rust gets away without them. :) * The tooling around Rust is pretty nice already considering it's such a young language: cargo, unit testing, benchmarking, doc tests. But it's not all roses either. I could list at least three issues that I have to work around and/or deal with somehow. Some of this is being worked on. For example, I'm not convinced that "exceptions" only being "catchable" at thread boundaries is a good thing. I've also not made friends with Rust's module system. I still need to figure out how to organize things properly. And I really really want types to mix better ("multidispatch") and have associated types mix well with lifetime parameters.
* "Guard clauses" would probably help with the rightward drift: instead of if entry.is_dir() { // do stuff } use if !entry.is_dir() { return; } // do stuff * though in this case, you'd probably benefit even more from using more Iterator stuff e.g. something along the lines of for k in contents.iter() .filter(|e| e.is_dir()) .map(|e| Path::new(e).join("Cargo.toml")) .filter(|k| k.exists()) { // stuff (note: have not checked if code is actually correct) (and it'd require getting the dirname back from the current file path) (also that's the kind of situation where unbound methods or UFCS would be really useful) * your matches on Command are not very useful, three of them are basically `unwrap()` and the one in the middle of visit_dirs could be something like `.map_err(|e| {visit_dirs_clean(&amp;k); e }).unwrap()`. * also I may be mistaken, but I believe .spawn returns a process which may be running, so e.g. your `cargo build` would not be waiting for `cargo update` to be done running. `.output()` or `.status()` might be better. * and it looks to me like `visit_dirs_clean` already gets a path to a Cargo file, so it's never going to execute. edit: this could probably be improved further (not too happy about returning a `Vec&lt;()&gt;`, e.g. you may want to return a `Vec&lt;IoResult&lt;Path&gt;&gt;` so you can know which directories have successfully built, and you may also want a custom error type, also visit_dirs_clean's possible failure is completely ignored) but I'm thinking something like this: use std::io::fs::PathExtensions; use std::io::{fs, Command, IoResult}; use std::io::process::ProcessOutput; fn visit_dirs_clean(entry: &amp;Path) -&gt; IoResult&lt;ProcessOutput&gt; { println!("{}",entry.display()); Command::new("cargo") .cwd(&amp;Path::new(entry.dirname())) .arg("clean") .output() } fn visit_dirs(dir: &amp;Path) -&gt; IoResult&lt;Vec&lt;()&gt;&gt; { fs::readdir(dir).unwrap().iter() .filter(|e| e.is_dir()) .map(|e| Path::new(e).join("Cargo.toml")) .filter(|k| k.exists()) .map(|k| { println!("{}", k.display()); let dir = Path::new(k.dirname()); try!(Command::new("cargo").cwd(&amp;dir).arg("update").output()); try!(Command::new("cargo").cwd(&amp;dir).arg("build").output() .map_err(|e| { visit_dirs_clean(&amp;k); e })); try!(Command::new("rustdoc").cwd(&amp;dir).output()); Ok(()) }).collect() } fn main() { let r = Path::new("..."); visit_dirs(&amp;r).unwrap(); } 
I thought this would be of interest to more then just the gamedev community...
I wonder what could possibly be the use of a private trait item. And what exactly would a private trait item imply? That only the module defining that trait gets to use these private items? Hmm... the concept of private trait items seems weird since the purpose of traits is to define a kind of (public) interface of things.
Making the operator traits take their parameters by value seems like a bad idea to me. At least its chance of being a regrettable decision is too high. And I don't buy the "there is no loss in expressiveness" argument because it just makes things complicated and less generic. Consider a custom numeric type which is not `Copy` but has an expensive `Clone`. Consider the possibility that the binary trait `Mul` cannot be reasonably implemented in a way to recycle resources of the arguments for the result. And consider the fact that nobody expects operators to consume their arguments (which then makes pass-by-value the less efficient approach). If I want to reuse some argument I used for `mul`, I would have to clone it. Do you expect people to write let (re1, im1, re2, im2) = ...; let re3 = re1.clone() * re2.clone() - im1.clone() * im2.clone(); let im3 = re1 * im2 + im1 * re2; instead of let (re1, im1, re2, im2) = ...; let re3 = re1 * re2 - im1 * im2; let im3 = re1 * im2 + im1 * re2; for some more "complicated" non-`Copy` type? As for "there is no loss in expressiveness", well, there is a loss ... a loss of genericity. You might want to `impl Mul` for `&amp;T` instead of `T` but then the compiler won't let you feed your type to some generic function because the author of this generic function has *not* written let (re1, im1, re2, im2) = ...; let re3 = &amp;re1 * &amp;re2 - &amp;im1 * &amp;im2; let im3 = &amp;re1 * &amp;im2 + &amp;im1 * &amp;re2; but without the borrow operators and with `T: Mul` instead of `&amp;T: Mul` as type bound. I don't think there is a right way. It's a trade-off. The question is: Which approach sucks less? Auto-borrowing for operators or pass-by-value? Personally, I'm not convinced pass-by-value is the right way if we have to limit ourselfs to just one way. If you want to concatenate two lists simply don't do it via implementing `Add`.
Thanks for the feedback. These issues were all considered during the RFC process. The point about expressiveness is to say that everyone can continue to work with these operators by reference -- both in providing an `impl`, *and* in writing generic code (once full where clauses land). So you continue to be able to write code with identical semantics/genericity as today, albeit with a bit more explicitness about borrowing. Rust makes borrowing explicit almost everywhere -- the main exception being `self`, of course. This RFC makes operators more consistent with the rest of the language (besides increasing flexibility). The expectation is *not* that you would clone everywhere, but rather that you would provide `impl`s that work over references (and perhaps *also* values, since multidispatch will happily work with that). Generic code should also be written using references in most cases, unless there is a reason to do otherwise. And we can also provide blanket impls for `Copy` data that lifts any by-value impls to by-ref ones automatically. That's what I mean by no loss: you can still do everything you can today. But you can also *do more* than you can today, since you will have the choice of working with data by value when appropriate. DList was just one example, and commenters on the RFC have given others. To put it differently, the loss of genericity you're envisioning would only arise if the library author explicitly chose to take things by value -- which is only made possible by this RFC -- as opposed to by reference. In general, the strong convention is to take data by reference unless you specifically need ownership, so I don't see this happening often. I agree that there's not a single right way, but draw a different conclusion: we should allow the traits to be used in both by-value *and* by-reference styles, so that library writers can choose for themselves. That's what this RFC does.
&gt;&gt;"I've also not made friends with Rust's module system." same here, I would like the option for a directory to be flat (like the opposite of declaring submodules in a file). files-as-namespaces is generally a nice idea but it seems overkill with directory::file::type::trait::method; I've found moving code around can be as bad in Rust as it is in C++ I have wondered if a 'fuzzy match' could be viable without being dangerous. whats the probability of name clashes on an ident, very high. but is the probability of name clashes with , say, one qualifier low enough, if it did a search from the current location? (and if it was ambiguous it could throw an error, and you could fix it)
Sounds very interesting! Can you just add SDL function signatures to the ffi modules and have them work just fine? Or would you also have to add more emscripten boilerplate code (which also doesn't look very safe, lots of mem::uninitialized...)
In C++11, I think std::vector&lt;T&gt; could safely memcpy/realloc if T satisfies some type trait (probably ```std::is_trivially_move_constructible``` and/or ```std::is_trivially_copyable```). Not sure about C++98/03. The situation is certainly simpler in Rust!
 fn main() { let v: Vec&lt;uint&gt; = std::os::args().iter() .filter_map(|x| from_str::&lt;uint&gt;(x.as_slice())) .collect(); println!("{}", v); } You are already dealing with references: your ref x is just making them double references. No point. You can remove them.
`.map(…).filter(|x| x.is_some()).map(|x| x.unwrap())` is equivalent to `.filter_map(…)`. The `ref` is completely superfluous; I expect it is optimised out of there. let list = std::os::args().iter().filter_map(|x| from_str::&lt;uint&gt;(x)).collect::&lt;Vec&lt;_&gt;&gt;();
I've run into this a few times, myself. If it bothers you, you can always define an iterator extension to do this for you, [like this `keep_some`](https://github.com/DanielKeep/rust-grabbag/blob/master/src/iter.rs#L746-L780).
Not unless they radically changed the semantics of `;` and never told anyone. `&amp;&amp;` only runs the RHS if the LHS succeeded. `;` unconditionally runs the RHS.
In order to call methods from a trait, the trait must be in scope; this is the meaning of the “in scope” part of the error message. The trait defining the `chars` method is [`core::str::StrPrelude`](http://doc.rust-lang.org/core/str/trait.StrPrelude.html), so you need this: use core::str::StrPrelude; … or you can just go for `#![feature(globs)]` on the crate, and `use core::prelude::*;` everywhere (see [`core::prelude`](http://doc.rust-lang.org/core/prelude/index.html) for more info).
alright, I'll give it a shot, thanx.
Wait, if you're gonna glob import everything in prelude, why bother with #![no_std] anyway?
That's importing the `core` prelude, not the `std` prelude. `core` is significantly slimmer than `std` and suitable for freestanding purposes because it has no dependencies on `alloc` and doesn't need any kind of platform integration other than a few C standard library symbols being present.
He glob-imports core::prelude, not std::prelude.
You need `Iterator` too for for loops to work.
In shell script, success means 0 exit code, yes. `a &amp;&amp; b` is the same as `if a; then b; fi`.
I'm a bit confused about the size of the data structures. I see it's inserting elements each iteration, and sometimes removing. What's the max size in this benchmark?
Just convert your tuple. match (tup.0.as_slice(), tup.1.as_slice()) { … } Your current method would also require extra allocations for the strings on every function call if it would work.
As I see. I was trying to match in the wrong direction.
Update: The mentioned above are not really disadvantages, but rather due to new language, some of these features are not optimized. 
That is just plain wrong, at least formulated as such general statements. Please note point 2 of the rules. Bake it at least with an example to make it constructive. Edit: Just to point 3, it has been mentioned here like hundred times: Rust binaries are statically linked but nobody holds you back from not doing that. zinc.rs boils their minimal example down to ~500 byte. That's a good start I would say.
[see also similar recent thread](http://www.reddit.com/r/rust/comments/2ifgsx/what_can_c_do_that_rust_cant/)
Wait, why isn't there a trait like MutableMap on the standard library? Nice benchmark!
You are right! It can be used as a case filter. A proof of this is the following code snippet (you can run it in the playpen too: http://is.gd/RkSZ58) fn main() { match Some(42i) { _ =&gt; { } None =&gt; { } } } And you get: &lt;anon&gt;:4:9: 4:13 error: unreachable pattern [E0001] &lt;anon&gt;:4 None =&gt; { } ^~~~
Do you have evidence for your claims? Here are some counter examples: &gt; Poor performance Here's a [list of benchmarks](http://www.reddit.com/r/rust/comments/2lzc9n/rust_serialization_part_21_now_with_more/clzjm5z). In those benchmarks, Rust is as fast as C++, sometimes faster than C++ or within 2x of the speed of C++. Of course, benchmarks are very specific, so we can't conclude that a language is faster/slower than other. &gt; Large binary size Here's a [151-byte Hello world program](http://www.reddit.com/r/rust/comments/2iwtjh/151byte_static_binary_for_x8664_linux_in_rust/). There's nothing stopping you from creating very small binaries that [fit in microcontrollers](http://zinc.rs/stats/). &gt; Consumes more memory Until now, I haven't seen any comparison in this area, but I'll love to see how we fare.
http://joshitech.blogspot.com/2014/11/fibonacci50-rust-slower-than-go.html?m=1 
Makes sense. Seems like it's all hanging on higher-kinded types, then. Releasing Rust 1.0 without them feels more and more awkward to me.
Here it shows memory usage http://joshitech.blogspot.com/2014/11/fibonacci50-rust-slower-than-go.html?m=1
&gt; ...are the cases in a match statement guarenteed to be exectued in order? Yes, they're. Try to compile: fn main() { match 1i { _ =&gt; println!("_"), 1 =&gt; println!("1"), } } &gt; Essentially, can rust match be used as a weird sort of case filter? Yes, you can even use additional guards: match (1, 2, 3) { (x, _, _) if x &lt; 0 =&gt; { ... } (x, _, _) =&gt; { ... } } 
I'll note that that is a microbenchmark that only really measures what the default runtime uses before starting your code. There's no real use of memory in that entire application - whatever the application uses is likely to dwarf that quickly in anything but the most trivial of programs. So do the algorithms in Rust's standard library allocate more than C++'s? I'll also note that C is not C++, and C++ has a separate runtime that might (probably does) use more memory.
Yeah I think the inner loop needs to be tweaked a bit. Our benchmark suite doesn't do a fixed number of interations, but instead waits for a statistically significant amount of the same sample size to occur. So really each Bencher iteration should do the same amount of work, or else it can be difficult to draw conclusions from the results. Only needs a minor tweak though and the results will probably be pretty similar to the numbers the OP got. 
Performance comparison of hash_map and tree_map is nonsense. Former is O(1) and latter is O(log(N)).
Thanks a lot for the comments. The benchmark is pretty dummy, I know. I shared it because I was interested on comments from experienced people in Rust. I forgot to mention that it is a benchmark from a newbie =( 
I used it to implement rules for a sort of game-of-life: let (x_new, y_new) = match neighbours { Neighbours ((_,_,_), (_,_), (_,o,_)) =&gt; ( x, y+1 ), Neighbours ((_,_,_), (o,X), (_,X,_)) =&gt; ( x-1, y ), Neighbours ((_,_,_), (X,o), (_,X,_)) =&gt; ( x+1, y ), Neighbours ((_,_,_), (o,o), (_,X,_)) =&gt; {if y%2==0 { (x-1, y) } else { (x+1, y) } }, _ =&gt; ( x, y ) };
Will this work as well? `match (tup.0[], tup.1[]) {`
I am surprised to see no one found anything better in C++ compared to Rust. Rather, everyone asked for a proof on my subjective comment. I know it is a Rust forum, but let's be open minded. In fact, I am learning Rust and Servo (https://github.com/servo/servo) so I can contribute to this great initiative. 
Do you not remember the bad old days when a poorly written program would crash the entire system just because M$ was lazy? Or do you still think Win 95 is a great OS because it doesn't have all that complex error handling? In C, it's not really that hard to write robust code, it's just pretty tedious sometimes. Basically, if you can, you avoid calling functions that can fail (like malloc), and if you can't, you check for errors and handle them. (that is the tedious part). You also get to sanity check all data coming from the user (or other untrusted sources), and you probably don't get to use recursion, so that you can set a proper stack size. I don't see why robust code in Rust would need to be any more complex than in C. I also don't understand why you think memory allocation could happen anywhere. Rust is designed as a systems language, so memory allocation should only happen when you allocate memory. If Rust would implicitly introduce sources of errors in random places, it would be rather useless as a systems language.
I automated a bunch of stuff when learning Python; like fetching wallpapers from /r/EarthPorn. But I want to do something not web related, like a game may be. But I have no idea where to begin. 
No, HashMap is O(n) and Omega(1). TreeMap is O(log(n)) and Omega(log(n))
Having something in mind is great. What kind of game should it be? Are you interested in graphics or more interested in the game rules?
There's also another subreddit for game development in rust /r/rust_gamedev.
Eventually I would like to be able to build a 3D like Super TuxKart for example. But as a beginning idea I would like to build something simpler like a pacman clone or something to learn the basics. And then something 2D with more physics involved; like a 2d game with jumping around and shooting stuff like Mario. I don't know if that makes sense. But that's what I have in mind for now.
Thank you. I'll definitely keep an eye on that subreddit. Edit: Looks really useful. Thanks again.
Huh, pretty sure Rust is beating C++ in over half of the benchmarks the last time I checked http://benchmarksgame.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=rust&amp;lang2=gpp&amp;data=u64q
There are so many low level things to trip on with gamedev that I feel like a person can't start too small. Go for pong! I find /r/gamedev is a supportive community with a good sidebar for getting started. A reasonable progression for 2d games might be something like this: * pong * tick tack toe * tetris * pacman * mario From there you'll probably feel comfortable starting topics for 3d games. You'll want to learn some elementary linear algebra. 3d graphics needs quite a bit more math than their 2d counter parts. Udacity has a 3d programming course. It's in javascript. So you might start taking that now while you are working on the 2d games. And so then by the time you finish the 2d games you can reimplement the homeworks from the udacity class in rust.
It should be noted that the performance characteristics for these maps are *very* sensitive to specific size, as well as access distributions. For instance, TrieMap is an absolute *beast* at linear insertion (1,2,3,...,n). This was a problem when I was trying to normalize the benchmarks a bit to get some comparisons in the standard libs. For a more robust analysis, I'd like to see several access/insertion/deletion patterns as well as "size classes". See [our own benchmarks](http://doc.rust-lang.org/src/collections/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libcollections/btree/map.rs.html#1362-1456) for an example of this.
A standard chaining-with-linked-lists HashMap is O(1) *amortized* (expected assuming a decent hasher etc etc). Most would agree that's the most relevant measurement for a collection. And it's legit O(1) worst-case if you use our capacity management functionality right. You can also do the-thing-no-one-ever-wants-to-do and have chaining with BSTs for O(log n) worst-case even with a bad hasher. However we use a Robinhood Hashmap, whose performance characteristics are more strongly tied to load-factor. It's a bit fuzzier. I'm vaguely aware of an O(loglogn) expectation as described [in this paper](http://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CB8QFjAA&amp;url=http%3A%2F%2Fcglab.ca%2F~morin%2Fpublications%2Fhashing%2Frobinhood-siamjc.pdf&amp;ei=y9xwVNHBDvK1sQS9iILABA&amp;usg=AFQjCNGsx_2hYRXCMlioUL0tgiSJ8dxpGg&amp;sig2=Rhv_Suw3gTUINNBqn5JpWA&amp;bvm=bv.80185997,d.cWc). Which is, admittedly, O(1) for any reasonable input. (loglog(2^64 ) is like 6.)
I need to ask: What are you trying to measure? (Please don't answer performance ;-)) Average insertion time?. It's not clear from the code. I also see a systemic error: The work per iteration is not constant, instead is increasing with the number of iterations, because your collection has a different size on each iteration. You can see that if you plot [the average time vs the number of iterations](http://i.imgur.com/e1fv2qU.png): the sample is heavily skewed to the right. This also means that the reported average time will depend on how long you measure. These are the results if you measure for [60 seconds](http://i.imgur.com/RmqFnDL.png): the average times got larger. The previous plot were the results for 5 seconds of measurement time. Measuring only the insertion time is a tricky: you have to avoid changing the size of the map, but measuring both insertion and removal is straightforward: // Start with a map filled with N elements // (For real-world-ness: the initial elements and `idx` should be randomized) // (Also the HashMap should have a different capacity on each run) // (Ideally `idx` should be random on each iteration, etc) let mut m: HashMap&lt;_, _&gt; = range(0, n).map(|i| (i, 0u)).collect(); let idx = n + 1; b.iter(|| { m.insert(idx, 0u); m.remove(&amp;idx); }) You should benchmark that for different values of `n`. These are some results for a [HashMap](http://i.imgur.com/wIiAIYE.png) and a [TreeMap](http://i.imgur.com/8AY3HxO.png). HTH (Shameless plug: The plots were generated using [criterion](https://github.com/japaric/criterion.rs) and [this program](https://gist.github.com/japaric/c16a573b8e371697c9f1#file-main-rs), you'll need to have gnuplot installed) (And in case you were wondering [this](http://i.imgur.com/zjVzmZX.png) is how a proper sample usually looks like: Like a unimodal gaussian distribution + some outliers)
[nickel.rs](http://nickel.rs/) seems like the start of what you're looking for.
Remember, 1.0 is about a stable base, not about being finished! HKT should be backwards compatible, so getting them in a 1.4 or 1.5 release would be totally fine :)
As the error message says, a variant `ElementType::$name` is not a type but a value. (When the variant has arguments, it refers to a constructor function.) You should have distinct dummy types (will be not trivial to do so, though, as `concat_idents!` doesn't work well with definitions) that implement a common trait instead.
assert would be confusing, since in other languages that stuff isn't compiled-in in release builds. Anyway.. when I first looked at rust (not so long ago), the unwrap() also confused me as to it's function.. personally I was thinking something like .v() or .value() would fit better, but that doesn't convey what it does when there is no value (None). 
Yes agreed. For the "expectation" I noted I intended "make a random hashmap, what's the expected worst-case perf". But I suppose you could also interpret it as "make a random hashmap, what's the expected expected perf". That is, what's the expected perf of a query that is *also* random. (I've only skimmed the paper, the math itself seemed over my head. Or at least, too much effort to parse for how much I actually cared.)
Here's the current master's [raw benchmark output](https://gist.github.com/Gankro/840b4a5eed0824d15d2b). Here's the relevant parts for this post: btree::map::bench::find_rand_100 ... 84 ns/iter (+/- 5) btree::map::bench::find_seq_100 ... 82 ns/iter (+/- 2) btree::map::bench::insert_rand_100 ... 231 ns/iter (+/- 5) btree::map::bench::insert_seq_100 ... 439 ns/iter (+/- 6) btree::map::bench::find_rand_10_000 ... 159 ns/iter (+/- 3) btree::map::bench::find_seq_10_000 ... 109 ns/iter (+/- 1) btree::map::bench::insert_rand_10_000 ... 619 ns/iter (+/- 8) btree::map::bench::insert_seq_10_000 ... 574 ns/iter (+/- 3) btree::map::bench::iter_1000 ... 34236 ns/iter (+/- 892) btree::map::bench::iter_100000 ... 3472849 ns/iter (+/- 176574) btree::map::bench::iter_20 ... 714 ns/iter (+/- 44) tree::map::bench::find_rand_100 ... 81 ns/iter (+/- 5) tree::map::bench::find_seq_100 ... 78 ns/iter (+/- 2) tree::map::bench::insert_rand_100 ... 120 ns/iter (+/- 7) tree::map::bench::insert_seq_100 ... 483 ns/iter (+/- 8) tree::map::bench::find_rand_10_000 ... 163 ns/iter (+/- 3) tree::map::bench::find_seq_10_000 ... 112 ns/iter (+/- 2) tree::map::bench::insert_rand_10_000 ... 979 ns/iter (+/- 16) tree::map::bench::insert_seq_10_000 ... 856 ns/iter (+/- 19) tree::map::bench::iter_1000 ... 23934 ns/iter (+/- 371) tree::map::bench::iter_100000 ... 6968887 ns/iter (+/- 388981) tree::map::bench::iter_20 ... 506 ns/iter (+/- 66) Somemeone refactored triemap's benches, so we've lost these :( trie::map::bench::iter_1000 ... 43388 ns/iter (+/- 4914) trie::map::bench::iter_100000 ... 2720104 ns/iter (+/- 218634) trie::map::bench::iter_20 ... 636 ns/iter (+/- 355) vec_map::bench::find_rand_100 ... 35 ns/iter (+/- 1) vec_map::bench::find_seq_100 ... 34 ns/iter (+/- 0) vec_map::bench::insert_rand_100 ... 61 ns/iter (+/- 0) vec_map::bench::insert_seq_100 ... 56 ns/iter (+/- 1) vec_map::bench::find_rand_10_000 ... 35 ns/iter (+/- 1) vec_map::bench::find_seq_10_000 ... 34 ns/iter (+/- 0) vec_map::bench::insert_rand_10_000 ... 65 ns/iter (+/- 0) vec_map::bench::insert_seq_10_000 ... 56 ns/iter (+/- 1) // Hashmap's benches are totally different As you can see, we should only ever use VecMap ;) (unf-- dat space-time tradeoff) Also of course I am required by law to point out that BTreeMap scales better than TreeMap, and you really should default to it as a general-purpose sorted-map :P A note: BTreeMap has a handicap in micro benchmarks. It's pretty much the only file in all of std that hasn't been blindly `#[inline]`d. There's also [a PR](https://github.com/rust-lang/rust/pull/18028) that's been in the queue for almost two months that optimizes it substantially, at the expense of lots-and-lots of unsafety. The other maps aren't exactly optimized either, though! Vec and HashMap are our only two *really* nice collections. Also if there are any benchmarks gurus who want to cleanup our benches, that would be super cool!
Hey no worries! This is how we all learn to be better programmers every day. My workflow consists pretty much entirely of thinking of something and getting ripped to shreds by people (or compilers) that are smarter than me. :D Benchmarking is a notoriously frustrating endeavour. Benchmarking collections is even trickier, since they're so complex! 
I may have misunderstood the problem, but this can be solved in a different way in Rust: enum Errors { ERROR1, ERROR2, ERROR3 } impl Errors { fn english(&amp;self) -&gt; &amp;str { match *self { Errors::ERROR1 =&gt; "error1 msg", Errors::ERROR2 =&gt; "error2 msg", Errors::ERROR3 =&gt; "error3 msg" } } fn french(&amp;self) -&gt; &amp;str { match *self { Errors::ERROR1 =&gt; "error1 in french", Errors::ERROR2 =&gt; "error2 in french", Errors::ERROR3 =&gt; "error3 in french" } } }
That's exactly what I was looking for ! This is the equivalent of the java like enum, event though the the java synthax (you can initiate all the constant value alongside the enum item). Thanks.
Great! You could write a macro to achieve a more Java like syntax if you think it's more convenient, but I, personally, like this method more because of how everything is grouped.
Note that Rust doesn't actually use the original robin hood scheme but something called "robin hood with backshift deletion". The difference is when removing an entry we shift later entries back to fill the gap if they aren't in their ideal position. This makes deletes (and inserts!) more expensive but ensures a low probe count as well as a consistent probe count. The original design only ensures a consistent (but possibly very high) probe count and assumes a clever lookup will take advantage of that.
That is because the compiler does not know how big an arbitrary struct implementing Fooable will be. Hence you can't put it as a value into the struct instead you have to use an indirection (it does not need a Box, &amp;Fooable is good enough). Furthermore, you could use generics instead: struct Baz&lt;T: Fooable&gt;(T); 
Short version: `&amp;'a Fooable + 'a`. That's because the type implementing `Fooable` could contain references, and the compiler needs to know how long any such *hypothetical* references are valid for. For example, it'd be unsafe to store a reference to `Fooable` with lifetime `'a` if it contains a reference of lifetime `'b`, and `'b` is *shorter* than `'a`. So, you need to tell the compiler what it can assume about the contents of the type that implements `Fooable`. You could use any of: struct Baz&lt;'a&gt;(&amp;'a Fooable + 'a); struct Baz&lt;'a, 'b&gt;(&amp;'a Fooable + 'b); struct Baz&lt;'a&gt;(&amp;'a Fooable + 'static); That second one is *unlikely* to ever actually be useful, though; assuming `'b` is *at least* as long as `'a`, you should be able to just use `'a` everywhere. The last one means "the thing that implements `Fooable` can contain references, but they must be valid for the entire lifetime of the program. The first means "the thing that implements `Fooable` can contain references that live *at least* as long as `'a`."
&gt; When the variant has arguments, it refers to a constructor function. interesting. so in `enum Foo { Bar { u: u8 } }`, “Bar” isn’t a type while in `struct Bar { u: u8 }` , it is? &gt; You should have distinct dummy types sorry, i can’t follow you. what should be distinct types?
i just noticed that i botched the title. it should be much more expressive than that, but i seem to have hit the wrong key at some point -.-
this post is a big bait, also generating traffic towards you blog is good, I assume?
That's a great explanation. Thank you. I'm going to guess that Fooable could contain references to a whole bunch of different stuff, And I may actually need something like struct Baz&lt;'a&gt;(&amp;'a Fooable + 'b + 'c + 'd +'e ...etc); I can't think of an example where that would come into play, but still. As you say, that's unlikely to ever be useful - a' is likely the lifespan I need. Thank you again. 
Rust doesn't come bundled with an http implementation, so you have to use third party libraries. [Hyper](https://github.com/hyperium/hyper) has an http client and is probably what you are looking for. Full disclaimer: as it says in my flair, I am one of the primary authors of hyper. Feel free to ask any questions you might have.
In the second case, would you need `'b: 'a` to express that `'b` should live at least as long as `'a`?
Dunno. I've seen some cases where I *assume* the borrow checker has been smart enough to work that out. You'd have to try and see. :P
Yes, if one uses: #![feature(slicing_syntax)] [Rust playpen](http://is.gd/kvVaYa)
See [this bug](https://github.com/rust-lang/rust/issues/19177) for documenting this technique.
You'll never need more than &amp;'a Fooable + 'a because the + 'a means that 'a is the lower bound on all lifetimes within the implementing struct. So, even if the struct contains ten references, all with different lifetimes, they will all last at least as long as 'a.
Disadvantages? Well, things are still in flux at this level of maturity. And C++ can do some things Rust can't. For example: Non-type and non-lifetime template parameters. I also still tend to miss overloading and ADL. It feels like Rust traits system is a bit more "rigid". On one hand traits are nice in the sense that there is modular type checking and the compiler is able to blame someone (the author of generic code or the user of it). But on the other hand, it seems more rigid in the sense that traits require a bit more foresight in the design and last time I checked it wasn't possible to implement trait X from crate A for type Y from crate B in crate C. I understand, this was a deliberate design choice. And so far I didn't feel the need to do that. Instead of comparing optimized versus non-optimized implementations on some microbenchmark, you should probably check out the language itself w.r.t. design choices regarding any kinds of unwanted overhead to get an idea of where Rust is headed.
I know you asked about HTTP requets, but your title says web crawler. You could use [html5ever](https://github.com/servo/html5ever) for parsing.
Another that hasn't been mentioned is failing to make new thread or process
But it wasn't Markdown back at those days.
Oooh, this is insidious. Here's the thing, the `normalize` function doesn't do what you think it does. Specifically, here's what you've *actually* written: pub fn normalize&lt;'a&gt;(&amp;'a mut self) -&gt; Result&lt;(), &amp;'a str&gt; Lifetime elision causes the compiler to assume that the `&amp;` in the output is supposed to have the same lifetime as the `&amp;` in the input, which is *usually* the case. This, in turn, leads the the freestanding `normalize` to mean: pub fn normalize&lt;'a, T: ..&gt;(v: &amp;'a Vec3&lt;T&gt;) -&gt; Result&lt;Vec3&lt;T&gt;, &amp;'a str&gt; In this case, the `'a` is simplified to basically mean "the duration of this call", which is what the "anonymous lifetime #1" thing is about. What you need to do is use `&amp;'static str` instead of `&amp;str`, which is what the *actual* type of a string literal is. Making that change gets the code to compile.
Changing the result types from -&gt; Result&lt;(), &amp;str&gt; -&gt; Result&lt;Vec3&lt;T&gt;, &amp;str&gt; to -&gt; Result&lt;(), &amp;'static str&gt; -&gt; Result&lt;Vec3&lt;T&gt;, &amp;'static str&gt; seems to satisfy the compiler. I assume that this is due to [lifetime elision](https://github.com/rust-lang/rfcs/commit/704f0060176418659698eb63642e2071b109e029?short_path=6b90c47#diff-6b90c47efb814adb642039adecdde442) (so the signatures were actually e.g. `fn normalize&lt;'a&gt;(&amp;'a mut self) -&gt; Result&lt;(), &amp;'a str&gt;`).
Thank you for your detailed explanation.
So long as you have `#![feature(tuple_indexing, slicing_syntax)]` on the crate.
`as_string()` doesn't allocate by itself though!
I'm not worried, I'm just pointing out that the performance test is meaningless. Like comparison of quicksort vs. gzip. BTW, tree_map is not ordered, it is sorted.
Rango? :) I'm not so sure if going the Django route makes so much sense in 2014, when the future seems to be heading more towards rich clients connecting to smart APIs. It might be a good idea for a modern server side framework to focus on making creating APIs really nice and robust, and forget about templating.
Also note that if you need to interface with C, it's possible to specify values for simple enums: #[repr(C)] pub enum Encoding { ISO_8859_1 = 0, // ASCII ISO_8859_2 = 1, // Latin2 ISO_8859_3 = 2, // ISO_8859_4 = 3, // Latin4 ISO_8859_5 = 4, // ISO-8859-5 ISO_8859_6 = 5, // Arabic ISO_8859_7 = 6, // Greek ISO_8859_8 = 7, // Hebrew ISO_8859_9 = 8, // ISO_8859_10 = 9, // JAPANESE_EUC_JP = 10, // EUC_JP // ... SOFTBANK_ISO_2022_JP = 74 } This can then be mapped to the integer type expected by the underlying foreign FFI using `Encoding::ISO_8859_9 as u32` (or whatever). But you wouldn't normally do this in idiomatic Rust code. Still, it's helpful when interfacing to other things.
The code in `Error.name()` and `Error.message()` is identical except for the name of a variable, perhaps you can put the shared code in a (private) helper function. And I think they could return &amp;str instead of Option&lt;String&gt;, e.g.: // in impl Error: fn name(&amp;self) -&gt; &amp;str { c_str_as_str(&amp;self.e.name) } // Somewhere else fn c_str_as_str(cstr: &amp;*const c_char) -&gt; &amp;str { if cstr == &amp;ptr::null() { "" } else { from_utf8(unsafe { std::mem::transmute::&lt;_,&amp;[u8]&gt;(Slice{ data: *cstr as *const u8, len: strlen(*cstr) as uint, }) }).unwrap_or_default() } } In `iter_append`, you have many lines like this: &amp;Bool(ref b) =&gt; self.iter_append_basic(i, *b as i64), You could just do &amp;Bool(b) =&gt; self.iter_append_basic(i, b as i64), Also in `iter_append` you have this line: let c = s.to_c_str(); let p: *const libc::c_void = std::mem::transmute(&amp;c); that could probably be rewritten to something without transmute: let p = s.as_slice().as_ptr() as *const libc::c_void;
You may also consider doing this: http://is.gd/nc0tIb One thing Im particularly opposed to is the "magical 'static" where adding 'static to code just 'makes it work'. Happens a lot for closure / trait / lifetime things. I think the thing to understand in this example is that youre implictly asserting the &amp;str return has the same lifetime as your clone. Therefore, you cannot return it *without* returning the clone, because it cant exist when clone no longer exists. If your function always returned clone, it would work as is. By using 'static (or simply 'b), youre asserting the &amp;str and clone have *different* lifetimes, which makes the return fine. ...of course, in this case your return *is* a static string, so you may as well use 'static; but the point remains that Vec3::normalize as it currently stands returns a &amp;str which has the same lifetime as the instance of Vec3.
I'm kind of in the same boat as you are. I come from the same Python and web development background and just starting to learn Rust. Let me know if you (or anyone else for that matter) wants to hang out and code something together.
malloc returns what you set overcommit to return.
Note that there is `std::c_str::CString` which you could use
No, what you suggest is not possible at this time. All return values of a function must be of the same type. A workaround would be to return an enum, e.g.: enum AorB { AA(A), BB(B), } impl MyImportantTrait for AorB { fn some_method(&amp;self) -&gt; bool { match *self { AA(ref a) =&gt; a.some_method(), BB(ref b) =&gt; b.some_method(), } } } fn get_important_trait(i: int) -&gt; AorB { match i { 1 =&gt; AA(A{i:2}), _ =&gt; BB(B{x:4}) } } 
[Oh really](https://github.com/diwic/dbus-rs/blob/master/src/lib.rs#L5)? ;-)
Can't help you with the details but I think that there'are usually two ways to get rid of verbosity: better abstractions and macros. Sorry for the lame comment, I really just wanted to fix your last sentence:( "without using the flexibility" -&gt; "without losing the flexibility"(?)
Fixed, thanks. I will think about how I could use more macros.
Thanks! I implemented some of these suggestions now (already in repository) &gt; The code in Error.name() and Error.message() is identical except for the name of a variable, perhaps you can put the shared code in a (private) helper function. And I think they could return &amp;str instead of Option&lt;String&gt;, So I did half of this by returning an Option&lt;&amp;str&gt;. That made fit nicer with the Error trait implementation. &gt; &amp;Bool(b) =&gt; self.iter_append_basic(i, b as i64), Right, because bool, i64, etc have copy semantics, there can't be any "can't move out of reference" problems here. I think. &gt; let p = s.as_slice().as_ptr() as *const libc::c_void; This compiles, but causes runtime failure. The lack of to_c_str() makes the string not null terminated. Also notice the "&amp;c", the dbus function wants a **const char as *const void.
&gt; You may also consider doing this: http://is.gd/nc0tIb can 'b be anything other than 'static in this example? i mean, assuming the return value wasn't a string literal, what else could it be?
&gt; I ran into the problem that by implementations become extremely verbose and changing a tiny trait parameter in one file will force me to apply many changes in other files. Yes. From my own experiments with maths in rust, I have found my self wishing trait bounds were optional.. add them if/when the error messages become problematic. This is basically why I lost some enthusiasm for the language when I discovered this is outside of the language's 'vision', not just missing features. (which is a shame because there's so much about the language - 90% - that I have always had great enthusiasm for). ( I've done things in the past building abstractions for different low level representations of similar types &amp; was left fearing what this would turn out like in Rust. e.g. a variation of a type is logically identical to another but optimised for certain operations, so some operations are missing, and there are variations of types that signify intermediate states that library users don't care about ) IMO When you have lots of single function traits (and you frequently want to group them together in different permutations) they just get in the way. I have 3 suggestions for the language [1] Maybe it is possible to workaround by defining larger traits to begin with, and having many unimplemented defaults. The downside is that leads to runtime failures when you'd have prefered a compile time error (i.e you're suppose to remove the calls that produce the runtime error..) - perhaps the language could gain a version of "panic!()" that produces a compile time error if the function is even called. Then you'd have the best of both worlds, and the behaviour thats outside of the original 'rust vision' is available but 'not default'. **EDIT** it seems this might be related to **negative trait bounds** (from comments below) however looking at the RFC that refers to extra detail in the trait system, not an adhoc 'this isn't supported' [2] at least allow changing the order of `impl &lt;trait&gt; for &lt;type&gt;` to `impl &lt;type&gt; : &lt;trait&gt;` - because in the single function trait the types read the same as the signatures, so its' less mental hoops to jump through [3] just allow adhoc overloading and make trait bounds optional. (I realise this will never happen, its too far from the design &amp; philosophy). Another benefit would be greater interoperability with existing C++ apis which would surely accelerate language adoption. it could be feature-gated, not part of the official spec. r.e. macros - they probably can help but IMO it shouldn't be necessary. After watching Jonathan Blows videos' I've given in to the NIH temptation to start my own language experiment... semantically more like C++ (so C++ prototypes can be translated 1:1) but superficially looking like Rust - with the unambiguous grammar &amp; cleaner expression based syntax I like so much. The basics seem to work now but I realise its' a futile exercise; I'm going to run into the same problem drawing me back to C++ - I'm not going to be able to produce the same tools support (IDE autocomplete, source level Debugger..) and the complexity will probably explode soon enough and I'm unlikely to get collaborators for yet another 'pet language'. At the minute my syntax has already diverged, := and foo[T](), but I might aim to read a subset of Rust programs, maybe let what I've done work as a transpiler. Some subset of C++ &amp; Rust should be a valid subset of my pet language, and some subset should be auto-translateable to Rust by inferring the trait-bounds. Imagine a fork of Rust or a fork of a C++ compiler with a different syntax &amp; new sugar - think of the .. [SPECS proposal](http://www.csse.monash.edu.au/~damian/papers/HTML/ModestProposal.html) - perhaps in future when C++ gets modules it might get easier to do such a thing - imagine a spectrum of languages (like there are many LISP 'dialects'..); The language I want is somewhere there: and there's no need to throw away existing source bases to use it
Oh you're absolutely right about that null termination, totally forgot about that :)
You can just write a trait that inherits all those trait bounds if you want to require them everywhere. Then you can implement it for all types with those traits using generics. Rust doesn't need new features to make this less verbose. I also find the new where syntax much more pleasant to read. trait All: A + B + C + D + .. { } impl&lt;T&gt; All for T where T: A + B + C + D + ... { } fn&lt;T&gt;(foo: T) where T: All { ... } This is how the `Num` traits work, for example. If you do want different bounds everywhere, then you kinda have to change the signatures, don't you? I mean, the signatures are different and Rust has explicit signatures. A good IDE would help resolve that, though (I agree that IDEs should not make up for language features, but I think explicit function signatures help *more* without an IDE). If you don't actually want type safe signatures and just want textual replacement, that's what macros are for... I don't understand why you're dismissing them here.
Oh, I see. I'm gonna try this. EDIT: Hmm, that doesn't really work. pub trait Semigroup&lt;S: Set&gt;: Magma&lt;S&gt; + IsAssociative&lt;S,S,S&gt; {} impl&lt;G:Semigroup&lt;S&gt;, S:Set&gt; Magma&lt;S&gt; for G {} pub trait Monoid&lt;S: Set&gt;: Semigroup&lt;S&gt; + HasIdentity&lt;S&gt; {} impl&lt;G:Monoid&lt;S&gt;, S:Set&gt; Semigroup&lt;S&gt; for G {} pub trait Quasigroup&lt;S:Set&gt;: Magma&lt;S&gt; + IsLeftInvertible&lt;S,S,S&gt; + IsRightInvertible&lt;S,S,S&gt; {} impl&lt;G:Quasigroup&lt;S&gt;, S:Set&gt; Magma&lt;S&gt; for G {} This raises `conflicting implementations for trait Magma`
the trait bounds repeat information you specify in the function body. `fn lerp(a,b,f){(b-a)*f+a} // this one line contains all the information // if have to specify what combinations of types it works for &amp; its longer than the impl ..maybe time for a new language or fix an old one ` macros are more verbose+different declaration/calling syntax, and it doesn't need to be a macro (why add that asymmetry?): its' a function you hope will inline ...and which may have specialisations for specific types. Leave the inlining decision to the compiler and have an optional override without changing caller syntax. I like writing lots of small functions, it clarifies whats going on. I understand why traits exist but they're not useful 100% of the time &gt; This is how the Num traits work, for example. &gt; `trait All: A + B + C + D + .. { }` &gt; `impl&lt;T&gt; All for T where T: A + B + C + D + ... { }` IMO thats' unecasery micromanagemnt - I've been through that process and concluded its' not worth it. Imagine if "All" was at least duck typed: if A,B,C,D exist, any :All+A+B+C is also automatically satisfied And I've seen Rust's evolution: those Numeric traits have shifted over time. The point is what happens when you're working on the unknown - you don't know how your program structure will end up - you want something fluid that can react to changing demands, and what you learn as you implement&amp; experiment. I realise Rusts' focus is "huge source bases"(multi million LOC) which maybe means more co-ordinated effort between huge teams of people. For clarity on the difference look at jonathan blows comments on the Rust module system .. he's clearly not stupid, he has significant acheivements, and for his use cases "priv" by default is a hazard not a benefit. (Maybe his difference in POV comes form dealing with 100's of klocs, not mlocs). In some ways I'm sure traits might also make consensus harder. +long_void in the piston engine was motivated to make his own simpler maths library "because he didn't want to figure out all the traits". But also traits are one centralised location, that 2 people must agree on (deference: "I accept maintainer X is now in charge of this concept"). They come with restrictions r.e. crates, who can implement what where. If they can simply work ad-hoc and rename what they did to match afterward, that might suit organic teams and emergent compromise. I realise the problem of people who do lots of string manipulation + for string concat, * for repeating.. and suddenly your maths code 'can be called on the wrong types'. Maybe another way to deal with that is a dedicated concat operator (binary ++?). or easier n-ary functions (concat(a,b,c,d)). a concat operator would disambiguate vec&lt;Num&gt; + vec&lt;Num&gt; from suggesting component wise operation. or just declare that 'multiplying a string' is taking things too far (or, just don't implement string-multiply for floats). if the language had the 'dont_allow_this_to_be_called!()' idea maybe you could provide a communicative error similar to a trait bound, but easier to organise .. operator*(s:string,x:float){error!()}
Great post, some notes: In the blog you mention you call .borrow() on Rc and then .deref on the RefCell. That's not what's happening; you're calling .borrow() directly on the RefCell (there is [no borrow method in Rc](http://doc.rust-lang.org/std/rc/struct.Rc.html)) thanks to autoderef: If you implement the traits Deref and/or DerefMut, your type acts like a smart pointer and you can call .some_method() on the underlying object directly. Then you're calling .deref() on the returned [Ref&lt;&gt; or RefMut&lt;&gt; objects](http://doc.rust-lang.org/std/cell/struct.Ref.html) which are proxies of the RefCell to keep track of whether the object is borrowed or not. The blocks should also not be necessary since the proxy Ref&lt;&gt; object dies 'at the semicolon'. In other words you should be able to replace ``` let last_keypress = { move_info.borrow().deref().last_keypress }; ``` with ``` let last_keypress = move_info.borrow().last_keypress; ``` Check out this [playpen](http://is.gd/linUsI).
I might end up using this! I do love tracebacks!
If e.g. `README.md` was rendered as Markdown (which would be useful anyway), people could use Travis’s status buttons.
&gt; I think this improves the situation somewhat, but comparing this to my StandardError experiment, it still requires way more bookkeeping. In which sense does it need more bookkeeping? The feedback generally was that explicit errors are a feature that people do not want to see going away. BTW: there is no reason a `StandardError` could not coexist with this. &gt; Are you intending for `FResult` to replace the Result type entirely? Don't think so, there might be good reasons for non error results. That said, `try!` might have to require errors or failures on the `Err` part.
This is amazing, and I'll definitely be using this.
cool, thanks for the details!
This looks good to me too! :) Havent read the code yet, just the interface.
I don't really mind having to be explicit about what the error type is, having to specify that is probably more inline with Rust's philosophy. It would be really nice to get away from having to implement casting for errors all the time. Every time someone implements their own error type they'll have to implement conversions from other error types, like IoError. That seems needlessly cumbersome. Ideally, creating a custom error type would be a single line. That's probably doable today with macros, but it feels like it shouldn't require Macros. But really, I'm nitpicking ;) This is great, thank you so much for putting this code together.
Basically you write `enforce!(x).is().equal(y)` or `enforce!(x).does().not().contain(y)` instead of the equivalents with `assert`. 
I think that [associated types](https://github.com/aturon/rfcs/blob/associated-items/active/0000-associated-items.md) will help a lot with this issue; take a look at the example for graphs. They're not quite working yet, but should be soon!
Not directly, but if you had a container perhaps: fn main() { let x = Bar; let y = Foos { f: Foo { s: String::from_str("Message goes here") } }; println!("{}", y.bar(&amp;x)); } #[deriving(Clone)] struct Bar; #[deriving(Show)] struct Foo { s: String } struct Foos { f: Foo } impl Bar { fn foo&lt;'a, 'b&gt;(&amp;'a mut self, f:&amp;'b Foos) -&gt; &amp;'b Foo { return &amp;f.f; } } impl Foos { fn bar(&amp;self, bar:&amp;Bar) -&gt; &amp;Foo { return bar.clone().foo(self); } } or something odd like: use std::kinds::marker::NoSend; use std::kinds::marker::NoSync; use std::mem::transmute; fn main() { let x = Bar; println!("{}", bar(&amp;x)); } #[deriving(Clone)] struct Bar; #[deriving(Show)] struct Foo { s: String } struct Foos { x: NoSend, y: NoSync, f: Foo } impl Bar { fn foo&lt;'a, 'b&gt;(&amp;'a mut self, f:&amp;'b Foos) -&gt; &amp;'b Foo { return &amp;f.f; } } fn bar(bar:&amp;Bar) -&gt; &amp;Foo { return bar.clone().foo(Foos::instance()); } impl Foos { fn instance&lt;'a&gt;() -&gt; &amp;'a Foos { unsafe { if INSTANCE == 0 as *const Foos { let tmp = box Foos { x: NoSend, y:NoSync, f: Foo { s: String::from_str("Message goes here") } }; INSTANCE = transmute(tmp); } return &amp;(*INSTANCE); } } } static mut INSTANCE:*const Foos = 0 as *const Foos; Really, what it comes down to is that &amp;str is awkward in rust unfortunately, because you constantly have to distinguish between &amp;'a str and &amp;'static str. Using error codes or String would work much better.
Thanks! That's awesome info =D I'll update my post =D
It's kind of odd seeing this long blog post, and not once seeing what the output assembly of the various options looks like. That could go a long way to explain why things behave as they do, along with performance counters.
&gt;C++ is a bit funny here of course with &lt;&lt;. It probably is the case more people want to read that as 'insert into stream' than 'bit shift', but again would n-ary functions have been a nicer solution .. output.write(a,b,c,d) instead of output&lt;&lt;a&lt;&lt;b&lt;&lt;c&lt;&lt;d). "&lt;&lt;" to me will always primarily represent bit shift. (i'm guessing rust would side with a macro here) The best solution is variadic templates. &gt;I dont hate either C++ or rust as much as he does , evidently. e.g. I don't mind the complexity in pointers so much (he doesn't even like C++ having references), and I like Rusts' lambda syntax. Jonathan Blow doesn't like pointers *or references*? Is he actually insane?
There is bindings to libcurl: https://github.com/carllerche/curl-rust
Huh, the performance regression is interesting
You cannot have cyclic references in both (provably) safe and optimal way, this is a consequence of the affine type system used by Rust. You can tackle this problem with two ways, by either giving up the (provable) safety or the performance. The former would involve an unsafe pointer and the latter would involve `Rc` and `Weak`. Still, I'm not sure why you need the pointer to the parent at the first place. Most tree operations are recursive and *you have a pointer to the parent in the previous level of recursion*. Making use of it will give both (provably) safe and optimal implementation. This equally applies to the immutable iterator. (I'm not sure about the mutable iterators, your mileage may vary.) Nitpick: `depth` and `print_node` need not be a consuming (`self`) method, as they can be called multiple times to the same tree. Consider making them an immutable (`&amp;self`) method.
So, when you want circular references (parent references child and child references parent), you have two options: 1) Use Rc&lt;&gt; and Weak&lt;&gt; to make reference counted objects, or potentially Rc&lt;RefCell&gt;. The Rc module has an example with "Gadget" and "Gadget owner" that implements circular reference. The downside to this approach is speed and memory usage being slower than just using Box. http://doc.rust-lang.org/std/rc/index.html 2) Go unsafe and use raw pointers. This will give you the same performance as C (which always uses raw pointers!), but you of course need to maintain the raw pointers yourself in order to avoid segfaults and so on.
Damn. I want to like Rust, but this stuff is just too much for me. I understand it, but I just don't want to have to deal with it. Maybe it will grow on me in the future, but at the moment it seems like busywork for little gain except supposed safety. 
Regarding the need of having the parent known to the child: It is to implement tree rotation from the leaves to the root. Of course I might be horribly wrong about doing it like this.
What's a problem with this code? (Note: Not tested. Might not work due to the moves, but the principle remains same.) fn rotate_left(&amp;mut self) { *self = match *self { // (a p (b q c)) to ((a p b) q c) Node { data: root, left: left, right: Some(box Node { data: mid, left: right1, right: right2 }) } =&gt; Node { data: mid, left: Some(box Node { data: root, left: left, right: right1 }), right: right2 }, // (a p _) to... what? I don't know. node =&gt; node, } } fn rotate_right(&amp;mut self) { *self = match *self { // ((a p b) q c) to (a p (b q c)) Node { data: root, left: Some(box Node { data: mid, left: left1, right: left2 }), right: right } =&gt; Node { data: mid, right: left1, left: Some(box Node { data: root, left: left2, right: right }) }, // (_ p b) to... what? I don't know. node =&gt; node, } }
&gt; Jonathan Blow doesn't like pointers or references? Is he actually insane? he's sane :) he likes raw pointers. He doesn't like wrapped smart pointers. he doesn't like the fact that C++ has references *in addition* to raw pointers ("two things for the same thing"). I disagree there, I know references are a useful distinction; He likes the idea of unique ownership to the extent that he's given his language a dedicated *! which is basically Box&lt;T&gt; or rusts' old ~T. I agree with him mostly: the simple syntax should be available for the features you're supposed to use most of the time. "modern C++"'s problem is its' shoe-horned into C, where the syntax is optimised for raw pointers &amp; low level work. if you're not supposed to use raw pointers, then it should be 'raw_ptr&lt;T&gt;' and *T is something safe. but they can't retrofit that. IMO Rust got it right originally with the sigils (and they should have generalised some sort of overloading rather than religiously purging anything nongeneral) The reason he doesn't like C++ stuff is actually quite sane: The complex data in games is mostly immutable preprocessed data structures (worlds, animation data etc), and simple buffers for dynamic entities. The complex stuff can therefore be concatenated into single allocations. The entity stuff doesn't need the generality of a heap allocator (think of threading) this is good for loading times, avoiding fragmentation,and data locality. std:: stuff just gets in the way of that. He's explained his language might get dedicated syntax for this ("joint allocations") its possible Rust macros could do this better than C++ at least... templates aren't really good at it since you can't parameterise identifiers. C++ makes it easy to switch between linklists and arrays, but round about 1999 games machines started getting really bad at pointer chasing and linklists became obsolete for 'production code', so there's no point having your pointer wrapped as an abstract iterator when you want to be really sure its not doing something crippling.
Totally spitballing here. I ran into a performance regression myself recently. I haven't investigated as much as you yet, but here's the code: https://github.com/BurntSushi/rust-csv/blob/master/src/bytestr.rs#L135-L145 I discovered this particular regression because I was stressing `HashMap` and `HashSet`, but nothing unreasonable (low millions of entries). At some point recently though, it started becoming dog slow. I realized that this happened after I switched code like `x.as_slice()` to `&amp;*x`. Once I switched back to `x.as_slice()` (in the pertinent `Hash` impl), things were rosy again. As the comment in my code says, I didn't investigate more. My guess is that there are spurious copies happening in `&amp;*x` that aren't happning in `x.as_slice()`. Food for thought...
Maybe the author isn't well versed in assembly? When I look at generated assembly I only get more confused than I already was :)
Is this still the case? It seems like a pretty basic thing to want to do with channels. I was trying to write my default new application with a language (a simple chat server) and ran smack into this problem (I want a message from one client to be written to all the other clients, which means I need to inform the threads that are handling writes to their streams), right after I ran smack into "no real non-blocking i/o". I really want to like rust, but this stuff seems pretty basic...
Ya, 40% regression is HUGE in any standard.
I ran all your benchmarks under [criterion](https://github.com/japaric/criterion.rs). (Here's the [script](https://gist.github.com/japaric/7fe70658d75a51930f1d#file-lib-rs) and here's the [full output](https://gist.github.com/japaric/7fe70658d75a51930f1d#file-output)) These are the results of the `copy_nonoverlapping_memory` case: Benchmarking bench_copy_nonoverlapping_memory &gt; mean [317.97 ps 320.38 ps] &gt; SD [3.0316 ps 9.5043 ps] Benchmarking bench_copy_nonoverlapping_memory_inline_always &gt; mean [317.16 ps 318.57 ps] &gt; SD [1.6473 ps 5.8108 ps] Benchmarking bench_copy_nonoverlapping_memory_inline_never &gt; mean [371.63 ns 374.29 ns] &gt; SD [1.5266 ns 12.460 ns] (ps is picoseconds and ns is nanoseconds) I'm pretty sure the compiler optimized away your code in the `inline`/`inline(always)` cases (picoseconds range only occurs when measuring no code, like `b.iter(|| {})`), so you shouldn't compare against those. Comparing the `inline(never)` cases makes more sense. Next I produced [this plot](http://i.imgur.com/Euw0tjC.png) comparing all your buf writers in the `inline(never)` case. (With [this other script](https://gist.github.com/japaric/e383ef056f5b9c8f2513#file-main-rs)) All these writers seem to be on the same ballpark as the copy_nonoverlapping one: buf_writer_{8, 9, 6, 1, 13, 4, 2} and the unsafe_writer Disclaimer: I can't comment on the implementations, all I did was measure :-) EDIT Just noticed that the plot is missing the labels on the X axis (bug in criterion). The X tics should be: 300, 400, 500, 600 and 700 nanoseconds
OK, so measuring `inline(never)` doesn't make much sense either because the function call overhead is muddying the measurement. So I redid the benchmarks for the `inline(always)` BUT filtered out the measurements that reported picosecond range (ie got optimized away): unsafe_writer, non_overlapping and buf_writer7 Here's the [new plot](http://i.imgur.com/f8ZL7fM.png) (produced with [this script](https://gist.github.com/japaric/23c9a2e5088942492112#file-main-rs))
I meant to dive into llvm bitcode and assembly (although I'm really am quite quite rusty with it) then I sort of stumbled into a fast version. Part of the reason I wrote the post was I was hoping someone could explain what exactly was going on :)
I chose those completely arbitrarily :) I just wanted some configuration. That didn't take too short or long to run. I'm not sure what the statistically best number to aim for is, but a spread from 50 to 1000ns seemed like a good spread to me. Yeah, it'd be fun to see how they look plotted out. I do know that when I used a SRC_LEN=128 and a BATCHES=128, inlining stopped mattering. But that was the extent of my experimentation with the numbers. 
why a separated lib? This should fit very well in hyper itself.
That's true, I think the `Deref` Did land between when when I started my benchmarking and now. I wonder if it's related.
&gt; I wonder why a lot of yours seemed to run roughly the same speed? That's because the plot in the parent comment uses the `inline(never)` variant. It seems the function call overhead keeps the times bigger than 300 ns. If you see the plot in my other comment where I use the `inline(always)` variant then some Writers go into the sub 100 ns range. &gt; Maybe it's a OS/cpu thing? I've tested on a MacBook Pro, which has a Core i7. What kind of computer do you have? I have a Lenovo Y510P, it's a core i7-4702MQ. I wonder it's the hardware is the responsible for the `nonoverlapping_copy` and other writers optimizing away in my measurements. Supposedly the LLVM backend is the same.
Rust has no runtime besides the usual minimum that is similar to C language runtime and there is none planned. I do not see why you need one. Rust code should be at least as memory safe as Java code. So if you manage to exploit program to run arbitrary code in Rust you should be able to do the same in Java and then you can do any syscalls you wish. Process sandboxing can be used for both Rust and Java.
&gt; Consumes more memory … I'll love to see how we fare The benchmarks game shows [memory used](http://benchmarksgame.alioth.debian.org/play.html#memory) by each program.
The way runtime safety can help is by reducing surface-area for audits. Let me give an example: Suppose an application written in Rust uses a library also written in Rust for a pure computational task (say CSS parsing of a string). Now, from a security point of view, it is not sufficient to know whether the operations performed in the library are memory safe. We also need to know, whether due to malice or negligence, the library is doing something unintended (such as reading a file or making a network request). That check is traditionally done by auditing the code of the library. If however, the runtime is able to sandbox the library, there is no need to audit it! You just give it the right permissions and let the runtime do its job. The above is a case of the library code itself having a vulnerability. There can be other scenarios. Say, the library calls some unsafe code and that code corrupts memory (maliciously or otherwise). Now, when we are back in safe zone, the app might be compromised. In summary, runtime safety can help reduce auditing effort.
How are you ensuring that your benchmarks get run on a "quiet" core, i.e. one on which you are sure the scheduler will never run another process? Is there a tool to make that easy in rust yet? I've seen it done on linux at the kernel-config level, but I bet there is also a way to set it up post-boot as long as you have root permissions. Once I sat down and tried to figure it out, but it seemed like there was still too much churn in the relevant parts of the standard library, and I figured whatever I wrote would break before anyone used it. Also, what's the big picture here? Is the point of all this scary code that you're having to write just to implement an efficient memcpy(3) that ~users won't have to because composable higher level abstractions (i.e. iterators) will usually be fast enough?
Rust's equivalent would be to audit everything that uses unsafe but provides a safe interface.
As long as the library is written without ever invoking unsafe surely that's as a safe as running on a safe runtime.
That is true but requires confidence in JVM sandbox. However, you won't get help with auditing from Rust. EDIT: On the other hand you can modify stdlib to do security checks in IO operatiaons and annotate each task with permissions. Then you only need to audit unsafe code in libraries as sigma914 mentioned.
You're boxing Nodes anyways, so, you could use vtables and trait objects too. Right now, you use one enum and one function to represent them all. You could alternatively, albeit quite verbose, use a trait and many structs for each *Node* and impl the trait (with eval) for each of them. You may want let x = vec![1, 0, 0, 0]; instead.
Do you mean file i/o calls are treated as unsafe in Rust? Genuine question. If no, then I am afraid you are not understanding my concern.
&gt;The Java runtime has a neat but little known feature called the Security Manager. Once a security manager is installed, any significant operation such as disk i/o, network i/o or execution of native code is audited. If the currently executing code stack doesn't have the requisite permission an exception is thrown. You could probably achieve something like it at compile time by creating a custom lint and simply not allow any imports from std::io except certain IO-related modules.
It won't work. I am copying my reply to sigma914: &gt; the first case of unintended file i/o can't be covered purely at compile-time. The call might be happening only under an if condition, say when a flag in the library is enabled dynamically. Hence, a check for this has to happen at run-time or human audit time. ... either that, or the library writer will be severely constrained.
I wonder whether checks could be done on the crate objects. (I'm not up to speed on the details, though). For example, if a Rust-compiled crate claims to not do any I/O, then there should be no calls to I/O functions from that crate object. In addition you could require that no syscalls are made directly from that crate (checked by scanning the machine code?) Then a requirement that there is no unsafe code to get around these checks. Seems like patching a leaking dam, but the JVM security approach is no more secure, only they've had more time and experience to think of all the ways people can get around their restrictions. (Plus security scares every time they missed a hole.)
From compiler theory, it boils down to the [Halting problem](https://en.wikipedia.org/wiki/Halting_problem). It is a theoretical limit on what can be analysed at compile time. You can't write an algorithm that can analyse the behavior of the universal set of codes in a sufficiently expressive language.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Halting problem**](https://en.wikipedia.org/wiki/Halting%20problem): [](#sfw) --- &gt; &gt;In [computability theory](https://en.wikipedia.org/wiki/Computability_theory_(computer_science\)), the __halting problem__ is the problem of determining, from a description of an arbitrary [computer program](https://en.wikipedia.org/wiki/Computer_program) and an input, whether the program will finish running or continue to run forever. &gt;[Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing) proved in 1936 that a general [algorithm](https://en.wikipedia.org/wiki/Algorithm) to solve the halting problem for *all* possible program-input pairs cannot exist. A key part of the proof was a mathematical definition of a computer and program, which became known as a [Turing machine](https://en.wikipedia.org/wiki/Turing_machine); the halting problem is *[undecidable](https://en.wikipedia.org/wiki/Undecidable_problem)* over Turing machines. It is one of the first examples of a [decision problem](https://en.wikipedia.org/wiki/Decision_problem). &gt;[Jack Copeland](https://en.wikipedia.org/wiki/Jack_Copeland) (2004) attributes the term *halting problem* to [Martin Davis](https://en.wikipedia.org/wiki/Martin_Davis). &gt; --- ^Interesting: [^Computability](https://en.wikipedia.org/wiki/Computability) ^| [^NP-hard](https://en.wikipedia.org/wiki/NP-hard) ^| [^Microsoft ^Terminator](https://en.wikipedia.org/wiki/Microsoft_Terminator) ^| [^Chaitin's ^constant](https://en.wikipedia.org/wiki/Chaitin%27s_constant) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cmbote2) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cmbote2)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Invoking rust-bindgen as part of the build is an option (if I do it just here, then it might not work on other archs), but I was thinking about rust-bindgen functionality directly built into the rust compiler...
Seems to me very similar to the idea of Google's PNaCl. Are you trying to clone it? 
Ah icic, I was thinking about going down that route. The main part that's getting me stuck though, is how I would pass a bound value for "x" all the way down the computation graph. Would we just use a hashmap for this?
I think you need something similar to chamber, url: https://github.com/brson/rust-chamber , a domain specific compiler, Or use a compiler plugin to add some permission checking code at all vulnerable call position, like ...eh sorry I forgot the name of library which gives you ability to attach tags like"#[precond(x&gt;0)]" to "fn vul_code(x) {}", and checking it at call site. 
Can you give an example of an attack you're trying to prevent? I don't understand the threat model here.
&gt; The way runtime safety can help is by reducing surface-area for audits. Let me give an example: Not really. Python has a runtime and no sane person would say that it reduces the surface area for audits, if anything it makes it larger.
The halting problem doesn't actually apply in most cases people seem to think it does. All it says is that it's impossible to write a 100% accurate algorithm that tells you if a computer program has a certain property or doesn't. But that doesn't prevent you from writing an algorithm that says "Yes, it has this property," "No, it doesn't," or "I dunno, give me something simpler," with 100% accuracy whenever it gives the first two answers. If you're willing to accept the third answer some of the time, usually on pathological cases, the halting problem doesn't get in your way. In particular, when you're compiling software, you generally have the ability to print out an error message and tell the software author to fix their code. You can require that code be marked up with annotations proving that the behavior of the program follows certain rules. If the annotations aren't there, possibly the program follows those rules anyway, but the compiler is free to not bother, and say "Figure out how to place the annotations." Here's an example of [working with similar restrictions in the Coq proof assistant](http://adam.chlipala.net/cpdt/html/GeneralRec.html): the proof assistant insists that all recursion in the language terminate, and specifically wants to see that any recursive call be on a sub-structure of one of the original arguments. So it insists that the program author spend some effort writing their recursive algorithms that way, and only then is it willing to prove anything about the code. There's no solution to the halting problem here, because Coq's purpose in life is not to prove things about arbitrary code.
The JVM doesn't solve the halting problem. It just has runtime checks hand-coded into all the library methods that give access to the protected functionality. Actually the JVM doesn't do anything much, it is all in the libraries. There is no reason the Rust libraries couldn't do the same thing, i.e. have hand-coded checks that the task (for example) is authorized for I/O. This doesn't protect against someone writing another I/O library and giving another path to the I/O functionality, though. (I think the same risk would be present in the JVM if someone implements another JNI interface to I/O that lacks the standard checks.) If you want something proved, then you could follow all the reachable code from the limited set of functions accessed by the code under test. This isn't a halting problem -- just a reachability test. If there is any reachable I/O code found, then it could fail it. With a bunch of partial solutions like this (and a cracker mentality to look for ways around it) you should be able to come up with something solid, probably either with or without runtime checks.
I am not saying JVM solves the halting problem! It is theoretically impossible. And I completely agree that it solves it at run-time. The GP was asking why it is impossible to do at compile time and that's why I quoted the halting problem.
Yes, I don't think we disagree. A practical implementation has a trade-off between analysability and expressiveness. JVM avoids the compile-time problem by doing the checks at run-time. This is a different tradeoff: speed v/s expressiveness.
I'm not that familiar with explicit programming lifetimes. I think part of the hassle is separating the concept from the syntax. I have a pretty good idea of what i want to say, figuring out how to say that in rust has been a little challenging, there are lots of blog posts referring to old syntax. I dunno if it's worth it or not for you. For me, yes. I like Rust. I like Haskell. A long time ago I wrote a c compiler in c in school, i wouldn't consider myself much of a c programmer though. Over the weekend i got a good chunk of a parsec style parser implemented. My two big takeaways, 1) rust is pretty concise. what i've written has a haskelly feel to it IMHO. [parser start](https://github.com/jfoutz/rusty-scheme/blob/master/src/main.rs) 2) the compiler is a big help in making changes. I really enjoy changing a method signature then just chasing the compiler errors till it's clean. I find that hard to do in C, partly because it's hard to keep all the alloc/free pairs in my mind while i'm changing structure. Rust avoids that.
It need not necessarily be an *attack*. It could be a passive leak of sensitive data too. A very simple example: A CSS parsing library follows an `@import` directive and accesses the network when it shouldn't. A runtime sandbox could prevent such a leak without needing to audit the CSS parsing library.
I think Hyper is a little lower-level than this. It focuses more on the requests than the data they contain. For example, to decode a normal POST request (`application/x-www-form-urlencoded`), you read it to a byte buffer and then decode with [`url::form_urlencoded::parse`][1]. Hyper doesn't provide a useful abstraction for this because it's beyond its scope, IMO. [1]: http://hyperium.github.io/hyper/url/form_urlencoded/fn.parse.html
I didn't say *all* runtimes reduce the surface area for audits! Please parse my statement again.
Tracebacks are awesome! Makes understanding the cause much easier. BTW this looks a lot like Python tracebacks. 
I didn't try to run things on a quiet core. Instead I just ran my benchmarks 10+ times and didn't run anything intensive at the same time. I found my results to band together in some pretty well demarcated tranches: 70, 110, 300, and 900, so I figured I could treat them all as having roughly the same runtime behavior, and not have to bother trying to build a precise test configuration. That and I'm not sure how do to that :) Regarding the big picture here, I wrote this up because: * I'm inspired by [Julia Evans](http://jvns.ca/) to write about my development stream of consciousness. Being a better writer should make me a better engineer. * I'm sure I've made various mistakes with my approach, so I'm hoping someone will correct me on my behavior. * I'm hoping I'll inspire someone to fix all my performance bugs :) * There's been a lot of conversations about people being worried about Rust bounds checking. I wanted to show that not only can you make a safe Rust interface for an unsafe memcpy, but also the overhead of bounds checked code doesn't actually matter in this super tight loop, so people should only be concerned about rust forcing bound checks in safe code only if benchmarking shows it's actually a problem.
Just following the rust development, you hear about lots of other cool projects!
It is not impossible to do at compile time if you accept enough restrictions on the code under test, e.g. use crude over-strong checks which give false-positives and fail the code when perhaps it is okay (but we can't be sure). Then code within those restrictions.
You could go for a more advanced version of what other browsers do with OS-level sandboxing. Simply split your application up into components that can communicate with each other, then compile each component into a separate binary. So your CSS parser would read CSS in on stdin, and output a datastructure on stdout. Then you can use the OS's sandbox to prevent each component from doing more than it's supposed to. Honestly, I'd say go ahead and write it in Java if you know it and trust its runtime. I will note that Java's runtime is *massive* and packed with bugs and hidden features, though, so I wouldn't trust it myself.
Does anybody know if there is an idiomatic way of bringing the enum variants into scope only when matching on the enum? Overall I really like this change, but I don't like either importing the enum variants into the module as a whole or having to specify the fully qualified name for each. 
Assuming we're working from the source, my intention was that the unsafe check was done before building.
Yes, that's a possible alternative solution; if a little hard to implement.
I think that if all errors are deriving from a common trait and you don't mind boxing then you should only need to be able to wrap that trait. Declaring a specialization for other errors should be when you want to do MORE than just embed then.
Indeed! Actually, I have we use tracebacks in Release too: errors are infrequent enough that the performance hit in case they do happen is welcome given the hindsights we get in exchange.
To add to that: the concept to use a sandbox as a protection for this sort of thing is a bit dangerous. This needs support from the OS to be secure.
A browser's more than a little hard to implement in itself - in comparison, sandboxing stuff is mostly an exercise of finding and reading the documentation. No idea what the speed would be like, though, given you're at least jumping into and back out of the kernel every time you send data between components.
Have you guys tried Ada? Ada seems like it is very strong when it comes to defining assertions, invariants and so forth, either dynamically or statically checked. Could that be extended to these *permissions*?
I would be very interested if Rust had a way to ensure a crate is 'pure', for the Haskell definition of pure. That is, no IO or syscalls or interaction with the outside world. A function in the crate could panic/fail but at least it would be possible to be certain it does not interact with I/O. Unfortunately it looks like there is no interest in enforcing purity in Rust, which I feel will impair its future. It is one thing to ensure safety by making sure that memory is handled correctly. It is another to ensure safety when working with an ecosystem of multiple modules and many hands working on code simultaneously. It would be fantastic if modules or functions could be declared IO pure (a stricter variant of safe) and have this purity be enforced recursively.
You can import at a function level: enum MyEnum { Variant1, Variant2, } fn match_on(e: MyEnum) { use MyEnum::*; // Needs #![feature(globs)] on crate match e { Variant1 =&gt; (), // Do stuff Variant2 =&gt; (), // Do other stuff } } I might have liked an attribute that switched the enum namespacing behavior, maybe on the enum itself: #[variant_escape] // Enable old namespacing behavior, equivalent to pub use MyEnum::* enum MyEnum { ... } I was excited about the new behavior but in some cases the old is cleaner, especially for enums that are primarily or exclusively used in the same module. I believe attributes can be implemented as compiler plugins so who knows, this might be neat to implement. Edit: [Unless playpen is horribly outdated, it looks like the old namespacing behavior still works in the same module...?][1] Edit 2: ~~This is not possible with a syntax extension, because the compiler doesn't give the plugin any way to mutate view items in a module.~~ Was using `Decorator` instead of `Mutator`. [1]: http://is.gd/eocWQ0
I worked on [something similar](https://gist.github.com/vks/099fa3fd9e7e15045743). Because I wanted to practice lifetimes, I did not use `Box` or `Rc`, but something equivalent to the standard library's `MaybeOwned`. It was kind of painful. I could for example not implement `Add`, because it does not have the required signature in terms of lifetimes. You probably want to use `Rc` instead of `Box`, because that allows you to create references instead of copying everything.
Personally I'd like to able to declare ``` use enum MyEnum { Variant1, Variant2, } ``` Which would amount to the old behaviour (only in the scope the enum was declared). Makes adhoc local enums more convenient.
All IO goes through libstd, so you just have to audit for unsafe blocks and uses of it. CSS parsing etc. should be able to use libcore/liballoc/libcollections instead.
 steve@warmachine:~/tmp$ cat safe.rs fn main() { println!("hello world"); } steve@warmachine:~/tmp$ rustc --forbid=unsafe_blocks safe.rs steve@warmachine:~/tmp$ cat unsafe.rs fn main() { unsafe { println!("hello world"); } } steve@warmachine:~/tmp$ rustc --forbid=unsafe_blocks unsafe.rs unsafe.rs:1:13: 1:48 warning: unnecessary `unsafe` block, #[warn(unused_unsafe)] on by default unsafe.rs:1 fn main() { unsafe { println!("hello world"); } } ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ unsafe.rs:1:13: 1:48 error: usage of an `unsafe` block [-F unsafe-blocks] unsafe.rs:1 fn main() { unsafe { println!("hello world"); } } ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: aborting due to previous error 
Can someone go in to detail how || can now be used for both boxed and unboxed closures? [similar to this SO ?](http://stackoverflow.com/questions/25445761/returning-a-closure-from-a-function) As seen here: [Unboxed closures can be written with the sugared syntax.](https://github.com/rust-lang/rust/pull/19113)
I'm really looking forward to the cmp and ops reform. I think along with the collection reform it really makes code easier to read and the API easier to remember. The new range notation is a very nice generalization, and IndexSet definitely beats C++'s behavior of always inserting an element when it is indexed. What exactly is the v[] notation used for? Explicitly converting to a slice?
&gt; What exactly is the v[] notation used for? Explicitly converting to a slice? Yes.
If you don't trust your libraries you should not use them. No sandbox could ever protect you from all malice. There is some point where you have to weight the effort against the gain. A per-library sandbox does not make any sense.
Not necessarily: Just have it be completely empty and without any creation permissions, and you can re-use it all over the place. It's not like tabs need to do file IO. If they want to do something fancy, let them ask. Secondly: Why would that be a problem, anyway?
I noticed higher kinded types discussed in the placement box rfc. Is higher kinded types making its way into rust? All I could find was [this](http://www.hydrocodedesign.com/2014/04/02/higher-kinded-types/), but it seems to not yet be in the rfcs?
I was thinking of some digital signature type of thingy. A binary is signed by the compiler is some way that proves that it was compiled using safe block only. Not asking that rustc can do this right now, just that it is even possible. If I remember correctly singularity (Microsoft's research OS) used something similar. The assemblies (or whatever they were called) were signed by their compiler. I don't remember the details, can someone prove me wrong or confirm this?:)
Very interesting. thanks for sharing. I just tried running your code and it works like charm. A few questions: 1) Is the `MaybeOwned` idea so that we can allocate some things on the stack and others on the heap? E.g. it's an optimization to avoid always allocating nodes on the heap? 2) Is there a way to bind a concrete value to one of the symbols and evaluate the function with it? 
I *suspect* this is just a transitional measure until the built-in boxed closures are removed entirely. (And the way to make a boxed closure under the new system is just to use `&amp;`, `&amp;mut`, `box` or whatever on an unboxed closure.)
`macro_rules!` is pretty lousy at parsing this sort of construct. There are a few issues I can think of. First of all, like you saw, you can't have a macro expand to a type. You also cannot turn a string literal into an identifier. It's also exceptionally difficult to do the sort of unordered, nested parsing you'd need here. Previously, I'd have recommended doing a procedural macro, but with 1.0 coming up, there's not really much point. Best bet now is probably to write a code generator that can be used as a library from a Cargo build script.
Just a reminder: there is a link broken on the site. Here http://this-week-in-rust.org/ "All Posts" is broken. I think that should point to http://this-week-in-rust.org/blog/archives/index.html
One of the things that is holding Julia back for this use case is its very slow startup time because all of the stdlib &amp; deps are compiled on startup. IIRC static compilation is on the agenda for 0.4.
Wow, I looked up the [crate](https://github.com/nick29581/libhoare) you were talking about, and it's really cool. It's like a macro to add assert! to function calls, but much easier to read over the bounds on the function.
... but people do really want HKT and so many someday we'll get them. Maybe.
`realloc` is not part of the allocator interface. But you could make a special exception for the standard allocator combined with trivially copyable types. But this won't cover many types. For example, std::vector&lt;T&gt; itself is not "trivial" whereas in Rust every type is trivially movable by copying the raw bits. That's the beauty of destructive moves.
Seems like a job for a lint. I don't see a reason why this would be impossible. You could scan for IO pretty easily, I'd think.
AIUI, it means that now you can pass boxed closures `|| {}` to places where an unboxed closure is expected. For example: fn foo&lt;F: FnOnce&lt;(), ()&gt;&gt;(unboxed: F) {} fn main() { foo(|| {}); } Works. But if you try that on [the playpen](http://is.gd/wBotNJ) (which has a very 18 days old version of the compiler), it doesn't compile. EDIT bummer, I was meant to answer to the parent comment
Thats pretty easy to do with monadic IO, you just define a custom "IO" monad that only has access to what you want via whatever means you want and code against that. The type system prevents you from ever doing anything the custom monad doesnt allow
This is problematic because currently `use` has to be at the top. Also I'm not sure whether the sugar is necessary, this could work too: enum MyEnum { Variant1, Variant2, } use MyEnum; 
I've come across these same issues recently and to be honest, they're damn frustrating because they're such obviously desirable use cases. &gt; you can't have a macro expand to a type. You also cannot turn a string literal into an identifier. *but why not??* A related problem is when you want to generate multiple types from a single name (e.g., accept "User" and generate a "User" and a "UserController"). As far as I know, not doable. &gt; Best bet now is probably to write a code generator that can be used as a library from a Cargo build script. To me this is exponentially worse than the original C solution of textual includes. So now I'm supposed to write a program to generate textual includes? *What?* I hope 1.0 doesn't ship with macros in this state. There are appropriate use cases for build scripts -- e.g., embedding other languages' code and type checking it, as in Elm with GLSL / Markdown. A separate library makes sense there. But concatenating two identifiers to form a third? Returning a type? Absolutely not.
**TLDR**: string -&gt; identifier can be done, but requires adding it to the standard library. The rest? It's either because it's hard and no one has done it, or because it would require *significant* re-engineering of the compiler that *no one* is likely to do any time soon. Sadly, wishing upon a star doesn't really *work* outside of Disney movies. :) I'll start with the *general* problem first: there are two kinds of macros in Rust: "macro by example" *a.k.a.* `macro_rules!` and "procedural macros". `macro_rules!` is basically *just* direct substitution with some syntax-aware matching. This means you can't do any real *processing* on what you match, without assistance from a procedural macro. That is, you *can* turn a string into an identifier, but *not* with `macro_rules!`. The "why" at that point is: because that's starting to get into CTFE territory which is *hard* and there's simply only so much that can be done with the available manpower. I know I'd quite badly injure someone to have CTFE in Rust, but *I* don't have the time, nor does anyone else, it seems. Why not just use a procedural macro, then? Because they *cannot* be stabilised in their current form. They are compiler plugins that depend on the internal, unstable API of the compiler. Why not stabilise that API? Because doing so would *literally* freeze the language forever. No more additions or changes, *ever*. As soon as 1.0 rolls out, your procedural macros would be rendered useless to anyone *not* using nightlies. Why not use a different, more flexible API that *could* be stabilised *and* extended? Again, manpower. I'd love to see an external-to-the-compiler parser that allowed for arbitrary extensions, but again, I don't have the time for it right now, and no one else has written it, so... The one bright point here is that you *can* have procedural macros exposed as part of the standard library, since that's shipped with the compiler. So you *can* write a `string_to_ident!` macro and get that added to the standard library. It's a much higher barrier to entry, but it can be done. That said, I don't know exactly how receptive the core team would be *right at this moment*. There's already not enough time for all the things they want for 1.0 before their release goal. Plus, macro names are kinda *global* at the moment, so I don't know what the policy for using up new ones is. Please don't take this as "don't bother; it won't be accepted"; it's intended as "right at *this moment*, I dunno if the devs have *time* for anything other than stabilising for 1.0". If I'm wrong, hopefully someone will correct me! :D &gt; To me this is exponentially worse than the original C solution of textual includes. How? I mean, presumably, there has to be a program that turns the input into something which can be included as valid C. And since `macro_rules!` isn't really going to cut it, the alternative (ignoring stability) would be to write a procedural macro... which are *also* effectively code generators... That said, the big difference is that you obviously can't pass arguments to the code generator from *inside* the code. Which sucks. But the original example looks fairly amenable to it. &gt; But concatenating two identifiers to form a third? Returning a type? Absolutely not. This is because macro invocations are part of the AST in Rust. As such, they have to be supported in each position explicitly. Currently, there is no support for type-position or identifier-position. You could potentially allow them in type position (specifically, you'd have to add something like `TyMac` to `syntax::ast::Ty_` *and* then adjust all code to deal with it), but identifiers is another kettle of fish: they're just raw `u32`s from what I remember, so... yeah, not sure how that'd pan out. Not having macros as a distinct, text-only pass has some advantages, but as you've found, some disadvantages, too. I'm not trying to say "this is better, stop complaining!"; I'm just trying to explain the why, and that if you want to improve matters, [I'm sure everyone would be appreciative](https://github.com/rust-lang/rust/).
Yep! It's not that crazy. Small writes like this can happen deep in the heart of a serialization library. Plus it was fun. 
Is there any reason to trust MAC (mandatory access control) at the OS level more than at the runtime level? I guess the problem with doing it at runtime is that any time you reach out to native code you break the sandbox... Doing MAC in the JVM made sense in the days of applets and Java ME, but those both seem to be dying. I think it's still appealing because it's crossplatform. For server software it doesn't really matter, but for client software I'd hate to have to write policies for each OS.
What of those trait specifications raise the possibility of double specifying `Magma`? I don't understand why this happens..
I feel like a lint is a partial solution, in much the same way that if types were removed from Haskell, proving purity by "scanning for unsafe" would be. For one thing, a lint would have to be constantly updated to safeguard against new changes in Rust's `std`. I suppose a whitelist could be maintained, though. Interesting idea, I'm wondering how useful that would be as an alternative.
I think it actually means that the syntax which previously always meant boxed closure now can be inferred as either boxed-or-unboxed closure, depending on the context. https://github.com/rust-lang/rust/pull/19113
the standard library is precompiled as of 0.3. package loading is still slow but being worked on.
Yes, that seems to be the case, which is really cool! #![feature(unboxed_closures)] use std::mem; fn size&lt;F&gt;(f: F) where F: FnOnce&lt;(), ()&gt; { println!("{}", mem::size_of_val(&amp;f)); } fn main() { let bc = || {}; //size(bc); //~ error: the trait `core::ops::Fn&lt;(), ()&gt;` is not implemented for the type `||` println!("{}", mem::size_of_val(&amp;bc)); // 16 (boxed) let uc = |:| { }; println!("{}", mem::size_of_val(&amp;uc)); // 0 (unboxed) size(|| {}); // 0 (unboxed) } It seems that if you bind a closure to a variable, it's always treated as a boxed closures (see `bc` in the previous snippet), regardless of whether you use it later where an unboxed closure is expected (see error in the previous snippet). Unsure of whether that's intended or a bug. But, once boxed closures get removed, that issue will disappear anyways.
Would it be possible to have a shorthand for specifying enum variants when it's clear from context what the type is? In Swift, you can write switch/match statements like: switch topic.children { case let .ContentItems(contentItems): return contentItems case let .Topics(topics): return concatMap(topics) { self.flattenedContentForTopic($0) } default: return [] } and similarly call functions like so: let lastHyphen = filename.rangeOfString("-", options: .BackwardsSearch) Could the same thing be done in Rust, perhaps with a simple `::` prefix?
Can Rust use the tool of clang? for example AddressSanitizer/MemorySanitizer/ThreadSafetyAnalysis.
Sometimes the old behavior is preferable, and this is a single attribute on a module/crate instead of glob imports for every enum therein. The most basic test case passes, however more test cases are needed and I'm having trouble thinking of some. Might cause a compiler error if used with enums that have private variants, as it doesn't check the visibility of the variant before adding it to the view list. However I'm not sure that's possible in today's Rust, and if it is, it doesn't appear to be very common. It's not clear yet how this plugin interacts with `rustdoc`. If the plugin is run before `rustdoc`, then the location of the generated `use` statements in the docs could get messy with large numbers of variants. A simple fix for this would be to use globs, though I'm not sure if that would require `#![feature(globs)]`.
Yeh, I gave myself a headache thinking about this. For what it's worth my point was that going the Java route would be more portable (because the JVM security manager works the same on all platforms), but you would obviously have to trust the JVM.
Assuming you're the author: you don't *need* to run `install.sh`; it's enough to just extract it somewhere, then add the `bin` directory to the `PATH`.
I am, that's good to know thanks. I'll update the blog right now. so what does install.sh do?
One can just import the names though... use module::Enum::{mod, Variant1, Variant2}; // i think `mod` works here. match enum_variable { Variant1 =&gt; { /* */ }, Variant2 =&gt; { panic!("how dare you be Variant2!") } }
There was [an rfc](https://github.com/rust-lang/rfcs/pull/176) but it was rejected due to lack of a true vision. You could submit an RFC for type-macros if you would like. There will be a dedicated macro-meeting soon with the rust team. It is being planned, but no date has been set yet, as I understand it.
I have no idea. It could be that it's there for people who, for some strange reason, really, really want to install Cargo through MSYS. *shrug*
It's cool that you did this, for those of us who like the old behavior :) It's really easy to replicate on its own, though.
Well... I'm glad Rust is making a lot of the changes I wanted, but every time I have to wonder why it wasn't that way from the get-go.
Any word on named parameters? This is a huge oversight if it doesn't make into 1.0 because of the long term impact on code readability.
I looked at Iron and the problem is `iron::Request`. The body is allocated to a `Vec&lt;u8&gt;`, but I built `multipart` around `Reader` since uploaded files can be arbitrarily large and loading them naively into memory will inevitably be problematic. I actually spent a lot of time getting `multipart::server::Multipart` working correctly and I'm very proud of it. It can read a multipart request of any size with `O(1)` additional memory, yielding them one at a time or passing them to a closure. I wish it could implement `Iterator` but you can't add a bound lifetime parameter to a trait method. 
Yeah, I know. It was mostly an experiment with compiler plugins, since I hadn't written one before.
&gt; IndexSet Thanks for hilighting this! 