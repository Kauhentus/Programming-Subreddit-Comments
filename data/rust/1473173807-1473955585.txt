This code can be simplified by using the [filter](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter) method. I'd start by writing tuples.iter() .filter(|&amp;(_,include_fn)| include_fn(i)) .map(|(concat_str,_)| concat_str)
&gt; Just Google de-optimized by Isaac Gouy to see how often that has happened. Just use [that Google search](https://www.google.co.uk/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=de%20optimized%20by%20isaac%20gouy) to confirm that the author's intent is *mischief*: "17 Mar 2016 - Quentin Barthelemy, Cédric **Gouy**-Pailler, Yoann **Isaac**, Antoine Souloumiac, .... Based on 2D Givens Rotations and a Newton **Optimization**." Suffice to say, a few programs *didn't do the right thing*, so a temporary fix was applied until programs that did the right thing were contributed - and we all (almost all) lived happily ever after.
I'm speculating, but it probably has something to do with having a nicer API: - Tokio is built on top of the new Futures API which has all sorts of positive implications - Tokio's service trait makes it easier to compose functionality 
Thanks, yeah, that would be good. It'll need some time though.
Maybe even with an eye towards turning it into a hard error someday, given that I can think of no reason that we would want to allow this. We do already error on shadowing other type parameters, after all. Seems this just might have been overlooked. I'd definitely support an RFC to remedy this.
&gt; If I understand you correctly you're suggesting essentially implementing my own pointers out of integers, which index into a vector of nodes? Yes. &gt; I mean that would work, but if I'm understanding your suggestion right I guess I don't see the point. Using indices you're still operating with safe code and have some guarantees. With pointers you're completely on your own. About the performance, you might even get better performance with indices, because between your node data might be a higher cache coherence. In these days it's really hard to tell which implementation will be faster without profiling.
Thanks! Fixed.
&gt; Using indices you're still operating with safe code and have some guarantees. With pointers you're completely on your own. Good point! Do you think thats better than using `Rc&lt;Cell&lt;Node&gt;&gt;`? (`Option&lt;Rc&lt;Cell&lt;Node&gt;&gt;&gt;`? Urgh what order do those go in?) The rc-based solution seems like it should be simpler (no free lists) and faster (no extra indirection). Way more pointy and a bit more magic though. What are the comparative advantages to using an integer pointer into a vector?
Golang has a similar feature, which is a nice way to keep this inheritance-like convenience.
 &gt; Do you think thats better than using Rc&lt;Cell&lt;Node&gt;&gt;? Can't say for sure without really seeing how the implementation in your case of both looks. &gt; The rc-based solution seems like it should be simpler (no free lists) and faster (no extra indirection). I wouldn't be that sure that it's faster at the end. It's IMHO not the indirection that's killing the performance, but an indirection outside of the cache. 
I thought I wasn't going to be able to go because of classes BUT as it turns out, I'm actually free those days! I just bought my ticket! I'll see if I can talk my TAs or students into coming with me :)
I mean, if bumping to 2.0 is not a big deal, then there is no difference between bumping 0.5 -&gt; 0.6 :) I will let the current API sit for a bit, but assuming it works out, the main blocker to 1.0 is to get rid of the deprecated code.
Thanks! I am so very bad at asking for help. I'm making it one of my personal goals for this semester to finally stop being a lurker around here, so I'll try to keep you guys updated on how this is going!
Unfortunately I think you need a @upenn.edu email address to sign up for the class on Piazza. Last semester we used this [Google Group](https://groups.google.com/forum/#!forum/cis198-public) for non-enrolled students. It wasn't super active, but we still respond to questions on there.
I'd love to have you come by for a talk again!! (And not organized at the last possible minute this time... &gt;.&gt;)
I really liked the material and homework. Feedback on where I stumbled: * BSTree (HW2 &amp; HW3): I stopped, read the "Too many linked lists" book, came back and - from the start - used Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt;. The iterators where very hard to implement due to lifetimes. I did not manage to implement in-order traversal iterators, only the fake "always take left" one :( * RPN Calculator: it was relatively easy, and pattern matching was put to great use on this one! * The Darkest Dungeon, I was initially complete lost. But after reading and studying the "starter code" I could make sense little by little. But it took some time... * BBS (HW 6): I did not feel confident enough in doing network programming, studying I managed to make half the homework. But I got lazy, and stopped here.
`&gt;&lt;` Thanks, edited.
&gt; For starters, you would still have to create a new Base type for every Base type that you wanted in any language... this is no different, since Base is baked into the macro. Yes, obviously. It's just that macros – in my opinion – are quite an intermediate feature. I want to introduce some coworkers to Rust and i'm not really comfortable to showcase macros in this type of situation. Writing macros looks somewhat to complicated in comparison. You're absolutely right but i am feeling we could have something "easier" and more convenient to use. If we could have something like compose!(Base =&gt; struct Inherit { x: f32, y: f32, }) i would be ok with it – to hide the complexity of a macro. actually i like the Deref example from /u/Gyscos it looks quite nice. EDIT: Now i tend more to the solution you presented due to the fact that this is a way i could use it with serde .. at least if i can find i way that i get this: {id: 12, meta: 22.2, a: 33.3, b:44.4 } easily parsed into the resulting Example2 struct .. i am struggling with the /u/Gyscos solution now and tend to use yours ... maybe i need to dig a little more into serde (custom de/serialization ) – what i did not (just a couple minutest for testing)
I really like this, thx! Rust has powers i still need to uncover. I know Deref but was not able to apply it on this problem domain. This would be a great example in something like "Rust for OOP Folks" ... or any other "Book" that not only show the building blocks of Rust but "How to use it" //EDIT: to sad, that this is not directly working with serde – without custom de/serialization – to encode or decode json like {id: 12, meta: 22.2, a: 33.3, b:44.4 } so i am loosing the purpose for doing this in the first place. But the marco from /u/antoyo does work in this scenario. 
==&gt; https://www.reddit.com/r/playrust
Sorry, new on Reddit. Reposted there !
Ok, thanks. I might try the Google Group if I have a question then :)
Let's make it happen, you know my email :)
Any word on the servo blog? It's been a few weeks now.
There are many aspects to the ML pipeline outside of model generation that Rust could really help. One (usually) neglected area of data science is the ability to provide safe, reliable, and stable feature engineering systems. This is crucial for optimal model performance in any domain. Check out my and /u/staticassert 's talk at RustConf this week to learn more :) 
I can't speak for the blog, but I have noticed a significant improvement in the usability of the nightly in the past week or so.
One easy source for this would be to compare against the [implementation in itertools](https://github.com/bluss/rust-itertools/blob/73ded4ee50b28d6d417ec96c4ab41218397c49c8/src/adaptors.rs#L1349-L1375). ---- The big issue: **this isn't a unique iterator adapter**. It's only a "mostly-unique" adapter. Hashing can collide and create false positives! Practically, this means that values you haven't seen before will not be returned from the iterator. For example, `[1,2,3]` could become `[1]` if the hash algorithm maps all three to the same hash code. If you are OK with that... 1. Use only `where` clauses once it gets longer; don't mix and match inline constraints and `where` constraints. 1. I tend to not specify constraints on the struct. 1. There's no need for inherent method for hashing. 1. There's no need to have a constraint of `Iterator` for hashing. 1. There's no need for explicit types on the closure. 1. There's no braces in a one-line closure. 1. There's no need for generics on the trait. 1. There's no need for explicit types on the `UniqueIterator` constructor, they are inferred. ---- use std::collections::HashSet; use std::option::Option; use std::hash::{SipHasher, Hash, Hasher}; struct UniqueIterator&lt;I&gt; { iter: I, seen_items: HashSet&lt;u64&gt;, } fn hash_once&lt;T&gt;(item: &amp;T) -&gt; u64 where T: Hash { let mut hasher = SipHasher::new(); item.hash(&amp;mut hasher); hasher.finish() } impl&lt;I&gt; Iterator for UniqueIterator&lt;I&gt; where I: Iterator, I::Item: Hash + Sized { type Item = I::Item; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { let seen_items = &amp;mut self.seen_items; match self.iter.find(|x| !seen_items.contains(&amp;hash_once(x))) { None =&gt; None, Some(item) =&gt; { seen_items.insert(hash_once(&amp;item)); Some(item) } } } } trait UniqueIteratorAdapter: Iterator { fn unique(self) -&gt; UniqueIterator&lt;Self&gt; where Self: Sized, Self::Item: Hash { UniqueIterator { iter: self, seen_items: HashSet::new(), } } } impl&lt;I&gt; UniqueIteratorAdapter for I where I: Iterator {} fn main() { let data: Vec&lt;_&gt; = vec![1, 12, 15, 1, 12, 3, 5].into_iter().unique().collect(); println!("{:?}", data); } 
As far as I know, yes! Looks like there was already a [PR](https://github.com/serde-rs/serde/pull/530). Edit: and here's [serde_derive](https://crates.io/crates/serde_derive)!
It's somewhere between bug and design, in that `Self` doesn't work in struct initializers like normal type aliases; it may not have been intended to but a lot of people (including myself) find it unintuitive that it doesn't: https://github.com/rust-lang/rfcs/issues/1588 There's also a slight difference between your examples in that you're using `Self::A` in your first example but you don't use an associated type in the second. It doesn't work either way. 
Oh wow, this'd mean it'd likely be stable in two releases right? Edit: it doesn't, see /u/Manishearth's response below.
One can only hope!
I've got a tutorial that does something similar, but on spam/non-spam text massages. It starts from loading data, goes through string vectorization, and runs and evaluates a simple classifier. You certainly can do something more complicated, but a linear classifier will be a good starting point. More complicated things like random forests or factorization machines are already implemented in [rustlearn](https://github.com/maciejkula/rustlearn) and I'd suggest you try them before trying to write something yourself. The tutorial is [here](https://github.com/rust-community/rustbridge/tree/master/workshops/machine_learning).
I'm not saying that everything should be rewritten in Rust, but some years from now Rust as a mature language will (hopefully) have the potential to replace both C and C++ in most situations. And you can even see it today, Dropbox decided to write the core of their software stack in Rust instead of C/C++, and Mozilla started supporting Rust to replace C++ with something safer, but equally fast in their codebase. It's true that I'm a bit biased because I really like the language, but I think the whole 'Rust hype' is all about people realizing this potential.
Woo! Very excited that the course is happening again!
Screw it, lets rewrite the world with Silver Bullets!
Not if you use a HashSet.
Suppose I have a &amp;mut Option&lt;Rc&lt;T&gt;&gt;. How do I get a &amp;mut T following these rules 1. If Some(rc) and rc is unique, return a mutable ref to the contained object 2. If None, allocate a new T::default() and return a mutable reference, **while placing this back into the original Option&lt;Rc&lt;T&gt;&gt;**. 3. If Some(rc) and not uniquely owned, clone the object and create a new (unique) rc to it and place this back into the original Option&lt;Rc&lt;T&gt;&gt;. It seems like there should be a rustic way to do this with the various option chaining methods and Rc::get_mut(), but I can't figure it out. Heck, I'm having trouble even doing it the ugly way, thanks to the issues with pattern matching on mutable options and NLLs. 
I'd highly recommend starting with Python. It's awesome and it is beginner friendly. 
I want to retweet this blog post so badly but I just can't figure out how!
Note that you'll need to use RefCell, not Cell. Cell is for Copy types, and since your Node must recursively include other Rcs, it can't be Copy. As for the advantages of indexing into a vector, you have three main advantages: better cache locality, avoiding ref count bumps, and guaranteeing that everything will be freed when the list is destroyed. It also has disadvantages: such as the extra indirection and making it difficult to immediately free memory of removed nodes. The big disadvantage though is that you forgo any assistance from the type system in managing lifetimes. Note that there's actually a large number of possible implementations with different combinations of features. For example, you could use a TypedArena. Or you could combine ref counts and vector indices. Or you could use two vectors. Etc. On a side note, there is an alternative to free lists. If each node keeps track of which other nodes are pointing to it (probably the case in a skip list), then you can relocate nodes within the vector by updating the indices appropriately. When removing a node, just relocate the node at the end of the list into the newly freed slot and shrink to fit. That approach lets you immediately free memory when removing a node.
Why would that be different?
I was using Rc::get_mut(), which doesn't require any feature gates.
Actually, a HashSet is [a wrapper for a HashMap](https://doc.rust-lang.org/src/std/up/src/libstd/collections/hash/set.rs.html#105-107), not a vector of hashes. That means that as long as hashes are consistent, this is perfectly accurate. There is no requirement for unique hashes from my understanding.
&gt; I'm just talking about using a faster hash function. Then I don't understand. I tried many different hash functions in all languages and the results were always the same: Rust was no faster. One of the hashes I tried in all languages was `i+2000+(j+2000)*4000` which should hash every coordinate used to a unique number but I noted that it didn't affect the results either. &gt; If you used FNV hash on both hash sets, the comparison would be reasonable because FNV hashing doesn't display pathological behaviour on either implementation - Rust was "slow" then because FNV hashing has time overhead that you weren't also requiring of F#, which didn't use FNV. Is FNV really that slow? I'd expect the performance to be dominated by memory accesses. &gt; I don't see how it's an ad-hominem to say that if you're looking for a language which supports purely functional language styles then Rust isn't for you. None of this has anything to do with me personally. My original program used recursion and purely functional sets and the `nth` function was just a few lines long: let rec nth nn n v = match n with | 0 -&gt; set[v] | 1 -&gt; nn v | n -&gt; let s1, s2 = nth nn (n-1) v, nth nn (n-2) v Set.unionMany(Seq.map nn s1) - s2 - s1 That's a neat solution and, best of all, with memoization you get to reuse the results when computing the neighbor shells for other vertices. I tried to translate that into Rust and found there is no purely functional set. I didn't want to start implementing my own AVL trees and fighting with the borrow checker and reference counting. So I found Rust's mutable hash set and used that instead, sacrificing memoization and sharing. Then I found it leaked memory and I had to either ditch recursion in favor of loops or add `drop(s2)` so I went with the latter. Then I found the hash function was very slow so I replaced it with Rust's "fast" FNV hash function. Then I found the performance of all the programs was basically the same. After all this (and the threads) I revisited the programs and tried various other inputs and found that the Rust is sometimes 3x faster than F# and sometimes 3x slower but no evidence that Rust was generally any faster. On the basis of this I made some observations: * Rust doesn't seem to bundle any purely functional data structures. * Recursion doesn't interact well with Rust's scope-based memory management and leads to memory leaks. * Rust is not significantly faster than F# in this case. None of these observations have anything to do with me. My personal conclusion is that Rust makes lots of easy things hard so you would need compelling motivations to choose it but I'd still choose it over C++ any day of the week and twice on Sundays (not that I have used C++ professionally for decades). &gt; I see what you're saying now. I still think it's absurd to call Rust leaky given the situation raised is unrealistic, relying on this in other languages is fragile since only trivial cases can be proved I disagree that this is "unrealistic" and I rely on this all the time. After all, what use would garbage collection be if it didn't collect the garbage! You cannot prove it, of course, but since when was anyone trying to prove that a GC would collect their garbage on time? We just rely on it to clean up and rarely have problems in practice. &gt; and Rust gives far more guarantees of timely deallocation than any other language I know. But such is life, you can't please everyone. "Timely" is an interesting choice of words. Qualitatively, it felt like the Rust introduces a big hang at the end of the program on this benchmark (though maybe that was with the memory leak). I never measured it but I'd imagine you would have to do a lot of work to get decent latency out of Rust, or is there an easy design pattern to follow? I once spent months trying to write STL allocators in C++ to reduce the pause times in a program. We ended up porting the whole thing to OCaml where the pause times were 5x shorter without putting in any effort at all. So, while I appreciate the theoretical advantage of timeliness, I'm skeptical of its practical uses in many cases. 
Thanks for the info, and that makes complete sense. I'm already using nightly so it's not any loss for me, but I'm looking forward to being able to use stable.
&gt; Your post certainly doesn't show that. Not sure which post you're referring to but I've posted the shorter and simpler F# solution using the generic built-in hash many times now. &gt; Do you actually believe the stuff you type? Where is the F# version using [Veedrac's "contiguous" hash](https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/d1s2tpp)? Or even the measured performance of it? &gt; Of course they were. I've added lots of links for you to read if you're at all interested in the truth. 
Interesting, thank you.
&gt;Not sure which post you're referring to but I've posted the shorter and simpler F# solution using the generic built-in hash many times now. Stop dancing around the issue. You know you faked the hash the first time. It's not my job to run your F# code; the fact that I can pick out one line of it doesn't require me being an authority on the language, or Rust for that matter. If you were interested in being honest you'd update your blog post, not yell at some random guy with underscores in his name. Sell your books to someone else.
A big difference: with left pad, the author was able to delete it, and that's what caused the problem. Not so with crates.io.
Nothing useful. Just a comment. The hygiene idea never really clicked with me. I mean - I understand how it is supposed to work, but I don't really want it. Plenty of times it was just annoying me with it's limitations. If my macro clashes with a symbols from outside of it - so be it - sometimes that is what I want. I would rather have a special way to generate names that are guaranteed to be unique, for names that must not clash with the symbols outside of the macro. Or - the other way around, easy way to opt out of hygiene and be able to introduce new names etc., breaking out of hygienic rigor.
It was windows wasn't it? :P
To clarify, the ability to break hygiene is essential (for generating usable items, at least). But the ability to have hygiene when you need it is also pretty important. 
&gt; x] it's a bit left-pad isn't it? People say this, but I don't get the problem. What would you prefer? The problem with left-pad being revoked doesn't apply to rust, so it's just a matter of whether "small crates are bad" - and I don't see why they would be.
The current Macros 2.0 RFC is just meant to sketch out the outline of the idea, with details to be fleshed out later. I believe the intent is to deal with hygiene issues like that, but all of the details haven't been written out yet. Right now, you can work around the issue by using a type parameter in the macro expansion that is unlikely to conflict, like `T_default__`; include the name of the macro in your type parameter to avoid this parameter from conflicting with others in other macros that use a similar convention. I think that the stopgap procedural macros work has taken precedence over macros by example 2.0, just because there is more work that is made difficult by them not being available, like serde being a pain to use on stable. Workarounds like the above cover pretty much all of the troublesome cases of macros by example 1.0, and they are mostly a pain for writing the macros, while lack of stable procedural macros makes things that need them like serde a pain to use.
I think I've met your requirements: https://is.gd/zU4nBR It _feels_ like there's a better way - this was just the first one I found that worked.
I think that hygiene by default is good and that the ability to break it is a must. However, I would like non-hygienic macros to be explicit at every step of the way: - when you declare them: since it is better to not let you break hygiene by default. You don't want to break hygiene by mistake, but rather get a compiler error saying "your macro breaks hygiene! if that is what you wanted you need to annotate it!". - when you use them: since whether a macro breaks hygiene or not is important for reasoning about the code surrounding the macro. 
That's exactly the reason I choose to implement it by storing the hashes themselves. An alternative would be to store references to the elements in the HashSet, but I don't think that's possible lifetime-wise while still preserving Iterator semantics.
It's possible if it's an iterator where the elements are references. Itertools' .unique() works fine with that too (we note that its cloning just clones references in that case).
&gt; I tend to not specify constraints on the struct. Can you elaborate on the tradeoffs between specifying them and not specifying them?
As far as I'm aware the plan is that macros 2.0 will use some variation on the "sets of scopes" algorithm which I'm fairly sure will make everything fully hygienic.
&gt; That approach lets you immediately free memory when removing a node. I don't think this will work, because if you allocated the whole memory block at once, then you can't just free only parts of it. 
I'm guessing the API might be more stable too, so there'd be less likelihood of bleeding edge crates breaking.
Many good things. Getting [static/consts a little cleaner with default lifetimes is nice](https://github.com/rust-lang/rfcs/blob/master/text/1623-static.md#motivation). Always had to look up how to write those lines, maybe I'll remember it now.
Ah yeah, you're quite right. I wish I'd called that out in the text. 
It is a feature! The wording got changed a bit in editing. My original title for that section was "The Rough Spots" not "The Cons of Rust". I agree that 'con' is too strong of a word for nightly/stable distinction. 
Didn't realize this existed, totally would have submitted for last month when we announced, but late is better than never... [Way Cooler](https://github.com/Immington-Industries/way-cooler/) A tiling Wayland window manager based off of awesome and i3. https://github.com/Immington-Industries/way-cooler/ Yes
Personally I wish the text was interrupted more with quotes from sentences that I've just read. It's very difficult to remember what I read 2 sentences ago
&gt; One of the hashes I tried in all languages was `i+2000+(j+2000)*4000` i + 2000 + (j + 2000) * 4000 == i + j * 4000 + 8002000 == The other hash + constant offset That changes nothing about its pathological behaviour. &gt; Is FNV really that slow? I'd expect the performance to be dominated by memory accesses. It only needs to account for 25% of the time. I just tried the hash function (((i * 3203431780337) + j) * 3203431780337) &gt;&gt; 15 which should distribute sufficiently randomly, and the time dropped locally from 2.6s to 2.0s - the precise difference you measured between Rust and F#. I wouldn't be surprised if using this hash function actually slowed F# down a little bit, because F# was probably benefiting from the overly uniform nature of its hash and multiplication isn't free. But who knows? I'll let you time. #[derive(Default)] pub struct PairHash { hash: u64 } impl Hasher for PairHash { fn finish(&amp;self) -&gt; u64 { self.hash &gt;&gt; 15 } fn write(&amp;mut self, _: &amp;[u8]) { panic!("Cannot hash arbitrary bytes."); } fn write_i32(&amp;mut self, value: i32) { self.hash += value as u64; self.hash *= 3203431780337; } } &gt; My original program used recursion and purely functional sets [...] I tried to translate that into Rust and found there is no purely functional set. In Rust you can do let s0: Set = s1.iter().cloned().flat_map(nn).collect(); &amp;(&amp;s0 - &amp;s2) - &amp;s1; You don't need a purely functional set - you're not even sharing data between the sets so why would you bother? FWIW, I tried running your code on Mono/Linux and found it took basically forever. The code isn't getting memoized, so the runtime increased exponentially. &gt; I had to either ditch recursion in favor of loops i.e. you wrote unidiomatic code in a way that makes no sense in Rust and blamed the language. Just write a loop. Why would you *ever* write a counting loop as anything but a loop in Rust? &gt; On the basis of this I made some observations: &gt; * Rust doesn't seem to bundle any purely functional data structures. You didn't actually need them, though, so your want seems to be entirely personal wishy-washy feelings. &gt; * Recursion doesn't interact well with Rust's scope-based memory management and leads to memory leaks. Aka. you want a purely functional language. Just use one. &gt; * Rust is not significantly faster than F# in this case. Rust's aim is to give you lots of tools and abstractions to write fast code. If you don't intend to use them, Rust's speed improvements will be limited. There's nothing surprising about that. &gt; I disagree that this is "unrealistic" and I rely on this all the time. Yes, because you keep trying to write functional code in Rust, despite the fact that Rust *is an imperative language*. Idiomatic Rust is clearer, shorter, faster and for Rust's target audience more obvious than how you wrote it. That's why I call it unrealistic: the code you wrote would never be written by someone in Rust's target audience. &gt; Qualitatively, it felt like the Rust introduces a big hang at the end of the program on this benchmark I don't see such a thing. The code ends instantly after printing its output. &gt; I once spent months trying to write STL allocators in C++ to reduce the pause times in a program. We ended up porting the whole thing to OCaml where the pause times were 5x shorter without putting in any effort at all. If you just intend to heap allocate hundreds of thousands of objects, there's no point using a low level language. Of course GC allocation and deallocation is faster than the equivalent `malloc`-`dealloc` code. That's not the point, and that's never been the point. The point (at least when it comes to memory management) is that these languages offer tools to control when allocations happen, and *avoid* most allocations altogether. There's a reason AAA games and real time software uses C++ (and similar languages) to avoid pauses. Is it really more likely that everyone else is insane, or that you've misunderstood something? 
I'll toss my hat in the ring: [`xsv`](https://github.com/BurntSushi/xsv) (fast CSV toolkit)
Oh, oops. Totally missed that.
If a SO post is out of date or broken, anybody in the community with enough rep can fix it. Only the crate's author can upload a new version.
Hey everyone! Just giving you all yet another reason to attend RustFest -- we're preparing a workshop where we'll explain practical aspects of implementing blockchain clients in Rust. For the past year or so, we've been professionally developing [Parity](https://github.com/ethcore/parity), and we want to take this opportunity to share some of the lessons we've learned along the way. Don't worry if you're new to the crypto space; we intend to make everything really newcomer-friendly. Check out the linked post or [github issue](https://github.com/RustFestEU/conf-2016/issues/2) for more details.
So [michaelsproul](https://github.com/michaelsproul) helped with the insert case. It turns out I was using the stable 1.11.0 locally, but nightly is a lot better. Also, in the case where `*t == None`, writing to `*t` was calling drop on the `None`, which wasn't ideal. Michael showed me the `forget` function that doesn't call drop. So instead of writing: *t = Some(Box::new(...)) We instead ended up with: std::mem::forget(std::mem::replace(t, Some(Box::new(...))); The [resulting code](https://is.gd/5mGcbx) produces perfect assembly on nightly.
tweet the link? Am I missing something?
The problem with this approach is that I would have to look at all elements to detect whether the sequence is sorted. This would remove the capability to perform lazy evaluation.
Nearly every paragraph had a quote box with Twitter links. OP was being sarcastic.
I haven't fully formed all my opinions in this area yet, but my main reasons against doing it: 1. It's "infectious" — if you embed the struct elsewhere, you have to continue to state all those restrictions. 1. You have to repeat yourself. If the restrictions change, then you have many places to update. This can be limited by creating your own trait that combines all the relevant traits. Reasons *for* doing it: 1. Potentially better / earlier error messages when trying to misuse the struct.
True, though SO posts being out of date is certainly a problem even if they can be updated. 
I'll throw [`clap`](https://github.com/kbknapp/clap-rs/) out there too. It's a very configurable command line argument parser.
I must have missed the video when you posted it before - it's really cool! I look forward to seeing the others. With that said: --- rusty-machine A pure-rust machine learning library. https://github.com/AtheMathmo/rusty-machine I may be available for interview but the next few weeks are a bit crazy as I'm moving country. If you're following the same format as last month then maybe I can put together a slightly more interesting demo!
I'm working on breaking up my serial terminal project into two parts, one is the GUI frontend and it communicates over channels with a backend that uses a thread to communicate with the serial port. I'm having problems with this abstraction, however, as I can't seem to figure out how to declare a callback when creating the thread struct. A minimal example of the failing code is here: https://is.gd/iHvXqx
If the package receives an update, you can find that out easily. There's no mechanism to be notified that your copy-pasted code from SO has been fixed.
There generally aren't more-than-minimal docs for things until they're stable; this has just landed on nightly. 
The `Send` trait marks types that are safe to move to another thread. In addition to this trait, `thread::spawn` also needs any captured values to have a `'static` lifetime, because the new thread's lifetime is not bounded by any particular scope. If you add both of these bounds, then the `thread::spawn` call will work: pub struct SerialThread&lt;F&gt; { rx_callback: F } impl&lt;F&gt; SerialThread&lt;F&gt; where F: Fn() + Send + 'static { pub fn run(self) { thread::spawn(move || { loop { (self.rx_callback)(); thread::sleep(time::Duration::from_millis(1000)); } }); } } A more complete example with some other minor fixes: https://is.gd/DBD482 For more details, the [book chapter on concurrency](https://doc.rust-lang.org/book/concurrency.html) may be useful.
The [`custom_derive`](https://danielkeep.github.io/rust-custom-derive/doc/custom_derive/index.html) crate seems like your best bet for now.
That's true. I guess I'm just not used to 5-6 line snippits of code needing to be "fixed" periodically. I literally just copy-pasted a snippit off SO to convert a .Net `DateTime` into Unix seconds. I would never include a package to do that because it's so simple and it will never change.
I really waited for this one! Guess, I'll, for now, use what I can from serde sources :)
I looked at this crate but didn't quite understand what it was doing, now I think I get it: It parses the `#[derive(...)]` attribute and invokes them as macros, feeding the entire token tree inside `custom_derive!` for you to match against and you just extract what you need. Yeah that sounds exactly what I meant! Would this work for Serde? It seems like it just needs to iterate over the fields of a struct. Perhaps more complex handling is needed for lifetimes and generics (I don't know).
&gt; I would rather have a special way to generate names that are guaranteed to be unique, for names that must not clash with the symbols outside of the macro. This isn't a substitute for hygiene. There are things you cannot do at all without a hygienic macro system (I forget the examples off the top of my head, but IIRC they have to do with macro-defining macros).
Is there any prefered/clean way to iterate over each pair in a `Vec` and get a `&amp;mut` to the two elements constituting the pair. The one solution I have found involves creating two mutable slices and accessing the `mut` elements from there (See below). I can't help but wonder if there is a more self-documenting way. let mut vec = vec!(1, 2, 3); for i in 0..(vec.len() - 1) { for j in (i + 1)..vec.len() { let (mut left, mut right) = vec.split_at_mut(j); let ref mut first = left[i]; let ref mut second = right[0]; println!("{} and {}", first, second); } } This prints, which is what is want: 1 and 2 1 and 3 2 and 3
One of the worst hold-overs from print media. Blegh.
This is more important in a language like JS than a language like Rust. There's a package (I forget which one...) that is on major version 3, because it checks if a given JS object is something. I don't remember, false, zero... but, it turns out that doing this in a robust way is really hard, and they messed up some of the initial cases, so they needed to release a major version bump. Rust won't have these kinds of problems all the time, but it does have a related problem: `unsafe`. For example, I have a package, https://crates.io/crates/ref_slice, that only exports a few functions. Similar to the package we're talking about here, it was something that was abstracted from the standard library when we deprecated it. However, since it relies on unsafe code, isolating it is a very useful idea: `unsafe` affects the entire module, so if you wrote this yourself, you'd _at least_ want it in a submodule. You'll also notice that it has a 1.1 release: I added re-exports that are the same functions, but with names that are a bit more conventional. Anyone who was using `ref-slice` 1.0 can see that they're now outdated, and upgrade to the slightly better names.
This is a good point and one that I hadn't really put a lot of thought into. There are a couple of ways to get around this with the new api. - Modify `train` so that it doesn't consume the `Trainer`. - Have the user clone the `Trainer` before calling `train`. Right now it is set to consume the `Trainer` as it is common to allocate large amounts of data during training (for example centroids for K-Means) and then we can pass these onto the `Model`. Changing this would probably lead to more allocations later. As for cloning before - it is pretty ugly but would get the job done I guess. A `Trainer` shouldn't have large amounts of data when it is instantiated and in fact we might even be able to implement `Copy` on some of them. I'm also not sure how common this is? We would need it in cross validation for example - but in this case I think the `clone` approach would be acceptable.
I made this internals post a while ago to try to move the discussion forward about inclusive ranges, but nobody commented. I want to write an RFC to get the `...` syntax out of unstable limbo (and hopefully change it to `=..=`/`=..&lt;`), but it would be great to get some more input! Other discussions: [RFC for `...` (which I implemented)](https://github.com/rust-lang/rfcs/pull/1192), [tracking issue](https://github.com/rust-lang/rust/issues/28237).
Please let me know how that works out for you. I had the idea for the macros and auto-delegation while I was writing that comment. I'm curious if it pays off in any way. I won't be surprised if my super quick implementation needs some ergonomics fixes at least.
For bool the case is clear, however some libraries have developed `i128` and `u128` so when they are introduced as primitive it would be great not to break those libs...
Hey, mbrubeck, that's really helpful! I didn't get around to resolving how to deal with the threading issue because I hadn't gotten past the Send/'static stuff yet, so thanks for resolving that for me as well! And thanks for the additional links. When I was experimenting with this I was trying not to parameterize SerialThread, because then I don't know how to store it. I have a thread local storage on my main thread, where I track the SerialThread object. And I need to define those datatypes, which I'm not sure what the correct way to resolve this is. See your revised code here: https://is.gd/mfLgAv This is now almost exactly how I use the code on my end. I need to have the SerialThread object available to various UI callbacks, so I need it global or in local scope.
http://stackoverflow.com/q/34612395/155423
More readable than a lot of code.. :D Looks like it could be cleaned up a bit with better pattern matching like `if let` and maybe slice_patterns though.
So, so, so excited for this.
One thing I don't get: &gt; The main reason we had separate structs in the first place was to allow static distinctions about which kind of range you support (rather than, e.g. having to panic on an unsupported style). What is meant by supporting different kinds of range? What sort of code does not work with inclusive ranges vs inclusive-exclusive ranges? 
Yeah, but is `if let` more readable if I'm going to immediately return anyway? Can you give an example on how to clean it up?
Maybe because exclusive ranges can't represent the max value?
Is there a performance cost to unifying the types?
a little bit, you don't have to write return as you can do it all in one expression: if let Some(c) = decide(*liberty1, *liberty2) { Some(c) } else if let Some(c) = decide(*liberty2, *liberty1) { Some(c) } else { None } 
Can we have list comprehensions too? That'd be great.
I really like the idea of the readiness model for futures; it rather reminds me of the rules based approach the RAMCloud folks used when dealing with some of the less deterministic parts of network programming: https://blog.acolyer.org/2016/01/19/dcft/ .
with slice_patterns (although not sure if this compiles - just had a look at the book) it looks like everything before the inline function would collapse to: match group.liberties() { &amp;[_, _, ..] =&gt; return None, &amp;[liberty] =&gt; return Some(Play(player, liberty.col, liberty.row)), &amp;[liberty1, liberty2] = { // do bit from my previous comment } }
I've been coding in rust a few months ago and have the 1.7...1.11 GNUs, but I feel like it'd be more convenient to be using the rustup toolchain. Would I need to uninstall the previous GNUs first, and then download rustup (If so what's the best way to do this?)? What are the steps I should take to make this change?
Belongs in /r/playrust -- this is for the Rust programming language.
Damn it. As a Rust fan and an active developer in the bitcoin world, this would've been a grand thing to attend. Shame it's a bit short notice for me :(
The poller will propagate calls to `poll` down the future chain, eventually reaching a root future that has some kind of I/O source, such as a `TcpStream`. This is not the libstd `TcpStream`, but a special `TcpStream` that knows about the event loop it's running on. [Before](https://github.com/tokio-rs/tokio-core/blob/master/src/readiness_stream.rs#L69) returning `NotReady`, the stream will send a [message](https://github.com/tokio-rs/tokio-core/blob/3794cf7f1db616ecf27b6841a0d32facc3dd4bea/src/event_loop/source.rs#L77-L79) to the event loop, via the `LoopHandle`, to re-register its associated mio `token`, inferring the event interest based on the I/O action that was requested (read or write or whatever).
That's actually a very good question. Basically, if a `Task` polls a future and the `Future` calls `socket.read()` and realizes that the socket isn't ready for reading, it can park its task by calling `task::park()`. This returns a `Task` handle which can be used to unpark the task. Then, the `Future` can tell some some async IO service (e.g., `mio`) "unpark this `Task` when this socket has data to read". Finally, the future returns `Poll::NotReady`. When a task sees `Poll::NotReady` it knows that it will be unparked when it should poll the future.
wow, I wasn't even aware of that!
[unborrow](https://github.com/durka/unborrow) heavily relies on hygiene.
hygienic macros are conceptually analogous to closures since both abstractions define a block of code with an enclosing environment. There are three kinds of identifiers in both: 1. Internal identifiers. A closure can have its own variables and macros have hygienic parameters. 2. Passed in parameters. Both can obviously receive parameters. 3. External identifiers from the surrounding environment. For closures these come from the enclosing lexical scope and for macros they are unhygienic identifiers from the macro's call site. Hygienic macros usually means that all the above types are supported and that identifiers that are not parameters are *defaulted* to internal and the user needs to mark the specific identifiers they want to toggle to become external identifiers. Theoretically one could design a macro system with opposite defaults but then internal identifiers would need to be declared in scope just as variables can be defined inside closures. IMO this alternative system would be less ergonomic for the usual macro use cases. Not having hygiene *at all* (i.e. no internal identifiers) and just generating unique names doesn't work because of recursion. Same logic as in regular functions: fn foo(x: i32) { let temp = x + 5; foo(x) } you have multiple instances with the same name - "temp". This can only work if temp is internal to each foo invocation. 
Will there be any penalty on Windows because of the readiness based design while Windows is using completion based IO?
/r/playrust
What's This ? I Opened it I coudn't find anything related to My thing xD
You're lucky! https://github.com/dpc/slog-rs is here for the rescue, with it's composable, reusable drains! :D So happen that `slog`, has a `envlogger` port - `slog-envlogger`: https://github.com/dpc/slog-envlogger BTW. Using `env_logger` like this (setting `RUST_LOG` from within your own program) is ... non-idiomatic. With slog-rs you can just build a filter: http://dpc.pw/slog-rs/slog/fn.filter.html to log only what you want, the way you want.
Thank you, that link explains all my remaining questions.
I made a quick-and-dirty demo project: https://github.com/AtheMathmo/image-compression It uses K-Means from rusty-machine to "compress" images down to fewer colors.
I'm announcing my first crate: RdxSort. It is a generic [radix sort](https://en.wikipedia.org/wiki/Radix_sort) implementation. People might wondering why someone decides to choose such a weird way to sort data. The reason is rather simple: it is fast (see [performance tests](https://crepererum.github.io/rdxsort-rs/rdxsort/index.html#performance)). And I like the idea that comparison-based sorting isn't the only option. I'm rather happy with the overall design of the algorithm. Currently it is limited to data types with a fixed number of sorting iterations (this for example does not include strings). I might extend the generic implementation to fix this. I also might include data structures that use radix information, e.g. sets and maps driven by trees. An important thing to point out is that the performance of the generic algorithm heavily relies on the optimizer (on the Rust-specific one as well as the LLVM-driven one). One major problem is that I'm not able to give the compiler hints about loop unrolling which prohibits some further performance improvements.
Here's the missing magic: https://github.com/tokio-rs/tokio-core/blob/3794cf7f1db616ecf27b6841a0d32facc3dd4bea/tests/poll.rs#L18 The following implementation works: fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { use futures::Async; use futures::task; if self.is_elapsed() { Err((self.f)()) } else { task::park().unpark(); Ok(Async::NotReady) } }
I'm not sure exactly what the benefit is for Mozilla besides the obvious benefits of the language itself for building it's products. Having an alternative to C/C++ that is portable, safe, concurrent, embraces updated syntactic styles, has a safe &amp; fast moving development cycle, an easy build system with package management and has near C performance will probably be useful for many developers down the road. CPUs are hitting the physics when it comes to performance and being able to write concurrent code safely is going to become way more important.
&gt; Tuesday, February 18, 2014
This post is from early 2014, which is a very long time ago in terms of Rust. Rust 1.0 was still over a year away, and the language and its surrounding landscape both looked a lot different at the time. I'm not sure why it's being posted now.
[removed]
Me too. I'm really happy in being able to contribute a little =)
Yes, the current task lives in TLS. We experimented with a more explicit design initially, but using TLS makes it much easier to work with existing traits like `Read`.
I believe `mio` does have a performance penalty on windows because of this, but I'm not sure how much
Well, they will be bigger if you put in a discriminant.
Looking at how Rust development works compared to Go or Swift, I don't believe there many, if any, other organisations that could have produced the language we have now. I've worked on a number of open source projects that are managed by big organisations and Rust is the only one that feels like the majority of decisions are made out in the open. It's not perfect, but many "sponsored" OSS projects end up with the core (i.e. paid) team just throwing stuff over the wall with little to no communication. The article is predicated on the idea that Rust is a large resource sink for Mozilla. This isn't the case. While I don't know exactly who is and isn't a Mozilla employee, there can't be more than 10 across all the teams (not all Mozilla employees are part of the core team) and not all of them are full-time Rust engineers. Ideally, Rust wouldn't be dependent on Mozilla support. In fact, this is the long-term goal. The thing is, Rust is still very new, this means bigger organisations are still evaluating whether or not Rust is worth investing in. However, if you look at the sponsors for RustConf, you can see that there are organisations out there that are willing to put *some* money behind Rust. These things take time though. Rust hit 1.0 less than two years ago, so we're still in the early days for adoption. Adoption is going to have to come first before any large organisation sees Rust as something worth investing money into. For some perspective, LLVM had it's first release in 2003 and the LLVM Foundation was created in 2014. While not a perfect comparison, it shows the kind of time scales these things work on. EDIT: I did not realise the article was several years old at this point. Though it makes many of my points *more* relevant, since at the time Mozilla was putting *less* resources into Rust than it does now.
So, I've been thinking about this. This post is entirely speculation, but.... So, there's going to be some overhead to map the completion-based design to the readiness based one. In that sense, what /u/connorocpu is right: there's some overhead. But this blog post _also_ talks about overhead with trying the completion-based model. In other words, while a _specific_ implementation of two programs might have some overhead through adapting the models, the inherent inefficiencies of the completion model here for a general abstraction might be more than said mapping. Again, this is entirely speculation, and I might be very wrong. I'm thinking about what the post says here; it's also possible it's missing something.
I finally discovered lazy_static and was able to use it in my offline Reverse Geocoder ( https://github.com/llambda/rust-reverse-geocoder ). This was the answer to a question I had been stuck on. Now with my statics, my loaded CSV city data and reverse geocoder live once as static variables, and my webserver can respond to queries much faster because the closure has access to those variables. Before this change, each web request would load the CSV file, build a KD tree, and then query the KD tree, which was naturally slower. Also, the author of kdtree released a new version 0.3 that changed its type signature and now allows kdtrees to work as static variables. I feel like I have gone from Rust noob to Rust intermediate.
IMO list comprehensions are much cleaner for constructing moderately complex lists. Ranges of even numbers, numbers divisible by x, etc.
I say stick with inclusive-exclusive. It's worked quite well in Python for a very long time. Creating all sorts of extra syntax and types increases the chance of a bug, and would make rust look a bit more like operator soup. Its just one concept to learn, too, and a concept that translates well between languages. 
&gt; Again, don't tell me, tell the people you misled in the first place. The code is freely available. Anyone can run it and see that you're wrong. &gt; The conclusion I set out to draw stands. There is no merit in pretending otherwise. 
Huh, I feel the opposite, especially once things start getting complicated. For a simple example, I'll take let evens = (0..100).map(|n| 2*n); over evens = [2*n for n in range(100)] any day.
yall need to be drawing some diagrams and control flow graphs and whatever, this is really confusing.
In the article it says "buildbot"
The idea is that `..` means `=..&lt;`, so no syntax would be removed or change meaning. We could either stabilize `...` as a synonym for `=..=` or remove it.
It's also worth noting that most people will be working at the tokio layer, a bit above this. That said, it's important for great docs here too.
It should be exclusive-inclusive with 1-based indexing. It works suprisingly well, and you can safely return 0 (or `Option&lt;NonZero&lt;usize&gt;&gt;::None`) for "object not found". I am only half-kidding.
The `Future` trait looks pretty different than it did a few weeks ago, with the only required method now being `poll`. I remember there also being a `schedule` method and both taking an explicit `Task`. Where's a good place to read about what the changes are and why?
There's no longer a `schedule` method because futures now implicitly schedule themselves when polling (specifically when returning `WouldBlock`). I don't know why the task is implicit now, other than what the parent comment already said.
Alternatively to `slog`, if you just want more configurable logging over the basic `log` layer, https://crates.io/crates/log4rs
Somewhat, but the kernel also has the ability to use the NIC's memory directly. I'm also pretty sure windows also needs the same amount of kernel buffering that linux does because data could come in from the socket before you provide the buffer, and the kernel still has to do TCP handling, etc...
Although this is an old article I have strong opinions about the subject. I don't want to address it point by point, but will answer the headline question, though the answer should be self-evident to anybody who has followed what's happened since this was written. Mozilla has always been transparent in its motivations for developing Rust: that it needs a safe and fast language for large scale application development, and no other language is considered appropriate to the task. Rust is a hedge against existential Risk to Mozilla's ability to remain competitive in the browser market, in the face of huge opposing forces. Brendan Eich made a daring bet _in 2009_ in anticipation of the needs of Firefox _today_. For a long time it was not clear that anything would come of that bet, but now Mozilla is committed to transforming Firefox in ways that other browsers are not prepared to match, because it has access to Rust, and multiple cutting-edge web browser components written in Rust. Mozilla has more options to keep Firefox relevant in the face of much larger competitors because it has steadily invested a relatively small proportion of it's operating budget in a moonshot. I am astounded every day that it looks like it might pay off. Some of the arguments made in this article are based on the idea that Mozilla's goal is to create a successful programming language. This is only a secondary goal for Mozilla, in service of its primary goal of keeping Firefox relevant; and in some sense Mozilla has not always even been aware of this goal, though the Rust team has always seen it as a strategic imperative in service of Mozilla's goals. Other arguments seem to be based on the assumption that Firefox can do the kinds of things it wants to do with Rust, but without Rust. There isn't any particular evidence to support this besides wishful thinking that Go and D are suitable languages to write a web browser. In fact, one of the arguments the author uses to justify Mozilla should not work on Rust - that it doesn't have enough resources - is _exactly_ why Mozilla supports Rust: maintaining millions of lines of code is brutally difficult and requires vast resources. When it comes to fighting the tide of bugs via brute force, Mozilla's competitors are always going to win. Mozilla wants to work smarter, not harder. That is why it invested in Rust. The point that maintaining a programming language requires lots of money and budget is true, and Rust would be better off with more and more money. What we've achieved on the budget we have, with the help of a lot of people, is frankly astounding, and I'm incredibly proud of it. As long as Rust remains strategic to Mozilla it will continue to be well supported and continue to grow, and someday we will reach an inflection point where significant other resources are supporting Rust. Mozilla develops Rust because it is the only viable solution it has to achieve it's strategic goals, and as a sweet side-effect is building an open community-oriented process to support the revolutionary technology it needs. 
You might find https://github.com/DanielKeep/cargo-script useful for this purpose. I'm not sure what happened to the inbuilt commands, so I can't offer a helpful comment on that, sorry.
If you don't want to wait for the doc situation to improve -- and honestly, I don't blame them for waiting to write extensive docs until things aren't changing so much -- I recommend the [tokio gitter channel](https://gitter.im/tokio-rs/tokio) or #mio irc channel. /u/carllerche and /u/acrichto are very responsive on there.
Well, I've gotten it compiling with your help! Finally have a clean interface between UI and Serial code with no interdependencies. Thanks a lot for your help!
Right now compile times are pretty slow across the board. A few things are happening to try and lower them though. Incremental compilation is going to make small diff compiles a lot faster, that is being worked on right now AIUI. MIR, the new lower level Rust representation, will be opening doors for making compile times faster in Rust, but it's still a ways to go i think. MIR is on by default in 2 releases I think. Sorry for the useless answer, saw you had ni replies and felt like giving something.
If you don't need to actually *run* your code, you can compile with `rustc -Z no-trans` (or `cargo rustc -- -Z no-trans` or use the `cargo-check` crate) to skip translation, this usually speeds things up drastically.
Would love to see either an RxRust implementation on top of these or the Rx combinators and schedulers in the rust futures.
The same could be said about iterators.
It could be useful for iterating over indices twice while skipping repeats. Like for i in 0..v.len() { for j in i&lt;..v.len() { // ... } } I think I'd still use for j in i+1..v.len() { though.
I do like the other answers in this thread, but to my mind this question seems to stem from an overloading of the terms completion and readiness, for two orthogonal concerns. In one setting, they're used to describe how an application interacts with the operating system when submitting I/O requests. Where as in the linked article, it feels like more of a push/pull distinction wrt the I/O driver. Put another way, do you keep asking ("pulling") to drive the state machine vs. having callbacks "push" the state machine through it's workflow. I'd suggest that speculatively, most of the overhead in dealing with IOCP would be (as mentioned elsewhere) that the buffers need to be lent out to the I/O system for the duration of the write, which usually implies heap allocation. Still, having the option to worry about the overhead of heap vs. stack buffers seems like a luxury compared to most runtimes. :)
&gt; It's worked quite well in Python for a very long time. And for C++, and for D, ... About the syntax, I wish we had chosen `[,(,),]` instead of `..` and `...`. Even if that introduces parsing ambiguities, the syntax that is being proposed for i in 0=..=len { ... } for i in 0 .. len { ... } for i in 0&lt;..=len { ... } for i in 0&lt;..&lt;len { ... } is way less readable to me than the actual mathematical notation for i in [0, len] { } for i in [0, len) { } for i in (0, len] { } for i in (0, len) { } so I would be in for keeping `..` as half open and then adding something like the mathematical notation with whatever "extras" are needed to disambiguate it.
&gt; You've used a Vec&lt;Vec&lt;T&gt;&gt; AFAIK the flat version of `Vec&lt;Vec&lt;T&gt;&gt;` is called a jagged vector, but I haven't been able to find implementations of this data-structure in rust yet.
I actually wanted to give it a try and rewrite rayon's backend using tokio (and make the API of rayon non-blocking), but since there are plans to decouple rayon's backend from the library I think I will wait for that to happen first.
First off - Use four spaces before code to correctly format it on reddit. The logic by which closure borrows are selected is pretty simple - They borrow the top-level variables they mention. This avoids a lot of complex interactions (especially involving cases where '.' deos a dereference of a smart pointer) Your closure variable needs to be mutable because it mutates the state (as has to be called via the FnMut trait, which takes `&amp;mut self`)
The article isn't bound to a concrete date, all the arguments still persist.
 thanks a lot for your quick answer. I kind of thought this was a conservative choice for security reason but I could not really figure a case where it would bite to borrow only the sub-field. i am not completely sure of what is a smart pointer but I see what kind of problems it could give. thanks also for the formatting tip. 
**unsafe:** You're right. I'm thinking about replacing them with `debug_assert` to get rid of the checks in release mode. Actually the reason behind this weird behavior is that I want to avoid the checks but enable people to plug in their own data types at the same time. I could introduce some kind of `do_unsafe() -&gt; bool` method to the trait so that the developer implementing the template could decide if he is sure that his implementation is good enough to not return wrong results. **16 buckets:** that was the one yields to the best performance in a test I did some time ago. The number of buckets depends on the data type though (e.g. is different for `bool`). More buckets mean more cache misses but more iterations. So it's an optimization "knob". **2-pass version:** You're right, that would work. The question is if this is faster or not, since some transformations from value+round to bucket number might be quite heavy (see floating point versions). I'll do some tests when I have more time. A similar optimization is the usage of binary bucket sort (= 2 buckets) starting with the most significant bit. It will require way more iterations but would even be possible to be executed in-place. I think that the high number of rounds would ruin the performance though.
thing is, that mathematical notation isn't even standard. In Europe we'd use: for i in [0, len] { } for i in [0, len[ { } for i in ]0, len] { } for i in ]0, len[ { } and such a notation is really bad for parsing anyways. I'd love to use it because it's very readable but nonmatching surround symbols are usually a bad idea.
The year is right above the content of the post, and nobody can modify the title of a post on reddit.
Hopefully "lol hygiene". 
It's certainly not a hold-over from print media.
How would this fit with Linux disk IO, where the readiness model doesn't work (regular file descriptors are _always_ ready)?
Whoa, is this the same park/unpark terminology as WebKit's WTF::Lock for a reason?
park/unpark are pretty standard names in any stuff related to threads. 
Well, only that one function requires the Clone trait bound though, so it shouldn't be part of the trait. &gt; Which makes perfect sense, because the Clone trait has to be satisfied for all types used as Inner. It doesn't. This doesn't happen without HRTB, as you can see here: [Playground Link](https://play.rust-lang.org/?gist=7fa20d95610922b159167fbfeb5ddeae&amp;version=nightly&amp;backtrace=0) trait Lel { type Inner; } impl Lel for f64 { type Inner = f64; } fn bug() where &lt;f64 as Lel&gt;::Inner: Clone {} fn main() { bug(); } 
On Mac/Linux you could make an alias, something along the lines of this: alias rust="cargo build --release &gt; /dev/null &amp;&amp; ./target/release/mysrc" 
Does task::park work through global variables or thread local storage or something like that? How generic is it?
Are you sure that the Rust trait system is supposed to work this way? In the C++ case you can write template functions that get only instantiated for the requested types, but IMHO in Rust the rules are a lot more strict and generic code has to be valid for all possible types, and therefore there seems to be a need for the `Clone` constraint on `Inner`.
Well it may be unintended due to the for&lt;...&gt;, I still need a different solution without the Clone constraint in the trait, as that's definitely not something that I can do. So I need to be able to constrain the type to Clone in some way that is close to the function that actually requires that constraint.
I'm kind of shocked by the amount of editing that questions get in the #rust tag. I've asked a few questions myself, and they were all edited without asking me first. It's incredibly invasive. I feel like people are just editing questions for internet points. In most cases the changes do not really improve the question.
Right, that's what I'm doing in C as well. However, other systems, not based on readiness are handling this much easier. I understand that today disk IO is not even 0.00000001% frequent as socket based IO, but as somebody who's developing disk storage engine, I always hope things will be easier for me :-)
It looks like a bug to me. Have you raised the issue on the bug tracker? Here's a hacky workaround: #[derive(Clone)] struct CloneWrapper&lt;T&gt;(T); fn bug() where for&lt;'a&gt; CloneWrapper&lt;&lt;f64 as Lel&lt;'a&gt;&gt;::Inner&gt;: Clone {} 
Working on my [WebAssembly interpreter](https://github.com/joshuawarner32/rust-wasm). It's currently passing all the official spec tests, but it's failing to run an emscripten-compiled hello-world correctly. On a positive note, it looks like it's failing in the same way as v8 when running that same module - so something fishy is going on. :)
I hope not. That view had made the community toxic and asking for help is so hard to get.
Julia might be a better fit then for one-off data analysis programs, as that's sort of what it was designed for. It uses LLVM to JIT-compile Julia scripts and executes them as native machine code, only recompiling when the script changes. Performance is pretty much identical to C, which is a big win over Python and MATLAB.
This is most likely [a known limitation](https://github.com/rust-lang/rust/issues/30472) (sorry if that's not the best issue for this but it's only one I can easily find). soltanmm and /u/nikomatsakis have worked towards extending associated types past this HRTB limitation, but I don't know when it will be solved, you'd have to ask niko (nmatsakis on IRC if he doesn't reply here).
I never viewed it like that, but it makes a lot of sense.
lol....love this guy's beard http://stackoverflow.com/users/1060004/ustulation
Keep in mind that SO is actually a wiki in the guise of a Q&amp;A site. Editing is to be expected, though since it still displays your username under edited questions it's understandable that one could become angry if edits imply that you've said something you object to (I have no idea if this is the case in your particular situation).
Agree 100% (I'm developing such an application), hence my previous comment that if you could control your tasks performing different types of IO from the same control point (I believe kqueue can help you with this, as it is generic enough not to care - but I never tried), that would be ideal.
It doesn't necessarily answer the question, but it's at least related, so I'll seize the opportunity to link to Dijkstra's neat [_Why numbering should start at zero_](https://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.PDF) (wherein he argues that ranges should be written with an inclusive lower bound, and an exclusive upper bound), in case there's someone here who hasn't seen it.
Possibly also related to https://github.com/rust-lang/rust/issues/23481.
In addition, it'd be great if some more Rust users would help with the [SO Documentation](http://stackoverflow.com/documentation/rust) site - any help reviewing changes or contributing new examples would be great because there's a massive backlog (some changes have been waiting for over a month due to a lack of contributors!) 
I don't have any concrete thoughts at the moment but I wonder if anything can be learned and applied from [Eric Niebler's range library for C++](https://github.com/ericniebler/range-v3).
Hygiene allows names in macros to resolve to names from the macro's scope, such as in fn main() { let x = "old"; macro_rules! old_x { () =&gt; {x} } let x = "new"; println!("{}", old_x!()); } This is not particularly useful with `macro_rules!`, because it only applies to let-bindings. In macros 2.0 it should also be usable for items.
Genial, gracias!
I've noticed this too. It's mainly one guy and he edits almost all questions sometimes even making titles complete nonsense and stuff like that. So it's editing just for the sake of editing. I bet he actually knows quite a lot tho since he also tries answering every once in a while. I find it very annoying tho. Unfortunate situation.
&gt;Anyone can run it and see that you're wrong. There is no "anyone" in this thread. You're wasting your time proving something to someone who never cared about the results of your benchmark in the first place because you feel guilty. That's why you posted the code here, and not in public, as I urged you to... uh, three times now? Four? I can't remember.
&gt; It's mainly one guy and he edits almost all questions sometimes even making titles complete nonsense and stuff like that Given the little activity on the Rust tag... are you talking about /u/shepmaster ? I've rarely seen edits in the Rust tag that mangled titles. I've seen many mangled/useless titles coming in but generally after a few edits the posts become usable (in no small part thanks to shepmaster).
Does Samsung still support Rust, as mentioned in the article? The reference is from April 2013, so not really up to date.
Yes that is him. At least if the SO username matches. I've seen some useless edits and some that imo made it worse. But I guess I'm kinda biased because I always get annoyed when people edit just for internet points.
/u/shepmaster doesn't earn internet points by editing. He edit's a lot, but I agree with him in nearly all cases. And yes he edit's titles, too, but I'm actually surprised how often he finds a super descriptive title for the question that certainly improves the old one. About that and the "annoyed by editing"-problem, please read [my comment here](https://www.reddit.com/r/rust/comments/51q8oa/5000_questions_on_stackoverflow/d7e985h). I really hope that the communication about "what SO is" get's better ... to avoid these kinds of misunderstandings which are bad for both sides :/
"TLS" as in "Thread Local Storage" in this case. It's usually provided by the threading library.
Hi! &gt; sometimes even making titles complete nonsense and stuff like that That's certainly not my intent! I'd love to get pointed to some titles that I've screwed up though. A lot of the time, my goal is to make the titles more descriptive to the question asked in the body. Otherwise we'd have a lot of "question about lifetimes", "question about lifetimes too", ... Creating a good title is **hard**! I actually think that we should write the body of the question first and then form the title most of the time. When I ask questions, I tend to revise the title heavily after typing up most of the content. &gt; So it's editing just for the sake of editing I don't believe so, and I'm sad that's the impression I've given. I want every SO Q&amp;A to be one that people are happy to land on from a search engine. Proper grammar, spelling, formatting, links to related questions, etc. &gt; I bet he actually knows quite a lot tho since he also tries answering every once in a while I do attempt [to answer questions sometimes](http://stackoverflow.com/tags/rust/topusers). &gt; It's mainly one guy One shouldn't assume gender on the Internet.
&gt; I'm actually surprised how often he finds a super descriptive title for the question that certainly improves the old one. Thank you very much! The fact that one person notices and thinks it helps makes it worth it!
I'm back working on my serial terminal program. With the help of people in thi s subreddit I've finished a refactoring I've wanted to do for a while: split the serial thread into a separate module and keep the UI stuff contained to main.rs. So this week I think I've accomplished the big stuff. Now I'm gonna actually start using the program for what I intended it for, to send text ASCII files to my serial data logger device I'm building.
The logical thing would be for incremental to only be used on debug builds, and for a full rebuild to happen with release builds, but I have no idea what they'll do in the end. I *am* sure that this will not lead to an unavoidable runtime performance regression. That wouldn't be acceptable to anyone.
I'm looking forward to fast compile times in Rust. In particular, I like that the compiler can tell that only a part of a file has changed (one method in an impl) which should make it vaustly superior to c++'s incremental compilation which has to recompile everything that references that file.
Let's put it that way: In general you get the best performance when you present the whole program to the optimizer at the same time. A great example of this is the recently emerging "link time optimization" in GCC and LLVM-based compilers. You can even improve that when you add profiling information which requires you to actually compile the program twice and run representative performance tests in between. What Rust could cache are some transformations that are not part of the optimization process (e.g. type inference). The problem with production / release builds is that these type of optimization are normally not the heavy-lifting part (just compare debug and release build times of your programs). So this type of conservative incremental compilation won't help you much. TL;DR; Incremental compilation has its performance costs.
 cargo rustc -- -Z incremental=&lt;dir&gt; You can also export env variable `RUSTFLAGS="-Z incremental=&lt;dir&gt;"`, or set the [`rustflags` key in `.cargo/config`](http://doc.crates.io/config.html#configuration-keys). You can also add `-Z incremental-info` to get some info whether it's actually reusing something (The info has a module granularity, but I presume the actual incremental compilation is more fine-grained)
Yes, in theory we should be able to do better than C/C++. We are basically using the same model but it is up to the compiler to decide what goes into separate compilation units, giving us a bit of an edge there. 
Thanks, this is the solution I ended up choosing, and works like a charm. All I needed is to add a Cargo.toml file to my analysis directories. Doing ",c" in vim now runs my program. 
If there's no option for that, then I'd call that a bug. I'd bet a nominal small amount of money that the maintainer(s) would be receptive to a fix or report of that. :) 
Not at all. The C JIT was introduced in JRuby as a way to port Ruby extensions into JRuby without having to pay for marshalling performance between JVM and external code.
The lasted versions of VC++ are more fine grained than that. Lucid C++ and CSet++ were able to do incremental compilation at function level.
Has been [proposed](https://github.com/rust-lang/rfcs/issues/1677).
C++ has syntax for ranges‽
Not, need test, thnx!
Not bad, Google Translate, not bad.
&gt; if a `Task` polls a future and the `Future` calls `socket.read()` and realizes that the socket isn't ready for reading, it can park its task by calling `task::park()`. Sorry, I'm still missing something. The `Future` "knows" about the socket, but not about the `Task` (since `poll` takes no other argument). So how can it `park` the `Task` and schedule the `unpark` as a callback on socket-ready? Is it that the `Future` trait is always implemented such that it gets a handle to the "parent" `Task` object during initialization (so that it "knows" by the time it is polled)? And does it also imply that all futures are associated with a unique task object?
This is true for programming languages, which are called "lenguajes", but human languages are called "lenguas" (as in "lengua española"), except when referring to foreign languages they're more often called "idiomas" ("idioma oficial", "aprender idiomas"). So there's no idiomatic Spanish way of making /u/fullouterjoin 's joke :( But if I had to choose I'd go with "lenguajes" too, as the analogy feels clearer that way. (This might be a European Spain thing, though.)
There's a separate option to enable debug info for jemalloc. So that's one part of the mystery..
Also Box, since it automatically frees its pointee on drop. In some way, even plain borrows in Rust are smart pointers, given the whole dangling safety thing.
France. But I've seen it elsewhere. Mathematicians are great at making but notoriously bad at standardizing notation unfortunately.
Great work! I did a quick test on a project of mine and realized, that WorkProducts get recompiled also on whitespace changes. Didn't the article say that the detection of changes is based on the AST?
Holy crap, those projects... I just barely finished TowersofHanoi implementation in Rust.
Are you also envious of go's error handling and type system? :P
I think the more pertinent question is - are you jealous of go's lack of generics? My understanding is that Rust's strong support for generics is where a lot of the compile time challenges come from. 
No, but if I have to pick one or the other I'm not entirely sure which I prefer. Rust's compile times can be abysmal to the point where it can seriously affect developer productivity.
Sure, Box too! Haha, good point.
Yes: `for (auto&amp;&amp; i : range)` where range is always inclusive-exclusive `[first, one past last)`. Inclusive-exclusive ranges make dealing with empty ranges trivial (`size([a, a)) == 0`). If you want to implement something different, e.g. an inclusive-inclusive range like range-v3's `view::closed_iota` (`[first, last]`), you do so by transforming it into an inclusive-exclusive range: `[first, last + 1)`.
[Large code base](https://github.com/matthiasbeyer/imag). And the 50 minutes are on travis-ci - locally it is a bit better, depending on which machine I'm building.
I don't believe that llvm has Og support, so by proxy I don't think rustc does either. I might be wrong though. 
This makes we wonder if parts of the (non-LVVM) compilation are feasible to parallelize... hmm, probably not worthwhile until the borrow checking works off of MIR... 
That is a terrible idea. Debug builds can be 1/100th the speed of release, incremental build might take a small percentage off the release performance. Iterating on something large and computationally intensive is a prime candidate for incremental compilation.
What helps Rust to have finer-grained nodes than C++? Is it thanks to language syntax (graph is simpler to parse)? Or just the approach/algorithm of building dependency graph is better than in C++ compilers?
Cool, thanks!
I'm not sure if C++ as a language poses significant obstacles to making the dependency graph more fine-grained. What I was talking about was the source &amp; header files model, where it is up to the user to factor things into files/compilation units. What the Rust compiler does could be compared to putting each function defined into its own header file, so that touching one function doesn't invalidate a bunch of other unrelated functions (Note that we are not quite there yet implementation-wise). In short: We are doing a lot more things automatically behind the scenes, giving us the opportunity to do stuff that would be way to tedious to do manually.
Yes, incremental compilation is not meant to be used for production builds. For those you want to build with `-C lto` and `-C opt-level=3`, which takes a while but will bring the best performance.
The AST is just a tree, so it sounds to me that one should be able to e.g. use rayon's join for parallelizing both building it, and recursive algorithms that work on it. /u/nikomatsakis you _must_ have actually thought about this a lot. What are the main issues?
I don't know where you are getting this from, `rustc -Og main.rs` works today... and it definitely both optimizes and has debug symbols. Edit: An example demonstrating both $ cat main.rs fn fib(x: u32) -&gt; u32 { if x &lt;= 1 { 1 } else { fib(x-1) + fib(x-2) } } fn main() { println!("{}", fib(40)); panic!("To show debug symbols"); } $ rustc main.rs $ RUST_BACKTRACE=1 /usr/bin/time ./main 165580141 thread 'main' panicked at 'To show debug symbols', main.rs:8 stack backtrace: 1: 0x55f916f305ad - std::sys::backtrace::tracing::imp::write::h29f5fdb9fc0a7395 2: 0x55f916f32e21 - std::panicking::default_hook::_{{closure}}::h2cc84f0378700526 3: 0x55f916f323bb - std::panicking::default_hook::hbbe7fa36a995aca0 4: 0x55f916f329f7 - std::panicking::rust_panic_with_hook::h105c3d42fcd2fb5e 5: 0x55f916f29766 - std::panicking::begin_panic::h5c54e97e5cda1cc4 6: 0x55f916f29ad9 - main::main::he714a2e23ed7db23 7: 0x55f916f326e7 - std::panicking::try::call::h5df3ac2979db3c90 8: 0x55f916f3a926 - __rust_maybe_catch_panic 9: 0x55f916f31b89 - std::rt::lang_start::hfe9ab243c60ffb9b 10: 0x55f916f29b09 - main 11: 0x7fb459ec7290 - __libc_start_main 12: 0x55f916f295b9 - _start 13: 0x0 - &lt;unknown&gt; Command exited with non-zero status 101 1.02user 0.00system 0:01.02elapsed 99%CPU (0avgtext+0avgdata 3420maxresident)k 0inputs+0outputs (0major+298minor)pagefaults 0swaps $ rustc -Og main.rs $ RUST_BACKTRACE=1 /usr/bin/time ./main 165580141 thread 'main' panicked at 'To show debug symbols', main.rs:8 stack backtrace: 1: 0x5564998c726d - std::sys::backtrace::tracing::imp::write::h29f5fdb9fc0a7395 2: 0x5564998c9ae1 - std::panicking::default_hook::_{{closure}}::h2cc84f0378700526 3: 0x5564998c907b - std::panicking::default_hook::hbbe7fa36a995aca0 4: 0x5564998c96b7 - std::panicking::rust_panic_with_hook::h105c3d42fcd2fb5e 5: 0x5564998c06e4 - std::panicking::begin_panic::h5c54e97e5cda1cc4 at /buildslave/rust-buildbot/slave/nightly-dist-rustc-linux/build/obj/../src/libstd/panicking.rs:338 6: 0x5564998c07a0 - main::main::he714a2e23ed7db23 at /home/normal/main.rs:3 7: 0x5564998c93a7 - std::panicking::try::call::h5df3ac2979db3c90 8: 0x5564998d15e6 - __rust_maybe_catch_panic 9: 0x5564998c8849 - std::rt::lang_start::hfe9ab243c60ffb9b 10: 0x7f8bde993290 - __libc_start_main 11: 0x5564998c0589 - _start 12: 0x0 - &lt;unknown&gt; Command exited with non-zero status 101 0.28user 0.00system 0:00.29elapsed 99%CPU (0avgtext+0avgdata 3488maxresident)k 0inputs+0outputs (0major+297minor)pagefaults 0swaps Notice the time difference and line number information in the second command.
A bit light on the details. https://translate.google.de/translate?hl=de&amp;sl=ru&amp;tl=en&amp;u=http%3A%2F%2Fzoom.cnews.ru%2Fsoft%2Fnews%2Fline%2F2016-09-08_doktor_veb_issledoval_linuxtroyannapisannyj How do they know it's written in Rust? How do they know it's cross-platform and simply recompiled? How do they know the IRC library is used? Where/how did they find it? - especially since the other article suggests the IRC channel is non-operational.
Started working on creating some functions from Haskell that I would like to have in Rust so I can start making my code more functional and less iterative.
I realize it is still in early development, but what's the advantage of imag over using the 'Contacts' or Outlook etc. ?
I guess this means Rust is ready for production
I have been saying I would contribute to this but then just got very lazy about it. I will put some time aside to help improve the SO docs stuff for Rust.
on the bright side, no memory corruption
Thread-local storage. Basically, it calls the *function* `task::park()` which internally looks up the task currently executing on the current thread. The devs considered threading through a reference to the task but that just complicated things.
&lt;3 
I know that the rust team does download crates with bots and compile them. This can uptick a crates download count not sure by how much.
This is pretty cool. I wrote a small backdoor Linux backdoor in rust at my old job to see if it would be a good replacement for our C programs. It worked really well in testing but didn't actually see use in production. I'm working on a rootkit in rust at my new job. Definitely more enjoyable than C 😁
Oh, sorry for the confusion. Yes, you can pass `-Og` to `rustc`, which is just another way of writing `rustc -O -g` and will give you an optimized build with debuginfo. GCC, however, has a distinct optimization level `-Og`, which optimizes the code in a way that has less impact on debuginfo quality than regular optimization levels. That's what we don't have in `rustc`.
You're very welcome! :-D I plan to add a curses-UI as soon as we have a few tools implemented, though I consider such an interface as nice-to-have: Everything should be doable via the pure commandline interface (and hence also from scripts). The curses UI would be a nice-to-have feature for visualization of some things.
Note: you only gain reputation from editing [prior to gaining the edit privilege](http://meta.stackexchange.com/questions/87572/no-points-for-editing-after-gaining-edit-rights), on SO it means prior to having 2,000 points. This means that none of the [Rust top users](http://stackoverflow.com/tags/rust/topusers) gain anything from editing.
I know this is not directly related to rust, but as a community that has a higher focus on avoiding bugs, I thought this was interesting enough to share. The tool is created by John Regehr, who is one of the most prolific bug finders/triagers for both LLVM and GCC, and it is one of many tools he uses to eliminate bugs. I guess while I'm at it, he's got a few others that would probably be interesting to the compiler team: https://embed.cs.utah.edu/csmith/ http://embed.cs.utah.edu/ioc/ While I'm not exactly a compiler writer, I have a background in Operations Research which involves a metric shit ton of Mathematical Optimization, so [this project](https://github.com/google/souper) (which [John Regehr helped spur](http://blog.regehr.org/archives/1109)) is of extreme interest to me. I'd love to maybe talk with other engineers that are involved in the new Mir optimization engine and see how we could build something similar that works on more macro-level optimizations that can benefit from the massive amount of metainformation that resolved types and lifetimes can provide to the optimizer. 
Wait, `[a, b)` is real C++ syntax?
But what is `range`? How do I express a range from 1 to 10?
C-Reduce is amazing, and it works reasonably well on Rust out of the box, e.g. [a few issues have had their test case reduced with it](https://github.com/rust-lang/rust/search?q=creduce&amp;type=Issues&amp;utf8=%E2%9C%93).
Rust is absolutely ready for production.
range is an object for which `begin(range)` and `end(range)` return objects of a type that model the `Iterator` concept. Using range-v3 you can use both: for (auto&amp;&amp; i : ints(0, 11)) or for (auto&amp;&amp; i : closed_ints(0, 10)) but the iterators returned from both objects always denote an inclusive-exclusive range; `closed_ints(0, 10)` is actually just `ints(0, 10 + 1)`.
And this is a big step toward fixing that. I don't see what the point of complaining here is--we do obviously care about compile times a lot, to the point of going *farther than just about any other language* in developing sophisticated techniques to improve them (incremental compilation).
Even if C++ had automatic tracking and modules, it would still be much more difficult to build the dependency graph. The problem is that C++ has function overloading/duck typing. Function overloading means that adding a new function with a given name requires re-performing the overload resolution for any single use of a function with a similar name which could access this namespace during name look-up. The same idea will also apply to templates (generics) which use duck typing rather than constraints, and thus pick symbols from all around the place. Once overload resolution has kicked in, you would end up in the same state as Rust (or most any other language, I suppose), but C++ would have a much harder time caching the transition from "AST" to "MIR" (and would re-use the cache less often).
What is your job? Genuinely curious
Pretty sure they were joking
What about femaleware?
asmx85's autocorrect is sexist.
if you look at this feature in isolation -- true. but it's more about the implication of accepting contextual implicitly declared magic things in your language. iirc the language isn't available to the public yet anyway
Working for the russian government.
Am I missing something? What makes this rust relevant?
That's pretty cool! What about dll injections with Rust? Is it feasible?
&gt; what happens to closed_ints((uint8_t)0, 255) Doesn't compile, because `closed_int` signature is `T,T -&gt; Range&lt;T&gt;` but you called it with `uint8_t, int` (so `T` cannot be deduced). If you were to call it like `closed_ints((uint8_t)0, (uint8_t)255)` it actually works. Just checked and `closed_int`'s implementation is a bit more clever than than `ints(x, y + 1)`, it stores both values of the closed range, a boolean saying whether iteration terminated, and for this range object `begin`/`end` return iterators of _different types_, that mutate the range object, and only compare equal after the last element of the range has actually been returned.
Rust is a pretty high level language if you're trying to learn assembly and/or shellcode. You might be better off writing C and compiling with no optimization and looking at what's generated with the -S flag, or just no optimization and using objdump or radare2 to disassemble it. If you want to learn low level linux stuff I highly recommend [this awesome book available as a free PDF](http://instructor.sdu.edu.kz/~konst/sysprog2015fall/readings/linux%20system%20programming/The%20Linux%20Programming%20Interface-Michael%20Kerrisk.pdf), the Linux Programming Interface. It goes into very deep detail and has example C code in it. Over a thousand awesome pages. I read through most of it except for the pseudo-terminal stuff because that is confusing as all hell. Another great book is "Introduction to 64 Bit Intel Assembly Language Programming for Linux" ([Amazon.com](https://www.amazon.com/Introduction-Intel-Assembly-Language-Programming/dp/1466470038)), and you also might want to pick up a newer Instruction Set Reference for intel processors. You also might take a look at MSFvenom and look at the encoder modules. 
[removed]
Heh, I'm too used to Rust type inference :) Except for the thing about comparing iterators, that's how `core::ops::RangeInclusive` works, FWIW (it's an enum discriminant instead of a boolean).
There's a [sliding windows crate](https://github.com/flo-l/rust-sliding_windows) that may do what you want.
I miss those. Oh well.
Given that a good way to get rid of Python's GIL is to write a Python VM in Rust (already happening), this can be very relevant. 
Does anyone know if there will be any talks related to this the day prior and also if video will be recorded at RustFest and made available to the public? I can't find a list of talks -- only workshops -- at their website http://www.rustfest.eu/ and also it doesn't mention video. I might like to watch recordings if they are made, especially if there are any talks about crypto or blockchain in relation to rust.
Meh, to-may-to, to-mah-to. *Lengua* is just fine, there are books with that in the name, like the *Diccionario de la lengua española*.
The `let mut tile = arr[2]` statement moves element 2 out of the array and binds it to `tile`. Due to deriving `Copy`, the move actually makes a copy of the element, and so you end up mutating the copy. If you want to mutate the element in place you'll need to make the array itself mutable and take a mutable reference to the element. #[derive(Copy, Clone, Debug)] struct Tile { num: usize } fn main() { let mut arr = [Tile { num: 0 }; 5]; println!("{:?}", arr); { let ref mut tile = arr[2]; // or: let tile = &amp;mut arr[2] tile.num = 6; } println!("{:?}", arr); }
Thanks I did end up figuring out that `&amp;mut arr[2]` invokes the `IndexMut` trait which lets me modify the object in place
He's very much a core dev
of course it is, in fact Windows exposes a number of API calls for injecting anything that exposes a `DllMain` symbol, which Rust can of course provide.
I've done a quick search and can't find any project. Link?
&gt; The modules/plugins/extension/modifier system in Iron, in particular, is woefully under documented... I have used Iron for a couple toy projects and this certainly is the case for me too. In order to understand modifiers, I had to read the source which re-exported the modifier crate and then follow that source too. Perhaps I should start writing documentation PRs or a blog post instead of complaining :)
Thanks. It's actually one of my concerns that someone uses my name if my crates become popular. Is there a way of filing a takedown if this is the case? Are there community crate curators?
If you look at a binary in a disassembler, you see *much* *much* more than in what language is has been written - you even see the compiler it has been compiled with, as they all have different patterns that they like and use a lot.
I don't know much about working with dlls unfortunately, but I haven't run into anything that I can do in C that I can't do in Rust.
A good place to start is searching for free CTF exercises online. They're usually preconfigured VMs that are purposefully vulnerable to attack. /r/netsec and /r/asknetsec are also good sources of information.
Unrelated, usually it's `-` not `_`.
&gt; it's an enum discriminant instead of a boolean Nice, LLVM should generate the same code for both then, since the enum "type index" can only have two values (and is semantically equivalent to just a boolean).
Not really, Rust's `HashMap`s are far more specific a datastructure than Python's dicts. When you ask for a dict you want a thing that can hold things (which can be implemented in many ways). When you ask for a hashmap you want a thing that holds things with various guarantees on complexity. Dicts in python are also used a lot more than hashmaps in Rust. You can often avoid the need for hashmaps in Rust by just using structs or generics. The cases they are optimizing for are less common in Rust. When you reach for a hashmap in Rust it's usually pretty certain that you're going to stuff a ton of things into it. In Python you may have relatively tiny dicts quite often. (Rust's hashmap algorithm is a pretty good one too)
Yes, you did post that there. That doesn't have much to do with anything, but hey, I'm not the one who's selling something.
Awesome! Good luck with that. Sounds like a fun project.
I'm not in charge of crates.io by any means, so I don't have a clue.
The documentation issue will get better after the foundation does. Its likely there will be a waterfall of documentation. First for Futures/Mio, then Tokio, then Hyper, then Iron/Nickel. One other curiosity is about how Iron fits into the new Hyper-Tokio stack. Iron is currently a framework for adding middleware, and also a collection of middleware. Now that Tokio exists and its primary feature is compostability of middleware, will Iron just become a collection of Tokio middleware?
There's also the very useful [`itertools`](https://crates.io/crates/itertools) crate, which adds some adaptors that allow multi-element consumption. Most generally the [`batching`](https://bluss.github.io/rust-itertools/doc/itertools/trait.Itertools.html#method.batching) adaptor.
It turns out to be kind of unfixable. To make ints fast, they implement the SizedRange concept (they provide O(1) size to compute their length). However, how many elements does `[0, u64::max()]` has? Well `u64::max() + 1`. So that cannot ever work :/
Eh, I guess I don't expect as much from a web framework. I built a mini server as a start for a game I'm working on which will have game rooms and chat and I just needed a router and TLS (well, TLS wasn't strictly required). I tend to avoid templates and databases (especially in the early stages of a project) and prefer to build boring HTML using JavaScript to make the dynamic content (I.e. use Angular or something). Who needs templates when you have JSON? I guess I expect less from my frameworks, but Iron is perfectly fine for me, especially in the early stages is a project when I have no idea what it's going to end up like.
Currently Iron 's handler is a synchronized one, which might not fit into futures/tokio model out of box. So I guess there will be some redesign to port Iron to tokio. Or use a thread pool.
There's a PR to get the package policies on the website, they're on internals. I'm on mobile or I'd link you. WRT the Code of Conduct, package names, etc, are subject to them, but we won't actively police it; we'll rely on the community reporting issues.
I had similar experiences with both Iron and Nickel, especially coming from batteries-included frameworks like Rails and Laravel. I now have an intermediate understanding of Iron, but only after much experimenting, deep reading of API docs, and banging my head against the wall. The ORM story is improving as well, gradually, as Diesel gets more mature, but again, that needs to be manually wired into your Iron app. I think if Rust had a fully loaded framework like Rails that tied all the necessary components together, it would drive adoption of Rust for web applications much faster. Web developers generally don't want to worry about setting up their framework, they want to be able to change a few config files then start writing controllers immediately.
Does anyone know why usize doesn't implement From&lt;u16&gt; or From&lt;u32&gt;? All these explicit integer conversions are driving me crazy.
Thanks
Definitely have to agree. I tried to get something going and after a lot of reading, trawling through the source and experimenting I got something going, but...decided that I'd rather implement my service in another language (in my case Go). It all felt over-complicated. Unfortunately, unlike the author I haven't helped out the Rust ecosystem via documentation, but I'll try take that on.
I don't know if Rust is a good fit for a web app. I suspect it's a bit too low-level, and one probably needs to know too many concepts to be productive in it. But, YMMV.
There's work in progress to support these types of conversions via [`TryFrom`](https://github.com/sfackler/rfcs/blob/try-from/text/0000-try-from.md), which is similar to `From` but allows for the possibility of failure. `usize`is smaller than 32 bits on some architectures, so a conversion from `u32` to `usize` could potentially fail.
The [Go malware](http://vms.drweb.com/virus/?_is=1&amp;i=8436299&amp;lng=en&amp;utm_source=golangweekly&amp;utm_medium=email) was less lazy thou...
Let me guess. This backdoor app was compiled using rustc? Rather than any of the other Rust compilers?
eyes are bleeding
I don't think the type system has a high overhead, IIRC most of the time is spent in code generation and optimization. `cargo check` usually takes about a second for me.
Using Hyper alone on the other hand is very easy. The api and documentation is good. You have to write some things yourself but you'll probably be fine.
Really? I assumed that usize was guaranteed to be 32 or 64bit. Are there any architectures where it is 8 bit though? Because it seems like at least From&lt;u16&gt; should be supported.
Is this a bug?: let mut a = Some([0, 1]); a.unwrap()[1] = 0; println!("{}", a.unwrap()[1]); The above code should print `0` but instead prints `1`
Why array of tuples of two `i32` is unsized? i32 should be 4 bytes, right? error: the trait bound `[(i32, i32)]: std::marker::Sized` is not satisfied [--explain E0277] --&gt; &lt;anon&gt;:1:44 |&gt; 1 |&gt; fn dda(x0: i32, y0:i32, x1:i32, y1:i32) -&gt; [(i32, i32)] |&gt; ^^^^^^^^^^^^ note: `[(i32, i32)]` does not have a constant size known at compile-time note: the return type of a function must have a statically known size
Another library which does something similiar is https://github.com/zslayton/lifeguard. It does not support thread local storage (for better or worse) however but I think that might be possible to add. As for this library I would recommend detaching `impl Recyclable for $t`and add `impl&lt;T&gt; Recyclable for Vec&lt;T&gt;` in the library itself (as well as implement it for `HashMap` etc). That should reduce a bit of duplication.
BTreeMap might be better, or a vec of capacity 5, or yes, a hashmap, though a vec in most cases could be cheaper. But, the case of stuffing a few things in a hashmap is far less common than in Python, where dicts are used as a stand-in for structs. Which was my point -- the design requirements for dicts in python is very different from that for Rust, and the algorithms they come up with aren't likely to be useful for us. 
 let ref mut tile = arr[2]; // or let tile = &amp;mut arr[2]; Which is more typical/idiomatic? Also, are these preferred over making the array an array of pointers?
The iteration just goes straight down the table, which is built by just appending to the end. So now the iteration order is always the insertion order. Previously, it depended on where the data was placed in the table, which was a function of the hashing.
Personally I'd say the second. Would also prefer over pointers.
 &gt;In Python you may have relatively tiny dicts quite often. Afaik every time you call any function a dict is built. Also, every object instance is represented as a dict as well as most of the scope objects. Dicts in Python are literally everywhere, which is why a size reduction for dicts may result in huge benefits. 
I would recommend coming up with a naming scheme and uploading empty crates so you can stake claim to all the namespaces you need at one time. Prefixing it with `pickles` is a great idea. Crates.io will have to address the namespace ownership at some point. 
`[(i32, i32)]` doesn't have a defined size. You have to declare the array size in the type. Try `[(i32, i32); 2]`.
/r/playrust
It looks like you are recompiling all dependencies for every binary https://travis-ci.org/matthiasbeyer/imag/jobs/158808454. Have you tried setting up a cargo workspace? Its only for nightly but I *think* it should let you avoid the recompilations.
Thanks, didn't know about that
Just assuming that you are not trolling, because I'd like to think you can read (the sidebar of the subreddit) and trolls are neither wanted nor needed here. This is the subreddit for the programming language Rust, the game Rust is (as mentioned by /u/K900_) at /r/playrust.
Lots of algorithms benefit from constant time _correct_ size/distance computations, and lots of algorithms depend on these. From the top of my head, the most basic and important ones are: - `equal(rng0, rng1)`: if you have O(1) size, you can break early since if the size is not equal, they cannot be equal - all the algorithms that partition a range some how: partition point, all the binary search ones (lower_bound, upper_bound, equal_range), need to find the middle (or something similar) of a range, for which they need to know the correct size in O(1). Lots of algorithms build on these (selection problem, stable/unstable sorting, rotate ...). - other lazy algorithms like reverse need this information as well. I know there are others, but random access without correct size only gets you that far. For lots of algorithms you need correct size. Range-v3 has RandomAccessRange which is independent of SizedRange for this reason. Rust has [`ExactSizeIterator`](https://doc.rust-lang.org/std/iter/trait.ExactSizeIterator.html) for this same reason, and in particular `0...5` implements this, which will probably fail with `0..u64::max()` since size will be stored in an `usize` that overflows triggering a run-time panic. Basically the only thing you can do here is to panic _earlier_, e.g. if someone passes `0` and `u64::max()` to `...`.
I'd just go by tags, that seems fast enough.
The second is more typical in this kind of usage. For something so simple I don't think it matters much and think the former looks better and conveys intent better so I chose that. Typically though `ref` is left for use in patterns. Yes, definitely prefer both over an array of pointers.
Workspaces are already [there as PR](https://github.com/matthiasbeyer/imag/pull/624) but imag is compiled with beta and stable only, so we do not yet have cargo with workspace support. I'm really looking forward to get the workspaces feature!
Hi. This is the wrong subreddit. You want /r/playrust. Make sure you read a subreddit's rules before posting.
Doesn't jemalloc handle this pretty well?
[removed]
[Wrong](http://imgur.com/gw3cMin.png) [subreddit](http://imgur.com/xCPcX1e.png). /r/playrust
The `regex` crate uses a wonderful library called [`thread_local-rs`](https://github.com/Amanieu/thread_local-rs) written by /u/Amanieu for *really* fast thread safe access to a pool of previously initialized values. The key is that it allows for dynamic per-object thread local values as opposed to statically known thread locals like with the `thread_local!` macro.
Thanks! :)
Why would this affect the optimization? If anything the effect of reduced memory usage should *increase* in importance as `HashMap`s get larger and fixed overheads are reduced, no?
[Said PR](https://github.com/rust-lang/cargo/pull/3057), and [said original internals post](https://internals.rust-lang.org/t/crates-io-package-policies/1041).
PyPy found the performance was break-even at small sizes and significantly faster at large ones, so at least in practice your claim doesn't hold. This isn't an optimization for small values. One difference between Rust and PyPy/CPython here is that Rust's maximum load is 95%, whereas it's a mere 2/3 in the Python implementations. This does mean that even in the worst case of a totally full `dict` the Python implementations save memory; Rust probably wouldn't. (That said, it would let Rust cheaply use a lower load, which should increase speed.) But the average case matters too. Reducing the active memory (even if the officially allocated memory stays roughly the same size) should significantly improve cache performance, since unvisited memory is free. FWIW, AFAIK Rust's "average" load is ~71%, the Python implementations supposedly average ~58%. &gt; Python can get away with this because it already has a GC, so deleting items by setting the item to a tombstone value is fine. Rust can't. I don't know what you mean by this. Rust can use tombstones too. It's even easier if you don't care about preserving order through deletions (and just want the memory improvements), since you can just swap with the last element. &gt; And the hash algorithm Rust uses may require reads from multiple buckets to find the key, so the cache behavior is probably important here. So do they all. 
Very interesting article -- I would love to see Rust in the comparison.
I agree with your skepticism. Even if it does end up noticeably slower, a similarly-fast ordered dictionary would be useful, though. &gt; It might be implemented as a linked list internally, which doesn't impose an additional penalty since everything is already boxed? There's no linked list, just double indirection. An array of offsets into the second array.
Note: A *slice* is sized, eg. `&amp;[(i32, i32)]`, since the data to be stored on the stack is just a pointer-length pair.
[**Edit**: Oh duh, you were asking about `ExactSizeIterator`. The answer there is that `(0...usize::max_value()).len()` panics; this is a bug and `RangeInclusive&lt;usize&gt;` should not implement `ExactSizeIterator`. See original reply below.] Good question. The `size_hint` method on iterators is [specified](https://doc.rust-lang.org/beta/std/iter/trait.Iterator.html#method.size_hint) to return `(usize, Option&lt;usize&gt;)` for exactly this reason. &gt; The second half of the tuple that is returned is an `Option&lt;usize&gt;`. A `None` here means that either there is no known upper bound, or the upper bound is larger than `usize`. So then we have $ cargo +nightly script -u inclusive_range_syntax -e '(0...usize::max_value()).size_hint()' (18446744073709551615, None) Other calculations should work fine (if not there is a bug). $ cargo +nightly script -u inclusive_range_syntax -e '(0...usize::max_value()).map(|x| x*x).take(10).collect::&lt;Vec&lt;_&gt;&gt;()' [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 
&gt;Crates.io will have to address the namespace ownership at some point. I'm not sure they will :/
Is this right? It seems like there are things in the stdlib that definitely depend on `usize` being at least 32 bits, like `Range&lt;u32&gt;` implementing `ExactSizeIterator`.
These are great suggestions. Thanks for taking the time to look over my code.
It's not that crazy, it's just a macro :) As far as I understand it, the general strategy of the lazy static crate is to create a type hiding an `std::sync::Once`, and it also implements `Deref`, so the first time you dereference the static it runs the initialization function and stores the object... somewhere (on the heap, I think). And then every time afterwards it returns a reference to the stored object.
I was confused about this comment, but I just had to disable uBlock Origin to see it (it was probably blocked by the anti-social filter)
I haven't looked at the binary (where can I get it?), but yeah, that's how you figure out something has been written in Rust.
Probably for learning purposes
I don't really know what specific problem you're trying to solve, but all I saw was "reuse allocations." `thread_local-rs` definitely fits there. `ThreadLocal` having a scope makes perfect sense for the problem it's trying to solve. :-)
Theoretically if Rust supported any 16-bit or 8-bit platforms, `usize` would be that size. It looks like [this](https://doc.rust-lang.org/book/getting-started.html#platform-support) list doesn't include any.
Too early (it's not born yet). But thanks anyway.
I have removed your post, as it does not belong here. Feel free to post it to /r/playrust
Compile with arm toolchain crash(( https://gitlab.com/snippets/25916
I've also used https://github.com/carllerche/pool in the past.
I'm just trying to understand: Is this better than coroutines? How? From a usability point of view, it is not. From a performance point of view, I'm really curious, so you don't need to care about explaining usability to me (really, don't even try). I've watched a talk from last year's cppcon and with just coroutines it was possible to solve all the problems you guys are struggling (and being successful although creating a lot of complexity, as a coroutine is JUST a function which can suspend and resume and this solution of yours is a big architecture change at the library level which needs lots of posts to understand to possibly provide no benefit performance-wise over pure coroutines): https://www.youtube.com/watch?v=_fu0gx-xseY I'm honestly willing to understand the difference. Could you expose them to me, please? I don't want to see Rust with an abstraction that would be inferior to the C++ one as until now Rust was better in every way and I want to fully migrate to Rust.
Is there any operating system for which the readiness model is useful for asynchronous file I/O? As a file is really always "ready". Is there any workaround you can think of to make this work beautifully using `futures-rs`?
How much would a Kickstarter campaign need to do something like this? I could imagine a GPL project that stabilizes a framework, documents, and tests then passes the torch to the community. 
You can use the minted package. It uses pygments to do the actual highlighting so it supports all languages supported by pygments (including rust).
I want too do something like a tree structure Basicly i want Struct Node { value : u32, left : Option&lt;Node&gt;, right : Option&lt;Node&gt; } impl Node { get_left (self) -&gt; Node{ if let Some(n) = self.left { return n } self.left = Node { value : self.value*3+1 , left:None , right:None } self.left } ...similar for get_right I left out any attempt at lifetime and mutability and/or Box and RefCell types as to not post an XY problem. But the basis is simple. I need a tree like structure (that is deterministicly computable) , but i would like to postpone this computation for when the specific node is requested through some root node and a serries of get_left , get_right . ( After which i would like to skip the computation next time ) 
I've used minted for Rust before [here](https://github.com/solson/miri/blob/master/tex/report/miri-report.tex) if you want some examples.
A *tree* structure (as opposed to a graph) is not really irregular: each node owns its children. You don't need GC at all, and Rust's borrowing story is sufficient (as long as you don't need parent pointers, in which case you still only need minimal intervention). That said, lots of small allocations tends to be slow, especially in non-GC languages. If you care about performance, a vector+index approach (or something Forth-like, perhaps?) is bound to be an improvement.
This will be really useful =)
Could you expands on the Forth comment a bit? I'm intrigued, but don't have enough info to do my own Internet searches yet. 
Have you tried using [futures](https://github.com/alexcrichton/futures-rs)/[tokio](https://github.com/tokio-rs/tokio-core)? It makes writing async code much more pleasant than handrolling state machines.
There are some arena allocator crates: https://crates.io/search?q=arena [`petgraph`](https://crates.io/crates/petgraph) uses the Vec + indicies approach under the hood.
Thanks for the link. Do you know if those arenas are safe? It isn't clear from the descriptions. I also couldn't see much indicating what their interface is like. I'll try looking at the code, but I'm still a rust novice :x
Forth is a stack machine, and stack machines are really just a fancy way of writing generalized *tree reductions* (though stack operations that produce multiple values don't correspond to trees). For example, the AST for `print(10 + num_cats)`, call / \ get_local + / / \ string int get_local / / \ "print" 10 string \ "num_cats" is isomorphic to the encoding "print" string get_local 10 int "num_cats" string get_local + call in a stack machine (aka. Reverse Polish) encoding. The advantages of the later include that it's extremely concise, it's *flat* and it doesn't need pointers. The downsides include that is that it's hard to do a top-down traversal. An equivalent syntax is Polish notation which is just the other way around (note: it's not just a reversal of the list): call get_local string "print" + int 10 get_local string "num_cats" I think this should be a bit easier to traverse, though still not without its challenges. Thinking on my feet here, this can be remedied a bit by having each tree encode its size: call get_local string "print" + int 10 get_local string "num_cats" 9 2 1 0 5 1 0 2 1 0 This structure can be built cheaply during construction of the tree. A nice feature of these is that each subtree corresponds to a *subsequence* of the total sequence, so you can still use slicing and such to share parts of the tree. You don't even have to find the end ahead of time if you can trust the function you pass it to is well behaved, though finding the end is effectively free if you have the auxillary size list. It really depends what you want to do with the AST, but I suspect there are a nontrivial number of transformations where the flat version affords much faster computation. Particularly, iterator-style in-order consumption to in-order output should map extremely well to this model. 
They're safe. The arena gives out references to the data allocated inside, and the references cannot out-live the arena itself: https://docs.rs/typed-arena/1.2.0/typed_arena/struct.Arena.html
and https://github.com/erickt/stateful
&gt; I suppose you can always have a generic expression node which carries an enum, or something like that. That's probably the sane thing to do. This had the downside of making the space overhead of every node the same size as the largest. [Rust had this problem with its DOM](http://smallcultfollowing.com/babysteps/blog/2015/05/29/classes-strike-back/#problem-1-memory-bloat) and uses double indirection to solve this; effectively there's an enum of `Box`es, and each `Box` is allocated with its specific data. Another option is to have one type per vector. Each node will be represented by a `(type, pointer)` pair of type, say, `(u8, usize)` (this is basically the same thing but with an index rather than a pointer). 
Yea I would have preferred iterative with Rust but some problems are just easier to reason about and write with recursion, and this one to me seemed like an obvious candidate for recursion. I just spent the whole day having rustc yell at me for type mismatches, before running into this so I wanted to be sure before I committed to it, or switch to Python or C.
I haven't written suffix() yet unfortunately, leaving that for tomorrow, but I'll try to get a quick thing out to get the first character.
Personally, I believe every LaTeX project needs a solid makefile, so thanks for mentioning this. I've had trouble get minted to work on Fedora, and I can't believe I never thought of this.
For suffix? I was thinking `String::new()`, then take `pattern.chars()` and iterate over it and put that into the new String and return that. Right now I've dug myself into another rabbit hole trying to fight the borrow checker because all ways I can find to concatenate 2 Strings take ownership. For the main algorithm, it's basically just assume you have the neighbors of the suffix, then if hamming distance is less than d, you add all of A, C, G, T to the front of each `suffix_neighbor` and push that onto the current list of neighbors. If not, just add the first character of pattern to each `suffix_neighbor`and push that on. The problem is here since I want a String to push but there doesn't seem to be an easy way to get this from 2 Strings beyond implementing Copy for Strings. If it helps, the name of the algorithm is Frequent Words with Mismatches. 
You've explained how you've tried to implement `suffix`, but not what it actually does. Does it just take the first character off of a string? The last 5 characters? Also, I have a feeling the answer to my previous question will help with this, but could you give me an example pattern and some valid outputs?
There is a ton more similar-looking code in my [Gatti project](https://gitlab.com/susurrus/gattii)
Lol because we didn't hear about it enough times today. ;)
For any project that requires more than just invoking pdflatex on a single file, I agree.
I haven't been able to use tokio due to the fact that there's no HTTP library that doesn't have a NOT PRODUCTION READY disclaimer on it. hyper 0.11 will, from what I hear, be the library we need (as opposed to the one we deserve), and once it exists I'll definitely switch to it. As for stateful, I didn't know it was still in development... that's pretty darned cool.
&gt; Personally, I believe every LaTeX project needs a solid makefile I rarely have to do anything that latexmk doesn't do better than a Makefile (e.g. looking into the log to know if there is need to recompile or not).
&gt; Is this better than coroutines? Mu. Coroutines are probably best thought of as syntax sugar around one of various styles of directly-written async code; which style is used - or whether it's possible to support multiple styles with the same language feature - depends on the language/implementation. This Rust post more or less describes a lower-level abstraction that a future coroutines language feature could be conceivably built to suit. There are some convenience methods like 'join' that native coroutines would mostly obsolete, but the fundamental questions of how async functions call each other and get scheduled are orthogonal to whether coroutines are being used. At a pragmatic level, I haven't looked at the C++ resumable functions proposal in too much detail, but from what I've seen it fundamentally depends on dynamic dispatch and, in some cases, heal allocation. Thus it should be possible for Rust to do a bit better, though I don't know how any current implementations benchmark.
I've made a [GitHub repo for Rust](https://github.com/Maplicant/RustGame/tree/master).
I second Latexmk ... pretty great tool.
Does the second thing have a performance impact? If we use two nested if's, and`func_a`returns`None`, then`func_b`wouldn't be called.
I think `RangeInclusive&lt;usize&gt;` / `RangeInclusive&lt;u64&gt;` should implement `ExactSizeIterator`, but they should panic on construction if the user creates one with `0` and `u64::max_value()`/`usize::max_value()`. The reason is that I don't think it is worth it to penalize `RangeInclusive` by not implementing `ExactSizeIterator` when there is only a single input that is troublesome. One really cannot do anything reasonable about this input, e.g., if there is no exact size an algorithm might still try to "count" its size, which will also overflow. Making it slower for all other cases might lead users to find workarounds... which might be even messier than just a simple panic. For these reasons I would rather have `RangeInclusive` panic early, on construction, for the input `0...usize::max_value()`. The panic should explicitly inform users that if they really want to do `0...usize::max_value()`, they should be using a `BigInt` type (like the one from the `num` crate). That is something that would actually be reasonable in this scenareo.
It's hard to tell: the pattern matcher will *probably* work left-to-right, so it will be fine, but I don't know what guarantees Rust provides here. (In this `func_b` won't be called anyway, because *somebody* is missing some parentheses.)
Yes. There was a macro recently posted here without this shortcoming.
It appears that this is a common pattern: GLOBAL.with(|global| { if let Some((ref ui, _)) = *global.borrow() { I would try to extract that into a function, helper trait, or macro.
What kind of library do we deserve?
At some point we will hopefully get guaranteed tail call optimization.
I'd be surprised if Stakkr can afford attorneys, but you should probably change the name. 
Works in any language: your `if` statements should check for negative conditions and then do error handling, and the happy path can go below that, in the top level block.
Suggestions? :v
I'm also wondering about the state of iron? 
Uff, I wouldn't be sure about that. I think the main issues with that were destructors. There is a mailing list thread about this from ages ago that details the problem, maybe somebody still has the link to that..
Heappr 
tweet-schedule rusty-tweets twitter-timer tweetr tweet-queue Something along those lines. I mean stakkr has no monopoly on sending tweets on a schedule so i wouldn't associate it with a company name really (except for twitter ofc) 
[RFC issue 373](https://github.com/rust-lang/rfcs/issues/373). There was [RFC 1303](https://github.com/rust-lang/rfcs/pull/1303) but was closed as postponed until `?` is ready. One could use the [`guard` crate](https://crates.io/crates/guard) for now.
Thanks for writing this, I found it helpful
It should work? I do it all the time line this: https://www.reddit.com/r/rust/comments/526xx5/managing_rightward_drift/d7i2s6q.compact
As long as moving to a 16bit system will break compatibility anyway, it seems silly to make things gratuitously difficult for every programmer and platform today.
How did you come to the conclusion `coder543-openldap` was a "spiteful" name?
&gt; This Rust post more or less describes a lower-level abstraction that a future coroutines language feature could be conceivably built to suit Maybe wrong. What is a "future coroutine"? I just want a coroutine. A "future coroutine", whatever it is, could be nice too. Coroutines is JUST a function which can suspend and resume. It'd preserve local stack variables state among suspend points. This Rust post has nothing to do with coroutines. This project doesn't allow me to write coroutines. &gt; At a pragmatic level, I haven't looked at the C++ resumable functions proposal in too much detail, but from what I've seen it fundamentally depends on dynamic dispatch and, in some cases, heal allocation. Thus it should be possible for Rust to do a bit better, though I don't know how any current implementations benchmark. C++ has several proposals, I just mentioned one, in a video talk, which is more worried about making people understand what and how it is better be it at usability level or at performance level. I highly suggest watching it. It's evidence on how much better in terms of performance coroutines can be. If by "C++ resumable functions" you mean Chris' proposal, I've read some time ago and what I understood is that will only do dynamic allocation if you don't want to implement things in header-files (Rust doesn't have this problem of header file or independently compiled abstraction). C++ had no real competitors in the area of system programming for a long time. Other languages like Python which provide an event_loop + future + coroutine abstraction don't care about performance as much as C++ or Rust. It's important to also look what it is being done in the C++ land, not only at Python land.
Quick summary: * It doesn't support conventional OOP: can't define base methods to re-use code, traits can't access fields of implementations. * Can't build cyclic structures (tree of widgets with parent/child pointers). * Can't cast `&amp;Rc&lt;RefCell&lt;T&gt;&gt;` as `&amp;Rc&lt;RefCell&lt;Trait&gt;&gt;`. Reading this, it feels like the author's issues largely come down to trying to bash an OOP-shaped object into a trait-shaped implementation and failing. It's clear they found the #rust-beginners IRC channel, so it's not like the community got no chance to help them. I don't know whether this implies we need docs *specifically* focused on how to not do OOP in Rust (*i.e.* "so you want to use an OOP pattern, here's what to do instead..."), or to just get better language support for "thin object"-y constructs.
&gt; Another example. For nanogui, and CEGUI uses this concept as well, each widget has a pointer to a parent and a vector of pointers to its children. How does this concept map to Rust? There are several answers: Answer number 5: don't try to map the exact concept to Rust. In this situation, the proper solution in my opinion is to have a `WidgetsTree` object that holds a `Vec&lt;Widget&gt;` (or a `Vec&lt;Box&lt;Widget&gt;&gt;`) from which you can manipulate widgets through some sort of identifier, just like you manipulate the content of a `Vec` through the indices or the elements of a `HashMap` through its keys. The `WidgetsTree` object is then responsible of maintaining correct links of parent-children. 
Something like that. But it would require changing cargo, whereas a new command could be implemented as a binary separate from cargo, like `cargo check`.
I think that's exactly what it implies. If you come to Rust looking for OOP you're going to be disappointed, and we need a fast landing page for that set of people - I expect there to be a lot of them.
Something like that. But it would require changing cargo, whereas a new command could be implemented as a binary separate from cargo, like `cargo check`.
Notably, the need for materials and features to ease the OOP-to-Rust transition has been a recurring theme of the 2017 road map planning thread. 
Definitely need better documentation. I hadn't gotten to needing anything that would traditionally be solved via inheritance yet, but I share his "WTF!?" reaction to discovering that [RFC 1546](https://github.com/rust-lang/rfcs/pull/1546) exists and hasn't been implemented yet. Requiring the developer to repeat themselves purely because default trait functions must be pure feels like a pretty big flaw in a language where it's not a matter of "By trait, we actually meant interface, and you can also have a superclass". (Disclaimer: I'm used to Python mixins from versions after they introduced the C3 method-resolution order to resolve the diamond problem and I will fully admit that `super` [has its warts](https://rhettinger.wordpress.com/2011/05/26/super-considered-super/).) My first impulse was "no wonder Rust still doesn't have bindings to a GUI library that will feel native on my desktop without breaking periodically because my desktop isn't GNOME." (ie. Qt or GTK+ 2.x)
Well, you can make downcast methods yourself and put them in the trait. Servo has things like `as_block()` for this.
&gt; It's easy enough (and only marginally tedious) to emulate base-class data-member access by simply implementing Deref or AsRef to go from &amp;Derived to &amp;Base. &gt; &gt; From then on, it's a walk in the park to give a default implementation of the trait methods working with Base fields. Would you be willing to give some example code? I've been procrastinating doing anything with that kind of complexity in Rust by dwelling on how serde and [Diesel](http://diesel.rs/) both lack schema migration and [qmlrs](https://github.com/cyndis/qmlrs) takes PyQt's lack of compile-time validation and adds on poor native widget support. (So why bother adding a Rust toolchain dependency when my apps are mostly portable, I/O-bound SQL-to-human adapters on rapidly-iterating schema or desktop utilities where a reliable, native, non-GNOME look and feel is mandatory and portability to Windows is preferred.) &gt; The cyclic structure is more problematic, obviously. This is typically the case for an arena... or the upcoming Gc types that Manishearth is working on. I don't have much direct experience with un-GCed languages, but he's talking about a tree with doubly-linked parent-child relationships, [not necessarily](http://xyproblem.info/) cyclic directed graphs in the general case. If `Gc` types are possible without compromising safety, shouldn't it also be possible to implement a standalone `WeakRef&lt;T&gt;` that allows non-owning child-&gt;parent pointers in a doubly-linked tree?
Can't you cast subtrait to supertrait?
Actually, /r/playrust doesn't appreciate server or group advertisements either. We should probably try not to direct traffic for these to /r/playrust, but the appropriate sub instead. They are actually looking for /r/playrustgroups or /r/playrustlfg.
&gt; If `Gc` types are possible without compromising safety, shouldn't it also be possible to implement a standalone `WeakRef&lt;T&gt;` that allows non-owning child-&gt;parent pointers in a doubly-linked tree? It is: https://doc.rust-lang.org/std/rc/struct.Weak.html
It actually might be better for you to learn rust without knowing C/C++. Those languages are kind of old and encourage habits that would be bad in rust.
Cycle: sorry, the structure is not a single cycle, it *contains* cycles. It is possible to use a weak reference (it's called `Weak&lt;T&gt;` by the way), however the inner struct is typically wrapped in a `RefCell` to allow mutability so the full type is `Weak&lt;RefCell&lt;T&gt;&gt;` (and `Rc&lt;RefCell&lt;T&gt;&gt;` for the corresponding owning type). It's quite a mouthful. --- Regarding the code, it's pretty simple actually. struct Base { value: i64 } struct Derived { base: Base, ... } impl AsRef&lt;Base&gt; for Derived { fn as_ref(&amp;self) -&gt; &amp;Base { &amp;self.base } } impl AsMut&lt;Base&gt; for Derived { fn as_mut(&amp;mut self) -&gt; &amp;mut Base { &amp;mut self.base } } Then, you can create a Trait if you want: trait SomeTrait: AsRef&lt;Base&gt; + AsMut&lt;Base&gt; { fn get(&amp;self) -&gt; &amp;i64 { &amp;self.as_ref().value } fn set(&amp;mut self, v: i64) { self.as_mut().value = v; } } impl SomeTrait for Derived {} [See it with your own eyes](https://play.rust-lang.org/?gist=631536b392187d1172490b155dd3e574&amp;version=stable&amp;backtrace=0). Could there more sugar? Definitely. Is it workable? Certainly feels so to me. (Note: using `Deref` would avoid casting manually, using `AsRef` and `AsMut` allows other bases, mix and match as you like)
Thanks. I'm still triaging my inbox but I'll look at that in detail later today.
The crate allows heap allocating arrays of objects, but handling them seperately. Perfect for game development! It's my first crate and rust code I share with someone else.
Serde creator here. Interesting idea. something like that wouldn't be that hard to layer on top of serde. /u/sfackler made https://github.com/sfackler/serde-value and https://github.com/sfackler/serde-transcode that can transform a thing into a generic value and back out to some new value. They could be combined to support data migrations. Someone would just need to come up with some glue magic to drive the process. 
On the other hand, there's significant contingent of folks who are like "wtf fix all the bugs and deal with the already accepted RFCs before working on frivolous things like associated fields".
Yes, I think the idea is viable, it "just" needs someone to write it. That person won't be me for a long time though.
... Thats actually pretty great :D
I didn't say it was a good thing.
A trait object consists of a pointer to the data and a pointer to the vtable. So you can't always coerce like that.
Assuming that you're using `Vec&lt;Widgets&gt;` where `Widget` is an enum, how is that slower? If you're storing pointers to widgets like in C++, you need to put them on the heap, which means one allocation per widget. A `WidgetsTree` on the other hand could put all the objects one behind another in the same memory allocation. This not only reduces the number of allocations, but is also more cache friendly. 
&gt; trait functions must be pure feels Not pure, just based on what's defined in the trait, the trait is an abstract structure, to be sensible default implementations can only rely on things provided by the trait or one of its dependencies. That seems pretty sensible to me.
While this is the problem at its core, please avoid making shallow "you're doing it wrong" posts on these subject matters. Not only does it not present any constructive solution, it will aggravate people coming from other languages, and also stops any conversation around the pro's and cons of either solution. 
I am one of those people. So, yes please to a "How to swap basic OOP to Rust idioms" guide.
Note that most folks consider that to be a pretty niche feature being requested. Traits are not meant to replicate OOP. They provide a form of inheritance, but that's not necessarily how you use it. In my three years of writing Rust I have felt the need for associated fields exactly once, and it was easily worked around. This is why that rfc is slow to happen. Not many have a strong need for it.
That would be fast, but it would also sidestep every guarantee of Rust. You'd essentially have a bunch of private maybe-checked pointers, and I'm not convinced that that's any better than a cyclic structure.
Why use pointers? I was thinking of storing the indices of the elements, and use getters to turn that into a nice API. Of course that means you'd be storing a `Vec&lt;Option&lt;Widget&gt;&gt;` in order to remove elements in the middle of the list without moving the indices.
Thanks for this. As someone who is (perhaps sadly) stuck in his OOP ways seeing things like this makes me more likely to dive fully into Rust. However it does seem a good deal more complicated than other languages make it.
&gt; Is that the norm for Rust, or is this an outlier and people are generally just as condescending as elsewhere in the CS world? I hope the helpful attitude is the norm. It's supposed to be, but I know it isn't always helpful all the time. Since I'm not an unbiased observer, I can't say if it's normally helpful or if the good parts are overemphasized to the point of looking more common than they actually are. But whatever the case may be, I suggest looking in the sidebar, particularly the Code of Conduct.
It is the norm. Of course, no community is perfect, and we don't always live up to our ideals. But we take being welcoming seriously, and try to take criticism as an opportunity to improve. You can see it elsewhere in this very thread: https://www.reddit.com/r/rust/comments/5295nf/why_im_dropping_rust/d7il0oc?st=isz2497c&amp;sh=fba90824&amp;context=1
There is `#rust-docs`. A `#rust-book` exists, but it's for the second edition of TRPL specifically. I actually own the domain for "Learn Rust the Hard Way"; following that model might be interesting.
Indicies are basically equivalent to pointers in this case, aren't they? No whole-program crashes, but no guarantees either.
I think learning assembler is easy and it explains everything you need to know about "low level" programming. C, C++, Rust are the way they are (comparing to eg. Ruby, Python etc.) because they need to map very directly to the machine code instructions of modern CPUs. You'll understand program counter, stack, heap, pointers, etc. and then you'll see how C, C++, Rust give you abstractions over these to make your life easier.
Or use hashing...
There's a spectrum: compile time -&gt; run time -&gt; no checks It moves it from one to two, but not the whole way to three.
&gt; like interrupt vectors tables On x86, you pass the memory location that the IDT lives in via `lidt`, not "the IDT must be at this memory location".
Same here, as it is right now it is usable, but I believe that with 'better' libraries it would be more awesome. Having the assurance to have made the right 'pick' at project start would be great from the side of the Iron project.
That's good to hear. I really like the idea of rust, but I haven't seen it as something practical to use for anything beyond a basic microservice (mostly due to my lack of understanding of the intended design patterns).
Every error message update has been locked, and [it doesn't look like any contribution is welcome](https://github.com/rust-lang/rust/issues/35233#issuecomment-243713652).
To explain petgraph, it's a data structure that describes a graph. (As opposed to "a graph of objects and their interlinks"). Using indices (also in the public API) trades away a bit of protection against logic errors and index invalidation, and in return it's easy to mix graph traversal with mutating through the graph (typically mutating node and edge payloads, not the graph itself).
Yup, you're totally right! And I just wanted to point out that the safe part of `unsafe` code may be the hardest to write correctly.
[Pronto](https://github.com/mmozuras/pronto) is nice automatic style checker written in Ruby. It has some runners available and now there is one for Rust! EDIT: For anyone interested [there is nice article what can be achieved with that (in Elixir).](https://medium.com/fazibear/automated-elixir-code-review-with-github-credo-and-travis-ci-986cd56b8f02#.yutxz4qan)
/u/sgrif ought to be able to comment on the diesel side. I believe it does have the ability to interrogate the database and generate structs for you. It might even be possible to write a plugin that finds all the database records and diffs them to figure out what migrations are needed. I'm sure /u/sgrif has already thought about this since he also maintains the Rails ActiveModel libraries.
I wouldn't say that Rust would be great as a first language book, but definitely viable as a second language. Getting up and running with Ruby and/or Python will be much easier IMO: they have minimal friction and vast resources (especially w/ Python for stuff targeting beginners).
[It's the norm](https://www.reddit.com/r/rust/comments/4vbmv4/can_we_talk_about_build_times/d5yhoa6?st=isz4kgch&amp;sh=c09543bb)
Rust people are generally even *more* condescending: In particular, we like to passive-aggressively bash other language communities for being condescending. That, of course, and inertia, which was at least party brought over from Haskell. 
It's entirely possible that Rust isn't a good choice for a first language. But I think it'd still be nice to have a guide for those who choose that path.
We've got [an issue in the new book repo](https://github.com/rust-lang/book/issues/109) for adding chapter along these lines (I just changed the title to make it a bit more clear). I'm not sure when we're going to get to this chapter, but we'd love suggestions for what it should cover on that issue! &lt;3
The problem of making Widget an enum is that it's not very extensible. The idea of most GUI systems is that you are able to make custom widgets (that e.g. are composed of more primitive widgets) and use them everywhere just like the basic provided ones. So Widget would need to be an interface, and for having dynamic dispatch it would need to be some interface pointer (trait object). If you then at first need an ID to fetch the Widget pointer out of the WidgetsTree and later access it by pointer you would have the dual indirection that majorgag is talking about. But that's just how I also would try to do it. If there's some better way I'm interested.
I'm asking for a way to tell rustc this: *hey, at 0x40003080 there is already a writable [u32; 384] array; give me an usable reference mut x starting there, so that I can do things like x[5]=x[4]* That is, the array does not have to be allocated/freed (the "mmap" syscall returns a pointer to "somewhere already in memory"). I defined a struct only to dereference a pointer to an array. But I found that while on some types it works (raw pointer to an *u64,* for example), with a fixed array it doesn't work *(&amp;mut[u8;8],* which uses the same space of an *u64).* In C you'd define that in a single line of code: uint32_t* x = (uint32_t*)0x40003080l; x[5] = x[4]; 
The problem is "How do I write a UI?". In OOP you use a hierarchical structure of objects each inherited from a base widget class and use subtyping for creating more specialized widgets. The blog author tried to use the OOP-way instead of directly finding a solution for the problem. On the other hand creating a good UI in rust is not easy. I have not yet seen a library with a very "rustic" api.
Is there an example of a project implementing something similar, where a tree would be traditionally used?
The problem here is that you also defined data to be a reference/pointer inside of ExistingArray. So ExistingArray is not struct { data uint32_t[MAPSIZE] } like you expect it in C notation but struct { data *uint32_t[MAPSIZE] }. With the result that you later do not attempt to read the first element from &amp;ExistingArray but from &amp;&amp;ExistingArray. If you remove the reference before data and adjust the remaining code it should work.
It would be nice, I agree, though the market for that would be much smaller, unless/until Rust becomes a mainstream language. Anyone particularly enthusiastic about creating such a resource should of course make one though, as I'm sure it would inform a lot of good practices for teaching Rust in general.
Writing the VM in Rust does not automatically make the GIL go away. To manage Python object I currently have a big pool with a lock (because Python objects have a lot of reference loops), so... that's basically a GIL. But the VM is still very small, so I'm hopeful in the possibility of doing something more clever (contributions welcome!)
But all professionals sell products or services. 
I recently posted exactly about this – i am one of those guys, too. Its not that big of a deal today but that's just because i did not use Rust that often. But one has a toolset in mind that does not work on rust anymore. Its like Windows users sitting the first time in front of a Linux machine – all of their previous knowledge is worthless (to some extend of course) .. i am all for an "Hi OOP Guys: feel concerned? come this way! We show you how to do X in Rust" 
I'll repeat what i say everywhere A good language don't force you into a specific design, it should just give you all the possible tool to solve your problems If you need to do OOP the language should give you the ability to do OOP, same for functional 
That is not a solution, if user A wants to do OOP to solve his problem don't tell him: "Do this instead, trust me" Language has to give enough tools so user A can solve his problem easily OOP or Functional or wathever XYZ
This is dodging the real problem, user want to do OOP not something else, the language should not dictate how people should sovle their problem This is repeating what Java did with OOP People should be able to do OOP,Functional or what ever in a language, if they can't that means the language is incomplete
This is why it's suggested to use a keyword for explicit tail calls, which changes the destructor ordering (and presumably errors if tail call is not possible).
&gt; Among other things, coroutines based on dynamic allocation of stack frames would be less efficient than direct use of futures in at least some cases, which would be unfortunate. Stackful vs stackless coroutines. And the initial link I posted shows an already implemented coroutine for which coroutines are stackless and are faster than pure callbacks and aren't as awful to use as coroutines when you need to combine `while` and `if` constructs. &gt; `|| { await do_a(); await do_b(); }` Now you implement futures and coroutines are language-level. I don't want to make a statement, but my impression is that you could provide generic great coroutines and at the language level you could leverage that to improve your future, but the other way around is restrictive and less performant. It has been evidenced in C++ and I don't see anything different that would invalidate the generalization to Rust. &gt; For more control flow more complex than a straight line you would either use other combinators And this will always be more unclear than simple `if` and `while` constructs. The promise of coroutines are to make asynchronous algorithms as readable and maintainable as blockyng synchronous algorithms. &gt; but it provides a lot of the framework for what coroutines should eventually desugar to. And why would this be a good coroutine at all? If you just plan to provide coroutine, you could even model something resembling go channels on top of that. A coroutine that desugar to that, in my understanding, will always have less performance than what the folks at C++ are developing (and already reached working code). From the original post: &gt; The space for this “big” future is allocated in one shot by the task In the C++ land, we don't have a future at all to allocate this "big future" and that's how they achieve a "negative abstraction" that is faster than pure callbacks.
That's the problem with people like you, you are so closed minded, you only like your design and hate everything else, grow up, that's not because someone can use OOP in rust that you have to use OOP..
This seems nice, but IIRC the last times I saw discussions about inheritance and someone mentioned Deref/AsRef there were multiple people saying it should not be used for that (though I must confess I didn't quite get why, or maybe it was just Deref?). (This is why I'd love a guide, by the way, not just to have a working solution but also to be fairly confident it's the idiomatic way to go.)
Right, but not necessarily to the public. 
Oops, I found the friendlier trickery I was searching for: std::slice::from_raw_parts_mut(addr as *mut T, len)) Now it works as planned. My error was trying to build a "reference to an array" when I actually only needed a slice. 
If you want to do OOP in Haskell, you're not going to have a good time. It does not mean that Haskell is incomplete. Supporting full gamut of OOP is a not a goal of Rust. Maybe associated values make it in for performance benefits, but not necessarily other things.
Are the videos going to be posted online? And where?
You have data, and you have functions. You create functions that work with your data. You don't combine the two and call them an object. Also the type system in rust as far as traits are concerned is nothing new and is overall pretty basic. Basically the powerscale goes like this: - **Level 0: Interfaces (C#, JAVA, etc..)** these are pretty useless and you might as well just cast your stuff to objects. First of all you have to add them before the fact, for instance if we wanted an `Addition`interface it has to be added to the `Int` type on the int type itself, meaning the type quickly becomes outdated or have to be updated all the time, or gets bloated to insanity because it has to care for every use case. They also lose their own type when used i.e. if `a` is an `int` and `int` has the interface `Addition` which has a function `+`. then `c = a + a` is an `Addition` and not an `int`. - **Level 1: Traits/Statically Resolved Type (Rust, F#)** I like to think of these as type of kind order 0. These types can now be added after the fact, and doesn't lose knowledge of their own type i.e. `a + a` is `int` and not `addition`. They are just barely above interfaces and are somewhat useful, for instance you can define a library that works with both floats and double, and you don't have to duplicate the code. However as they are not higher kinded you can't have `M&lt;A&gt;` where both `M` and `A` are polymorphic. Such that the function would work for any `M` and any `A` where `M` could be types like `List&lt;A&gt;`, `Set&lt;A&gt;` or `Graph&lt;A&gt;`. I should note that traits are a bit more powerful than the **Statically Resolved Type** of F# as F#'s doesn't always work when added after the fact, and is overall more like a hack to a problem than a feature in the language. - **Level 2: Modules (Ocaml)** Or first order kinded type, you can now have types with 1 level polymophism. Meaning you can make the `M&lt;A&gt;` function but you can't make an `M&lt;C&lt;A&gt;&gt;` function. This type system is pretty good and will get you through most problems however there will always just be those few times where you have to hack a bit to get something working. - **Level 3: Type classes (Haskell, purescript)** Finally we reach the level of higher order kinded types or n order kinded types, the sky is the limit and there is milk and honey flowing in the street, however sadly we get articles like the ones above missing basic points of a type system and preferring to reverting back to something that is completely useless.
The first line I wrote explains that, the rest show what you can do in rust and what you can't.
It seems to me that should be possible (meaning I believe it IS possible).
I agree with answer 5, but it is sometimes hard to think of a suitable alternative. I've had a similar problem with other trees (in a filesystem). Is there a good zero-cost abstraction to use as an identifier for a node in an arbitrary tree? 
&gt; A good language don't horse you into a specific design, it should just give you all the possible tool to solve your problems This is how you get C++. Most people in this industry realised what a mistake it was to try to do everything with one language, and that it's better to specialise.
This talk was sooooo good
&gt; but people can't be expected to throw away an entire design paradigm for one new language You're asking me to throw away functional design, though. If you want to support both at the same time you end up with a monster indistinguishable from the monster that is scala, with tons of fickle semantics... and you still haven't made checking the Liskov substitution principle decidable.
Besides, trying to play with this method, I am not sure how you are supposed to handle the case where `Derived2` should inherit `Derived`'s behaviour. In particular, with only one trait, we can specify default methods in `Base`; `Derived` can now override some of them and keep the default ones when it is needed. However, `Derived2` will have to manually implement methods and call `Derived` implementation if it wants to use it, else it will fall back to the default implementation. (Example: https://play.rust-lang.org/?gist=98b6da78b193ae04aeae2db5b6991b91&amp;version=stable&amp;backtrace=0) Obviously, this can be solved with a bit more of boilerplate, but for a complex hierarchy with "classes" that have a lot of methods it's a bit tedious.
On the one hand, no; it'd be *ridiculous* to make a language hard for the sake of being hard. On the *other* hand... Rust cares *a lot* about requiring you to explicitly state what you're doing. This makes it a lot clearer as to what's actually going on in the code. Which sometimes results in obtuse-looking code that would make Java blush. It's a trade-off, and Rust is pretty unusual (in my experience) for favouring explicitness over terseness; it's one of the reasons I like it! After all, if I want to just bang code out quickly, I already have Python. None of this is to lambast you, by the way. I sometimes feel the hardest part about learning Rust can be accepting that almost all the weird, seemingly deviant stuff it does, it does for a *good reason*. Developing a certain degree of Stockholm Syndrome helps with accepting that; before you do, it certainly *can* feel like the language is doing this to spite you.
Hey, this subreddit is for the [Rust programming language](https://www.rust-lang.org/en-US/)! I think you're looking for /r/playrust instead :)
Rust OOP: ROOP. [It'll give you *just enough*...](http://idioms.thefreedictionary.com/give+enough+rope)
I'm pretty sure all languages are incomplete by that definition, then! I don't know any language that makes it easy to do OOP, functional, AND what ever.
Right, I agree that we need better documentation. The equivalent design pattern is usually dubbed "composition over inheritance", a form of which is detailed in pcwalton's answer.
OOP is a very language specific pattern. Many languages don't support it, but do support other patterns which do not translate well to other languages. Wanting OOP in Rust is likely an indicator that you have the XY problem.
Exactly. I learned very quickly when starting out with rust that it is a bad idea to try to shoehorn OOP concepts onto rust. Once you accept that structs ARE NOT objects, and traits ARE NOT interfaces, and that it is in fact intentional they are that way, then you can make some progress.
You can have the child struct store a raw pointer to the parent, but traversing a raw pointer is unsafe. This, to me, is just another reason it should be stressed for beginners that there are situations where it's totally fine to use `unsafe`. It's a very frustrating experience to work with any sort of recursive structures without it.
There's one big difference in the rules: C/++ allows mutable aliasing. It's not even UB if you're careful.
As someone who has grown to distrust and almost loathe OOP I think that Rust has a good opportunity to teach the paradigm changes via tutorials for OOP pain points. As someone still learning Rust I'm not the guy to create them, though :(
You shouldn't try to cater to everyone's workflow, it's neither possible nor desirable Relevant xkcd https://xkcd.com/1172/
So that would be similar to: let a = try!(func_a().ok_or(())); or let a = func_a().ok_or(()) ?;
Not a raw pointer but something equal to C++ [weak_ptr](http://en.cppreference.com/w/cpp/memory/weak_ptr).
You mean, like this: https://doc.rust-lang.org/std/rc/index.html? See the second example. You can do this as well.
I suggest to get the guys and gals from Redox involved. There is something going on (e.g. orbTk) but it is in its early days. Would be cool if we join forces because I think a toolkit just standing there is not going to gain any popularity if its not actively in use. On Redox it would be..
Here are my thoughts, goals, and discoveries so far. Your goals may be different. I'm mainly focused on applications that are very GUI-centred. # Design Goals - Not immediate mode. IM GUIs are convenient for certain tasks, but are not always adequate. - Distinction between **Content**, **Styling**, and (maybe) **Layout**. - **Content** refers to the information the UI is displaying (text, images, controls, etc.). - **Layout** is where that information appears, or how it is 'layed out' in relation to other 'components'. - It may not be possible to completely separate this from content. It isn't that important of a goal, but it's a goal nonetheless (HTML seems to work well enough, and it ties layout and content together). - **Styling** is how it appears (colours, textures, borders, backgrounds, etc.). - Backend-agnostic. And not only that, but writing a new backend should not require porting hundreds of algorithms to a new environment that might not be able to handle them as easily (eg: raster graphics vs OpenGL). - It may be possible/convenient for the styling aspect to be tied to the backend, allowing developers to either lock themselves into one backend and get access to all its capabilities, or try and stay as generic as possible, even if that limits the styling features available. - Sticking to Rust's ownership rules as much as possible. No `Rc`s, `RefCell`s, etc. unless really needed. - User-defined widgets. # Things that generally don't work in Rust - OOP-based widgets. Sure, you could probably make it work using `Any`, but that doesn't result in a very nice user API. - Handling events at the component level. This might work fine in languages that support circular references and garbage collection idiomatically, but Rust doesn't. - Identifying and indexing components by a unique identifier. Doesn't work because Rust doesn't really support heterogenous lists. # Problems due to Rust's design *(but not problems with Rust's design)* - Components cannot access their parents due to the ownership model (primarily a layout issue). - Can't store a list of heterogenous components without losing type information (well, unless you use `Any`, but again, not a very nice user API). - Can probably be solved with generics/macro abuse. # Information - Information can be edited by the software 'model' and/or the user of the software. - Changes the user makes should automatically be propagated to the software model. - Changes the software model makes should automatically be propagated to the user. - These two requirements suggest shared mutable data (Looks like we need `Rc`s, `RefCell`s, etc. after all). Using listener patterns would be too error prone. - Information can also come in the form of events. eg: when you click a button. These generally are triggered by user actions and are handled as part of the underlying software rather than the UI. - This is probably best handled with an event queue which the underlying software model can then pull events from. # Designs that might work in Rust - Abuse of macros to generate the entire UI structure at compile time. - This is bad for so many reasons but it might just work. There must be a better solution, though. - On the plus side, static dispatch for everything and all type information is available. - Event handling is done outside the UI structure. - The user receives events from a channel, and then manipulates the required components within the UI structure. - Child components can send events to modify their parents. - Events can be implemented as a user-specified enum. - Probably will result in generics everywhere (but it's probably going to be generics everywhere no matter what the design). - Might cause issues with extensibility (widget libraries can't specify their own events). - This can probably be worked around with careful design. ie: widget libraries can only make use of user-supplied events. # Miscellaneous unresolved issues - How to handle focusing components without being able to index them out of a list. - Is choosing the focus every frame acceptable? - Once you've chosen the focused component, you have to unfocus *everything* else. - Should we rethink the concept of focused/active/whatever components?
Swift, OCaml and F# All are ML style functional languages, particularly the latter two. Apple promotes the ML style, they call it protocol (ie trait) oriented programming. All have OOP support. The issue is this provides two separate methods to model every aspect of the problem, methods which dot always work well together, and in teams this can lead to an ugly mix of coding styles. 
Oh. Ouch. Thanks for the correction. Pattern matching *could* be lazy just like `&amp;&amp;` and `||`, and for the same reasons; obviously I am surprised that this is not the case. But there we are. 
I've been using gtk-rs for my GUI toolkit needs. I think it maps pretty well to Rust, though I'm still fairly new to it. There are some pending issues that they'd like to tackle to make it more rusty. I'd suggest communicating with them to see their thoughts on building a rusty wrapper round the low-level GTK calls. That might be a quick way to prove out some of the architecture of your GUI toolkit without having to do the heavy lifting of actually doing all the drawing and dealing with platform-specific issues until the architecture has proven itself.
Hmm. I've occasionally used composition over inheritance in Python but I'll have to think on the implications of that when combined with a strong type system. The risk of having to break and rewrite APIs because I wrote myself into a corner with a novice mistake is part of the reason I've been procrastinating doing projects in Rust as opposed to unit testing heavily and ensuring PyPy compatibility. (That's what better documentation would help with.)
I've been thinking about it too. Some of my ideas: * Use `Future`s as event source. Edit: I mean `Stream`s of course. * Nobody will design UI in shell, so instead of making code super-nice, make an IDE with visual editor and make it generate the code. * Use cassowary algorithm to do layout. * GUI should be just mapping from model. Don't change widgets directly, connect them to model and then just modify model. * Think hard about file format, so it would work well with git (or write merge tool)
&gt; Isn't this just like Java 8 interfaces that can now have default method implementations? Possibly, I haven't looked into them so I don't know what limitations they put on their dependencies. I would expect so though. &gt; I feel like it's not that simple though, since everyone seems to agree that this guy is being reasonable. There are multiple axis of agreeing, for instance with respect to trait limitations I do agree with some of the *problems* outlined but I think the issue is traits not having "internal" protocols (if you add an accessor/provider to the trait it's public and can be accessed by trait users, not just the trait itself).
Agreed. Rust does often force you into pre-designing your API beforehand, but I don't think that's a bad thing, and this is common in most typed languages.
I don't understand the last two paragraphs, such as this statement: &gt; r first order kinded type, you can now have types with 1 level polymophism. Meaning you can make the M&lt;A&gt; function but you can't make an M&lt;C&lt;A&gt;&gt; function. and this statement: &gt; Finally we reach the level of higher order kinded types or n order kinded types Can you explain this?
Let's assume we have a function for a list called `add&lt;T&gt;(T elem, List&lt;T&gt; list )` it takes a List and an element and adds it to the list. Now if you have first order kinded types you can make a function add but it will work for any type of collection. I.e. `add&lt;M&lt;T&gt;&gt;(T elem, M&lt;T&gt;)` where M could be set, list or even graph. Btw I am massively simplifying the whole thing, because I am phone. There are plenty much better articles out there explaining it much better than me. 
I'd like to echo phayzfaustyn in saying that I fount gtk-rs to be almost everything I would want from a GUI library. To me personally, GUI in rust is a solved problem.
Wouldn't using Servo as a UI framework necessitate writing our UI logic in javascript?
Right, but I don't understand your statement about OCaml, for example. You say that OCaml has 1 level of higher kinded polymorphism, but I do not think that is true. OCaml's normal Hindley-Milner type system doesn't support higher kinded polymorphism. You can simulate it with modules, but those are fully higher order (a functor can take a functor as an argument). The main difference between OCaml modules and Haskell type classes is that type class dictionaries are inferred based on the type structure whereas OCaml modules are constructed explicitly. There are other differences but with various extensions Haskell type classes come ever closer. Higher kinded types are actually mostly orthogonal to modules/type classes. You can have either one without the other.
&gt; Backend-agnostic. And not only that, but writing a new backend should not require porting hundreds of algorithms to a new environment that might not be able to handle them as easily (eg: raster graphics vs OpenGL) Depends on what you mean by "backend": **a) backend = another GUI tookit:** Then I actually would rather stay with a single backend. The problem with multiple backends is the following: You're limited to the subset of features that is supported by all of them. Furthermore, the result always looks and feels bad, because QT/GTK/... have different ideas on how they want to structure GUIs and how they should work. Using one umbrella to cover all of them results in something that violates the basic assumptions of the frameworks. Also, the result is likely to be heavily over-engineered. Therefore, I would propose a single, slim backend that is driven by OpenGL or a 2D engine sitting on top of it. **b) backend = 2D accelerator** That's OK, but keep in mind that writing something that takes some kind of scene graph and renders it in a performant way is quite a heavy task. Servo basically does the same and as far as I can see, they decided on using a single backend (webrender2, ..3, ..4) for their "final" product. Working around the lack of OpenGL on Windows platforms is something which is already solved [Google's Angle](https://github.com/google/angle), which btw is also used by Firefox to get WebGL support on Windows. A note on "native look": I actually don't know any program that simultaneously * is cross platform * uses one single GUI framwork (w/ or w/o multiple platform-specific "backends") * has a native look * provides a look and feel that is actually enjoyable Small points where OS-dependent solutions are good and helpful (and don't mess up the entire UI) are: * file open/save dialogs * print dialogs
&gt; Nobody will design UI in shell, so instead of making code super-nice, make an IDE with visual editor and make it generate the code. Some goals to keep in mind which are important to me once the "native look and feel" and "hard to accidentally introduce a crash bug" requirements are met: * **Building custom composite widgets for code reuse and clean APIs must not carry the full baggage of writing a widget from scratch.** * **Qt Designer** gets this right in two ways: * By allowing bare `QWidget` objects and containers like `QTabWidget` to be root widgets (a subset of Qt's ability to instantiate any `QWidget` or subclass as a top-level window) * By providing a "promote widgets" feature (which PyQt supports) where you can work in widgets Qt Designer is familiar with and have the code generator or PyQt's runtime `.ui` loader substitute a custom widget ([for a variety of purposes](https://doc.qt.io/qt-5/designer-using-custom-widgets.html)). However, it doesn't seem to go the last step of allowing a "QWidget as top-level" to then be dropped into other windows in the same `.ui` file as a reusable widget, so some code glue is still necessary. (I'm partial to using "promote widget" to map a placeholder to something which does nothing but instantiate the "QWidget as top-level" as its child and proxy signals and slots. Ugly, but better than nothing.) * **Glade** gets this wrong to the point that not only do you have to write boilerplate code for a custom widget, they didn't make it easier to load plugins from the project directory to compensate, so you still have to fiddle with environment variables to manipulate the search path. For GTK+, I rarely use Glade because most of my widget tree is often the child of a customized container widget Glade can't easily represent, so why bother. * **If I'm using a visual editor, I shouldn't have to do a lot of ugly plumbing-together by hand in the code side of things.** * In **PyGTK**, they do a mixed job: * Glade uses a combo box for event handler names, allowing you to enter arbitrary strings of text into the event-binding GUI while also providing suggested method names and `gtk.Builder.connect_signals` accepts your choice of an object (method introspection) or a dict mapping textual names to methods or functions of your choice, which means that it's up to me whether I bind to methods on a common Python object or arbitrary functions elsewhere. * Unlike Qt, GTK+ doesn't provide a mature "no code needed" approach to making common signal-slot connections like "On button click, close the dialog". * Glade doesn't provide a suitable way to specify userdata from the GUI, so I sometimes have to fall back to binding manually anyway. * In **PyQt**, you have three options, all feeling less than ideal in some way or other: * If you follow the advice of StackOverflow posters, you resort to manually including a bunch of lines like `self._button.clicked.connect(self._printMessage)`in your code. No automatic binding at all but, hey, at least you can bind to non-method functions. (There's also a shorthand which allows you to bind multiple slots on a single widget in one call. `widget.pyqtConfigure(slotName1=func1, slotName2=func2, ...)` * Qt's equivalent to `gtk.Builder.connect_signals` is `QMetaObject::connectSlotsByName`, which only accepts a QObject and recursively searches the given widget tree for slots (the `@pyqtSlot` method decorator under PyQt) with the magic `on_widgetName_signalName` name structure. This will not show up in the visual editor and you can't map to non-method functions with it. * If you use the "promote widgets" feature in Qt Designer, it will allow you to add custom signals and slots to the promoted widgets. This is the only solution that allows event binding in the GUI, but it took me a while to even discover the UI for adding custom signals and slots because it's in a non-intuitive place. (It only shows up in the dialog-based signal/slot editor accessed from the dedicated "edit signals and slots" mode while the sidebar panel signal/slot editor, available from "edit widgets" mode, is good enough that I never knew the dedicated mode had extra features beyond "drawing connector arrows is pretty".) * **Data-binding should be as visual as possible** * **Qt Designer** lets you add custom widget properties via the visual editor, but abdicates responsibility for binding widgets to models as far as I can tell. For bonus points, the icon view is just a display mode of the list view, so no fancy code tricks are required for the main widget in a simple file manager-style UI. * **Glade** is all over the place: * It doesn't let you add custom properties and its list and icon views are separate widgets, which basically makes it necessary to implement a custom widget to manage shared state and add/remove widgets at runtime (see my complaints about custom widgets it Glade). * Glade does have a half-assed data binding system. (ie. You can define your columns, column properties, and cell renderers in Glade if you realize that seemingly equivalent GUI elements actually behave very differently and you can sort of declare models and bind to the columns, but there are all sorts of weird shortcomings, like having to drop to code and write `widget.set_search_column(1)` and `widget.set_sort_column_id(1)` when there's no defensible reason not to put that in the visual editor.) * The documentation relating to data views is a mess. One of my projects resorted to manually re-parsing the GtkBuilder XML to map the click callback's `(cellrenderer, row_path)` arguments to a `(row, column)` position in the data model because the checkbox cell renderer can't be manipulated by default and I couldn't figure out why when the APIs to enable that seem to all be there. * **If I have to let it freeze my UI into code and then edit the code from then on to avoid losing my customizations, I'm just going to write code from the beginning.** (ie. Any solution must be sophisticated enough for the visual editor's source file to remain the authoritative copy as the application becomes more advanced.) * **GTK+:** Before I started migrating to Qt, I'd gotten in the habit of just skipping Glade from the beginning because Glade has neither "promote widget" nor a Python code generator and I wanted to avoid the hassle of having to inevitably manually translate my UI definitions to raw Python for lack of sufficient extensibility. * **Qt:** Apps like QTads have run into problems in the past when the Qt Designer schema changed under a `.ui` file that hadn't been loaded and saved during the entire deprecation period, breaking builds. (And I always choose runtime `.ui` or `.glade` file loading over code generation in Python to avoid the need for a "compile" step while iterating and because I'm already used to using `RESOURCE_PATH = os.path.dirname(__file__)` for other things like bundled icons.) 
No, but then there's an extremely good reason that people use c++ for game development (as op is) and very little else. Often you need the kitchen sink because the problem is a multiform hydra
I think the problem should be worded a little differently. People may not expect OOP from Rust, but they expect that it will allow them to solve the problems they have. Since OOP is the most prevalent paradigm right now, people know how to solve those problems with OOP tools, but have no idea how to do that in Rust. So the proper documentation would state a common problem first and then show how it's solved in OOP languages and an equivalent solution in Rust.
Some of the ideas here match my thoughts on a reactive-imperative DSL with a high degree of specialization based on static pieces of the UI structure and control-flow inversion for event propagation to parents or siblings. Technically it would be a component DSL that could work for things other than UIs but it's not clear yet how many UI needs (e.g. 2D layouts and graphics primitives) have to be baked in. I've postponed exploration of this design due to resource constraints and in a year or two I may be able to organise a team for this purpose, but nothing is certain at this point. If you are interested in what I have in mind, you can find me on IRC.
I'd recommend getting some input from veteran toolkit hackers before making too many decisions. There are (usually 20 year old) reasons why current toolkits work the way they do, and lots of recent work on next gen toolkits that should influence your design. I suspect you'll find that functional reactive or scene graph approaches will suit Rust better than OOP-derived designs. To integrate properly with widely deployed "native" toolkits and their behaviour (rather than half-arsing it by just matching the pixels), it'd be worth considering a model like React Native. Beautiful Rust out front; dirty, dirty imperative toolkit out back.
Yeah, unlearning and relearning design patterns is hard. Took me a while with Rust, took me a while with Python, and took me a while with Go. Python at least lets you hammer things into a putty, these languages don't :)
I was also thinking about making `clippy` optional, so one could run it also against stable. Unfortunately I am not aware of stabilization of linters API. But I can think about solution to provide any additional flags to the compiler. It shouldn't be that hard.
I agree with Tynach. I'm afraid I didn't find enzain's post helpful, but I thank him for trying. Like Tynach, what I would find helpful is a Rosetta stone to translate between language idioms. The Rust docs are currently written in Rust jargon, which is totally understandable. But you don't write a Beginner's Guide to French in French. You write it in the language the reader already knows. Only once they've picked up enough French do you direct them to a textbook that is actually written in French. Where I differ slightly to Tynach is that I'm happy if the Rust examples don't use exactly the same methods as long as the outcome is the same - i.e. feel free to convince me that OOP idiom A can be achieved *better* by Rust idiom B.
&gt; Then when you make those classes, and objects of that class, none of the objects you created are 'of type Addable', they're of type Int or whatever else implements Addable. You can't have an Interface returning a concrete type though, if your `Addable` interface has an `add` method, it doesn't know who is going to implement it, so it has to return `Addable` and not `int` or whatever. Something which Traits solve by allowing type parameters/associated types, so in Rust when you implement `Add` for `i32`, you write impl Add for i32 { type Output = i32; //here you define what `add` is going to return fn add(self, rhs: i32) -&gt; Self::Output { ... } } So it is the same trait but different implementations are going to return different concrete types. &gt; Please do not use any single-letter variables either. As a nitpick of a nitpick, `a + a` is hardly the kind of example that would look better as `number + number`
Sometimes I take humour seriously, sometimes I meet seriousness with humour. Any way it's the same.
the blog has been deleted?
One surely-crazy thought I've had is * The gnarliest part of a cross-platform GUI library with native integration for each platform is... the native integration for each platform * As far as I'm aware, Qt has the best and most mature implementation of all this * Qt has all of its native platform support [abstracted](http://doc.qt.io/qt-5/qpa.html) behind a common internal API into so-called "platform plugins" So maybe you could just take that and run with it and build your own GUI library on top? These are internal APIs with no stability or compatibility guarantees between versions, but even if you just vendor it or fork it you could presumably start out pretty far ahead. Someone please tell me this is a horrible idea. (sources of uncertainty (a.k.a. I don't actually know what I'm talking about): never looked at what these APIs are actually like; haven't ever implemented a GUI library before)
&gt; Why is that a major goal? Did you ever feel the need to write a custom widget in a GUI toolkit? Because I didn't. Wut. So all you ever need in terms of GUI are simple dialogs? No text editor components beyond a basic text box that even has line numbers? None of those things an audio player has? Or DAWS? Something with node editors? Or anything somewhat complex that shows content in a custom way, like a word processor?
User defined widgets are necessary to attract users, I think there are plenty of use cases, for example if someone wants to build charting widgets or other visualisations. I think I writing a custom widget was literally the first gui program of ever wrote. 
Object-oriented design is quite a bad fit for games because decomposing things along that axis ends up using tons upon tons of dynamic dispatch, which kills your cache. Worse: It doesn't even show up on benchmarks as the performance degradation gets smeared all over the code. Which is why people like to use entity-component-system based designs: They both provide very, very, very high composability while simultaneously allowing you to use techniques from database land to move your data with a dump truck instead of tweezers. And you can easily re-engineer that dataflow because the systems only ever know what to do with the data, not where they're getting it from or where it's going to end up. C++ is kinda appropriate for that because of templates. Not because of inheritance.
 &gt; is cross platform &gt; uses one single GUI framwork (w/ or w/o multiple platform-specific "backends") &gt; has a native look &gt; provides a look and feel that is actually enjoyable On the plus site, since the majority of people find all those electron based applications acceptable, despite even only delivering 2/4 one might settle for 3/4 and drop the native look. &gt; on native dialogs Good idea, but that needs care as well: I believe one version of the Windows folder dialog is that terrible that many developers hijack the file dialog instead.
The difference is between "you're doing it wrong, do it differently" and "you are used to something else, we have to thing hard how we teach you better to do these kinds of things in rust?". Also, we need to come up with solutions for this problem, but "bolt OOP to Rust" doesn't sound like a good one.
Pick one thing and be amazing at it. Don't try to pick 50 things and do them all so-so; you'll end up with something like python's tk. :P React native is probably the right place to look on how to do this properly. Another possible approach would be to wrap servo up and do some kind of electron-thing with js/css as the frontend and rust as the backend. Reimplementing widgets and layouts from scratch is fun and all, but its fundamentally the wrong approach to be taking here; the kivy project went down that path and it's been (despite being quite portable and quite good looking), largely unappreciated and unused.
&gt; The idea of higher-rank types is to make polymorphic functions first-class That was the sentence I needed to read to finally get what this is all about. Any explanation of HKTs &amp; co has always been very abstract without ever concretely stating what the consequences are.
How about using servo's webrenderer? Though there is a slight issue with it not having any documentation. I tried using it for a custom gui and gave up due to not understanding how to use it.
Finally an explanation in plain English. Now I understand why people are clamoring for it in the compiler.
Been hacking on [combine 2.0](https://github.com/Marwes/combine/pull/62) and I have finally gotten the performance down to within 5% of [nom](https://github.com/Geal/nom) (tested on the http parser in [nom_benchmarks](https://github.com/Geal/nom_benchmarks)). Going to release what is hopefully the last beta version this week with a proper 2.0.0 release a few weeks after. Also always busy with [gluon](https://github.com/Marwes/gluon) and I am hoping to finish the [last few issues](https://github.com/Marwes/gluon/milestone/2) issues and write up some release notes before releasing 0.2.
a) you mean LGPL? b) being \_plugins_ you could link them dynamically...
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/redox] [Xposting from \/r\/rust Designing\/Brainstorming an Idiomatic &amp;amp; Flexible Rust GUI Toolkit Library](https://np.reddit.com/r/Redox/comments/52e26s/xposting_from_rrust_designingbrainstorming_an/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Nice. Any reason `--mode tile` hasn't been implemented?
Just tried it and there's at least a 2 second delay from running the program and it terminating, whereas feh immediately sets and exits.
Trying to re-implement most of Lex as a Rust-Macro. It's an interesting project for self-education. [rlat](https://github.com/valarauca/rlat) Need to rebuild to the regex core. Eventually switch to tt muncher to better handle previous states/initial states.
Exactly right. The OOP things that Rust isn't allowing the article writer to do are the very things that allow it to statically dispatch every function call except trait objects.
Because the Rust community is amazing.
Not really, no. In the example, `randomChoice()` is used both with booleans and strings. It's supposed to be a `T -&gt; T -&gt; T` function, not `String -&gt; String -&gt; String`.
What are your thoughts on [Conrod](https://github.com/PistonDevelopers/conrod)? It's very simple and I think it's cross platform (though I've only used it on Linux). It definitely needs some work, but it's stabilizing.
Oh, right, I missed the part with booleans. It makes more sense now, thanks :)
Could you give more details about what kinds of limitations you faced?
Rather than writing something really low-level, could you have an electron style application and have HTML elements call rust code? Kind of like https://github.com/browserhtml/browserhtml Come to think of it, would a wrapper for [electron](https://github.com/electron/electron) work? It would look fine on all platforms.
What's a good way to convert a `* libc::sockaddr_in` to a `SocketAddr`-style thing? I think I want to use something similar to the private `from_inner()` but dunno if I should just re-implement the same thing. "But why would you want to do that?" -- I'm writing a fault injection library -- [`libfaultinj`](https://github.com/androm3da/libfaultinj) -- and I want to intercept calls to e.g. `bind()`, and check the address against an environment-var specified addr/hostname/mask. It's easy to go from `env::var("SOME_VAR")` to `SocketAddr` but going the other direction is murky. I probably don't have to promote the `sockaddr_in` to a `SocketAddr` but comparisons are relatively simple in that domain, so that's the way I picked.
&gt; Parametric polymorphism allows for “functions” from types to values. This didn't really click for me. The function still needs values to produce a new value, why is it "from" types to values?
C++: std::vector&lt;std::unique_ptr&lt;Container&lt;T&gt;&gt;&gt; 
Like /u/ConspicuousPineapple said above, no, you cannot do this in Rust. In C++ function overloading allows you to "emulate" this and get superficially similar behavior: template &lt;typename BiFunction&gt; string chooseStringWeird(BiFunction randomChoice, string head, string tail) { if (randomChoice(true, false)) { // monomorphize overload (bool, bool) -&gt; bool return randomChoice(head, tail); // monomorphize overload (string, string) -&gt; string } else { return randomChoice(tail, head); } } string bestEditor() { auto chooseValue = [](auto&amp;&amp; a, auto&amp;&amp; b) { /* random ... */ return a; }; return chooseStringWeird(chooseValue, "emacs", "vim"); } But note that this is not at all the same. Each overload is a different function, with a different address, etc. There is no way to have them both be the same function. Furthermore, with the Concepts TS enabled you can constrain it against particular types: template &lt;typename BiFunction&gt; string chooseStringWeird(BiFunction randomChoice, string head, string tail) requires IndirectlyCallable&lt;BiFunction, bool, bool, bool&gt; &amp;&amp; IndirectlyCallable&lt;BiFunction, string, string, string&gt; { ... } but there is really no way to say: `IndirectlyCallable&lt;BiFunction, T, T, T&gt; for all T` where `T` is another type parameter (possibly constrained as well, but not concrete). I really don't know of any low-level language (as in close to C) that has real higher-rank types. It seems a too hard problem to solve for too little benefit. Arguably function overloading in C++ gives you the most useful patterns of higher-rank types (e.g. for creating visitors). C++ also has Higher-Kinded types, which is what most people would like to see in Rust.
I believe this is what you are looking for? https://doc.rust-lang.org/stable/std/rc/struct.Weak.html
The 2nd point is somewhat troublesome to me. I've worked some in game and UI development, where these kind of data structures are used a lot. For example, you have a directed acyclic graph which is walked downwards when rendering and upwards when dealing with collision responses. Seeing how verbose even a doubly linked list is, these kind of data structures sound like something where the easier solution would be to just whip out the unsafe{}. This would be moving away from idiomatic Rust, however..
IIRC, Rust has higher rank lifetimes (via for), but not higher rank type parameters.
&gt; Of course anyone designing coroutines for Rust should keep in mind what C++, among other languages That's my point (and I think I only have a point and a question). Til now I have not seen any comparison with other approaches. Until now I've saw "futures are the way and we will do better futures than other people so we don't need to look at anything else". Congratulations to the future team, btw. It's an interesting work and I like it (even if I don't find any use for me). &gt; but I took a quick look and what they're talking about does indeed use virtual dispatch and heap allocation. There is one type std::future&lt;T&gt; (for a given T) of a fixed size, so the future's data must be behind a pointer and the code must be behind a vtable. (Not to mention that one of the slides shows atomics and a mutex being involved at some point.) This is their future, not their coroutine. &gt; If one async future-returning function calls another one and awaits on it, that's two allocations, and so on; this is the issue that "big futures" are supposed to avoid, by putting the whole 'stack' in one allocation They compare futures to coroutines at usability level (and futures are not as readable as blocking functions). Then he compares a pure callback-based approach with coroutines (and their implementation of coroutines won). If you're curious about allocation, that's the part you may be interested in: https://youtu.be/_fu0gx-xseY?t=22m5s
Good catch! Yes, it's true that your example would be a problem. However (and that's not mentioned explicitly in the blog post) those tables that do the tracking for reads, they also do the tracking for insertions into them. If want to access something in a tracked table, you do something like `table.get_or_insert_with(key, factory)` where the key determines the dependency node and factory is a closure that generates the value if it is not present already. This `get_or_insert_with` method takes care of setting the correct node active. You can only go benignly wrong with this pattern.
This was also posted here 2 days ago: https://www.reddit.com/r/rust/comments/523i1o/taking_out_the_garbage_the_cost_of_allocation/
Nice work! It definitely has the look off a (more or less) one to one translation so there is definitely room for some more idiomatic code. There is a few places which could be written as a `match` to reduce indentation and make it more readable. https://github.com/kradical/rusty-pl0/blob/b49906e2a94838313c2edfa33b8a9545162fddc6/src/main.rs#L692 https://github.com/kradical/rusty-pl0/blob/b49906e2a94838313c2edfa33b8a9545162fddc6/src/main.rs#L356 No need to allocate a `String` here https://github.com/kradical/rusty-pl0/blob/b49906e2a94838313c2edfa33b8a9545162fddc6/src/main.rs#L84. Just use the bare `&amp;str` instead. You could pass a `&amp;[Option&lt;Instruction&gt;]` instead of a `Vec` here https://github.com/kradical/rusty-pl0/blob/b49906e2a94838313c2edfa33b8a9545162fddc6/src/main.rs#L974 Those are quick and easy code style fixes but I would also like to suggest some improvements to the error handling which would take a bit more work. Instead of just using error codes you could use enums. Not only does it make error handling more readable but you can also store some additional information in them which can be used to make the errors more readable for the user. If you store all information in an enum such as the example below then you can also delay pretty printing of the errors until it is actually required which is also really useful! enum Error { MissingSemicolon, UndeclaredIdentifier(String), // ... } struct Parser { errors: Vec&lt;Error&gt;, // ... } I will omit a larger example of this and instead just link to the error definitions for the [typechecker of my own compiler](https://github.com/Marwes/gluon/blob/12721da373ae515a97c4f6e367f0d93f239e7430/check/src/typecheck.rs#L25-L127). While I don't use it in that particular error definition you may want to use a crate such as [quick-error](https://crates.io/crates/quick-error) which makes it really easy to define [new error types](https://github.com/Marwes/gluon/blob/12721da373ae515a97c4f6e367f0d93f239e7430/vm/src/lib.rs#L58-L92)
[removed]
Hmm, but (T, T) -&gt; T satisfies that trait bound...
Sadly, due to $circumstances I can't make it to RustFest. Have a blast without me anyway! :-) In the meantime, I'll do TWiR, try to get flamer to work again and try to `impl Add for Cow&lt;str&gt;`.
You can use `from_str_radix` to parse from a hex string to an integer, and then use the `{:b}` [format specifier](https://doc.rust-lang.org/std/fmt/#formatting-traits) to format the integer to a string as binary: fn hex_to_binary(byte: &amp;str) -&gt; Result&lt;String, std::num::ParseIntError&gt; { let n = try!(u8::from_str_radix(byte, 16)); Ok(format!("{:08b}", n)) } fn main() { assert_eq!(hex_to_binary("0F").unwrap(), "00001111"); }
Fair enough, tbh I haven't written any ocaml and only heard about this from someone who used it professionally. Maybe I misremembered and it was the number of type parameters that's limited. However it doesn't change the overall narrative of my comment which was, that there is a scale of expressive power to these things. Also are you absolutely sure of it as I am pretty sure of what I remembered.
I would strongly advise looking at React and copying a lot of what they are doing. IMO, the way you write React/Redux could be translated to Rust in a pretty straightforward way, perhaps even using macros to achieve something similar to [JSX](https://facebook.github.io/jsx/). React supports a more OOP style programming via classes and mixins but the React team recently announced that they no longer recommend writing React code that way: https://facebook.github.io/react/blog/2016/07/13/mixins-considered-harmful.html
Yes, but you lose the type in that case; `GraphSearch` may expose `frontier` to the user and the user may require the type of `frontier` to be `MyListWithExtraFunctionality&lt;Node&gt;` and not just `IList&lt;Node&gt;`.
I'm absolutely new to Rust and have noticed some code runs fine using unwrap() whereas if it is removed it throws an error. What is going on the allows unwrapped code to run (seemingly) fine when it is included, but become error prone when it is removed? Is using unwrap() a best practice or should I avoid including it? Are there alternatives?
Rust code wraps objects in an `Option` or a `Result` when the calculation that created the object might fail, so nothing was created or there is an error to report. There are several ways to handle the latter possibility when what you really want is the successfully created object. `unwrap()` is the fastest way, it just ignores the possibility of error and causes the program to crash (panic) if the object isn't there. It's better to actually check both possibilities and do something sensible (like print an error message to the user) in the error case. If you simply remove `unwrap()` from the code and try to use `Option&lt;T&gt;` as if it were `T`, you'll get a type mismatch, which is the compiler telling you, "Hey, your T might not be there! You need to check!". For lots more detail, read the [error handling](https://doc.rust-lang.org/book/error-handling.html) section in the book.
&gt; So I guess you are saying that you can't do object oriented programming in Rust? Or are you saying that you do a different kind of object oriented programming in rust? Because you can add me to the list of people who don't get it. I am saying there is no such thing as OOP it's just data and functions, and all you do when you force them together is you constrain yourself. &gt; it's a via class inheritance (maybe virtual and maybe of an abstract base class, but that's a detail). It's largely seamless. Is there a seamless solution in rust or is it a "suck it up, buttercup" and you have to implement the font_size method in 47 different structs when the implementation is exactly the same everywhere? Abstract classes are a fallacious design methodology, that always leads to pain. Never use them as they constrain design much more than anything. They are just slightly better than having global state in your code. Secondly never do -- shoe that fit all -- design, add font_size when you require it and if it turns out it was 47 places then so be it. What would you rather look at when you opened a struct? `struct A impl baseWidget` or struct A - font_size fs 
&gt; can they actually design industry-scale software like GUI widgets in Rust. They seem to imply this is just a matter of documentation... but is it really? The [Servo](https://github.com/servo/servo) project, intending to be an industry-scale browser engine, seems to be moving forward. That said, the GUI prototypes for servo like [browserhtml](https://github.com/browserhtml/browserhtml) are just built with HTML and JS, but the browser engine itself certainly has traditionally OOP-like constructs like the DOM working just fine AFAICT.
I'm afraid the article has actually confused you, and now you're mixing up higher **rank** types and higher **kinded** types. Read the article again, more carefully...
Poe's Law. Which side of Poe's Law I don't know, and I guess that's exactly the point of the law :p
Please allow me to provide a dissenting opinion (at least dissenting relative to the predominant voice here). I recognize this is going against the grain for a lot of UI development of the last decade, and I recognize that this may come across as emotional and ranty, but I feel like there have been a lot of babies thrown out with bathwater as of late and I would like to stop this progress into hell. My claim is this: OOP is the *best paradigm* for GUI programming, and the failings of GUI programming using OOP in the past has more to do with faulty and inferior language constructs than it does the paradigm itself. Specifically, inheritance as the only means of abstraction and code reuse has crippled OOP GUIs into terrible APIs. However, current language constructs, especially those found in hybrid (functional *and* OOP) languages like rust and scala, have the ability to turn OOP into a near perfect abstraction for GUIs. The basis for the claim is that all interactive UIs are state machines. They are not *like* state machines...they literally *are* state machines. Many people don't see the connection right away, but they also don't see the connection between human arms and levers. The *entire* functionality of an interactive UI can be described using a state machine diagram, because UIs *are* state machines. And because one of the most important principles of software engineering is to model the real world using appropriate abstractions, I would submit that any language construct that is excellent for modeling state machines is likewise excellent for modeling UIs. Likewise, any language construct that is poor at modeling state machines will be a leaky abstraction when used to model UIs. In particular, the trend towards "stateless" pure functional UIs is a gigantic leaky abstraction. You can't get rid of state in your UIs, you can only move it somewhere else. When you pretend that state doesn't exist, you haven't actually succeeded in eliminating it, you've just moved it somewhere where it is less obvious, less controllable, and more dangerous. A common example of what happens when you try to eliminate state through FRP-ish constructs is that you end up storing your state in the rendered object, where your best hope for manipulating it is querying, diffing, and constantly reconstructing it. Check out [this example](http://jamesknelson.com/state-react-1-stateless-react-app/) of a pure "stateless" UI. There is no state actually stored in javascript memory, but that doesn't mean the UI is stateless...the author explicitly recognizes this in a follow up post. All that has been done is the state has moved from javascript into the DOM, where it is much harder to reason about. You could do very similar things with other forms of FRP, where instead of explicitly modeling state, you end up moving it into non-obvious places like URL routes, observable listener closures, or similar. There is a small subset of functional UI programmers that actually do acknowledge the role of state in UIs, and their solution is mind baffling: global state atoms (aka the Redux model). All of the reasons that programmers have shunned globally-scoped variables as an anti-pattern also apply here. Global state atoms: 1. Are hard to reason about because they don't adequately scope state. 2. Are dangerous because they are manipulated using globally scoped events. 3. Are hard to manipulate because the of the massive surface area. Apart from their general anti-pattern behavior of global state atoms, the way that they are rendered makes them extremely hard to refactor. This is because this model falls on the wrong side of the [Expression Problem](http://c2.com/cgi/wiki?ExpressionProblem). In UI programming, you are far more likely to refactor your state than you are to refactor the functions acting on that state (your renderers). Any change in the structure of your state atom requires every single function touching it to be modified to accept its new arguments. In other words, every time you want to add a new UI widget, you change your state atom slightly, and now all of a sudden you have to rewrite either all of your reducers or all of your renderers. If you actually want to critically analyze this model, search for criticisms of Redux pretty much anywhere on the internet. The common theme is *boilerplate, boilerplate, boilerplate*. This isn't just because javascript is a bad language and redux is shoddily written...it is because the entire concept is poorly suited to modeling UIs. However, OOP is a near perfect solution to modeling the inherent state machine behavior of UIs. A state machine can be described as State^0 + Event =&gt; Side-Effect + State^1. In other words, a state machine couples (mutable) state, events, and behavior. Kinda sounds like the first sentence to the first chapter of a book on OOP, doesn't it? OOP was basically invented by a language (Simula) that is best described as a DSL for simulation...a task that involves modeling large quantities of state machines. If you have components, the best way to manipulate those components is with actions (methods) directly coupled to the state(properties) that it manipulates. An object, if you will. I actually think inheritance provides a much better API than the Redux model, but it is still inferior to what modern hybrid languages like Rust can provide. In particular, traits allow for breaking down both sides of the expression problem, and algebraic data types allow you to model state in vastly more expressive ways. You still associate state (data) with behavior (directly coupled methods), but traits allow for much better semantics for handling some of the messier cross cutting concerns that inheritance-based APIs would make difficult. You can still have pure renderers, but they are *methods coupled directly to the state that they are modeling*, just like they would be in an OOP class. And all of those weird cross-cutting concerns (like component lifecycle methods) can be mixed in and modeled as they are needed, instead of messily inherited. And with algebraic data types, you get much more expressive state machines. For example, instead of having properties and computed properties on a form to determine if the submit button should be disabled or have a loading spinner, you can directly model those states (forgive the syntactic artistic license, I know this isn't exactly valid Rust): enum FormResponse { Success, Failure(msg: &amp;str) } enum SubmitButton { ActiveButton(submit_event: Future&lt;void&gt;), SpinnerButton, DisabledButton } enum EmailField { Empty, Invalid(email: &amp;str, msg: &amp;str), Valid(email: &amp;str) } enum EmailForm { Empty(email: EmailField::Empty), Valid(email: EmailField::Valid, button: SubmitButton::ActiveButton) Invalid(email: EmailField::Invalid, button: SubmitButton::DisabledButton) Submitted(email: EmailField::Valid, button: SubmitButton::SpinnerButton, response: Future&lt;FormResponse&gt;) } Think about it: any data type can become a component with nothing more than a RenderComponent trait. Each enum can have its own impl to control state transition behavior. The state remains local and cannot leak. Event listeners can have compiler-enforced lifetimes and bindings. You can mix in lifecycle traits at the individual enum level if you want finely tuned lifecycle behavior. Need to log UI interactions? Implement the EventLogger trait on your events! Need to call an API to fill in form values? Implement the ComponentWillMount trait! Need to cache and restore state when switching between pages? Implement the ComponentWillUnmount and ComponentWillMount traits! So my plea is this: don't throw out the baby with the bathwater. Model your state directly and with a component-local scope, and use the superior modeling power of modern language features to bring pleasant composability to object-oriented UI design. 
That is a way for defining stateful functions. It can describe how a widget updates itself, but in most UIs you want to read and modify other widgets, so you need shared, mutable access, which is against the design of rust and functional languages.
No, you really don't need shared, mutable access. The worst case scenario is that you need to do more than one pass. Look at how Elm does it. Or the layout part of Servo (the DOM is, obviously, dictated by existing specs).
I imagine the reason for that is the image crate (the library I'm using for manipulating the images) is just slower than imlib2. I don't think there's anything I can do about that. If you use `-m full` it'll probably be noticeably faster. Oh, and be sure you're compiling in release mode with `cargo build --release`.
Just haven't gotten around to it yet. Soon, though!
Hi! I've been working on [Cursive](https://github.com/gyscos/Cursive), a TUI for rust. While being constrained inside a terminal does change a few things, many concepts are actually shared with a regular UI. For instance: * I have component-level event handling. This is basically done in a `fn on_event(&amp;mut self, event: Event) -&gt; Option&lt;Callback&gt;`, so components can only mutate themselves, but they can return a callback that will be called by someone higher up in the tree (the callback takes the parent as argument but should be `'static`, so it can't reference `self`, but since the component already had the opportunity to self-modify, it's not a big deal). This avoids the need for bidirectional references while providing most of the features. * I use `Any` to retrieve views of arbitrary types by identifier, but that's hidden under the API, the user only gets a `Option&lt;&amp;mut T&gt;`.
I'm not sure about the exact relation to FRP, but Qt certainly has the idea of signals/slots built into it's core, and it is a nice way to separate gui code from more decoupled work code.
Thanks, we would welcome any help on OrbTK we can get. OrbTK meets most of these requirements - it is multiplatform, supporting Redox, Linux, OS X, and Windows, it is idiomatic, it is not immediate mode, but it is very young. You can see the project at https://github.com/redox-os/orbtk. Some examples can be found at https://github.com/redox-os/orbutils
Saw it in the enum after commenting. Thanks for working on this, I sorely needed a wallpaper setter with these characteristics.
It was in release mode but I'll check again. Maybe if you do benchmarks of the same operation done with imlib2 vs image crate or libjpeg vs jpeg-decoder and submitted them to the projects, we could get a fix or conclusive explanation. If there's a gap, this looks like a good test case.
Hey there, I love your blogs and your work with rusty-machine!! Sorry, about the lack of comments! The solver as an object that takes ownership of the neural net sounds like a wonderful idea! Right now I am at a loss of when to terminate my gradient descent algorithm. I have it going for 60,000 iterations which comes up with some nice results (96% accuracy with the iris dataset). I mainly went with my own linear algebra library just because I have never programmed one with so many features (and I was following a video on youtube which was programming in python &amp; numpy). As far as GPU support Leaf has a great one and there is also one https://github.com/tedsta/gpuarray-rs. I guess in short, my biggest problem right now (I think) is the efficency of my matrix library (which can be solved by using other linear algebra libraries) and the way I am minimizing my cost function. Is there a solver that is generally the best?
I believe the point is that all the nodes will be the same thing, but at a different level from "type". In the languages you list, I think they will all be a pointer of some kind. Similar code can be accomplished in Rust using boxed trait objects, potentially with downcasting via `Any`.
I think the fundamental issue is that composition and `Deref`/`AsRef`/`Borrow` were never designed to cover all the things that inheritance gives you in many OOP languages. You don't get the subtype relationship/hierarchy, you don't get the access controls, you don't get the overriding. Using `Deref` gives you implicit delegation to a single target type, which sort of emulates single inheritance of a supertype, but without the accompanying semantics of being a supertype, and prevents a `Deref` implementation consistent with Rust's intended semantics for the trait. Rust takes a much more explicit approach by separating the concepts. You share data representation through composition. You share behavioral semantics through traits. Then you explicitly associate the behavior with the representation with an impl. Nothing is inherited, only delegated, and that is done explicitly. Rust could certainly use some improvement on the reuse front, like some short syntax for delegating a trait implementation in its entirety (or even partially), but trying to shoehorn OOP-style reuse via `Deref` is likely to result in more confusing code that doesn't scale beyond the simplest cases because it's working against the language. Composition and traits just aren't designed to grow in the same manner as inheritance in OOP; they scale along separate axes (e.g. data, behavior) instead of along multiple axes at once (e.g. data + behavior + subtype).
Rust does not try to be OOP, so there is some impedance mismatch (which requires boilerplate to overcome, and I would expect sometimes would run you into a wall). Rust is a multi-paradigm language, and despite its imperative nature borrows heavily from the functional languages, so it might take some time to adapt :)
I believe you from experience. It seems less painful, not from experience, than trying to somehow abstract over and having to manually rediscover all the stupid quirks of every single major platform's roughly-corresponding APIs. 
I adore how engaging the slides are!
Is it ok to solicit help from someone specific on these threads, e.g. /u/acrichto ? Sorry, Alex, if it's not legit to page you for a question like this or if you're not the right resource.
Is higher kinded polymorphism just a matter of making [this](https://is.gd/gRAPAR) compile?
I apologize for the title (which can seem rather clickbait-y). My hope was that it would help spark (especially critical) discussion and comments.
&gt; I think "absolutely" then is needlessly aggressive, but I suppose you see that. Yeah. I didn't submit the post to /r/rust, so I can't change the title, but I agree. &gt; Constructively, would you propose forming an alt-std::collections crate that contains most of the items in here? So much of what you describe (including crossbeam items) are works in progress waiting for the Rust ecosystem to evolve, and maybe we've reached the point one year on where it's time we propose a collection of crates that could be the next standard. Sounds like a good idea. First of all, a good implementation of each primitive is needed, then it would be ready to stabilize into libstd.
(Non-interface) inheritance is viewed as an antipattern in OOP since the 90s by people who care about design. Lots of modern languages (Rust, Go, ...) just leave it out because the few cases where designs benefit from it are massively outnumbered by the harm it does in other cases. OOP is not inheritance. Good OOP design prefers composition.
Will macros 1.1 only support custom derives or procedural macros as well? If first, does an item need to be semantically right (so it would compile in pure rust), even without derive, or just needs to be lexically right? 
&gt; Hey there, I love your blogs and your work with rusty-machine!! Thank you very much! For termination you'd generally define a stopping condition. A fairly standard one would be to check when the cost function stops changing by some significance level, i.e. ``` if (curr_cost - last_cost).abs() &lt; eps { stop(); } ``` Some algorithms may have more complex stopping criteria. But generally it's fairly low cost to do the above and you can keep a high iteration cap like 60,000 as you have now. For the linear algebra I totally understand wanting to roll your own - that's precisely what I'm doing! As for making it more efficient there's obviously a lot that should be done - but one easy step would be to use [matrixmultiply](https://github.com/bluss/matrixmultiply). It's written entirely in Rust and is waay more efficient than a naive implementation would be. You can also make the `x * y.transpose()` calls even more efficient by swapping the strides (I don't do this in rulinalg yet). I'm by no means an expert on gradient descent solvers. I think SGD, AdaGrad and RMSProp are all popular options but I can't speak for which is best. It likely depends on the application.
Currently there's unfortunately no easy way to do this, you'll have to basically match out and parse the fields of the `sockaddr_in` yourself. Hopefully that's not too onerous though! 
Thank you. Additional question: Where can I read more about Result&lt;&gt; and Ok()? and how can I add the 8 digits padding as an optional parameter to hex_to_binary?
&gt; You have to monomorphise the function to do this. But _do you really_? You could monomorphise it once for `void*` and erase the types. Using RTTI and `dynamic_cast` you could do a run-time type switch to call different functions (the original `BiFunction` monomorphised for different types if the compiler detects that the set of types is closed, or embedding a JIT), but all this magic would be hidden behind a higher-rank type (and painfully slow).
It's like being able to write: for&lt;T&gt; fn(x: T) -&gt; T Which could be interpreted as _for all types `T` there is a function that takes a parameter `x: T` and returns a `T`_. Note that this is not the same `for` as in loops or impls. This allows us to write something crazy like: fn foo&lt;U&gt;(f: for&lt;T&gt; fn(x: T) -&gt; U) -&gt; U
Then you're not taking a `bool`, you're taking a `bool*`. And you can't do this implicitly because the argument can be *anything*. What about a `Vec` where you assert `Vec&lt;T&gt;: VectorLike&lt;T&gt;` for all `T` and then return a `Vec&lt;bool&gt;`? `std::function` is a bit different; it's more equivalent to the Rust action of casting a function to a trait object. The cost here is a lot more explicit and necessary.
Isn't this kind of the point of the rust nursery? A place for crates to grow and gradually become part of std?
If I'm looking at the dom structure correctly, `Node` is a more typical OOP Node with a parent and children: https://github.com/servo/servo/blob/master/components/script/dom/node.rs#L89 `MutNullableHeap` is just a wrapper for `UnsafeCell`: https://github.com/servo/servo/blob/master/components/script/dom/bindings/js.rs#L343
Anyone knows why hyper needs to do IO at all? something like http://sans-io.readthedocs.io/how-to-sans-io.html comes to mind, but maybe I just don't get hyper at all :)
For future notice, please refrain from submitting links with clickbait titles.
This doesn't make any sense. You can't "easily" solve *all problems* using *any paradigm* with only a single language. You've basically just demanded that Rust be a magic Genie that can grant wishes. "I should be able to solve my problem using the wish-based paradigm." Engineering and design are about managing trade-offs, and if it weren't, there'd be no reason to do either.
I see it as an advantage that rust doesn't have the equivalent of empty interface.
HKT would be great for things like smart pointers, but it still remains to be seen how useful they will be for collections. We need the efficiency that iterators provide, but unfortunately iterators don't really conform to Haskell-style interfaces. That's not to say it's impossible - it will just take a bit of work to figure out.
I kind of like the fact that the standard collections are so minimal. I think it is better to supply these extra collections through blessed libs rather than standard libs.
Between this and yesterday's OOP ragequit post, it seems that /r/rust is finally getting the kind of hyperventilating masturbatory nitpicking shitposts I come to Reddit to read. 
That's the rub though with breaking changes in standard libraries. Even if it is an "easy" fix, it is still a break that requires some work to fix potentially by many people. That said. To the OPs problem why not last(); or last_mut();? There are no bounds checking there.
Aren't you just reimplementing a significant subset of Rust's memory model and pointer semantics at that point?
I knew Rust was going to make it when I started seeing very passionate posts insisting Rust was going to fail. 
&gt;Now, you want to be able to retrieve the element with the highest or the lowest priority, with a reasonable performance. This is an... unconventional definition of a "priority queue", e.g. [`std::priority_queue`](http://en.cppreference.com/w/cpp/container/priority_queue) in C++ and [`PriorityQueue`](https://docs.oracle.com/javase/8/docs/api/java/util/PriorityQueue.html) in Java expose the same API as Rust's `BinaryHeap`: access to just one end (it's in the name: it's a queue, not a deque). &gt; My only criticism has to do with cache efficiency, which is notoriously bad I don't believe it (where does the 60% number come from?): the cache behaviour might not be the absolute best possible for many cases, but B-trees are *explicitly* designed to reduce the number of memory accesses required, being quite flat. The data structure was originally introduced for on-disk storage, where a "cache miss"/memory access is seriously bad. I'm sure there's many ways to improve this, especially for specific use-cases, but I suspect "just" using an arena has more downsides than the article implies. As one example, the memory management gets harder for a tree that is being mutated a lot: one ends up having to basically reimplement an allocator to avoid unbounded memory growth, tracking free locations etc., whereas just using a modern allocator directly gives most of the stated benefits (e.g. jemalloc has large slabs internally, and even groups similarly sized allocations, so it is likely that tree nodes will be close in memory anyway).
I've been reading a bit about implementing something like higher kinded types in Rust with macros and was wondering if it would be possible to define something akin to Haskell's deriving functor in Rust. If some, would anyone want to collaborate on a project implementing a nice crate for higher kinded types in Rust? Thanks!
I wonder when something like: v.push(f); v.last().unwrap(); doesn't optimize the check out?
I'm pretty sure it meets your requirements. Besides it not being done yet (it's certainly better to help out than reinvent the wheel), what about it is unacceptable? For a retained mode renderer, I'd probably just use Gtk+ since it works well. For smaller projects where a simple UI is important, I'll definitely go with Conrod.
&gt; You can't have an Interface returning a concrete type though, if your `Addable` interface has an `add` method, it doesn't know who is going to implement it, so it has to return `Addable` and not `int` or whatever. That isn't true at all. Usually you do this with interfaces, at least in Java (found a Java example first, so using that as a reference; though I wrote my own example): interface Addable { public double getValue(); public Addable add(Addable rightSide); } class MyInt implements Addable { public int value; public MyInt() { this.value = 0; } public MyInt(int value) { this.value = value; } public double getValue() { return (double)value; } public MyInt add(Addable rightSide) { return new MyInt(this.value + (int)rightSide.getValue()); } public String toString() { return Integer.toString(value); } } class MyFloat implements Addable { public float value; public MyFloat() { this.value = 0.0f; } public MyFloat(float value) { this.value = value; } public double getValue() { return (double)value; } public MyFloat add(Addable rightSide) { return new MyFloat(this.value + (float)rightSide.getValue()); } public String toString() { return Float.toString(value); } } public class AddableExample { public static void main(String[] args) { MyInt foo = new MyInt(42); MyInt bar = new MyInt(20); MyFloat baz = new MyFloat(1.1f); MyFloat buzz = new MyFloat(2.2f); System.out.println("foo.add(bar) returns: " + foo.add(bar)); System.out.println("foo.add(baz) returns: " + foo.add(baz)); System.out.println("foo.add(buzz) returns: " + foo.add(buzz)); System.out.println("bar.add(baz) returns: " + bar.add(baz)); System.out.println("bar.add(buzz) returns: " + bar.add(buzz)); System.out.println("baz.add(buzz) returns: " + baz.add(buzz)); } } The parameter for `add()` does have to be `Addable`, but it can return `MyInt` or `MyFloat` instead of `Addable` if it wants. This isn't very useful for floats/ints, though... And Java doesn't actually give you operator overloading (even though they use it in the `String` class). So at least for this example of an `Addable` interface in Java, the whole thing is pointless.
It's certainly a bit longer, but I don't really feel that the current API is *hard*: use std::collections::HashMap; use twox_hash::RandomXxHashBuilder; let mut hash: HashMap&lt;_, _, RandomXxHashBuilder&gt; = Default::default();
I guess that's one way of saying Rust is doing well, but it's also a bit incriminating of the political climate in PL design and software in general that clickbait titles and angry rants would be a measure of a language or library's success. With that said, this article's content itself isn't nearly as full of hubris as the OOP one, and is generally a fair critique.
Please do. It's not so surprising there's no bindings because there are pure Rust crates for what imlib2 does.
Honestly I have no issues with Conrod itself. I just got tired of people suggesting others use it while it wasn't ready.
Maybe hash libraries should just export type aliases: fasthash crate: pub type FastHashMap&lt;K, V&gt; = HashMap&lt;K, V, FastHashBuilder&gt;; main.rs extern crate fasthasher; use fasthasher::FastHashMap; let x = FastHashMap::default(); //... 
Ah, basically that would require creating a type but filling in some of the types ahead-of-time? You can *almost* do that right now with a type alias: extern crate twox_hash; use std::collections::HashMap; use twox_hash::RandomXxHashBuilder; type FastHashMap&lt;K, V&gt; = HashMap&lt;K, V, RandomXxHashBuilder&gt;; fn main() { let mut h = FastHashMap::default(); h.insert(0, 1); println!("{:?}", h); } Sadly, you can't use `new` or `with_capacity` because those are already "specialized" for the default `SipHasher`. &gt; I observe that this cycle of discussion is never-ending Note that I'm not trying to knock on you (or anyone else) for Rust being too difficult. I've been using Rust almost daily for ~2 years at this point, so many things have become ingrained to me, and I've forgotten what it's like to be on the upward slope of that first Rust complexity curve. My intent during this discussion was not to say "no, it's not", but to point out the current state and learn from you how it could be made easier.
I'll try to make a [tui frontend](https://github.com/little-dude/xi-tui) for the [xi editor](https://github.com/google/xi-editor), using [termion](https://github.com/ticki/termion). I just started, and I'm still struggling with rust concepts, so it's hard to evaluate how far I will be able to get in a week.
Humans don't hyperventilate, masturbate, nitpick, or shitpost?
The problem with that is that it introduces dependencies. If you look at C, the wheel is reinvented over and over and OVER and has been for 40 years. Dependencies in programs are bad, but dependencies in libraries is even worse. I think avoiding that as much as possible is extremely valuable long term. 
Okay so you dont have the whole picture. Ignoring something [dependently typed](http://learnyouanagda.liamoc.net/pages/introduction.html) system F has something absent in most functional languages (type inference becomes undecidable) called *type application*. It works like this: id : ∀a. a → a id = Λa.λ(x : a).x idBool : Bool → Bool idBool = id Bool -- Type application When you bring System Omega (ω̲) into the mix you can now give types types (kinds) like the example you gave. So ∀a. a → a in just System F has an implicit kind siginature (:★) as ★ is the kind of all (base) types.
[It does](https://github.com/meh/xenu/blob/master/xenu-background/src/main.rs#L95-L105)
Alright, that problem is being caused by the `image::open` function using the file extension to guess. I'll switch to either `load` or `load_from_memory`, and that should fix it Edit: Problem should be fixed now
/u/meadowfire said it was a measure of /r/rust, not rust the programming language.
Some languages call it [optional typing](https://www.wikiwand.com/en/Option_type) or nullable typing. Note that in Rust it is merely an application of enums (and a convention), not a separate type-system feature.
I think part of the concurrent datastructure issue is that the tradeoffs are harder there. There are many ways of implementing a concurrent hashmap. Some are lock free, some are not (but are otherwise better). Depending on the use case you may want one or the other. This is not to say that everyone should roll their own, but to say that picking one to be *the* concurrent hashmap is hard. But yes, we do need more libraries for this; I was hoping crossbeam would be this but it seems like it's focused on lower-level primitives. &gt; MPSC This sounds like an optimization that could be made without changing the API -- do you have the modified mpsc code you used for your benchmarks? We should try to land it in Rust. &gt; BTreeMap .. My only criticism has to do with cache efficiency, which is notoriously bad. To be fair, you know this going in. All datastructures have tradeoffs; choosing between BTreeMap or HashMap involves deciding how your load is going to be and if you want to optimize to avoid cache misses in the case of heavy load, or unnecessary allocation in the case of light load (among other things). This doesn't make BTreeMap "horrible", it makes BTreeMap a B-Tree Map. &gt; Let’s move on to doubly-linked list. IIRC Gankro tried hard to remove doubly linked lists from the stdlib but it didn't happen (stabilization? idk) Neither kind of linked list should exist in the stdlib IMO. The safety semantics of the doubly linked list depend on use case, and for a singly linked list you can just use a boxed enum. &gt; Priority queues This is a binary heap. This exists. "Lowest priority" isn't generally a thing with priority queues. &gt; If something is crucial for keeping the ecosystem together, it deserves a place in the standard library. Most of the things listed don't seem "crucial". You have to draw the line somewhere, and overall the line drawn by the stdlib seems sensible -- you have a few lists (LinkedList should not be there, really), a few maps, a few sets, and binary heap. Each one of these performs a different, but very common function. There are a few choices within each type of datastructure for some flexibility. The majority of stdlib consumers just want a map or a set or whatever. This caters to them. If you want something more specific, you probably have various specific tradeoffs in mind and I don't think it's the job of the stdlib to address these. Double-ended priority queues, skip lists, and treaps tend to go down this path. IMO it makes sense for them to exist as separate crates. The distinction is basically drawn at "I need a map!" vs "I need a &lt;specific datastructure&gt;!". There's also a whole bunch of stuff at https://github.com/contain-rs/ and new datastructures should probably go there. &gt; Hiding Box I'm not really sure what you're talking about here, are you talking about functions which return `Box`? The `box` syntax (Now the `&lt;-` syntax) lets you do this, it's just not stable yet. 
Yeah I was about to say, at 90% load you should get more like 2-3 probes on average, which will fit within one cache line if you get lucky with alignment. I usually run my RH tables at 95%, fwiw. One thing that might help is to dynamically switch between 32 and 64 bit hash sizes based on the size of the table. Yes, it means there's a branch, but it should be 100% predictable (= probably free in practice) except that one time you go from 4B elems to 8B elems (or whatever your growth factor is). With 32 bit hashes you get 2x the number of hashes per cache line which should hopefully speed things up nicely. 
Since apparently the author is in here: what's wrong with a doubly linked list? You've got an extra 8 bytes of overhead, sure, but weren't you mentioning how memory is cheap anyway? A doubly linked list is a pure superset of singly linked lists, so there's nothing lost there save the 8 bytes/node. This is assuming 64 bit pointers, of course.
I think it comes down to: * Transparency: whatever the rule turns out to be, I want a) a lint that tells me where the compiler left performance on the table and b) an escape hatch to get at it anyway and damn the Torpedoes. * Usability: Setting up an `unsafe || optimized` dichotomy smells of a trolley problem variant. More analysis or being more explicit about the guarantees the user is expected to uphold (btw. that'd be an instructive lint – warn the user about how the compiler expects the lifetimes around a piece of unsafe code to be!) sounds like a workable solution. * Teachability: we want Rust to be a good on-ramp (edit: to systems programming) for newcomers. I personally don't think that needs to encompass unsafe code.
I'd agree this rule (no clickbait titles, even if it's the title of the article) is a good idea. But the existence of undocumented rules make this subreddit less welcoming, so I feel this rule should be documented in the sidebar (by "undocumented rules" I mean rules that would not be inferred by an average person from the written rules under the 'Rules' section of the sidebar, site rules, or common sense). Undocumented rules make a community unwelcoming because people who are less familiar with the community are more likely to be punished (treated explicitly as an outsider) even when they make a good faith effort to get involved. That is, they read the rules, they read the code of conduct, they make a good faith submission, and they're still upbraided (in no matter how friendly a tone) essentially to remind them that they are a newbie (too new to be familiar with these undocumented rules which are only mentioned once in a blue moon).
Yep that would work too!
We don't want to fill up branch predictor slots. Remember, HashMap is generic, so "a branch" means "a branch for every datatype that the hash map is instantiated for," which can easily be more than one.
&gt; It's quite difficult to exhaustively enumerate the things one is allowed to do. Perhaps I have not successfully communicated my suggestion, which I think is pretty simple and more scalable than you're suggesting here: "No clickbait titles" is a distinct rule about what *not* to do, and a rule that most people would not guess; not from the rules on the sidebar, not from intuition, not from site rules. It should be in the sidebar. It's unwelcoming and has a feel of an exclusionary in-crowd to have a bunch of secret rules that you're just supposed to know, and if you don't know not only will it highlight how much of an outsider you are, but a mod will helpfully come along and point out your "mistake," and everyone will be able to see how new and bad at this you are. Am I exaggerating for effect? Sure. Is making sure everyone feels they can participate as an equal essential to having a welcoming, inclusionary subreddit? Yes.
Is there a timeframe for 0.4.0?
After all, files could be downloaded from the network and sent to local processes, stored, passed around, etc.
It might not be a bad rule to add to the sidebar on the submission guidelines. HN, for instance, leans very heavy on the "original article titles only" rule.
Incidentally, you can implement double ended priority queues with a min-max heap, a slight variation on the traditional max heap that has the same complexity, but a higher constant factor.
Not officially. I personally think that once PR [#107](https://github.com/amethyst/amethyst/pull/107) gets merged, we'll be ready for 0.4.0.
I'll consider it, but I'm reticent because I honestly expect this sort of thing to be obvious. :P In my years of moderating I've never had to explicitly make this comment before!
&gt;Unfortunately, they cannot provide an essential property of the standard library: standardization. Standard libraries serves for making sure t he ecosystem is uniform. If something is crucial for keeping the ecosystem together, it deserves a place in the standard library. How are data structures crucial for keeping the ecosystem together? Arn't standard iteration traits, Vecs, HashMaps good enough for maintaining API interoperability? When do you need more than that?
[Rust documentation on the Result type](https://doc.rust-lang.org/book/error-handling.html#the-result-type)
AH! gotcha. So `∀a. a → a` is the type, and `Λa.λ(x : a).x` is the implementation. Can larger universes than ★ be in the forall in λω̲?
How about... a web browser rendering engine?
I think that's pretty much it, yes.
Have you called implement_vertex!(); for the elements in the colors buffer?
Some of the default and most popular subreddits (such as r/news) have the opposite rule: the post title must always match the linked-to article's title. Since r/rust doesn't make a specific statements on the topic I can understand why someone would assume that their post title should match the article title.
Has any part of this line of questioning been relevant? You want to disparage me because I caught you in a lie, which certainly reflects well on you. 
&gt; You seem to have confused Robin Hood with Cuckoo hashing No, I do discuss that. :) Robin Hood is a lot better than Cuckoo, but it's still carries some of the same trade-offs. &gt; Most of your other suggestions are super niche, and certainly should not go in std. Which ones in particular?
&gt; I am astonished that the author criticizes both the inclusion of binary heaps and the lack of a priority queue. Do they not realize that priority queues are usually implemented as binary heaps? You're misunderstanding me. I'm saying that you choose either: 1. minimalism of the standard library 2. covering many usecases which needs standardization What really stands out is that binary heap is too niche for a minimal standard library, **but** I'm actually advocating a bigger set of collections, hence proposing that. &gt; Also, TypedArena is standard, albeit unstable. Bit of a research fail there. Oh, no. I'm aware, but TypedArena is not well-known nor is it stable. What I'm looking for is granting it a place in libstd.
&gt; I think part of the concurrent datastructure issue is that the tradeoffs are harder there. There are many ways of implementing a concurrent hashmap. Some are lock free, some are not (but are otherwise better). Depending on the use case you may want one or the other. I agree. Concurrent hash map is not exactly well-defined, but nor is hash maps. It is mostly a trade-off, but I would like to see it in libstd. &gt; This doesn't make BTreeMap "horrible", it makes BTreeMap a B-Tree Map. Don't get me wrong :P. B-trees are not expected to have top cache performance, but I present a simple optimization, which can avoid many cache misses by keeping the allocations near to each other. &gt; This is a binary heap. This exists. "Lowest priority" isn't generally a thing with priority queues. There's a bit of a naming confusion here. Priority queues are often built upon binary heaps, but they allows for things like insertions and removals, as well as lookup (by their index), so it is strictly broader. &gt; Most of the things listed don't seem "crucial". You're right here. &gt; I'm not really sure what you're talking about here, are you talking about functions which return Box? The box syntax (Now the &lt;- syntax) lets you do this, it's just not stable yet. I'm talking about functions accepting unboxed parameters and then internally boxing them. I dislike that because it hides to performance cost.
Oh, nothing is wrong with doubly linked list. They're not significantly more expensive than singly linked lists. My point is merely that their usecase is more niche, and it doesn't deserve the place in a standard library going for a "minimal set of primitives".
The title is shit, but I don't think the content is circlejerky?
&gt; It is mostly a trade-off, but I would like to see it in libstd. My point is: _what_ would be in libstd? There are multiple ways of doing a concurrent hash map, and the tradeoffs involved for each are major enough that there's no single one you want to make "canonical". Making this choice isn't really the job of the stdlib, I think it's preferable that these be handled by libraries. &gt; but I present a simple optimization, which can avoid many cache misses by keeping the allocations near to each other. The optimization gets rid of one of the main benefits of btreemap -- it doesn't allocate much in the case of small load. It's all about balancing tradeoffs, and the fact that you want a btreemap that is optimized for different cases than what I want it to be is precisely why libcollections is tiny -- it only gets worse from here. 
I'm sorry. Edit: Changed the title of the blog posts.
Great response. Much apreciated. I'll get back to writing a reply later today :)
&gt; I disagree. B-tree map's strength is really the ability to grow without reallocation etc. the ability to grow without a realloc+copy (like a probing hashmap or a vec), but you still need to alloc semi-regularly when adding new elements. &gt; The most popular form of concurrent hash maps is a hashmap with chaining slots instead of open addressing. These works in a similar way to mpsc, by being lockless etc. Right. But I'm not convinced it belongs in libstd. Most popular does not mean that it's used in the majority of cases; and the tradeoffs here are fiddly enough that I don't think we should be making a decision on this. It's a point reasonable people may disagree upon, but it doesn't make libcollections horrible. 
I just want to say thank you for your contribution. I actually just read your HashMap implementation and associated papers last week. While I have heard criticisms surrounding rusts hash map I just wanted you to know I thought the whole thing was exceptionally well written. It embodies everything that I have come to appreciate rust for. It implements a new or novel technique not often (or ever) implemented in a real world scenario and goes to great lengths to back the implementation in hard CS resources. Even if you, OP, or the community at large end up being unhappy with this implementation of HashMap no one can say that it was for lack of ingenuity in design, thoroughness in documentation, or elegance in implementation. What is even more striking to me is that this level of code documentation and research are pervasive throughout so much of the language and ecosystem. If nothing else rust has made for an enlightening experience as my first systems level language and foray into systems engineering.
Cool! I'd been thinking of writing a very similar post. "Higher-rank types" = Higher-order generic functions "Higher-kinded types" = Higher-order generic types 
&gt; It's a point reasonable people may disagree upon, but it doesn't make libcollections horrible. To clarify, I don't think it actually is horrible. The title was intentionally exagregated.
There were 78 contributions...just one off from the weekly numbering scheme. This doesn't mean anything, but I really wanted them to match...
&gt;Binary heaps are not the same as priority queues. Priority queues allows for insert/removal of arbitrary keys. I don't think this is the common definition of a priority queue. Neither Wikipedia or "Introduction to algorithms" mentions removal of arbitrary keys.
&gt; While priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is an abstract concept like "a list" or "a map"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array. 
&gt; Does rust merge functions that are identical? It used to, but it was removed (like years ago) due to (a) it was a maintenance burden (b) LLVM can handle it. I don't know whether LLVM *does* handle it atm? &gt; Seems like you could do the hash probing off in a function that doesn't depend on the key type [This bit](https://github.com/rust-lang/rust/blob/5531c314a2855aec368e811da6fcd9e98365af51/src/libstd/collections/hash/map.rs#L136-L146) from the big comment in the code seems potentially relevant.
I'm really not sure what point you're trying to make, but just because a priority queue can be implemented with an array it doesn't mean that the API of arrays becomes part of priority queues in general.
You're right. I've only called it as `xenu-background wallpaper`, which panic'ed, and didn't bother to call it as `xenu-background another_wallpaper.png`. Timings show `wallst` and `xenu-background` to be equally slow, so it's safe to assume `image` or one of the decoders is the culprit.
I'd be interested in writing something like this but I'm probably not the best person for it as I went from C / Python / erlang -&gt; Rust. :(
Things other than the title that strike me as hyperventilating, masturbatory, nitpicking, or shitposty (exercise for the reader to decide which): You claim a binary heap can't be used as a priority queue and posture like the rest of us are idiots for not understanding why. You cry about the lack of specialized exotica like treaps, skiplists, or bipbuffers. You opine on the failings of HashMap w/o bothering to understand how it actually works. You casually call for a breaking change to a stable api. The 'Hiding Box' section asserts a controversial 'rule' as if it should be obviously true, and completely handwaves past what such an api would actually look like. You have a big section title "The good parts" over three short sentences, one of which restates things are horrible. 
Ignoring "horrible", it's not even bad, really. Libstd has chosen to remain lean precisely because folks have different needs for collections -- your post and the threads here illustrate this adequately. It doesn't make sense to have blessed collections because folks will feel compelled to use this, and then get annoyed when it doesn't make the same choices they need. Having a few _very common_ datastructures and nothing else seems like a sensible path, which is one that has been taken (ignoring LinkedList; that should be removed if it was possible to do so)
Really? Where? Pretty sure Roadmap 2017 is bug fixes and benchmark improvements.
I only found three differences. Pool allows destruction of objects and reusing the memory. Pool is thread-safe and can be used in multithreaded loops. Pool allows putting each element on seperate CPU cache line.
You have these options for optional function parameter: * define different methods with different names * using Option&lt;T&gt; * use a macro Links: * http://stackoverflow.com/questions/24047686/default-function-arguments-in-rust * https://internals.rust-lang.org/t/ergonomic-optional-parameters-impl-t-from-t-for-option-t/3112/16 * https://github.com/rust-lang/rfcs/issues/323 * https://internals.rust-lang.org/t/pre-rfc-named-arguments/3831 
I am giving the OP the benefit of the doubt and recognition for the amount of work it took to write the post, let alone perceive the problem. This is totally orthogonal to the wrongness of anything contained in it. It is important to criticize Rust, Rust can take it. This isn't someone trolling the community, trying to tell everyone else to not use Rust. The OP thinks the std::collections can be greatly improved, this is true. The OP could have been a little less flammable, and probably spent a little bit more time on how trait system is awesome for generics, but on the whole I didn't find it as jarring as most of the posters here. Now lets go write some concurrent, lock-free data structures.
Cool video, is there any closer explanation what kind of website it's rendering?
Which line of the code are you talking about?
That'd be great. While you could simulate those using deref[mut] it's not nearly as easy.
Doesn't have to do with lifetimes: The problem is that match arms have incompatible types now (one returns a `&amp;mut`, the other has type `()`).
&gt; but the technical definition is clear: an indexable structure in which every element has a priority such that the highest and lowest priority element can be retrieved cheaply That sounds more like a *double-ended* priority queue (a special kind of priority queue). 
[Red black trees are just B-trees with worse cache properties.](https://en.wikipedia.org/wiki/Red%E2%80%93black_tree#Analogy_to_B-trees_of_order_4) That makes them strictly more niche than B-trees.
Thank you! All of that sounds very cool. I'd *highly* suggest adding that to your README. I'd also add performance tests that show the speed benefits, some example usage, list out which processors / cache sizes you target, and a link to the generated documentation.
I like how the answer to all questions that start with &gt; rust is pretty good, but it doesn't have/do X Is always &gt; on its way Seriously impressed by the designers/community so much. 
I'm impressed with the quality of the RustType text rendering! Kerning looks good.
Not really. What I am in particular talking about (sorry for the bad wording) is keyed priority queues (often implemented as Fibonacci heaps).
What I'm requesting is really just any keyed priority queue (plain heaps are not sufficient), like a Fibonacci heap. I've clarified the wording in the post.
How do can I call `get_mut` on a type similar to `Arc&lt;RefCell&lt;T&gt;&gt;`? I've read this is a common pattern, but it currently results in a compiler error (in v1.11) as both Arc/RefCell implement a `get_mut` method.
My point is this: Priority queues are not just heaps. What I'm requesting in particular is a keyed version of priority queues, like a Fibonacci heap or a modified pairing heap. The point here is that it acts as both an associative array and a heap, i.e. you can retrieve elements based on its key, while being able to pop based the highest (value-wise) entry. Sorry, my wording was probably not clear enough. I've tried to clarify it in the article.
Definitely. It's **really** well-written. No concern whatsoever there. My disagreements are mostly of the design choices of the implementation.
I assume you'd confused the two because you said Robin Hood uses double hashing, and that "The “opposite” approach is linear probing". That's not true - Robin Hood is just linear probing with a tiny modification to keep probe lengths short. That makes its cache actually better than naïve linear probing, and certainly better than quadratic probing, which was only ever needed because naïve linear probing has bad worst-case behaviour. Cuckoo hashing is what uses double hashing, and is completely different. &gt; Singly linked lists That's just a more niche version of `LinkedList`, which is already much too much so. By the time you're paying the huge cost for a singly linked list, a doubly linked list, which is just one extra pointer per node, is basically free change. And if you do want a singly linked list, they're easy as pie to write: `struct Node(T, Option&lt;Box&lt;Node&gt;&gt;)`. &gt; Priority queue Your use of "priority queue" seems to refer to something much more specific than anything I seem to be able to google by that name. It seems like any such queue (though I'm really going to need you to specify it more explicitly, since I'm not able to infer it) is going to want to be specific to the given context. Ironically, your 83 results seem to mostly refer to `std::collections::PriorityQueue`, the old name for `BinaryHeap`. And you do say &gt; Note that priority queues are almost always implemented with binary heaps as the backbone. which implies that binary heaps are the correct structure to expose at `std` level. &gt; Treaps, Skip lists. Yet more trees and tree like structures. Super niche. If you fill `collections` with worse versions of B-trees, all you're going to do is make people use worse versions of B-trees. That's not a good thing. &gt; SLOBs (aka. pointer lists, memory pools, typed arenas, etc.) *Not niche.* 
Ah crap, now I really have to finish that GUI framework don't I? :P It's taken a bit of a backseat lately but I really hope to make some more progress on it soon, especially since the core of it is mostly done.
&gt;What I'm requesting in particular is a keyed version of priority queues, like a Fibonacci heap or a modified pairing heap. Fine, but that's not at all what you were saying. Your statements were quite clear, but wrong.
&gt; There is solution that is often ignored which changes behavior of ? from diverging to pipelining which I think is much saner solution. I would prefer if the main reason for giving such a proposal (especially over an RFCed and accepted feature) wouldn't use hand-wavey words like "sane". The proposal for `?` is sane. Disagree on other grounds. ( For reasons like this, I advocate striking the word "sane" from your vocabulary when comparing technology/solutions )
lmao
And you can have multiple graphs with the same structure and indices, but different node/edge payloads 
Is there any possibility of substituting your own hasher / allocator in the rust hashmap, like you can do in c++? I understand changing allocator might not be as useful in rust, but being able to easily change the hashing function seems like it would be useful
Are you telling me you actually build something with Rust? 
The hashing function is very easy to change. Allocators aren't yet.
I'm managing my locks via AtomicUsize. The issue is I'm tying to *safely* share mutable state between threads. But I want the memory allocation to be handled for me. AtomicPtr works, but requires unsafe and rolling my own Arc implementation. I guess you either use Arc&lt;Mutex&lt;T&gt;&gt; or roll your own everything? 
Looks very nice. I liked the original design, and I'm sure it's only gotten better. A quick question, though: does a conrod application still loop continuously (consuming 100% CPU), or has that been solved? For me, it's a bit of an important issue before I taken another look at conrod.
I don't agree that this is somehow "much saner" that the accepted RFC. Here are my two pence. If I'm reading a piece of code for the first time, I want the cognitive burden to be as low as possible. Errors are distracting, so I'd much prefer it if the error handling didn't interfere with understanding the program logic. Therefore, if a piece of code is willing to handle errors, it should handle them explicitly. If it's unwilling, it should diverge to an error handler with as little fanfare as possible. To me, the RFC `?` fulfills this purpose perfectly. Safe navigation doesn't do this. It keeps the errors floating along the trail of the code. Some function calls will operate on the whole `Result`, but some will only operate on `Ok`s (and silently ignore the `Err`s). As far as the cognitive burden goes, your proposal is only marginally more understandable than setting `errno` and keep going. 
Personally I don't like this syntax. It seem still awfully verbose. I think it is even worse for readability than the status quo. It's only goal is to make chaining easier but IMO, chaining is not the main interest of the `?` syntax, only a cool bonus. What is great with the `?` syntax is that it make the nominal case natural to read, while the error returning is still explicit. 
My understanding is that the issue is: if Rust were super strict with its aliasing guarantees it gets really hard to write unsafe code. So an attempt is made to compromise so that broken unsafe code doesn't lead to disaster. But as far as I can tell, this is only an issue for lifetimes *created* in the unsafe block, if the lifetime originates from a 'safe context' (eg passed as argument or `'static`) there is no issue (beside the usual stuff). Could it be better to punt the issue and put big warnings on creating lifetimes in an unsafe context? Kind of like `transmute`. Instead recommend people to use raw pointers which presumably don't have these pitfalls? Totally crazy idea: make `&amp;` and `&amp;mut` create pointers in unsafe context instead of references and require some other mechanism to opt-in references.
I'm not sure what's the advantages over `?` (or even plain `try!`) are here?
Yeah, I've written some react code and it seems pretty nice, seems like it could match Rust's style, as it's clear that components own their children, and so on.
withoutboats also wrote a very nice section explaining higher-kindedness in the RFC on associated type constructors: https://github.com/withoutboats/rfcs/blob/associated_hkts/0000-friends_in_high_kindednesses.md#background-what-is-kindedness
Sorry. 1.1 is only custom derives. 2.0 is the whole thing. But I've heard rumor that you might be able to terribly hack everything through custom derive, to get the full procedural macros via 1.1. It is not what you'd actually want to do, though.
OK, got it. I'm not sure how extensive the checks are that the item you pass to the custom derive is a "real" item. But I guess you could just do: #[derive(Whatever)] #[whatever(x = y, z = 5)] struct Decoy; and the macro outputs whatever it wants to.
It is just example of how to use proposed syntax with `try!`. Macro per se would look like this https://is.gd/au6TQw (the `chain!` macro).
&gt; Someone please tell me this is a horrible idea. Ok. This is a horrible idea. Not only due to C++ being a PITA to interface through the limited, C based ABI you get on most platforms. Not only, because it doesn't behave native in all cases. Qt has transcended a long time ago from a C++ GUI framework into their own, merely C++ based platform. Not only there is moc which not only adds signals but also extending C++ with reflection/introspection and other stuff similar how to GObjects extends C with classes. I also have the feeling like the Qt team embraces Qml a little too much for it to be friendly to bindings. For example, some Widgets only exist for Qml, like the map widget. Who can blame them.
Main disadvantage of `try!` comes when there is chain that can return error. With this macro: try!(try!(try!(foo()).bar()).baz()) looks like this: ttry!(foo()=&gt;bar()=&gt;baz()) Desired syntax would be: try!(foo()?.bar()?.baz());
Just curious, what exactly do you mean by keyed? Do you mean you want to be able to look up an object given its priority? Because I don't see how Fibonacci heaps support that.
Now go and try to use it in practice and you'll find that your version is pretty much useless in rust as-is. As I commented [here](https://github.com/rust-lang/rust/issues/31436#issuecomment-244607172), one almost never writes try!(try!(try!(foo()).bar()).baz()) in rust because we don't have easy to implement extension functions. Instead, one usually writes: try!(Baz::baz(try!(Bar::bar(try!(Foo::foo)))))) --- Take the concrete example: try!(File::open(try!(maybe_dir_entry).path())) With your pipe operator, this can be rewritten as: try!(File::open(try!(maybe_dir_entry?.path()))) IMO, is even worse because you now have to visually parse 4 closing parentheses right next to each other instead of 3 (and you keep the same number of `try!`s). With `?` as-is, you can write: File::open(maybe_dir_entry?.path())?
I was just looking at this library last night and it looks awesome! I'm planning on contributing to this as well as RustAudio in the near future. Thanks for all the exciting work you're doing!
&gt; So an attempt is made to compromise so that broken unsafe code doesn't lead to disaster. Well, more like so that naively-written `unsafe` code *is not broken* but perfectly valid according to the spec.
I think the idea here is that `?` be more akin to `flat_map`. Rather than returning out of the whole function immediately, it just short-circuits the individual expression.
Couldn't you write `try!(maybe_dir_entry.flat_map(File::path).flat_map(File::open))`? (Just playing devil's advocate here, as I also appreciate the convenience of `?`).
&gt; your proposal is only marginally more understandable than setting errno and keep going How it comes that this proposal is "only marginally more understandable than setting `errno`" and current `?` implementation is not?
When it comes to ownership semantics there isn't a single doubly linked list that works, you need to choose between a bunch of tradeoffs on the API. With other collections you can just choose tradeoffs on the performance characteristics, but here there are multiple types of doubly linked list you may need. Also, linked lists are pretty niche in practice, and in the cases you actually need to use them, the one in the stdlib might not always work (because of the API thing). http://cglab.ca/~abeinges/blah/too-many-lists/book/ Singly linked lists are already easy to do with an enum. A dedicated datastructure might be nice, but I'm not sure if it's worth it.
That sounds nice in theory, but at some point you most likely will need to know gruesome parts of the Qt documentation. Oh well, at least it mustn't be too terrible to create custom widgets, thanks to graphicsView. Another aspect, though: What if the Qt devs decide to get rid of moc - for example, as soon as C++ supports reflection - which would make it incompatible to C bindings?
Thanks.
Of course you can but you'll notice that you aren't using `?` there as a pipe operator at all. The point of `?` is to make handling cases like this easier.
&gt; Dropped from 11 crate dependencies to 4. I'm curious to know more about this, especially since adding RustType means you actually must have gotten rid of eight dependencies over the past year. Is it because some dependencies subsumed each other, or because you got rid of some functionality, or because you implemented their functionality directly in Conrod, or because additions to the stdlib obviated some crates?
I totally agree-- I was just trying to point out that there are other ways to keep your code reasonable and understandable without resorting to an early return.
From my (admittedly very picky) point of view, no to both. If you look at modern cross-platform frameworks like React Native and Xamarin, they share quite a bit right up to the UI layer, then they provide a pleasant wrapper for the toolkit. Depending on your application, there are some approaches that can share more. For instance, if you're making something very data entry oriented, you may be able to construct a pleasant enough native user interface from, say, a form definition. (But generated user interfaces have their own problems.)
Now that parts of tokio are in early release, I took a look at using futures in Rust. I realized that using futures requires being comfortable with using combinators on the `Result` type. I blogged some examples to help people grok `Result` with the basic combinators.
You are talking about doing nothing? I don't think it's a good thing at all. I just think it is better than introducing another new syntax that don't solve what is, in my opinion, the main problem.
Yep, my mistake. In ttry! Err is returned early which passes to try! and returns from the function. 
I try really hard to keep my iterator use to only pure functions.
Problem is, that I cannot provide example for monadic operator as it need to be introduced in standard library (I cannot implement trait on `Result`).
I haven't quite played with macros, but I've got a specific task that would really improve some ergonomics for me, and I'm wondering if what I would like to do is even possible with macros and attributes before I try to solve the problem with them. What I would like to do is an ever-so-limited case of refinement types when applied to enums. Specifically, I would like to be able to do this: #[my_custom_enum_variant_typifier] enum Response&lt;'t&gt; { Success, Failure(msg: &amp;'t str) } enum Validator&lt;'t&gt; { IsValid(email: &amp;'t str, Response::Success), IsInvalid(email: &amp;'t str,Response&lt;'t&gt;::Failure) } as it stands, Response::Success and Response::Failure are not types. However, I would like to create some way to make them types, preferably through a minimal boilerplate attribute. Possibly make them compile down to some solution like those found in [this thread](https://www.reddit.com/r/rust/comments/2rdoxx/enum_variants_as_types/). Am I going to run into any major pitfalls? Is this even possible? 
Hashing is done by trait. The trait can be auto made for you and the default is siphash.
thanks, that was indeed the issue.
[Like this?](https://doc.rust-lang.org/stable/std/collections/struct.HashMap.html#method.with_hasher)
Well, macros 1.1 is still nightly-only for the time being too. My understanding is that `#[derive(Whatever)]` can strip out the `#[whatever]` attribute from its output, so it will never reach the custom attribute checker. I might be wrong. I know that it can definitely strip out custom attributes from _inside_ the item, because serde uses it to delete attributes like `#[serde_rename]` on struct fields.
Oh! Thank you very much! Stripping custom attributes did the thing! 
Oops! I fixed it thank you.
I may need to clarify more. The `map` future combinator is doing work underneath the hood to poll for readiness. I don't think you can easily do this match. If I am way off here, I am happy to update the post.
I believe I suggested something like this in a [thread on the users forum](https://users.rust-lang.org/t/poll-try-syntax-changes/3483/10?u=lambda), with a [followup clarification](https://users.rust-lang.org/t/poll-try-syntax-changes/3483/21?u=lambda). Didn't seem to get much response, and the RFC thread was too exhausting to go through and see if it had already been discussed, so I didn't push it.
This! My criticism with the std::collections would be that there are too few Traits. Why is there no `Map` trait that people can build custom implementations against that I can then plug in if I want a different Hashing algorithm or whatever?
Related: Can it be used in a way it receives input purely by OS callbacks/without polling while not showing animations? If so, please document that this actually doesn't work with SDL2 anymore.
Reducing the number of bits stored in a hash table is a bad idea because when they collide, the whole key must be strcmp()'d. 64 bits is a pretty good compromise. For 64 bits of hash + 64 bits of value (pointer), you can fit 4 to 8 buckets in a cache line. That makes it 2 to 4 cache misses per insert/lookup/delete at high load. It's still pretty good. Going to 16 bits would make collisions more likely but wouldn't dramatically increase the number of buckets you fit in a cache line (you still need at least 32 to 64 bits to store the value). If you do collide, it's going to be several cache misses to do strcmp(). I think that would be a bad trade-off, but if someone wants to change my view I'd be happy to see a benchmark to prove me wrong!
I would look at tokio-rs and futures-rs.
I think the point is more that you won't be calling `poll` directly in most cases; instead, you'll be using the future combinators.
Nice post. Why should we "go back" to and_then and map instead of using `try!()` or similar? I've gotten used to try-based code and I vastly prefer it to method chaining.
I don't have this feeling, no. I mean, my little example fits in ~100 lines of code, and the borrow-checker doesn't!
I guess he means that you can have a pointer to a node in a Fibonacci heaps (or a binary heap implemented with a tree) to do removal without search. That's not possible with an array-based heap.
Would this be sufficient? fn map&lt;A, B, F, L: Borrow&lt;List&lt;A&gt;&gt;&gt;(f: &amp;F, lst: L) -&gt; List&lt;B&gt; Edit: I meant `Borrow`, not `AsRef`, I think? I find the docs pretty confusing. Also there is no blanket implementation `AsRef&lt;T&gt; for T`, which seems a bit silly. Edit 2: Full code: fn map&lt;A, B, F: Fn(&amp;A) -&gt; B, L: Borrow&lt;List&lt;A&gt;&gt;&gt;(f: F, lst: L) -&gt; List&lt;B&gt; { match lst.borrow() { &amp;List::Null =&gt; List::Null, &amp;List::Cons(ref h, ref t) =&gt; { let new_head = f(h); let new_tail = map(f, &amp;**t); List::Cons(new_head, Box::new(new_tail)) } } } Mind you that this is all just a more convenient way to call the function but doesn't make it any easier to read. In general you should just have your function accept `&amp;List&lt;A&gt;` if possible (`List&lt;A&gt;` only if required) and create the reference yourself at the call site, but there are some exceptions to this rule. See for example how `HashMap` uses `Borrow`.
&gt; You did read the code and the design articles before writing your "critique", right? I did, of course, but you're right there is a mistake in confusing double hashing and "modern" Robin Hood, I've updated the article.
Note that Rust implementation uses, at the moment, 3 tables: one for hashes, one for keys and one for values. This means that for the initial probing, only the size of the stored hash counts.
Because higher kinded types is needed, in my understanding.
We've been wanting to extend `cargo doc` to just support this type of docs natively, but haven't quite powered through. There's been two or three attempts so far.
&gt; SipHash is not a cryptographic hash function. It's a hash function, it's designed by cryptographers, but it's not &lt;technical term&gt;. [...] Basically none of what you're saying is right. I'm gonna copypaste [another comment of mine](https://www.reddit.com/r/programming/comments/52k29h/a_critique_of_rusts_stdcollections/d7lg7tt) in the /r/programming thread, because why the heck not: &gt; &gt; SipHash is a [**pseudorandom function**](https://en.wikipedia.org/wiki/Pseudorandom_function_family), usually abbreviated to "PRF." As such, its contract is the following: &gt; &gt; &gt; &gt; * If you pick a **random secret key**, then no efficient adversary can distinguish SipHash from a randomly chosen function of the same domain and range. &gt; &gt; &gt; &gt; This property implies that it's good as a [**message authentication code**](https://en.wikipedia.org/wiki/Message_authentication_code) ("MAC"): &gt; &gt; &gt; &gt; * If you pick a **random secret key**, no efficient adversary can forge a valid `(msg, mac(key, msg))` pair. Even if they have seen tons of such pairs beforehand. Even if they pick the messages to compute the MAC for. Even if they pick the messages *adaptively* (use the MAC results from earlier messages to choose new ones). &gt; &gt; &gt; &gt; And being a PRF also implies resistance to hash table multicollision attacks. Meaning that for a hash table with *n* buckets, it takes on average *√n* guesses to find a first collision, but then *n* guesses for each subsequent collision with that first one. &gt; &gt; &gt; &gt; SipHash is distinctive among such functions because it was designed to be very efficient with small inputs (hence the title of the website and [the paper](https://131002.net/siphash/siphash.pdf): "SipHash: a fast short-input PRF"). Hence its use in hash tables in a number of languages in the past few years. Its other uses are as a MAC, where two parties who share a secret key use it to validate that messages haven't been maliciously modified in transit. &gt; &gt; &gt; &gt; Calling SipHash a "hash function" isn't inaccurate ("hash" is in the name, after all!), but it's at best vague, because there really isn't one precise, consensus definition that all hash functions meet. At worst calling it a "hash" is confusing or misleading, because SipHash can't be used in place of something like SHA-256, because it has a *completely different contract*: SipHash only promises security **for randomly selected secret keys** (if your key is not secret or not random you're on your own), while SHA-256 doesn't take a key to start with. SipHash can be used as a lightweight, less secure, less proven alternative to [HMAC](https://en.wikipedia.org/wiki/Hash-based_message_authentication_code), OTOH. The other bit I'd address is this remark of yours: &gt; The point about SipHash is not about its quality, that it lacks collisions (it doesn't) but that it is infeasible for the attacker to figure out the hash function's current secret key (and use that to generate input that collides in the hash map). Resistance to key recovery is *necessary* to SipHash's security, but it's not *sufficient* to defend against hash table multicollision attacks. In the SipHash paper they discuss [*key-independent multicollisions*](http://emboss.github.io/blog/2012/12/14/breaking-murmur-hash-flooding-dos-reloaded/) against functions like MurmurHash, where an attacker can generate multicollisions *without having to recover the secret key*.
This is https://github.com/rust-lang/rust/issues/24355
I think this is mostly what you're asking for: https://is.gd/heVjBw The idea is to move the requirements to a trait with associated types and put bounds on those. Then you write `map` in terms of the trait and implement the trait differently for `T` and `&amp;T`.
For one, `try!()` doesn't currently work with `Option`. Assuming all of these `get()` calls return an `Option`, this is possible without writing your own macros: let quux = foo.get() .and_then(|bar| bar.get()) .and_then(|baz| baz.get()) Assuming we get a `Carrier` that works with `Option`, that code will likely become let quux = foo.get()?.get()?.get() And honestly the `and_then` version seems to be maybe more clear (at the expense of verbosity). Of course, your initial question was WRT `try!` based code. So, assuming again that `Carrier` works for `Option` and assuming you don't like the chaining, something like this would work let bar = foo.get()?; let baz = bar.get()?; let quux = baz.get(); I'm not actually arguing any of these approaches but merely exploring your question. Edit: Oh, and the major issue WRT futures is asynchrony.
FWIW, [I have been playing around with a Flux-like library](https://github.com/Ogeon/flow) in Rust. It works surprisingly well and it's easy to ensure that stores are only manipulated through dispatching. The only problem, at the moment, is providing different levels of thread safety. I'm messing around with it locally, but it's hard to get a nice API. I did also manage to use Cursive in a reasonably React-like fashion, in the examples, to show how it can be used. That did also work surprisingly well, but it's a small and silly example. Anyway, the point is that this model seems to work quite well with Rust, and it can definitely be improved to work even better, but I haven't tested it properly in a complex system. I'm still optimistic.
I cannot find the docs supporting this, but I thought `try!()` with `Option` was landing at some point?
If anyone is interested I finished it [here](https://github.com/deg4uss3r/rust_whois/blob/master/src/main.rs)
Note that try!-based code or similar also includes introducing try-like macros for other types than Result, if needed. It's just a style that you can use regardless if it's with Result or if it's with something similar.
`?` solves the very practical problem of seamless error-handling at large, i.e. it's a minimal explicit "rethrow" operator - as it could be described if it appeared in a "checked exceptions" language. And I can also say that concepts from FP are themselves XY because they shoehorn problems into various forms of function composition, just like LISP shoehorns everything into parenthesis. But is it that constructive? What's important is that a very real problem has an attractive solution and the bulk of the situations I have yet to see a *single* proposal that doesn't make things worse compared to `?`. I'm not sure how we're expected to take such proposals seriously when they don't take into account the reality that the problem `?` solves arose from. [HKT won't give Rust the magical ability to actually express "monads" either.](https://www.reddit.com/r/rust/comments/4x8jqt/zerocost_futures_in_rust/d6dll2f?context=3) One could even argue that Rust *can't* express FP because it doesn't have "functions" in t hat sense, only "procedures" with typeclasses to abstract over them and data structures. In fact, the only language feature that really qualifies as "functional" is closures, mainly used to parametrize imperative-procedural abstractions and contraptions (as sugar, if you will), instead of being directly composed. Rust is not, and *will never be* Haskell or F#: you may be able to borrow some good things, but certainly not *everything*. At the end of the day, FP cargo-culting is just as bad as its OOP cousin, and therein lies the irony of XY: why are you following a big-idea design pattern to solve your problem instead of attacking the problem directly, in a sugary imperative world?
Yes. Because of this I think the `catch` block is the vastly more interesting part of the `?` RFC.
&gt; Can't we just use another font for now? I agree so much. If a font is causing problems like this, just choose a different font. There are lots of great fonts.
Perhaps /u/valarauca8 could supply some context as to how this link is relevant to Rust, and what their insights are on the subject?
ok Rust isn't OOP, but the Rust-y approach to solving the problem seems less ergonomic than other (OOP) languages. Can't Rust be made more ergonomic so that the Rust-y solution is just as nice as the solution other languages provide?
explicitness at the cost of ergonomics? I was considering learning rust and i was hoping that the 'rust way' of solving problems was just as nice as the oop way -- ie that the only reason things were weird was cos people were forcing oop into rust. But it seems that the actual 'rust way' of solving problems is itself kinda awkward and overcomplicated. can't this be fixed? i think it's going to really hurt adoption.
&gt;What lie? You faked the hash function.
Have you thought about dispatching using futures? You could even build a tokio core message pump for event subscription and propagation. It could all run within a single thread. (You could still spin up threads for long-running jobs) 
You are looking for /r/playrust.
Yes. There's even a rule in the subreddit for this (but it's not like this is the only post that runs into it). &gt; ***Submissions must be on-topic.*** &gt; All submitted posts must explicitly reference Rust or link to repositories written in Rust. If you have a link that does not mention Rust but you believe would be of interest to the community, then please wrap the link in a self-post that explains its relevance. We prefer this approach over linking directly to the material and leaving an explanatory comment. I don't even agree with this rule. I think the explanatory comment would be enough, doesn't need to be a self post (and it's not realistically followed anyway). cc /u/kibwen
Non-UI code. Say you have an image sharing app. Your image filters and server interface code might be the same across every platform, but the specifics of your user interface would be coded for each platform (Windows, macOS, iOS, Android, Linux) separately.
If this is a common enough issue that people remember this, maybe it's time to use a font that isn't locally broken on a lot of people's computers.
It'll probably take more than two days (some conferences take month to post the videos…). I'm counting on /u/steveklabnik1 to post here/on Twitter when that happens.
Nope (Or I'll get angry at all the people who didn't tell me! 😉)
- Converts everything to Addable - Check - Requires crazy unsafe casting everywhere - Check - Must be put directly on the data value (in this case forcing you to create a new int and float type) - Check This is why I said interfaces are useless and you might as well just cast your stuff to object, as all your doing is bloating your codebase with useless non-sense. Just for comparison here is the haskell version class Addable a where add :: a -&gt; a -&gt; a instance Addable Integer where add a1 a2 = a1 + a2 instance Addable Float where add a1 a2 = a1 + a2 notice code is simple, not bloated, and have thrice the features of interfaces.
I don't think I understand what you mean by "they will all be a pointer of some kind". I mean, everything's a pointer of some kind. My point is that Python makes it easy to do stuff like this: ~ cat graph.py #!/usr/bin/env python3 class graph: nodes = {} def add_node(self,node): self.nodes[node] = set() def add_edge(self,node,edge): self.nodes[node].add(edge) def has_edge(self,node,edge): return edge in self.nodes[node] if node in self.nodes else false def display(self): for node,edges in self.nodes.items(): print(node,":[","|".join(map(repr,edges)),"]") g = graph() s = "[This is a string]" i = 12 t = ("this","is","a","tuple") f = lambda x: x*x print(f(i)) print() g.add_node(s) g.add_node(i) g.add_node(t) g.add_node(f) g.add_node(g) #why not? g.add_edge(s,i) g.add_edge(s,t) g.add_edge(i,s) g.add_edge(i,f) g.add_edge(t,s) g.add_edge(t,i) g.add_edge(f,g) g.add_edge(g,f) g.display() print() print(g.has_edge(12,t)) print(g.has_edge("[This is a string]",12)) ~ ./graph.py 144 [This is a string] :[ 12|('this', 'is', 'a', 'tuple') ] &lt;function &lt;lambda&gt; at 0x7f2a82f6ef28&gt; :[ &lt;__main__.graph object at 0x7f2a81833550&gt; ] 12 :[ '[This is a string]'|&lt;function &lt;lambda&gt; at 0x7f2a82f6ef28&gt; ] &lt;__main__.graph object at 0x7f2a81833550&gt; :[ &lt;function &lt;lambda&gt; at 0x7f2a82f6ef28&gt; ] ('this', 'is', 'a', 'tuple') :[ '[This is a string]'|12 ] False True One might argue that ints and strings and lambdas are "just pointers" but I think that's missing the point. You don't use them as pointers, you don't cast them or box them, you just shove "stuff" into your graph and that's it.
I'm not sure if you saw the footnote, but it is undecidable. This means that there isn't a single algorithm that can define a yes or no answer given some input. It will run infinitely. [Here is the article the author linked](http://www.sciencedirect.com/science/article/pii/S0168007298000475)
Hash tables that need to worry about attacker-controlled data are probably a small minority, so it's tempting to optimize more for performance by default. But I think tables that bottleneck the program (and specifically with time spent hashing) are *also* a small minority. The deciding factor isn't really which case is more common, but rather what tools are available in each case. Programmers worried about perf have profilers. Programmers worried about security have...nothing nearly so simple. Python is an interesting case study for this question. Every object is a hash table under the covers, but most of those don't have variable keys at all, much less attacker-controlled keys. That might make performance relatively more important than security than it would be in Rust. Nonetheless [Python switched to SipHash](https://www.python.org/dev/peps/pep-0456/) in version 3.4.
Good stuff. Looking forward to www.areweguiyet.com in the future ;)
In that case shouldn't you be talking about [Future::map](http://alexcrichton.com/futures-rs/futures/trait.Future.html#method.map) and [Future::and_then](http://alexcrichton.com/futures-rs/futures/trait.Future.html#method.and_then) instead of Result::map and Result::and_then?
I've updated the bug to indicate that patches are welcome to fix this.
For those interested, I'm writing up a summary of my and /u/staticassert's playrust classifier talk about rust data science. I'll post it on my blog as a accompanying resource to the eventual video in the next day or two.
Great post! A small correction though: you say in the post: "The and_then function can be given Ok(T) and return Ok(T) or Err(F)!" But and_then() is more flexible than that; it can return Ok(U), so you can map your Ok value to both different Ok type and different Err type :)
It was not streamed live, but was recorded. We can't commit to a timeline for them being available yet, but I will tweet and post here as soon as they go up.
I ran into some problems working on my functional programming library and couldn't find a lot of info on what I needed to solve it. This blog post steps through my learning process to help others understand and solve the problems I was having with (hopefully) less hassle than I ran into.
&gt; I mean, everything's a pointer of some kind. They are in Python (as far as I understand the Python interpreter, which isn't far), but they aren't in a language like C / C++ / Rust. When you insert a value into a `Vec` or a `HashMap`, it takes up that many bytes, which could be a different size than a pointer (larger *or* smaller). There's also no need to dereference to get to the value. For example, a Rust `Vec&lt;u8&gt;`: +----+ +-+ |ptr +-----&gt;+1| +----+ +-+ |len | |2| +----+ +-+ |cap | |3| +----+ +-+ And a Rust `Vec&lt;Box&lt;u8&gt;&gt;`: +----+ +----+ +-+ |ptr +-----&gt;+0x50+---------&gt;+1| +----+ +----+ +-+ |len | |0x10+----+ +----+ +----+ | +-+ |cap | |0x90+-+ +----&gt;+2| +----+ +----+ | +-+ | | +-+ +-------&gt;+3| +-+ A Rust `Vec&lt;Box&lt;Trait&gt;&gt;` would have bigger leaf nodes (IIRC 2x pointer size, one for the data and one for a VTable). I believe this is similar to a Python vector, but Pythong may require even more work at the leaves, not sure. &gt; You don't use them as pointers, you don't cast them or box them, you just shove "stuff" into your graph and that's it. Absolutely, and that's one of the great things about language like Python (or Ruby, which I have more experience with). The downside is that by making everything uniform, you lose out on the ability to do things specially. You also often add one layer of indirection, as the container holds a pointer to the real value. Many interpreted languages actually [use special tags](https://en.wikipedia.org/wiki/Tagged_pointer) to denote that a pointer isn't actually a pointer, but it's some (smaller) concrete value. This avoids the extra dereference but you still have to make some kind of switch on the tag. &gt; One might argue that ints and strings and lambdas are "just pointers" but I think that's missing the point. In a language like C / C++ / Rust, this distinction is important. "Accidentally" losing out on performance because something was autoboxed would go against the spirit of such languages. If you need that particular type of flexibility, you can opt into it by boxing up a trait object that can be dynamically decided at runtime. If you need a small amount of flexibility, you can use an enum. This would conceptually match the tagged pointer, but differs in implementation details. I believe it to be true that most of the time (let's say 80%), a collection contains one type. Other times (let's say 15%), it contains a small set of choices. The remaining cases can be handled, but probably not to the detriment of the first set.
&gt; Priority queues allows for insert/removal of arbitrary keys. So do binary heaps, you just have to pay O(n) to find the key in the removal case. Which, in many applications, is just fine. If you're e.g. doing A* it's the least common operation you're doing on your queue, the most prevalent being, unsurprisingly, find-min. And linear search is actually pretty fast on any architecture with a cache that isn't completely stupid as the cache logic starts shovelling data to the CPU before you even ask for it. In practice, thus, the number of cache misses is constant. Other data structures can do it in O(log n) memory accesses -- however, they're also going to cause that many cache misses. That alone can make the thing slower, not to mention the constant overhead you get. First rule of data structures: Never trust asymptotics you didn't falsify yourself :)
Sounds interesting! Would like to find out more but I never have time to be on irc. Reason for my interest is that I also have some thoughts on DSL that could be good for UIs, and also for robotics/industrial control. Those ideas have been baking in my head for many years but time for coding has been very limited. Inspired by what a success QML has been for Qt, I was thinking of doing something similar for rust (so, a reactive and declarative language). Main difference would be to make it a live programming environment running in a browser. Sorry to say but I don't see when I will find time to realize these ideas.. Would have been so much fun to form a team and make it happen.
I would make `uncons` have the signature `fn uncons&lt;A: Iterator&gt;(mut iter: A) -&gt; Option&lt;(A::Item, A)&gt;` instead of unpacking and repacking it into a collection each time, and let the caller choose how to generate the iterator. This means that infinite iterators can be `uncons`'d too, and also has `O(1)` complexity instead of `O(n)` or worse.
I didn't even consider that. I didn't consider that the complexity of an infinite iterator would be worse in this case. I'll have to update the package with this version. Thanks for pointing that out!
I didn't realize that collect actually forces the rest of the iterator. The idea being that it's supposed to return what it was before without the first item in the iterator. I though it just went back and didn't evaluate it but I guess I'm actually wrong and I'll have to tweak the implementation. Thanks for pointing out how where does work out a little bit better. Always nice to learn something new!
The fundamental attitude can't be fixed because it's a *feature*. Specific issues *can* be addressed, however. For example, we didn't used to have deref chaining (*i.e.* what allows `value_of_A.method()` to be called if `A` derefs to `B` and `B` has `method` defined). It was added because *holy cow* was it a pain in the backside not to have it. Right now, there's a big discussion about adding `?`, which is a shorthand for the `try!` macro that makes error handling easier. Some people don't like `?` because it's not explicit enough. So Rust *does* address verbosity to make life easier, it's just that the bar for how troublesome something has to be *before* it gets syntax support is generally much higher than in other languages. Again, for many of us, *this is a good thing.* Now that having been said, and having pondered on it for a few days, I think Rust *is* seriously lacking in composition; specifically, overridable mixins. I'm swinging toward it being something Rust *does* need to address, though probably *not* via inheritance.
[Where were you?](https://users.rust-lang.org/t/crates-for-functional-programming/7213). Now there are two.
All of the above! I knew that we'd dropped a lot of crates, so I jumped back in time 1 year on github to see exactly how many there were: bitflags = "*" clock_ticks = "0.0.6" elmesque = "0.5.1" json_io = "0.1.2" petgraph = "0.1.10" pistoncore-input = "0.5.0" piston2d-graphics = "0.6.0" num = "*" rand = "*" rustc-serialize = "*" vecmath = "0.1.1" - **bitflags** used to be used for tracking modifier keys I think, though now we get these types from the minimal **pistoncore-input** crate, so we don't need the dep ourselves. - **clock_ticks** was subsumed by **std::time** - **json_io** and **rustc-serialize** were used to support de-serializing the Theme type (acts as shared styling configuration), however I decided to abandon supporting this as it wasn't really any easier than writing the Theme constructor manually. I think a macro would probably be a better bet if it turns out that simplifying this becomes important. - **elmseque** and **piston-graphics** used to be required for graphics, but we've since added support for everything that we needed them for in a more conrod-friendly manner. - **rand** was solely used to generate random Colors! It's pretty easy import rand yourself and write `color::rgb(random(), random(), random())` so definitely overkill just to import it for a one-liner. - **petgraph** was replaced with **daggy** (a DAG wrapper around petgraph) - **vecmath** was only used for adding and subtracting 2d vectors, so I moved those functions into our utils module.
Which is exactly what I did in my other comment! :D
To clarify, the complexity for _all_ iterators would be `O(n)` for the version in the post, not just infinite ones. My point is that your function cannot be used with existing iterators without collecting them into a collection first, which adds even more overhead, since `Iterator&lt;Item=T&gt;` does not imply `FromIterator&lt;T&gt;` (and in most cases the iterator structs do not implement `FromIterator&lt;T&gt;`, `std::vec::IntoIter` does not for example). You cannot collect an infinite iterator, so they cannot be `uncons`'d using your implementation.
&gt; there is no blanket implementation `AsRef&lt;T&gt; for T`, which seems a bit silly. My understanding is that's by design. Despite signatures being identical, the semantics of `Borrow` and `AsRef` are different, and they have different blanket implementations (`T: Borrow&lt;T&gt;` and `&amp;T: AsRef&lt;U&gt; where T: AsRef&lt;U&gt;`) according to those semantics. See aturon's comment [here](https://github.com/rust-lang/rust/issues/24140#issuecomment-90626264). However I don't fully understand the coherence issues to which he alludes and would appreciate any clarification on that front.
Submissions will close on the 20th of the month (I just decided this today based on time I will need to edit the video, interview people and such). I will PM you over the coming days to let you know if you have been chosen.
Features are *additive*, so `std` support should be a feature (that should also probably be a default). If `slog-rs` is depended on by two crates, think about what happens if they ask for different features. If you use a `no_std` feature, and only *one* of them asks for it, the other will find support for `std` suddenly disappear! If you use a `std` feature, and only *one* of them asks for it, the other will... not care, because nothing it depends on will disappear.
&gt; A note about "syntactic sugar": the `where` clause is almost, but note quite, syntactic sugar. It also should be noted that [as per the RFC](https://github.com/rust-lang/rfcs/blob/master/text/0135-where.md#associated-types), where clauses should also have the ability to express type equality constraints (eg. `T == U`), but that feature has not been implemented yet. That makes working with constraints a little awkward still :(
I'm working on a little experiment with machine learning boolean satisfiability problems. I'm trying to predict accurately if a boolean formula is satisfiable or not, and it's surprisingly going well! The hardest part is getting large datasets, I'm using a Satzilla competition, but there's no easy way to get their data besides hand copying it. On the plus side, I've been using /u/SleepyCoder123's rusty-machine library and it's been super easy and fun to work with!
`Fira Mono` is gorgeous and `Fira Code`with the ligatures has really grown on me. I'm surprised that https://docs.rs doesn't use either one and jumps straight to SCP. For those that are curious, here's https://docs.rs with `Fira Code` on the left and `Source Code Pro` on the right: http://i.imgur.com/36B4W9t.png
Not a long time ago I've got almost the same question. You can read discussion [here](https://www.reddit.com/r/rust/comments/50znfn/why_optional_dependency_gets_compiled_with/). Short version: use `use-std` feature and enable it by default, so your `Cargo.toml` will look something like that: [dependencies] some_crate = {version = "*", default-features = false} [features] default = ["use-std"] use-std = ["some_crate/use-std"]
But why replacing `try!`?
&gt; You faked the hash function. There is nothing fake about the hash function. You can easily verify this for yourself. Pick any hash function and use it in all languages and you always get the same result: Rust is no faster. There is one single exception which is Veedrac's "contiguous" hash function and it only works on this specific input graph. So tell me again which hash function is "fake"? I've given you all the code you need to verify it for yourself. You can easily verify this. It doesn't make sense to keep claiming something is "fake" when the experiment is objectively reproducible. 
Nice! Are we talking amortized performance here? How does the latency compare? (One of the reasons rust intrigues me is the potential ability to write video games that won't crash except during calls to things like flakey graphics drivers... But nothing I can be blamed for myself! ;)
I think it was a design decision, because joysticks and gamepads could only be polled and they didn't want to separate them out of the normal events.
My poor Firefox, so busy rendering it takes 10 second to even close the tab :-)
In your examples in which you use a pair of consecutive `map` and `map_err` like let desired = given .map(|n| { n as usize }) .map_err(|_e| { "bad MyError" }); a good old `match` seems like the more convenient and readable approach: let desired = match given { Ok(n) =&gt; Ok(n as usize), Err(_) =&gt; Err("bad MyError"), }; Just saying.
I don't think you need a trait for that, just don't declare `munge` `pub`.
Just to confirm—do features work that way, that a feature spec is defined as “at least these features”? Could this cause issues with mutually exclusive features (e.g. where two features define different implementations of the same function)?
I can't find any documentation that makes it explicit, but I do believe that is how they work. The actual set of features used for a crate is the union of all the requested features by its dependents.
For me it is not an issue if we achieve some way to work with `try!` better.
Yes, it will cause issues with mutually exclusive features. https://github.com/rust-lang/cargo/issues/2980
I'm not a rayon user, but having read about it I thought `par_iter` was a naive parallel implementation, so this change would make it work the way I would have expected. I'm not a fan of the name `sequential_threshold` but I can't think of anything better.
oh supernatural, definitely :)
Hardcore Java dev here. Two of your choices - Rust, and Lua, are polar opposites. I have to say, if you just want to "do stuff with collections" without caring about a whole range of programming concepts which you allude to not needing, the Lua route will be dramatically simpler. It's elegant, minimalist, cute. You can learn all there is to know about Lua in a week. I personally really like Lua. Rust is a solid step in the direction of something like Haskell, where a whole range of concepts - especially aroung the type system - are required understanding if you are to become proficient. If you're bored of Java because you're looking for a higher-fidelity type system, and a language with which you can express very complex computations elegantly while retaining enormous control over resource usage, Rust is for you. But Rust is dramatically more complex than Java, in my experience. If you're bored of Java because you want to do stuff more simply, without all the boilerplate and complexity, Rust will "bore" (scare?) you even quicker. In that case, I'd use Lua + a good game engine. Rust - and inevitably Haskell, from which it draws some inspiration in terms of type system etc - are life journeys that enrich your abilities and outlook on programming. Good luck!
``collect`` is used to get all elements of an iterator into a concrete data structure (which is O(N)). In this case, you may prefer to return an iterator just without the first element (as you don't need such a structure in this case). In Rust, asking iterator to provide an element mutates it to skip that element. So, it may be enough to just return an iterator after getting its first element. fn uncons&lt;T&gt;(collection: T) -&gt; Option&lt;(T::Item, T::IntoIter)&gt; where T: IntoIterator { let mut iterator = collection.into_iter(); iterator.next().map(|first| (first, iterator)) }
Yes, it means that crate features are not designed to support mutually exclusive things.
A future looks very much like arbitrary slots and signals. You could do something like. ``` mouse.click().and_then(|x, y| do-something-with-coords();); ``` What tokio does for futures is it "pumps" the io event loops, so that any polling based IO can complete the futures. I reckon that a GUI would probably want to perform some evented IO. In addition, you'd need an additional "pump" to handle windows and OS events for each platform. I'm not sure, but you might be able to unify these somehow, although I'm not sure if you'd need to. 
I'd recommend trying Scala first, to get used to some higher-level functional programming concepts (but without sacrificing all those libraries you know and love, or OOP) and then jump to Rust from there. It will ease the transition. I've been coding in Java for 20 years and am now using Rust full time but only for a month or so. The learning curve is steep IMHO but worth it if you want to make the jump to an elegant systems level language.
Would you happen to know where it states that collect would be O(n) at all? I haven't found it in the docs at all. I'm guessing I'd have to look at the source? Also thanks for the clarification!
I don't think that's something you can fix appropriately in a binding... This is what happens in original SDL: https://forums.libsdl.org/viewtopic.php?t=10957&amp;highlight=waitevent The joystick thing is more of a guess, since SFML goes the same way: https://github.com/SFML/SFML/blob/master/src/SFML/Window/WindowImpl.cpp#L125
These would be great to hear and see recorded!
You surely mean r/playrust? This subreddit is about a programming language ;)
Hmm, I think you're on to something. If I give someone &amp;Self, then noone will be able to destroy me for the lifetime of the borrow...
&gt; But Rust is dramatically more complex than Java, in my experience. Java generics are quite complex. I don't think the difference in complexity is so large, even with the need to learn the borrowck. &gt; are life journeys that enrich your abilities and outlook on programming. That sounds nice for some programmers, while for (most?) other programmers that sounds scary or off-putting. So better be careful with similar comments.
I'd like to get better at functional programming with Haskell style but cant any lang do that. A function that creates objects performs algorithms on them and return something or nothing. Or is it more 1 liner magic?
I think it "goes without saying" -- how can you collect N things without looking at each of them? It must be at _least_ O(n).
I have a guestion pertaining to Diesel. Can Diesel change its schema at runtime? Say you `infer_schema!` and your table changes (either outside or inside the program). Can you alter your existing schema by calling `infer_schema!` again?
&gt; I also feel like this isn't an inheritance problem. If you called the HTML implementation in C++, it wouldn't have any reason to back out to the EPub implementation; exactly the same thing we're seeing here. I haven't touched C++ in ages, but I'm pretty sure that in e.g. Java, if `EpubRenderer` inherits `HtmlRenderer` I could do something like `super.render_token(...)` and it would then call the Epub implementation when calling virtual methods. (E.g., a class implementing `AbstractCollection` can call the `addAll` implementation which will then call the appopriate `add` virtual method). As for code duplication, I see your point, but I am not really convinced. In my mind, the Epub and HTML renderers are part of the same project and should mostly look the same. I guess this is a bit controversial for Epub, but I have the same problem for multiple-files VS single-file HTML, and in that case it would really feel weird if the two renderers gave drastically different layouts.
And the use of traits seems superfluous here. Pure functional programming seems to do well here: https://is.gd/YzFsMi (this can be compressed even more by moving the `format!`s into `render_vec()` edit: fix link
I guess I was secretly hoping that the implementation would be a smart thing with memory with the iterator. It knows that it starts and ends an certain memory addresses (I would guess that it would be contiguous with all of the items together) so if it went over by one then it can just say create a new structure that aliases to the new start and the old end without having to reevaluate each thing. This is of course assuming a lot about the internals, so assuming O(n) is the correct thing to do.
No no, `.clone()` is the answer ;)
Great talks
&gt; Instead, the /u/mitchmindtree would waste months trying to get all the same backends working directly with conrod... Woah there :) I’ve obviously considered this having depended on piston-graphics for a while, and I’d be more than happy to steer these changes to the piston-graphics crate if it turns out to be feasible, practical and what everyone wants. It would make my life easier. However, my understanding is that what I want for conrod is a little orthogonal to piston-graphics’ design direction. What I'm proposing in conrod is to provide a few totally optional, feature-gated functions that help to fill vertex buffers and cache text in textures in an efficient manner that is specific to each back-end, given some conrod primitive widgets. These functions should be able to slip right in next to existing rendering code that is idiomatic to that back-end. I'm not attempting to abstract the 2D drawing process with traits that attempt to enable a unified, generic drawing API between each graphics back-end, which I think is the goal for piston-graphics. I want it to be easy for existing gfx and glium users to drop conrod into their existing projects - hence the simple 2-step approach mentioned in the post. I don’t want to require that those users wrap their whole window and graphics backends in new types or to have to switch their entire application loops over to piston’s just to use conrod. You make it out as though I want to re-implement 90% of an API that already exists, however from my perspective it seems more like it would be at least as much work attempting to overhaul piston-graphics just to suit conrod’s needs, which might not match those of the pistoners who are already using it. Do you kind of see where I’m coming from? /u/long_void please correct me if I’m wrong on any of this btw - it’s been a while since we’ve discussed this!
That's a good point, it'd be great to clarify that. I think a lot of the documentation could do with specifying that it does or does not allocate, plus give the complexity for any method that you can reasonably calculate it for. Basically, the `fn from_iter` in `FromIterator&lt;T&gt;` is parameterised to work with _any_ `I: IntoIterator&lt;Item=T&gt;`. The collection can't know that it's being collected from another instance of itself and apply some fancy optimisation, so it must iterate through the argument (since the only way to extract values out of an `I: IntoIterator&lt;Item=T&gt;` is to move each value out of the collection with `.into_iter()`) and cons each one to the collection in turn. Besides, even if it could know what collection was being iterated over Rust's collections aren't immutable so it would have to create a clone anyway (which has `O(n)` complexity).
Thanks! As I hope I made clear in the post - I love using cargo profiler as it makes things so much easier! There's definitely more I'd like to see from it - like being able to use [callgrind_annotate](http://linux.die.net/man/1/callgrind_annotate) to highlight hot lines in the source code. I had a quick look through some of the source and it does look fairly easy to add more tools. Maybe I'll try to play around with it a little soon.
My issue with `std::collections` is that they make it very difficult to implement a custom sort for the type. You are better off creating a *different* type and implementing your own comparators and equality functions - which is an awful lot of boiler plate to get through just for treating your strings a little different to the default. What if you don't want a completely different type? The best part of C++ was its collections. And it made sure, from day one, that custom sorts were made available. I think Rust should, too.
nice maymay. but ive banned u for r 3, u mad?
makes me sad to be on windows. :-(
Gracias, le echaré una leida
Agreed, comparators would be nice. Comparators are also more powerful. For example, I might want to sort a BTreeMap by distance to some number x determined at runtime. Edit: I guess this is a bad example, as you can just compute the distance before. Anyway, my point is that your sorting method might rely on some runtime value.
Is it kind of too late for you to go back to piston-graphics at this point? How far are you invested in the current affair? &gt; What I'm proposing in conrod is to provide a few totally optional, feature-gated functions Could you point to these function signatures, just to get a feel of it? &gt; I'm not attempting to abstract the 2D drawing process with traits that attempt to enable a unified, generic drawing API between each graphics back-end, which I think is the goal for piston-graphics. I can see a wide range of graphics features that Conrod needs: different 2D shapes, colors, scissor/blending modes, maybe some textures. Pretty close to "generic drawing API". &gt; I don’t want to require that those users wrap their whole window and graphics backends in new types or to have to switch their entire application loops over to piston’s just to use conrod. I'm not talking about the application loops or even windows. I'm talking about the graphics backends. Even if one needs to wrap a Glium context within piston-graphics, this is only a minor inconvenience if the latter exposes the underlying context in public (while retaining safety). GFX backend does it, for example, allowing you to mix piston-graphics with manual GFX calls. &gt; however from my perspective it seems more like it would be at least as much work attempting to overhaul piston-graphics just to suit conrod’s needs This may very well be true. But extending piston-graphics would have a much greater community benefit in the long run.
I believe you ought to take a closer look at Rule 3, my friend.
About the SipHash: I don't understand how one can oppose it being a default due to "performance". Security *must* by the default. Performance is irrelevant. People routinely write their stacks in Python, and live happily ever after with 10% of performance that they would get with lower-level language. **Computation power increases every year, while security degrades because new methods of breaking cryptography and rise in computation power.** So you must start with as strong cryptography as you can, as in 10 years, your hashmap will be 10 times faster, but 10 times easier to abuse. It's very easy to use different hasher, when you are sure, you don't need secure hashmap. But the default *must* be secure. We're building software that might be powering things in the next century: IoT, self-driving cars, medicine appliances, etc. We can't systematically risk people lives, just because someone forgotten to change hashing function to cryptographically secure one.
I'm curious, how Btree cache locality can be improved?
To be fair, even Rust makes _some_ tradeoffs. Panic on integer overflow is arguably more secure/correct than wrapping, and we do it in debug mode, but we don't in release because it's too expensive.
Not sure if always 'mernes' but buzzword stack seems plausible...
Not really, rounding allocations up doesn't free you from being able to find the usable size from the pointer. Any general purpose allocator (including jemalloc, tcmalloc, ptmalloc, etc) have to store it somehow. Obviously, they use various techniques to avoid paying the 8 byte for every allocation (stuff like keeping N segments for each of the smaller sizes, and doing a lookup in a LLRB tree to find which size class it is), but ultimatively you cannot remove the burden without changing the API. The way I represent is really only for bigger buffers.
SLOBs (see my previous post).
Thanks Manish! I have edited the article.
Just come to the Darmstadt Rust meetup on Friday. Llogiq is the organizer.
I think we agree, in that your comment seems to mostly be a rephrasing of mine. &gt; rounding allocations up doesn't free you from being able to find the usable size from the pointer Rounding up is a stepping stone: it makes it feasible to have separate slabs for each possible (small) allocation size, since it reduces the number of possibilities to 20 (or something of that order), rather than thousands. As the rest of my comment describes, this allows cutting how much metadata is stored from 8 bytes per allocation to 8 bytes per thousand allocations or more. This is a *massive* improvement, and ignoring/dismissing it gives an extremely misleading view of allocators. But as I said, the new API definitely does allow cutting out even that small overhead. &gt; The way I represent is really only for bigger buffers. Thinking about the overhead is only relevant for small allocations: 8 bytes out of 1 MB is pretty trivial, the complete other end of the spectrum to the 2x the article suggests. (The main downside to storing it inline for these sort of allocations is shifting the start of the allocation from probably starting at the start of a page to being only 8-byte aligned, which is why I believe allocators generally still store the metadata out of line for huge allocations.)
No. Declaring that performance doesn't matter and that security is immutable is how we ended up with C and C++ in their dominant positions. Every single attempted replacement over the last 20 years ignored performance (and control) and lacked any credibility. Performance is paramount, and if users are complaining about the performance of a safe default, we need to take that seriously. But that's the reason to use siphash (1-3): its performance is very good. It's not the best in all cases though, and bridging that gap matters.
You're generally right on the purpose. However the `Borrow` trait itself doesn't ensure `Eq` and `Hash` and consistent between owned and borrowed, but the collections API _assumes_ and relies on that consistency. There are probably some interesting things you can do by intentionally violating the contract. I think a simple macro would reduce the boilerplate. Something like this untested one: macro_rules! asref_self { ($ty:ty) =&gt; { impl AsRef&lt;$ty&gt; for $ty { fn as_ref(&amp;self) -&gt; &amp;$ty { self } } } } &gt; it could conflict with the other blanket implementations for AsRef Oh. Duh. Actually seeing it written out it makes total sense. You'd have two implementations for `&amp;T`.
&gt;There is nothing fake about the hash function. You can easily verify this for yourself. I don't have any responsibility to verify anything for myself, and you *obviously* faked the hash function, or my original post would never have gotten recognition in the first place. You're just trying to prove something meaningless to someone who doesn't care.
&gt; Ah, you're right there, but note that "huge allocator" of jemalloc isn't based around that approach, and regardless, making use of a fixed set of sizes introduces internal fragmentation (i.e. you're still going to pay). The benefits of not having a large metadata overhead for small allocations, and generally packing things in tighter in memory, will vastly outweight the downsides of having a few extra "unnecessary" gaps for some allocations, for almost all use cases. Also, in practice, there's a strong bias toward allocations with power-of-two sizes (or a small multiple of that), so I imagine there's less space wasted than one might naively expect. (Other benefits of the mmap'ed slab approach of jemalloc come from facts like: typically the nodes in a tree are the same size, and so they may end up adjacent or otherwise close, helping to minimise the pointer jumping cost without making the scarifice of having an explicit arena/needing to write another memory manager.) &gt; note that "huge allocator" of jemalloc isn't based around that approach Indeed, huge allocations are large enough to just be considered in a "slab" by themselves in terms of reducing overhead, and jemalloc keeps this in mind. (My comment also mentions this case.) &gt; How does it suggest that? You wrote: &gt; say you allocate 8 bytes, then you get a 2x memory burden without acknowledging that small allocations do not actually behave like that in practice (at least, not for most platforms, maybe glibc uses the poor scheme?), nor giving a strong indication that the overhead is irrelevant for large ones.
Great review! I agree that Rust Conf was really well organized, and the talks were fantastic. I do have a couple of corrections for the post though: - Safe Rust guarantees no _data races_, but does not guarantee a lack of race conditions. - The idea of zero-cost abstractions isn't tied to calling C code. It's a general focus of Rust's design that ergonomic APIs like iterators compile down to the best assembly possible.
Hopefully! The first question is to identify what problems are difficult to solve in Rust (I am specifically saying "problems" not "tentative solutions"), and then see which additions/modifications to the language or libraries would solve most of those problems at once. It is not a trivial task, as you may imagine.
&gt; Assessing whether this is a good idea or not would need benchmarking with real workloads. Well, on the one hand it's good to have a fast standard library, on the other it's better to have a reliable standard library. By that I mean that rather to try and optimize for some workloads (at the detriment of others), I would rather have an insurance that no workload has abysmal performance. A bounded worst case is what I am looking for in the standard library. So, rather than benchmarking with real workloads, I would rather see benchmarking with synthetic workloads, and try to minimize the worst case.
Good points. The zero-cost abstractions when calling C were a particular benefit to me, but you're right. I have updated the post to address both points. Thanks!
I like how the trojan post is part of project updates. I can see it now, "Hey guys, I'm working on a trojan and I'm running into lifetime issues, can anyone help?" :P
I would actually propose NOT having any weight/threshold. As Amanieu mentioned, an adaptative strategy is one solution. Another would be to select items depending on how much are remaining. I am not sure it is easy to pull off, but the basic idea is simply that if there's a lot of items there, there is little chance that selecting nb items/N/2 per thread would have one thread prevent the completion: others would have to process 2x as much. This means that if the parallel iterators could start with "large" batches and only reduce the batch size toward the end (to maximize parallelism), then the user should not have to tweak any knob which is ideal.
You'd need to pick a ratio of lookups vs. insertions and removals. Not a trivial choice to make, but coming up with a synthetic work load would be quite easy. My gut instinct says storing 64 bits of hash because it has predictable performance and a bounded worst case. No sudden perf drop when you get to 64k + 1 elements. The drop might not be that big, though. There's not a lot to gain by leaving out a few bits.
I found this video the other night while looking to learn more about the LLVM IR. I thought it was a be great introduction to what LLVM does to optimize our code. I know that there has been a lot of talk about introducing optimizations now that the MIR is starting to mature, and there is some good dicussions taking place for establishing a well-defined memory model for Rust, so I felt that some others might find it interesting to see the kind of work that the LLVM will do to optimize our code as well.
I'm not a Haskell user but it's approach to functional programming is a lot more elaborate than simply using functions to process data. It has an extensive type system that ensures things like "correctness". Certain programming languages like Javascript can be made to behave in a kind of functional way but Haskell is built from the ground up to embody certain classic academic functional programming concepts.
Not on any rust docs site but https://play.rust-lang.org has a similar issue for me. Also on the ampersand. http://imgur.com/a/8tIel
Very cool, and thanks for the contributions you made to termion. Other Rust text editors which you might want to steal ideas from are [sodium (disclaimer: I'm the author)](https://github.com/redox-os/sodium) or gchp's [iota](https://github.com/gchp/iota). Looks very promising so far, especially the fact that it runs in the terminal.
So rust does wrap in release mode (by default), right?
Do you consider switching to another window backend? I think Glutin has support for waiting for events.
The [Vec extend_from_slice and extend_from_element](https://github.com/rust-lang/rust/pull/36355) performance de-regressions were great to finally get in, too.
More of theoretical inquiry.
Actually the problem is that it's difficult to "split" the renderer trait in two, since "render_token" will use "render_vec" and vice-versa. But looking into this made me eventually find a solution, so thanks anyway ^^
Glad to hear it!
I would just jump into it. I'm primarily a Java and JavaScript dev, but I don't regret any time I have spent with Rust at all. It is definitely challenging, but the payoff is huge. You will get frustrated and you won't be able to go as fast, but the concepts in Rust and getting introduced into it are pretty advanced and mind blowing and that is the reward of sticking with it.
Performance matters, and I think you need to always expose a way of "putting the pedal to the metal" and throwing security out the window. But I agree with the parent post that being reckless should generally be a choice, not a default. There are few exceptions. Things that are performance drags and easily become insidious, such as integer operations failing/panicking when under/overflow happens would not be a great thing. In this case it means that if you have a HashMap whose hashfunction is performance sensitive, you can always have it use a different [BuildHasher](https://doc.rust-lang.org/std/hash/trait.BuildHasher.html) that creates a more performant Hasher. With that said, I think that the proposal in OP's article is very reasonable. Just because you are safe doesn't mean you couldn't be faster.
How soon will new errors be accepted into stable Rust? And how long before JSON errors will be stabilised? Can one hope for JSON errors to be stabilised earlier or, at least, as soon as the new errors are?
Doesn't interior mutability with Cell/RefCell/Arc/etc make this a hazard? e.g.: if your distance function is computed from the struct's fields or a trait, it could be nondeterministic, nonidempotent, or altered by any side-effecting function with a mutable reference to those fields. I'm not sure this is a problem Rust can solve (at present). If Rust had a trait for immutable structs and/or annotations for pure functions, enforced at compile time, it seems solveable. e.g.: Functions involving interior mutability are `impure`, comparator must be `pure`, and so the comparator cannot perform `foo.borrow()` (for `foo : RefCell&lt;T&gt;`) or `*bar` (for `bar : Rc&lt;T&gt;`).
 struct HaHa(i32); impl PartialOrd for HaHa { fn partial_cmp(&amp;self, other: &amp;HaHa) -&gt; Option&lt;Ordering&gt; { if thread_rng().next_u32() % 2 == 0 { Some(Ordering::Less) } else { Some(Ordering::Greater) } } } You can already screw up trees.
MOD ABUSE!
&gt; That's what many other languages do behind the scenes, though I was hoping there was a more flexible way to do this than my solution while still keeping the efficiency :(. I guess the performance impact seems bigger than it actually is.
We == the programming language development community. Everyone needs to take this problem seriously.
&gt; But I agree with the parent post that being reckless should generally be a choice, not a default. The point is that being reckless shouldn't be necessary in a well-designed system. Yes there are compromises but this isn't the stance we should take by default. When we see a hard problem we should try to actually solve it.
Purity doesn't solve this problem. I can define partial_cmp as something nonsensical but otherwise pure that behaves just as poorly as rand() (e.g. `return false` will probably break everything too).
Sometimes there's a value to being reckless. For example doing an operation on a video game that causes a minor glitch to happen once every very-large-number frames but reduces the time to render a frame by 5% it certainly would make sense to "screw up every so much". Again though, the cases when this happen are rare, and should be opt-in.
This was a major theme of one of the talks at RustConf. We've had some iterations on the RFC process, and it's certainly not perfect, but I'm glad several people have been having a good time with it, even though it's their first time participating.
Your comment on `mem::transmute` made me wonder if we were using it at all in Diesel. We were and now we're not. Thanks!
`mem::discriminant`: the never-ending FCP.
I wouldn't say it is dramatically harder either as a professional java dev who rusts by night. I think some of the biggest hurdles to get over would be lifetimes and familiarity with stack/heap/pointer concepts.
&gt; only 1 byte of the tag space will be used, the other 3 bytes are most probably used as a padding to meet the alignment restriction for the 32 bit integer Red/Green/Blue data field which comes next I would think this as well, but if you add an additional byte in there, the size will jump up to 12.
&gt; Can you create such code dealing with two non-mathematics custom types, where each type stores data in a different format but both use the same function header? That's the whole point. &gt; Also, I don't know why you're posting Haskell code when we're talking about Rust. Because I wanted to exemplify the difference between verbosity and simplicity, if you want the rust version check https://doc.rust-lang.org/std/ops/trait.Add.html, rust as one should remember is pretty low level, so you can't expect them same level of grace. However since the example presented was in java, a language meant to have a high abstraction I think it was more fair to pit it against haskell.
The Visual Studio profiler works pretty well if you are using the msvc toolchain.
"Why I’m dropping Rust" was so meh and it got into TWiR? Rdedup never got to TWiR, while it is way cooler than a guy that just couldn't force Rust to be Java. :P And I recently decided to [release 1.0.0 of rdedup](https://crates.io/crates/rdedup). hint, hint
Previous post, http://ticki.github.io/blog/horrible/ &gt; SLOBs (aka. pointer lists, memory pools, typed arenas, etc.) * https://en.wikipedia.org/wiki/SLUB_(software) * https://en.wikipedia.org/wiki/SLOB * SLUB https://lwn.net/Articles/229984/ * http://events.linuxfoundation.org/sites/events/files/slides/slaballocators.pdf * Hotspot "Hierarchical PLABs, CLABs, TLABs in Hotspot" on http://cs.uni-salzburg.at/~hpayer/ The next big win for Rust will be fine grain control over the allocator(s) used in a program. After that, it would be pretty easy to add profile guided allocator hints. Someday almost all allocations will be a `bump` https://github.com/rust-lang/rfcs/pull/1398
What if the function returned an Option/Result instead? With the error case being when the argument does not have a discriminant.
&gt; you obviously faked the hash function, or my original post would never have gotten recognition in the first place Your post got recognition because the Rust community set out to draw the conclusion that my results were invalid because they believe that Rust is blazingly fast despite any evidence to the contrary. There is nothing fake about the hash functions I used. As I've said, you can easily verify the results for yourself so it makes no sense to claim that I faked anything. 
Anyone else recently notice cargo recompiling even when there are no changes?
It can be very subtle too. In a code review at work, I came across a comparison function with the following logic: 1. Compare by field 1, then field 2. 2. Field 1 is nullable, so if either is null, compare by field 2. x = ('a', 2) y = (null, 1) z = ('b', 0) x &gt; y y &gt; z z &gt; x Unless you actually have a crazy language like Idris which lets you write and enforce proofs about invariants like transitivity, you will have potential programmer errors that have to be checked by hand.
have readed the report. Reviewing code users means the users comment on the PR? Am I right? 
It also eliminates the ability to perform multiple different sorts over the data. Imagine I have a list of every opaque instance the graphic engine will render this frame. I sort it in a preorder traversal of the scenegraph so that when computing model-to-world matrices I can traverse linearly over the instances and only have to compose my parent's model-to-world matrix with my own local transform, improving cache locality. I then sort it from front to back in view-space to take advantage of early Z-testing, improving my rendering performance. If I want to do this in-place to avoid the time/memory overhead of draining the vector into a map into a second wrapper type and collecting again (and perhaps hoping the compiler is ridiculously smart and can optimize that down to a no-op), as far as I can tell, my best approach is... a mutable static variable holding a reference to the active comparator function, and using dynamic dispatch inside the wrapper type's comparator function? That feels awfully gross. `map_in_place` doesn't exist anymore, so that's out. Perhaps I can delve into the world of the `unsafe` and just cast my Vec&lt;A&gt; into a Vec&lt;B&gt;, and hope I'm not calling up that which I cannot put back down. (It's a poor example, because usually the scenegraph preorder is saved between frames and only altered when parent-child relationships are altered, while z-ordering is recomputed every frame, so you likely wouldn't do this anyway, and the two lists won't even contain the same data... but my example isn't *entirely* contrived...)
I believe that would be the metric used considering the new code review tool only rolled out today. I think it's a good metric of community engagement even if the language itself isn't as big as Go or Swift based off the other metrics shown.
Something has broken... all I get when clicking the link is a Wordpress php fragment.
Methods aren't exported; they're either public or private, and I need something in between. What would make this a lot simpler is to just place MyStruct directly in mod foo and make munge private. The problem is that I want to put it in a separate file so I don't end up with a single file that's thousands of lines long.
If I don't use a trait and munge is not public, then it's not visible directly inside foo.
Thank you! I hadn't thought about usize, and the use of "offset" is new information for me. Will update the article to include these points.
Haskell doesn't have the boilerplate of rust though, many times the haskell solution is dramatically more concise than java or even ruby/python. But in rust....blergh. Welcome to boiler plate city.
[Wrong](http://imgur.com/gw3cMin.png) [subreddit](http://imgur.com/xCPcX1e.png). /r/playrust
"you have chosen to open: which is: application/x-httpd-php (418 bytes) from: http://www.agildata.com" ??
Should tokio really have been featured this week? I am sure it will be featured again when the whole suite is ready to go.
I don't think that's a battle worth fighting. There's a reason Rust doesn't include a full theorem prover. Besides, it's useful to use impure sorts sometimes.
Vectors have sort_by_key. I guess the real problem is BTree.
This blog post proved to be too popular ... we should be back up soon
I've now also added a link to [responses from Rust community](https://www.reddit.com/r/rust/comments/5295nf/why_im_dropping_rust/) for balance. I'll get rdedup mentioned in next week's issue. And if you ever find something that should be included in TWiR but is missing, you are always welcome to submit a pull request. :-)
I once discovered a remote code execution vulnerability in a prominent Java application that ultimately stemmed from integer overflow in Java code. Specifically, it was serializing a 16 bit integer into a binary file without checking that the value being written was in bounds, leading to the file later being parsed in an unintentional manner. Unfortunately, I don't think Rust would help in that case, since `as u16` doesn't check overflow AFAIK. The equivalent Python code (struct.pack) would have thrown though. 
I had a lot of success using perf to profile my rust code.
Bit of a tangent, but having data power of 2 aligned isn't necessarily a good thing. It makes it easy to accidentally get high cache contention, since accesses separated by a large power of two will all be competing for the same few slots.
[MEGA BANNED](https://i.ytimg.com/vi/hcIzcMxRzFg/hqdefault.jpg)
Small nit, but the syntax highlight colors used in the blog post are a little difficult to see at times. I had to highlight the `...` in `gl.VertexAttribPointer(...);` in the first code sample to see what was inside the parenthesis. My pedantry aside, great article! Its really nice to have stuff like this explained as approaching a large codebase in a foreign domain is pretty intimidating and this really helps demystify it.
Someone else mentioned this to me too. I was using `unwrap_or_else` for this, but there are use cases for keeping the value wrapped in `Option` or `Result`. Thanks!
This rather pompously titled thing: https://www.packtpub.com/application-development/mastering-rust Just finished a draft of chapter 7, on concurrency. 5 or 6 to go. Tackling macros next. Independently of this, am thinking of writing a log stream+cache server based on cap'n'pnp, to be used as a basis for any CQRS-system. The buzzword ratio should be off the charts! 
To be quite honest, I was already considering it, but all the more reason to use it.
That's a great example. Perhaps this should be posted more widely as it is applicable to any language with comparators.
I agree that Scala is a nice language for Java devs who want to try out something new, especially since you will get decent results much faster than with Rust. You won't have to fight the borrow checker and you can use the eco-system you have gotten used to. One thing to mention though is that the scala compiler is really slow, so if you find C++ compilation too slow, you won't be very happy with scalac compile times.
I was not aware of that ... thank you!
Ah! Well, still cool regardless :)
So what's the rusty way of doing this? Using a tuple-struct like this: https://play.rust-lang.org/?gist=460824f1c6699e46401f3e12eb3ee7a4&amp;version=stable&amp;backtrace=0
JSON errors are stabilised with the next release (1.12).
Interesting. What post processing is done?
Did you find any good tool for symbol demangling? I sometimes end up with method entries like _$LT$rdxsort..tree..NodeInner$LT$T$GT$$GT$::insert::hfb945d4639d2f452 On the other hand, for some methods I get perfect results: test::run_test::h176e832712cd4328 (the trailing ID is not garbage, it is the SVH or something related)
You already can, in the form of `Rc` and its associated `Weak` pointer type. You could also do it with arena allocations (theoretically). The problem is that you can't do it without overhead or inconvenience of *some kind*, and I don't believe that can be worked around without `unsafe`.
I've definitely heard a great deal of praise from other language communities.
It *has* to, but that's not really relevant. Depending on `unsafe` code is absolutely unavoidable; it's whether you have to write it *yourself* that matters.
Thanks for expanding on it :)
All right, we can regard `Rc` as part of the language itself. In my mind, the question then becomes whether `Rc` and the rest of the language is enough to enable you to implement all the other things young might want to do (like the cyclic structures you mentioned), analogous to how a test-and-set operation in the hardware enables you to implement other kinds of thread-safe behaviour.
I hope they only do this in examples if it's an example of how they want the library to be used. The common thing to do is of course to define crate::prelude so that the list of things imported is easier to overview. One remedy is to use the jump to definition command that racer implements, that works fine internally and across crates.
This is particularly important when it is something that relies on trait imports.
The compiler can combine lifetimes by taking their intersection. That is, if you have `'a` and `'b`, then there is an intersection `'c` of `'a` and `'b`, during which both lifetimes are 'alive'. I believe it is always the case that this intersection is equal to the shortest of `'a` and `'b`, because of the way scoping works, but perhaps I am mistaken. In practice, this means that, when you see a `fn&lt;'a&gt;(x: &amp;'a T, y: &amp;'a U) -&gt; &amp;'a V`, you can put in a `&amp;'static T` and a `&amp;'b U`, and you will get a `&amp;'b V`, because the intersection of `'static` and `'b` is `'b`. So, why does your method cause the compiler to complain? Because to the compiler it kinda looks like this (it's not valid syntax): fn extract_bar2&lt;'a&gt;(foo: &amp;'a Foo) -&gt; &amp;'a Foo { 'b: { let foo1 = Foo { f: 22 }; 'c: { // The next line is wrong let foo2: &amp;'a Foo = make_bar&lt;'a&gt;(&amp;'a foo, &amp;'b foo1).bar1; 'd: { return foo2; } } } } I've made the scopes more explicit. What happens? The compiler knows that `foo2` must have type `&amp;'a Foo`, because that is what the function returns. So the `Bar` returned by `make_bar` must have had lifetime `'a`: we couldn't get a `&amp;'a Foo` out of it otherwise. So we must have called `make_bar&lt;'a&gt;`. But one of the arguments is wrong! `foo2` does *not* have lifetime `'a`, it has lifetime `'b`, which is outlived by `'a`. So the compiler complains. When you use your second definition of `Bar`, the code will work, because `bar1` and `bar2` need not have the same lifetimes in that case. So your second `Bar` definition is strictly more flexible, but in practice you rarely need that extra flexibility, and the extra lifetime annotations are annoying.
Depends on whether we're talking about pure rendering or game state updates. Sure, in the renderer that's no biggie. But in the update, you just increased your chance of having follow-up bugs that take decades to trace by magnitudes. (and yes I noticed that you said "time to render a frame". That, however, generally *does* include the update pass as it doesn't make much sense to render the same game state twice)
Good luck team bike shed painters!
In the last week I started using LLVM in my [DBKit](https://github.com/mtanski/dbkit) engine project. My goal is to generate host optimized code for a lot of the execution. I'm starting with Expressions (what you feed into the Filter or Compute relational operations). Right now I'm compiling C code using clang to IR using a spir64 target (because it's pretty generic) with optimizations disabled. Right now I need to do surgery of the IR to remove the target specification and target attributes for functions. I've successfully loaded it and JITed it from Rust code. For some reason right now I cannot get the Rust code / C LLVM code to optimize for my host CPU. I have to externally run the LLVM: opt -O3 -mcpu=native to transform it, and it spits out nice IR that gets compile to AVX2 when I load it in my Rust JIT. I think this problem has more to do with the C LLVM API, there's no easy way of saying ... just optimize this for me with the same way llvm-opt does. Once I get this working and the Filter/Expression framework further along I'm going to try to JIT nvptx code to to run it on the GPU. 
You may want to consider posting this good answer to SO as well, either under [OPs question](http://stackoverflow.com/q/39495717/155423) or what I think it is [a duplicate of](http://stackoverflow.com/q/29861388/155423).
The common frustrations I have * Name your top level Error enum, Error. This just forces the importer to rename it on import. Which just makes code maintenance harder. * Excessive usage of Traits. I get abstractions have no cost, but `Trait-&gt;Trait-&gt;Trait` can lead though a lot of clicking though boilerplate docs. * Free Functions. Rust-Doc doesn't make seeing these *that* easy. Especially when you have 1-2, and they're doing critical things. * Build in casts, but hid them behind a `Trait-&gt;Trait`. So it isn't obvious which types are available to cast. * Have your Rust-Doc direct first to `Trait` not the underlying `Type` that implements the Trait. Just makes your docs harder to read. * Use a type from Crate A, but don't expose Crate A's type as public from your Crate. **BONUS ROUND!** Fall behind Crate A's version, so a user has to read your crate's Cargo.toml so they can synchronize version numbers. * Fork a crate to expose an otherwise non-public type as public in a totally different crate, but don't document this change. **BOUND ROUND** Direct to the unforked mainline crate's docs. 
It will show the new errors by default. I'm sure /u/CryZe92 will chime in about the JSON aspect :)
(I'm on mobile sorry for formatting) I agree with some of your posts, but wanted to argue the reasoning for Error naming. It was decided to use Result instead of `IoResult` because you can more easily say `io::Result`. If it were named `IoResult` then if you were to type it out you would have to type `io::IoResult` which is weird. To make it easier, when you import things, you can import `self` so you don't spend an extra line on imports. Once again, wish I could expound more, but mobile. Have a nice day!
Exactly :) I think the exact location the error is displayed may depend on the order of the type inference doing its thing, either the `let foo2 = ...` line is wrong, or the `foo2` return statement, but the root cause remains the same.
"No, the curly braces go *after* the function definition!" "No, they go *before* it, you're objectively wrong!" Happy bike shed painting indeed :) Looking forwards to many cody style arguments!
Again, methods can't be re-exported. (You can't "pub use" them.) I need MyStruct to be visible to the world, but munge to be visible only within foo. Since MyStruct is visible everywhere, its public methods are automatically visible everywhere. 
Can't say I've written much code but I mostly like [the style](https://ubsan.github.io/style/) described by ubsan. 
I tried answering Rust questions on SO for a while, but I found it a rather unrewarding experience. "Best answer" is never awarded, and many of the questions that show up as 'unanswered' in the search are actually answered in the comments. Besides, half of the questions get edited by someone with a Kirby-avatar who clearly has no life :-p
&gt; Then he compares a pure callback-based approach with coroutines (and their implementation of coroutines won). The advantages of coroutines there seem to be exactly those advantages of Rust's readiness-based, monomorphised futures.
Good post. Did you get plausible results with callgrind? On my system, the cost model is a bit out of the ballpark.
What's worse is a non trivial code sample without any import statements at all.
On the topic of unreliable and ultimately meaningless metrics, the [Rust Github repo](https://github.com/rust-lang/rust) has more commits than the [Ruby](https://github.com/ruby/ruby), [v8](https://github.com/v8/v8), [OpenJDK](https://github.com/dmlloyd/openjdk), [Roslyn (C# and VB)](https://github.com/dotnet/roslyn), [Swift](https://github.com/apple/swift) and [Go](https://github.com/golang/go) Github repos. Rust is catching up on [Clang](https://github.com/llvm-mirror/clang)'s repo, but has a long way to go before it reaches [GCC](https://github.com/gcc-mirror/gcc)'s astronomical commit count.
Yes! With these to try any example you have to do an extra search to attempt to make something compilable/runnable. They drive me nuts especially when the thing that needs to be imported has a hard to search for name.
I've used callgrind to get some believable results in the past - though maybe someone more experienced would have found some issues with it. When I needed to profile this time it was a fairly large program and I needed to compare a new build to an old one to find a regression. That's why I stumbled across the gperftools profiler so I could profile chunks of the program and figure out which part looked most suspicious.
I've never used it before but it looks good. Maybe I'll check out the free trial next time I need to do some profiling.
Thanks! I'm not sure. I haven't implemented chess yet. I've been developing this as part of a tak AI, which is the game linked above. If you sign in as a guest on that site, you can see the bot I wrote: TakkerusBot (tak-rs bot... get it? heh..). It offers a joinable game. Edit: Theoretically, it should stack up pretty well as it matures. The only thing it will lack are chess-specific search optimizations. Otherwise, the biggest gains in a minimax search are pretty general.
AAHHHHH SO CUTE
You can! Silly me. I forgot to look through the methods from `Deref`. So I suppose what I said is still valid if you want to do something like reorder a heap or b-tree in-place. (Which would likely require some amount of auxiliary storage anyway, but could still be optimized when you're doing it in-place.) Probably the best option for that is to have an `unsafe fn` that takes a `Container&lt;A&gt;` and gives you a `Container&lt;B&gt;` and relies on the caller to ensure the two types can be safely transmuted.