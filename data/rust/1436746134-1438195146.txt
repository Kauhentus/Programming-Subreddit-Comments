To give a concrete example: you have a stack-allocated object with a buffer in it. You create a new, scoped object that contains a pointer to the earlier stack-allocated buffer. Rust will (statically, at compile time) not allow the use of this pointer past the point where the buffer no longer exists, and since it's embedded to an object, the object as whole cannot be used beyond it's scope. Also, if the pointer to the buffer is mutable, Rust gives the object with the pointer in it exclusive access to the buffer, making sure no-one else can touch the buffer while you're mutating it, until the scope of the object containing the pointer expires. People took this to mean that since you have exclusive access, you own the buffer and can "break" it by placing it in an inconsistent state in your constructor and fix it again in your destructor, and no-one will ever notice. However, if you leak the object containing the pointer, for example by putting it in an unreachable reference cycle, you can exit it's scope without calling it's destructor, leaving the buffer in an inconsistent state. Someone figuring how to do this with functions shipped with the standard library is what caused the freakout about leaking destructors near release time. The solution taken was to make sure that all the safe functions shipped with the Rust standard library do not place borrowed buffers into states that cause loss of memory safety if a destructor leak occurs. 
Well, one reason is that it's hard to do this while following the [FHS](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard). In Nix, every package is installed in something like `/nix/store/hash-packagename`. For example, a PCRE package might install, among other things, those files: /nix/store/zx0hnalq0sqg98wc1w2g4wjc553sy2ba-pcre-8.37/lib/libpcre.so /nix/store/zx0hnalq0sqg98wc1w2g4wjc553sy2ba-pcre-8.37/lib/libpcre.so.1 /nix/store/zx0hnalq0sqg98wc1w2g4wjc553sy2ba-pcre-8.37/lib/libpcre.so.1.2.5 A package that links against this specific version of PCRE will link against one of those files (and not something in `/usr/lib` or `/lib`). That way, one can have multiple PCRE installations (even with the same version): compiling with different options, or with a different compiler, etc. will yield a different hash. For PATH, what Nix does is to create a directory with symlinks to the files. For example, `~/.nix-profile/bin/ghc` here is a symlink to `/nix/store/w4nk4jiad6i2mhhnq04jyn8spy2q9lgj-ghc-7.8.4/bin/ghc`, so I can have `~/.nix-profile/bin` in my PATH for running per-user installed packages. Since Nix doesn't use any files from FHS directories like `/lib` or `/bin`, it doesn't even care which distro you're using: there's a distro that uses Nix called NixOS, but I'm using it on Arch Linux. Anyway, this approach works fine for source packages distributed in nixpkgs, but binary packages (like Steam or Matlab) expect to link against files in standard locations. The usual approach is to either [patch the ELF files](https://nixos.org/patchelf.html) to link against files in the nix store, or [create a chroot](https://nixos.org/wiki/Steam#chroot) with traditional FHS directories. There are some articles on how to use Nix to manage Haskell packages, like [this one](https://ocharles.org.uk/blog/posts/2014-02-04-how-i-develop-with-nixos.html). The problem Nix solves in this case is that Cabal (the Haskell package manager) can easily break when installing packages on a global directory, and Cabal sandboxes compile every package for each project you're working on, instead of sharing packages (Cabal sandboxes are like Cargo). Nix lets one have the best of the two worlds. PS: shockingly, Nix doesn't do version resolution, and Nix packages don't depend on a package given a certain version bound (like &gt;= 4.x). When you write a package, other packages depend on *that* (like importing a module in a programming language), since every package is on the same git repository. And merely by changing a package you also change the hash of everything that depends on it, making them recompile on a upgrade.
Yes. snake_case is for variables/functions/modules, CamelCase is for types and traits. Pretty much just follow whatever rustc says, if it warns about style.
Latest version, with changes merged from AerialX, does just this: https://github.com/arienmalec/newtype_macros/pull/1, and fixes an egregious miss by allowing `pub`, and also allows explicit control over preliminary `#[define]`s
It looks like it may only build on nightly releases of rustc, not the stable releases: https://github.com/indigits/scirust/blob/master/.travis.yml#L2
You don't add the `#![feature(raw)]` to the `Cargo.toml`, you add it to the root module of the crate that uses that feature; generally `src/lib.rs` for libraries, or `src/main.rs` for applications, if they're using the default layout. In `Cargo.toml`, this is just being interpreted as a comment. Since this error is coming when compiling the library, you would likely need to add this to the library itself. From that CI link you included, it looks like it was build with 1.2-nightly, but you are building with 1.3 nightly. Between 1.2 and 1.3, [the `core` feature was split up into several more fine-grained features](https://github.com/alexcrichton/rust/commit/c14d86fd3ff3ba2d01a6e859290b30e74081313b), to make it easier to stabilize them one at a time. So, to fix this, you would want to replace the `#![feature(core)]` in `scirust/src/lib.rs` with `#![feature(raw)]`.
Why do you think that is? Do you have measurements?
thanks a lot, I'd totally forgotten to try that
First of all thanks for your comments! I am planning a blog post to explain some of the design decisions, but just for now: Initially, there were no special traits in the game, just some methods implemented by `Vector`. Each model implemented `Deref&lt;Target=Vector&gt;` and `DerefMut`, so all methods on the `Vector` were available in the models (this looks like your proposal, but more ergonomic IMO). Since this implementation was the same for all models, I used a macro for it (`impl_deref_vector!`). The only "problem" with this approach is that it is too magic and that it feels like poor man's inheritance. IMO traits are more flexible (e.g. what if there is an object that has a position but not a direction?) and explicit (no magic). Since I was already using a macro, implementing the new traits (`Position` and `Advance`) was trivial... I didn't even need to change anything in the models. That was very nice. Regarding separation of concerns, I think you could approach to the problem in different ways. I considered creating a `views` module containing implementations of a `View` trait (with a method for rendering) for each model. But you could take it even further and introduce a `Controllers` module to separate the logic of the game and have MVC. However, I think the current implementation is easier to understand. For a bigger game I would seriously consider a real MVC approach. If you have more questions feel free to ask!
there's already [rustkit.io](http://rustkit.io/) with libraries around. We'd better concentrate our efforts on either something really new or on supporting existing cool projects, not on creating similar ones
This is awesome! I was just looking for something like this. How might users go about adding delegation implementations for other traits? [This](https://gist.github.com/SkylerLipthay/4abba4c083bc691bf547) is what I'm using; `ToSql` and `FromSql` are the two non-standard traits. You extend the `delegate!` macro by declaring any other macro `delegate_TypeName!`. My approach requires syntax extensions, unfortunately.
**Edit:** Looks like you are already doing this. Ignore me; I just saw the example above. &gt; newtype!(A(B),Display,From,Into,Deref,Add); I'd change it further to newtype!(A(B): Display, From, Into, Deref, Add); so that the type and the derives are a bit more separated. (If that is possible with the macro identifiers you are using.)
I wrote something similar a little while back, though never published it. Here's the [latest version I can find](https://gist.github.com/DanielKeep/da5edf8a79c6c092e3ca); feel free to scrape it for ideas. A few things it does differently: * The syntax is instead `num_wrapper! { pub Coin(u64): Copy, Clone, One, Zero, Debug }` * It allows you to explicitly derive additional traits; useful if you want to provide the struct definition yourself: `num_wrapper_derive! { Coin(u64): Add, Add&lt;u64&gt;, Add*&lt;u64&gt; }` * It has an explicit "reverse trait" syntax (*i.e.* `Add*&lt;u64&gt;`) for cases where you want `impl Add&lt;NewType&gt; for u64 { ... }`.
&gt;In C++ you can skip destructors as easily as in Rust, so there's not much difference. The only way you will skip the destructor in a RAII wrapper (at least in C++, I'm not sure about rust) is that you allocate the RAII wrapper dynamically... auto ugly = new std:shared_ptr&lt;int&gt;; If you do this you are banned from the C++ community for eternity. 
I'm not familiar with "`&amp;'a mut T =&gt; &amp;'a Cell&lt;T&gt; where T: Copy`". Am I right in assuming that this is supposed to be a transmutation?
&gt; there's already rustkit.io[1] with libraries around. [rustkit hasn't been updated since Nov. 2014](https://github.com/robertg/rustkit). I'd recommend [Awesome Rust](https://github.com/kud1ing/awesome-rust) instead.
I looked briefly at the asm generated and I think all the panic cases were removed, for example. (And yes, I checked that before commenting in this thread!)
So you have a datastructure that you want to use across threads that is does it's own "thread-safety" (i.e. not wrapped in a mutex)? Are you aware of std::marker::Sync? http://doc.rust-lang.org/std/marker/trait.Sync.html
Adding `#![warn(missing_docs)]` is a good way to make sure you doc every public item.
I just wanted to share this link -- I didn't really understand the connection that you made between in-place initialization and `unsafe`. It should be safe once it's supported by rust.
A way that could be implemented within Rust's current system (but isn't) would be for HashMap to offer a method that returns a mutable smart pointer to the element which acts like a mutable reference, except that you can also immutably borrow the HashMap back from it. During this reborrow of the HashMap, the smart pointer would automatically be downgraded to a shared reference by the borrow checker, so you could safely have immutable access to both the HashMap and the element at the same time. Then, once you're done with the reborrowed HashMap, you can let it go out of scope, at which point you'd have mutable access to the smart pointer again without a second lookup. Unfortunately, HashMap doesn't implement this functionality, and I'm not sure if the team would be interested in implementing it.
Changed this too :)
It would be really cool to include benchmarks so you can compare your version with the C++ implementation. Theoretically they should be equally fast, but more often than not there's some tweaking that needs doing.
Except that, afaict, there's no way to borrow the map back from the entry so you can get shared access to other elements. Edit: If `insert_or` and `insert_or_else` instead returned an `OccupiedEntry`, and `OccupiedEntry` implemented `Deref&lt;Target=V&gt; + DerefMut`, and had a `fn collection(&amp;self) -&gt; &amp;HashMap&lt;K, V&gt;`, then yeah, it would be what I'm talking about.
That is a wonderful answer, thanks!
At a glance, it looks like you need shared mutability across multiple threads. In most cases, that means you want a `Mutex&lt;Context&gt;`. (I find the docs on `Context` suggesting an `Arc` strange given that most methods require a mutable `self`.) However, in your `worker_routine` function, it looks like you're just using the `Context` once to produce a socket. A `Socket` impls `Send`, which means you can send that to other threads. So why not something like this? for _ in 0 .. 5 { let socket = ctx.socket(zmq::REP).unwrap(); let t = thread::spawn(move || { worker_routine(socket); }); ts.push(t); } Warning: code provided without the service of `rustc`.
This is my first rust in the wild, so I'd love to hear any feedback on how to be more rusty or any other ways this library could be improved. Thanks for taking a look!
There's a tradeoff here: if you don't implement `From` and `Into` and implement `Add`, you break encapsulation -- that is, `Add` gets direct access to the member, without the opportunity for conversion. For a newtype, a `new` method and `From` trait are semantically equal. `Deref` provides (perhaps encapsulated) access to a reference to the underlying member. But `Add` is consuming, so getting a reference doesn't get you anything. I remember a Tuple1, etc. trait that exposed the foo.0 tuple syntax, but I don't see that in the docs.
Say you have a complicated algorithm that works with data in a way that would lead to aliasing-mutable-borrow errors if you give it a `&amp;mut T`. Using this pattern you can give it a shared reference instead. For example, nested mutably iterating over an array and updating each element based on every other. let mut original_vec: Vec&lt;T&gt; = ...; let ref_cells: Vec&lt;RefCell&lt;&amp;mut T&gt;&gt; = original_vec.iter_mut().map(RefCell::new).collect(); for e in ref_cells.iter() { for e2 in ref_cells.iter() { if /* e is a different element than e2 */ { e.borrow_mut().update_with(e2.borrow()); } } } 
Cool. Not a real fan of farm hash, since it's overly complex and isn't stable across hardware archs. Note: Benchmarks are a must in these libraries.
There's an issue for that: https://github.com/erickt/rust-zmq/issues/64
Neat! It looks like it might be a bit difficult to provide the `filename` method while using chain, since the point at which `read` switches files will be hidden away from me. Does that sound right?
Ah, that's true, I didn't think of that.
All good points and thanks for taking the time to write out a detailed explanation! For now no more specific questions, but if I do have more I will ask. And if you do write a blog post or article about this and want some early thoughts on it from somebody in your target audience, I'd be happy to help.
https://gist.github.com/anonymous/c9c392a8b8d4a5bd82f1 the benchmarks results are farmhash required 0.06485036987578496 s with 0/235887 collisions fnv required 0.12042406422551721 s with 1/235887 collisions siphash required 0.235458227340132 s with 0/235887 collisions
Hear, hear. What's the deal in Rust with the single letter names? Why are all lifetimes `a` and `b`, while all generic types are `T` and `V`?!? Are we conserving disk space?
My point of view is that a lot of things are already verbose and explicit in Rust, from casts to function / variable naming. What lifetimes refer to is easy enough to understand from a quick look, no need to make the code *even* more verbose by using explicit names. 
If you create A, then B, it may be the case that ~~A's destructor may want to access a reference to B~~B's destructor may want to access a reference to A, so you need to destroy A after B. struct Foo&lt;'a&gt;(Option&lt;&amp;'a str&gt;); impl&lt;'a&gt; Drop for Foo&lt;'a&gt; { fn drop(&amp;mut self) { println!("{}", self.0.unwrap_or("nothing")); } } let a = "hello".to_string(); let b = Foo(Some(&amp;a));
This is because of dependencies: if B depends on A, then B can only be created after A, and by necessity it needs to be destroyed before A otherwise you'd end up with a brief period where the dependency requirement is not satisfied.
Cool, that worked, thank you! But such strategy might not work for more complicated use cases, alas. So I guess the only thing I could do here is to fork rust-zmq and fix https://github.com/erickt/rust-zmq/issues/64, right? Or are there any way to work around that?
Been thinking more about this. So I think we need something like an "explicit OIBIT". pub trait Pod {} impl Pod for ?.. {} Which implies that you can only `impl Pod for MyType` if all members of `MyType` also `impl Pod`. Alternatively, you can force-impl it using `unsafe impl Pod for MyType`. Then in `std` we'd have `unsafe impl`-s for primitives and etc. I raked my brains for a while, but I don't think there's any way to achieve this with regular OIBIT-s only.
`export` means that the shell should not only have this variable set, but also processes it starts should have it.
That would work if all you cared about was whether two slices pointed to the exact same sequence of bytes in memory. But for some arbitrary arbitrary `A` and `B` such that `A: PartialEq&lt;B&gt;`, then checking for equality on `&amp;[A]` and `&amp;[B]` must necessarily call `eq` on each corresponding pair. Certainly, [you can check the length first](https://github.com/rust-lang/rust/pull/26884/files), but otherwise, the definition of equality on component types needn't be constrained to its byte representation in memory. If we had specialization then this sounds like it might be a fast path for checking slice equality on certain types, e.g., trivially `&amp;[u8]`.
&gt; If we had specialization then this sounds like it might be a fast path for checking slice equality on certain types, e.g., trivially &amp;[u8] Yeah, that's what I mean. Checking referential equality would be a good fast-path for checking two slices of the same component type. It seems like it could be done without specialization if we had an intrinsic to compare equality of types at compile time (returning `false` if no such determination can be made). Why don't we have specialization yet, anyways? Isn't it kind of a highly desired feature like HKT or type-level integers?
Referential equality would be OK for `Eq`, where there is a *full* equivalence relation defined, but it’s not for `PartialEq`, where there is no such guarantee; for example, NaN ≠ NaN.
&gt; So how can we keep reference to an object that is created in an inner scope like a function? The answer lies in heap allocation which in Rust is achieved via `Box::new`. We will explore that in Part III. This seems really misleading (or maybe you've meant `Rc::new`?). You certainly can make an object escape a function without boxing it. And actually, it seems that all your examples would work fine with initial `Option&lt;Car&gt;` (even the one which currently doesn't), as a `Car` is never owned by more than one person (and idea of owning a car fits really well with Rust's concept of ownership). Overall nice post, but I guess it would be better to show some examples that wouldn't work with `Option&lt;Car&gt;` and do really require borrowed references.
Writing a *concurrent* data structure is, to my knowledge, completely different from writing a "normal" one like HashMap. Java and C# benefit massively from excellent concurrent garbage collectors in this case (I'm actually guessing in the case of C#... I know Java's is great). You generally don't want to fill your collection full of locks for performance reasons. However this raises the issue of memory reclamation. Java just goes "lol GC" and wins -- but Rust doesn't have decent concurrent garbage collection (Arc, again, probably has too much overhead). Crazy things like quiescent states or hazard pointers may be required to get really competitive concurrent data structures in Rust. Doing this safely in Rust is an open problem that someone should really work on! /u/aturon is hoping to work on this eventually but as far as I can tell is trapped in an infinite hellish loop of meetings herding the rest of the Rust team. Someone at the latest Rust SF meetup linked me this really interesting paper on the problem and tradeoffs: http://csng.cs.toronto.edu/publication_files/0000/0159/jpdc07.pdf
Search for AnyMap and TypeMap on github, they may give you some insights to building a data structure wrapping or emulating a map. And I believe both were built by either Rust core devs or people close to the Rust project, so not just Joe Random. I'm on mobile right now but once I get to computer I'll provided links. Edit: [TypeMap](https://github.com/reem/rust-typemap) [AnyMap](https://github.com/chris-morgan/anymap)
Hot off the presses https://github.com/rust-lang/rfcs/pull/1210
http://reactiongifs.com/?p=4082 (does this violate the "No Memes" rule?)
Yup, I'm saying what you want is a dead simple template engine. Mustache comes close, but I see that two rust impls of it are both no longer maintained. Handlebars is a decent second alternative. There's probably some value in a template engine with exactly the same syntax as format, but with runtime checks, you're right. Doesn't seem to exist yet.
Treating them as one line strikes me as pretty weird behavior.
Yeah, that's definitely a difference. The problem is that this is just implementing `Read` which has no concept of lines, while the Python API is dealing specifically with lines. I was sorta hoping I could punt on doing that kind of thing in this layer and maybe add an extra thing (like `Lines`) that would handle it. It definitely does seem important if processing line-wise, which is the likely use case of this kinda code.
It depends. That's not the worst, but personally, I prefer to tag on an `unwrap`, as then if my assumption is wrong, I will at least get a crash, rather than blindly continuing. I got in this habit after `let _` led to a few bugs that were much harder to track down than if I had just crashed, showing me exactly where the issue was. 
Good point. Thanks!
Yes it does. I'll let you off this time, but please don't do it again :)
That's too heavyweight for this use case, but I'm glad you showed it to me. :)
I was worried specialization was ruled out in the name of simplicity. I'm glad it's not, and this looks like a sensible way to put it on the table.
Would it be possible to give an ELI5 of why GC makes concurrent data structures much simpler? Granted, I haven't spent long thinking about it, but it's not immediately obvious to me what the trouble is with a lack of GC. I *thought* /u/pcwalton had essentially ported Java's ConcurrentHashMap at one point for servo. Also, there's no reason rust couldn't get gc at some point, and thus gc-enabled data structures. I guess the main drawback with that is a divided-universe situation.
Cool! I believe it isn't entirely fair though as the farmhash part doesn't go through the Hasher machinery.
It's easiest to see with a lock-free linked-list. ... -&gt; X -&gt; Y -&gt; Z -&gt; ... Thread A has a pointer to Y and decides it wants to remove it. Thread B is just walking through the list and happens to have a pointer to Y as well. It's easy enough for A to logically remove Y from the list, just rewrite X's pointer to go to Z: Y ---v ... -&gt; X -----&gt; Z -&gt; ... Now anyone at X correctly sees that Z is the next node, and anyone on Y will *eventually* proceed forwards to Z correctly (because Y still points to Z -- similarly if Z gets removed you can keep walking until you get back into the list). However we'd *really* like to free Y's memory (or equivalently put it in some pool to be reused later). Unfortunately, we can't correctly do that until B actually *leaves* Y. Oh look this is exactly just garbage collection. If your language has garbage collection there's "no overhead" to handle this. The GC trivially handles reclamation of the node once B leaves. If you *don't* have GC, you need to do manual book-keeping to do reclamation. That's hazard pointers or quiescient states. It's also not sufficient to *just* have GC; your GC has to be thread-safe. For instance Boehm isn't thread-safe by default: http://www.hboehm.info/gc/scale.html Edit: I forgot to say: most literature *assumes* you have a good thread-safe garbage collector. It makes no attempt to address the problem of reclamation. It just... *happens*.
Awesome, this really helps visualize it!
Okay, thanks for the link! I actually have more experience in the concurrent part of my concurrent map than I do in the data structure design part. I've done some lockless programming before (cough ring buffer cough) in C++, but I was just wondering how hard it would be in Rust. There aren't any hard requirements for this map, other than I'd like it to act as a thread-safe dictionary where multiple writes could happen simultaneously, and then start working on the problem of global operations. Anything that could be lockless gets a 2x time bonus over mutexes, but again, I wonder more about the global operations than the individual buckets (to start with). I also don't know if the Robin Hood stealing for buckets would lend itself well to any sort of concurrent pattern. So really there aren't any hard or fast requirements, other than I'd like to emulate those other two hashmaps. I wouldn't mind getting involved in more Rust development anyway (since that's what I wanted to build up to doing). I had thought that some sort of reference count would be necessary to keep track of objects that were deleted, so that they could be freed when the last reference disappears (like your example below), but maybe there is a better way in that paper. I hadn't necessarily considered the memory problems that it would encounter in the case of a orphaned node.
Cool, thanks! Those definitely seem to be on the path of what I'm looking for in guidance!
In my C++ version of the same code, I use separate unordered_maps in each thread that keep track of the local count, and then merge at the end. A locked or lockless implementation won't necessarily kill performance, but it won't be as good as a single-threaded hashmap. The performance bonus comes from being able to simultaneously write several buckets at once, and read from any number of buckets at the same time. This part is the easiest to follow, it just gets harder when you try to work on global operations.
So the *best* news for concurrent data structures in Rust is that the borrow-checker lets you make sure that when someone's supposed to have discarded all internal references you *know* they've discarded them. That means you need to frob ref-counts or equivalent info a lot less to be safe, which is great! e.g. if you have an Arc you can grab a bunch of &amp;'s into its internals, and you know the data will never get dropped early because the outstanding &amp;'s prevent *that* Arc from being dropped. Tons of shared references and no need to mess with ref-counts. I don't know a lot about the details of concurrent collections, but Robinhood seems pretty nasty for it. Lots of shifting elements around and false sharing.
On a debug scenario, or if you're only developing for yourself, then it makes sense. However, if your developing for thousands or millions of other users, then I don't think it's such a clear cut. The end user will probably prefer the application to continue on a best effort basis, rather than having it crash. That's my experience, at least. I would also like to add destructors to the list of things where errors should be - in general - ignored instead of panicking. A resource leaked is the least bad option here, IMO. Of course, the best thing is to actually think every situation through thoroughly and always make the right choice :-) So let me propose a middle ground that might be adequate for many situations: debug_assert!(x.is_ok()) 
Yeah, that was going to be my basic problem with it. My first stop was to start with a very basic hash map implementation that didn't necessarily do any load rebalancing (or very simple), and start introducing the different operations one at a time. Probably just start with adding elements and reading them (searching for an element), and see if that can work using locks on the buckets. At least, that would be a start. Then I can start worrying about removing things and the like. The Robin Hood scheme just looked like a huge pain for any type of concurrency, unless you were willing to lock the entire map at once (which always requires checking that lock on entry, blah blah blah). I'll have to think some more about how I would do the resizing of the buckets array.
I get the impression that this is *instead of* negative trait bounds. Which I prefer. This only requires reasoning about specificity, rather than "doesn't-implement" constraints.
Gankro how does this relate to integers as part of the generics? It's absence kills fixed sized array usage for me. 
This proposal is totally compatible with negative trait bounds, and their use cases are different and complimentary (there are some relations which can be implemented using either, but even for these I think one of the two implementations would be really strained and hacky). For example, you can't use specialization to provide implementations of the same trait for all `T: Integer` and all `T: Float`, and you can't use negative bounds to provide implementations of the same trait for `Vec&lt;T&gt;` and `Vec&lt;u8&gt;`.
Do you mean that this RFC would make fixed sized array usage irrelevant to you? I don't see a relationship between specialization and type parameters which are dependent on integers, could you elaborate?
I fear that limit would fail to provide either the desired simplicity nor would it allow the desired performance. :/ Regarding Servo, the DOM usually has a fairly deep inheritance hierarchy. Building it using a thusly limited number of overlapping impls could prove interesting at best.
Can you share this particularly evil example?
Unfortunately not, as the copyright lies with the university, and I'm no longer an employee. Anyhow, I'm pretty sure I can construct a similar example if I find the time.
Just for inspiration: https://www.threadingbuildingblocks.org/docs/help/reference/containers_overview/concurrent_hash_map_cls.htm https://www.threadingbuildingblocks.org/docs/help/reference/containers_overview/concurrent_unordered_map_cls.htm
&gt; In such scenarios that you don't want to panic, Which is approximately every production ready program, or...? &gt; you should have code that handles the error the best it can That I totally agree with... &gt; (and then if all else fails, panic). ...and that I think depends on the situation. Again, sometimes e g leaking a resource is better than panicking. Using `unwrap()` by default might be convenient for the programmer, but use it one time too much and your end users - who might be very far away from contacting you with a bug report - will get a very crashy experience.
Under this RFC, they're only required to follow the standard orphan rules, so the short answer is no.
Yes, but the weak reference somehow has to find out if the Rc box was destroyed or not, in order not to return a wild pointer ... (Reading the source, I found out that the Box is not actually freed, just the content gets dropped, so using weak references on [u64; 1024] or so would not really help with memory usage)
I changed that now. Same results
&gt; A's destructor may want to access a reference to B Do you mean that B's destructor may want to access a reference to A?
Update: I have [whipped up](https://llogiq.github.io/2015/07/14/inheritance.html) a blog post containing a simpler and less evil example. It's still evil enough to demonstrate the problem.
I've tried to clarify this in the explanatory text when you hover over it. Thanks :)
&gt; Compiler insists on us providing a lifetime parameter for the other input variable. Why? fn trade_with&lt;'b&gt;(&amp;mut self, other : &amp;'b mut Person&lt;'a&gt;) { let tmp = other.car; other.car = self.car; self.car = tmp; } This code is really confusing imo.
TBH, I don't see the problem. A bit of concentration was needed to keep the intermediate results in memory, but at no point is it unclear which specific method will be called.
&gt; A bit of concentration was needed to keep the intermediate results in memory This is exactly what I wanted to show. Keep in mind that this is a small, almost trivial example, and you still needed "a bit of concentration". Now go up a few orders of magnitude and you have a typical code base. Do I need to make the example more evil or can I otherwise help anyone to see the problem?
I wouldn't say the risk for silent data corruption is higher compared to panicking. (Also, Rust should be memory safe in both cases.)
I think memory safety has nothing to do with this. Consider `let _ = file.write_all(foobar)`. This error-ignoring might indeed lead to silent data corruption, with `unwrap` it would at least be noticed.
&gt; The end user will probably prefer the application to continue on a best effort basis, rather than having it crash. That's my experience, at least. I agree that it would be ideal, but we were discussing a case where the erorr should never happen. If it does, we weren't writing error handling code anyway, so the chances of this actually working are pretty low. Also, `panic! == abort` is a really commonly requested change to Rust, so it seems like many people want even stronger crashes. I personally think having unwinding is good in many cases.
This is a great idea. For a program focused on performance like rust, GPU programming is a must have.
This is indeed hard to track mentally, but I'd argue it's specifically because of mutable state, not because of the specialization / inheritence. If you know the rule that in Java, everything's virtual by default, knowing which methods get called shouldn't be a problem. If the rules are conceptually simple, I'm okay with specialization.
If you want async, you'll take `Arc`, not `Rc`. `Rc` is mostly used for (relatively) cheaply moving the basic rust guarantees to runtime in a single-thread scenario (e.g. you want multiple parts of your code to independently work with something and have it dropped when the last one is done with it). By the way, Manish has a nice [article](http://manishearth.github.io/blog/2015/05/27/wrapper-types-in-rust-choosing-your-guarantees/) about what the different available wrappers are, where to use them and how to combine them.
Cool! Any ideas for more use cases? Off the top of my hat: impl ToString for str {...} // shortcut avoiding format!ing I think the `Iterator` trait could benefit from this as well. But this will very likely be a breaking change. The modified `Iterator` trait I could come up with looks like this: pub trait Iterator { type Item; default type PeekInner = Self: Iterator&lt;Item==Self::Item&gt;; type PeekIter = Peekable&lt;PeekInner&gt;; // result has to be a Peekable&lt;...&gt; ! default type SkipIter = Skip&lt;Self&gt;: Iterator&lt;Item==Self::Item&gt;; default type TakeIter = Take&lt;Self&gt;: Iterator&lt;Item==Self::Item&gt;; default type RevIter = Rev&lt;Self&gt;: Iterator&lt;Item==Self::Item&gt;; ... } impl&lt;I&gt; Iterator for Peekable&lt;I&gt; { ... type PeekInner = I; // instead of Peekable&lt;I&gt; fn peekable(self) -&gt; Peekable&lt;I&gt; { self } ... } impl&lt;I&gt; Iterator for Skip&lt;I&gt; { ... type SkipIter = Skip&lt;I&gt;; // instead of Skip&lt;Skip&lt;I&gt;&gt; fn skip(self, n: usize) -&gt; Skip&lt;I&gt; { Skip { iter: self.iter, n: self.n + n } // possible overflow issue :-( } ... } impl&lt;I&gt; Iterator for Take&lt;I&gt; { ... type TakeIter = Take&lt;I&gt;; // instead of Take&lt;Take&lt;I&gt;&gt; fn take(self, n: usize) -&gt; Take&lt;I&gt; { Take { iter: self.iter, n: cmp::min(self.n, n) } } ... } impl&lt;I&gt; Iterator for Rev&lt;I&gt; where I: DoubleEndedIterator { ... type RevIter = I; // instead of Rev&lt;Rev&lt;I&gt;&gt; fn rev(self) -&gt; I { self.iter } ... } But I'm not sure if that's going to work at all. It seems a bit like overkill and the optimization for skip obviously comes at a price here (integer overflow problem). The constraints for the associated types to be iterators are important, I think. Otherwise, other generic code might break. Without the constraints the trait would not force the result of `.skip()` to be an iterator. You could override a default with something completely different.
&gt;I think if someone placed the Rust and Go community in a room and asked them to fight, we'd probably just all order pizza and geek out over languages. I'd doubt the two groups would interface with each other smoothly, and there would probably be a messy fight with pointers that we'd have to own up to. I'd hate to be the one to collect all the garbage after that fight.
&gt; it seems that all your examples would work fine with initial Option&lt;Car&gt; This is true. But the goal of the tutorial is to explain reference pointer containment. So I had to steer the requirements towards that. In this case the rational was performance (that is assuming ``Car`` is a big struct and expensive to copy). &gt; You certainly can make an object escape a function without boxing it. The objective is to make *reference* to an object to escape the scope where it was created. That is not possible with stack allocated objects. You can certainly copy objects out but that is not what we are trying to do here.
If there's garbage, it's the Gophers' fault. We always clean up everything we own.
&gt; [`Rc&lt;RefCell&lt;T&gt;&gt;`] seems so ugly and temperamental, like a hack to avoid the main reason to use Rust in the first place I havn't used this kind of type a lot. Almost never. So, personally, I can live with its ugliness. ;) Whether it's a hack depends on your definition of the word. ;) If "hack" implies something unsafe in the sense of circumventing the type system, well, that's been isolated to the implementation details of `Rc` and `RefCell`. They *do* provide a safe interface, however. You could say, that's what Rust is about: Building abstractions with safe interfaces. `Rc` is for when you can't reasonably make one part of your code the sole owner of something. `RefCell` is for when you can't rely on static borrow checking w.r.t. mutability due to restrictions in what the compiler can check. In these cases this checking has to be done at runtime which is what `RefCell` is for. Remember: [Sharing + Mutability is the root of all evil](https://air.mozilla.org/guaranteeing-memory-safety-in-rust/). 
 trait U8 {} impl U8 for u8 {} impl !U8 for .. {} trait Trait {} impl Trait for Vec&lt;u8&gt; {} impl&lt;T: !U8&gt; Trait for Vec&lt;T&gt; {} I don't think there's anything that specialization allows that negative trait bounds doesn't; it's just a lot cleaner in some cases. After all, with negative trait bounds, traits will have all the important set operations (intersections, complements, and cartesion products). They don't have unions, but I don't think they'd be particularly useful anyway.
&gt; The answer is 42? Man, you ruined it for everyone else... :-P &gt; Seems like this is only a problem when you add `super`. Not necessarily. The evil thing is that the most specialized class' method gets called, so when you call code in A from B from C, and in A call a method that C overrides, C's method will be called. Imagine that A, B and C are larger, and you cannot see all of them on one page. Note that during my time as consultant I have seen actual code in the wild (which I'll call extremely overengineered) that worked similarly, through 7 classes in 3 packages. Finding a bug in that maze of twisty large classes, all different has to be one of my least favorite activities, right up with being waterboarded.
Awesome. And what have you learned from the exercise (well, apart from "llogiq is a really evil person")?
The user got an error though and it's not "silent data corruption" any longer, that's what I generally aim for. I concur with the second point, it looks fine in that case. Generally, ignoring errors is a bad idea though, if you're fairly certain that it won't happen, `unwrap`, if it's okay if it errors, give a warning, if it's expected that it errors, do nothing (that's very uncommon), and otherwise just handle the error.
I think by mutable state he meant that you have to keep intermediate results in memory which makes it hard. The example you provided does not rely on specialization / inheritance and is equally complex as your original example. The only thing that's added through inheritance in the original example is that you have to know the difference between `a()` and `super.a()`. A reader can ignore the base class implementation of `b()` and `c()`. However, `super.a() + c()` adds complexity in the sense that a reader needs to know that evaluation order is fixed left-to-right in Java which makes a difference since `super.a()` mutates `x` (this is where additional mutable state complexity comes in as well). I would argue that neither this nor the original example show added complexity through specialization / inheritance well.
I played around a bit with writing [a concurrent hash table](https://github.com/veddan/rust-concurrent-hashmap) in Rust a while ago. Maybe it would be useful to have a look at it. It's much less sophisticated than the `std` hash table and might be easier to understand. It uses open addressing (so no bucket chains). The basic idea re. concurrency is to split the table into a number of parts ("shards") and have one lock for each shard. When searching the table, a shard is selected based on a couple of high bits in the hash, and the remaining bits in the hash is used to index the shard. Each shard is essentially an ordinary (non-concurrent) hash table. The number of shards can be tuned based on expected concurrency such that lock contention for an individual shard should be rare.
I'm not really good with creating puzzles. How about this? class A { int a(int x) { return x * 3 - 1; } int b(int x) { return c(x) - x; } int c(int x) { return x - 1; } } class B extends A { @Override int a(int x) { return super.b(x) + b(x); } @Override int b(int x) { return x * 2; } } class C extends B { @Override int a(int x) { return super.a(x) + b(x); } @Override int c(int x) { return super.b(x) + 2; } } And the question is: What is `new C().a(8)`? I tried to reduce the mental load by making it so that you can substitute a function with its body easily and then come up with an easier representation for `a()`. I added some methods for noise (like in your example) that can trip somebody up. Also, there is a small gotcha :) 
Yes :)
Strange enough, the compiler seems to have a hard time figuring out the correct lifetime, I don't really know why. I think your tiny code is roughly equivalent to this, which also fails: http://is.gd/rY4qlX While this equivalent code compiles: http://is.gd/qMNzhR So I think if you make an additional function, you can explicitely tell the compiler the correct lifetimes; you could have a function like fn split_up_string&lt;'a&gt;(string: &amp;'a str) -&gt; Option&lt;Vec&lt;&amp;'a str&gt;&gt; { let re = regex!(r"[ \t]+"); match re.split(x).collect::&lt;Vec&lt;&amp;str&gt;&gt;().as_slice() { ["v", v0, v1, v2] =&gt; Some(vec![v0]), =&gt; None, } } and then just do let data: Vec&lt;Vec&lt;&amp;str&gt;&gt; = mesh.lines_any().filter_map(split_up_string).collect(); I *think* this should work :)
So here's the problematic code: extern crate regex; fn main() { let mesh = "\ a b c d e f "; let re = regex::Regex::new(r"[ \t]+").unwrap(); let data: Vec&lt;Vec&lt;&amp;str&gt;&gt; = mesh.lines_any() .map(|x: &amp;str| re.split(x).collect()) .collect(); } This *should* work I think. The lifetime of the strings returned by `lines_any` is tied to `mesh`, and the lifetime of the strings returned by `re.split` are tied to `lines_any`. But for some reason, the compiler doesn't seem to be making that connection when inferring the type of `x` in your map closure. So if you help it out... extern crate regex; fn main() { let mesh = "\ a b c d e f "; let re = regex::Regex::new(r"[ \t]+").unwrap(); let data: Vec&lt;Vec&lt;&amp;str&gt;&gt; = mesh.lines_any() .map(|x: &amp;'static str| re.split(x).collect()) .collect(); } Then it works! I don't know enough about how borrowed lifetimes are inferred to know why the type hint is necessary here.
Oooo. This works: fn main() { let mesh = "\ a b c d e f "; let re = regex::Regex::new(r"[ \t]+").unwrap(); let data: Vec&lt;Vec&lt;&amp;str&gt;&gt; = mesh.lines_any() .map(|x| re.split(x).collect()) .collect(); } Hmmmm. So I think this means that lifetime elision is biting you. It might be that the lifetime of `&amp;str` is automatically connected to the life time of the `LinesAny` iterator, which surely will not live long enough.
That function calls with bad names can be much more confusing than overloading.
&gt; If there's garbage, it's the Gophers' fault. We always clean up everything we own. Au contraire. We know Go has Garbage collection built in. 
I mean I guess you COULD use Rc&lt;RefCell&lt;T&gt;&gt; everywhere, but it doesn't mean you SHOULD. I think that's why there's no default shorter name for it, because then it becomes more apparent you are using it and that there's a runtime cost tied to it. 
It would probably be better if your constructor would take an iterator (or better IntoIterator…), as you do not require the properties of a slice. Like so: ``` impl FileInput { new&lt;I: Iterator&lt;T&gt;, T: Borrow&lt;str&gt;&gt;(paths: I) { ... } } ```
`Rc&lt;RefCell&lt;T&gt;&gt;` is almost exactly like `Arc&lt;Mutex&lt;T&gt;&gt;`, except that you don't pay for the atomic operations. If you don't need the atomic operations, why pay for them?
It is interesting to note that Microsoft is sidelining the Java implementation of Minecraft and [bringing the C++ Pocket Edition to Windows](http://www.rockpapershotgun.com/2015/07/06/minecraft-windows-10-edition-beta/)
I just ran into this last night! Unfortunately the way `extern crate` and `use` work together in modules is poorly documented. You don't need the `extern crate` in main.rs. Just keep it in other.rs, but change the use line to `use self::rand::Rng;`.
Yeah, I think you're right. The iterator thing doesn't have anything to do with impl specialization. It's just associated types.
Okay, that makes more sense! I ended up keeping it in main so that all of the external crates I use are in one place. Thanks for explaining both ways
Thanks, I ended up keeping the extern crate in main as suggested by Quxxy
Allowing you to use negative reasoning to close over the entire space of traits into two types is a serious backward compatibility hazard in that it makes implementing that trait a breaking change. It turns silence into a breaking change. Because of this, under [the most recent Negative Bounds RFC](https://github.com/rust-lang/rfcs/pull/1148), the line `impl !U8 for .. {}` will **not** cause types which have a `u8` field to meet the `!U8` bound.
People over-rely on `Rc`because they miss garbage collection. It's very rarely necessary.
Bah! Thanks. I knew there was a good chance I'd be embarrassed by something about this question. :) I hadn't run into mention of `--release` yet, and never thought to look for an optimization flag. Come to think of it, I was assuming optimization would be by default, with debug being flagged for. Not sure why! But yes, this helps reclaim all the performance, and then some because the surrounding code is optimized too.
It's all good! We had a huge debate about what the default should be, and this is what we ended up with. Either way means some people make the wrong assumptions... (you're lucky it was only 5x slower! usually it's more...)
I wonder if partial implementations should counted when determining which impl is the super impl and the super super impl.
&gt; Plenty of times you need to share data where the final owner of the data isn't deterministic. Would you mind providing a simple example of a scenario where this is the case? Thanks!
&gt; For any trait T, the negative bounds RFC divides all of type space into three disjoint sets, T, !T, and ?T. Right, but because types for which no code has been written are ?T, and it is impossible to rely on a type being ?T, it doesn't present any backcompat hazard. &gt; I'm not quite sure what you mean by this. Are you saying that something like Vec&lt;u8&gt; is not !U8 as a result of that impl? That doesn't break the example, just some edge cases like Vec&lt;Vec&lt;u8&gt;&gt; and it seems like a very poor decision (if I'm understanding it correctly). This is correct. Or a struct which maintains a u8 counter for some reason, say struct Foo { n: u8, ... }. This has to do with the basic implementation of default impls and the way they are used for soundness guarantees in cases like `Send` and `Sync`. Though this terminology isn't used in the OIBIT RFC, any type which has a field which contradicts the default impl is inferred to be `?T` unless it has an impl itself. Its *inconvenient* that default impls don't work that way, but it is necessary for soundness. &gt; In any case, the code I posted currently works, if you do it backwards Yes, but the limitation is the same: A Vec&lt;u8&gt; does not impl NotU8, nor does an Option&lt;u8&gt; or a (u8, f64), or anything else that has a u8 field. Also, there might be a further limitation that this reasoning is only allowed locally right now; that is if you try to impl Trait in a different crate it will not compile. I'm not 100% on this though, you'd have to try it.
I get confused why the compiler is nagging? There is the lifetime elision rule: &gt; Each elided lifetime in a function’s arguments becomes a distinct lifetime parameter. You'd think that 'b would be needless in this case? Or does the &lt;'a&gt; there prevent elision?
Any graph structure?
Reminds me of hearing that it took Bethesda something like a year to release an -Ooptimized build of Skyrim. (Haven't confirmed this to be true) Regardless, I think debug by default was the right choice.
Are you saying that trait A {} trait B {} impl&lt;T: ?B&gt; A for T {} would not be allowed? I've read through the RFC a couple times and not gathered that.
Is there any reason why this wouldn't live within the Hyper project / repo? I know it's good to separate stuff into crates where possible, but this feels like a fairly standard feature of a web server.
If you are creating a reliable scenario you'd better have a really good excuse to ignore the error. Also it shouldn't be really ignored, instead it should be logged and then you should keep going. I replied to the parent with some code showing a way of doing this. And many times a `panic!` is better than to "try and keep going". If an error that should be impossible happened, something akin to 2+2 giving you NaN, then we can assume that the program is in a FUBAR state and it should just terminate instead of creating more flawed data that could bring the greater system down. OTOH the reliable system should run this program and be prepared for it crashing and be able to recover from that. Asking a system to be prepared for a program that might crash under impossible circumstances (that end up happening) is more reasonable that asking a system to be prepared for a program that will, under impossible circumstances (that somehow end up happening all the time) act in an erratic fashion and give you faulty results.
So... is it now just as fast? Inquiring minds want to know.
&gt; I think he means "What about type level integers". Ya! Sorry, I was sleepy at the time. Without the type level integers I can't ever derive Debug for something that contains an array with 33+ items.
It's faster -- "reclaims all the performance, and then some". The abstractions of num::complex are optimized away, plus all the other code is optimized. It's such a huge difference, and I'm always working with performance-sensitive code, that I'll probably do release builds by default until I need to debug -- and then I'll be looking to only build specific parts of code for debug.
Rc&lt;RefCell&lt;T&gt;&gt; basically lets you have a safe* shared mutable state for something that doesn't need to go between multiple threads. It's usually not the best way to do something, and if you see it a lot it's probably because people aren't used to doing things the Rust-y way or maybe are interfacing with non-Rust libraries (not sure of other reasons to use, as I've personally never used it at all) \* Only safe because Rc&lt;T&gt; can only ever be on a single thread, so the RefCell&lt;T&gt; being mutably shared is safe If you want to have a safe threadable shared mutable state type thing, then Arc&lt;Mutex&lt;T&gt; or Arc&lt;RwLock&lt;T&gt;&gt; are what you'll want to look into.
Yeah, I think there should be a comment in that bit of code to the effect of: match guess.cmp(&amp;secret_number) { // This causes a compiler error. Ordering::Less =&gt; println!("Too small!"), // Read on to see why! Ordering::Greater =&gt; println!("Too big!"), Ordering::Equal =&gt; println!("You win!"), } }
AFAIK there's no way to promote an immutable reference to a mutable one, but that wouldn't be out of place in the language.
I'll double check the crates guide, I thought it was in there...
It might be. I didn't actually go check.
Oh, ok, cool. Sorry, I should have looked at the context first!
You can do type level integers (and even arithmetic) now. It's not the prettiest, but not too bad. [Example] ( https://github.com/paholg/dimensioned). I, too, would like built-in support for type level integers, but there's nothing stopping you from writing the code now with plans to migrate in the future. 
It also takes two clicks to get to the actual quote. I'm not sure why they reference themselves referencing the source.
TL;DR: Rust's secret weapons are safety and kindness.
Nah, they also need to pay their staff. For better or worse, this is the web we've chosen.
That's fair, but making a worse user experience purely to increase page impressions rubs me the wrong way, especially since they already have a giant pop-up the moment you load the page.
I myself am guilty of mirroring bad attitudes back at people (including in my pseudo-official capacity as moderator of this sub), but that hardly makes it an admirable trait. It's possible to refute bad actors without stooping to their level, and it's a philosophy that I'm increasingly attempting to practice.
People can choose to respond in kind, or try to hold themselves to a higher standard. I think the latter is the best course of action always, but especially for community leaders: their actions are particularly influential in setting the tone of a discussion/project. Rust's leadership has always tried to take the latter route as much as possible (starting with Graydon), and I strongly suspect that this was a key part of creating the good natured Rust community we have now.
This is the maximally uncharitable interpretation. In a past life as a not-quite-professional-but-still-got-paid-to-do-it tech blogger I linked to my own past posts all the time, not out of financial incentive but because I legitimately thought people would find them interesting. And I can't speak for InfoWorld, but I sure didn't get paid according to page impressions. I had a post quota for which I received a fixed sum per post, and was awarded a bonus for each post above my quota.
You're setting yourself up for disappointment if you expect links to unfailingly indicate citations. :P We don't fault, say, Wikipedia for linking to other Wikipedia pages instead of directly to primary sources.
If you have `let x = 1; let y: &amp;i32 = &amp;x;` there's no way to explicitly write the lifetime of the `y` reference.
That's fair. It's a minor thing, and I gave that exaggerated response largely in jest. I don't always think about how things are recieved when tone is not attached.
&gt; picking always the most specialized functions without context forces you to have the whole inheritance/specialization hierarchy in "view". Exactly.
The compiler needs to know the size of the thing that `valT` returns. As *anything* could `impl Item`, it has no way to know this. What you can do is return an owned pointer to an item, as pointers have known size. This works: fn valT(val : i64) -&gt; Box&lt;Item&gt; { Box::new(Val { v: val }) } 
I try to think of these challenges this way: Every hour I fight with the rust compiler saves me at least two tracing that segfault if I hadn't used Rust.
I don't think that Rust will ever be specificially mandated, though it could someday land on a list of vetted languages (like currently Ada and Java) to be used for defense applications. As a pacifist, I'm not sure I like that idea. More to my taste is the possibility of Rust code in space. Some people in the automotive industry also seem to have set an eye on Rust, though I know of no official project to evaluate it yet. In any event, those use cases usually require a (mostly) fixed memory layout, so the "memory-safety" aspect is already moot. On the other hand, the "controlled sharing" aspect could prove particularly interesting.
&gt; Rust's leadership has always tried [...] And mostly succeeded, I'd say. :-)
It seems the community is still learning how to teach. One thing I just realized about Python, is that since it is a popular language and growing an a great rate, with many domain experts who are not programmers is that it has lots of smart people who aren't programmers. This also means that, since it has exponential growth, it will always have the same ratios of new programmers. This isn't bad, and rust has the same ratio right now. We need to learn how to teach and how to explain the core tenets of Rust. We should always welcome newcomers with open arms. They bring new ideas, new perspectives and keep us honest.
So assume that you're making a web service where each connecting client has a shopping cart. Now, a client makes a request to remove an item from the shopping cart, but that item was not in the shopping cart in the first place, so the function that was supposed to remove it returns an error. * You could panic. That would bring down the entire web service, then thousands customers have to log in again to find their shopping carts empty. * You could log the failure. That would enable an attacker to fill up your hard drive. * Again, the most reliable course of action is to just ignore the error (at least in a release/production build). I'm not saying that `panic` is inappropriate in *all* cases. There sure are cases where panicking is the least bad option. I'm just saying that using `unwrap()` as a somewhat sloppy routine, or a default fallback when you are too tired to consider whether something could actually happen or not, is a bad choice and will lead to a crashy program. And I would hate to see Rust software - which all its guaranteed memory safety - to get a reputation of being crashy due to bad advocated practices.
You should try `perf` as well. Comes with the kernel (in `tools/perf`), and on e.g. Debian it's in the `linux-tools-*` package.
No, but I'm going to. Next semester, in my robotics course :)
I have perf installed on my system, and in fact oprofile uses the same mechanism, but is able to annotate source/assembly. This is why I prefer it to perf.
....... NOBODY EXPECTS THE MANISH INQUISITION!
I don't know if this counts, but I'm using Rust to develop the GUI for our rover: https://github.com/tedsta/rover-gui Pretty primitive stuff. Uses human-readable text rather than binary for packets. It talks to a python middle-man running on an Arduino, which then talks to the Arduino C code. Super hack job. But it's fun to drive the rover with an xbox controller and live camera feeds!
Perhaps the confusion stems from the mixing of two concerns: a) military/aerospace applications and b) robotics. For (a), to my knowledge Java *is* certified to be used in military applications, while Ada was purpose-built for them. Regarding (b), C (and to a lesser extent assembly) is used in robotics because such applications are mostly real-time. Thus GC pauses are a deal breaker, and Java won't fly there. Rust has a good chance to become a viable contender in that domain, once the toolchain pains have been figured out.
AFAIK the problem is working with types parametrized over integers, not types parametrized "with a particular" integer. The simplest example I could think of would be implementing a trait for an array of arbitrary N. Is there a way to do this in rust?
You've been waiting a long time to say that haven't you?
I remember I tried various sampling profilers on Linux (not with Rust) including perf before callgrind and the main problem was the unreadable output (this guy summarized my impressions pretty well https://lwn.net/Articles/379949/). I was staring into perf output with call graph for hours trying to understand what a hell am I looking at and where is my hot paths. Then I found some script on the internet for converting perf outputs to dot and visualizing call graphs and spent times with graphviz, it was better but still uncomparable with KCacheGrind's graphical representation of callgrind results. "The purpose of computing is insight, not numbers." (Besides Linux, Visual Studio's profiler is probably the best I've seen, but I haven't tried the Intel's one)
Thanks, that's good to know. Perhaps you want to blog about setting it up for Rust?
There may not be a simple answer (yet) to this. My advice is to try to solve a simpler problem: how can you use `rust-websocket` with an HTTP server provided by `hyper`? I took a quick peek at the `rust-websocket` docs, but the API surface is huge and I don't see how to use it with an existing HTTP server. (But I'm also not terribly familiar with web sockets---I vaguely recall that you're supposed to be able to "upgrade" an existing HTTP TCP connection to a websocket.) You might file an issue on `rust-websocket` asking for an example. :-)
As the oprofile page states: &gt; There is a delay between the counter overflow and the interrupt delivery that can skew results on a small scale - this means you cannot rely on the profiles at the instruction level as being perfectly accurate. For example, if you are profiling an application with an event that counts L1 cache misses, a sample attributed to a particular instruction in the application doesn't necessarily mean that exact instruction is responsible for that event; instead, it means the sample was taken in the dynamic vicinity of that instruction, usually with a margin of error of a few instructions.
Now that's an impressive patch, kudos!
Meta answer (sorry). For just about every language I know of, I've heard arguments for and against teaching it to beginning programmers. Frankly, I've never been convinced by any of them. I've seen people learn starting with Haskell. I've seen it with C. C++. Python. Perl. SAS. Java. Scheme. I've even taught some of those myself. There are just *so* many ways to look at this problem. What are you trying to optimize for? Maybe CS majors should learn Haskell first because of its heavy focus on leveraging types. Maybe English majors should learn Python because it's so easy to write small programs that do something useful. Maybe engineers should learn C first because it will keep them close to the machine. Or maybe all of that is wrong and everyone should learn C++/Java first because that will give them the best job prospects. And so on... I'm *not* saying: "well people have succeeded with learning C++ first, so who cares." What I'm saying is that to answer this question you need good data, and *even if you had good data*, the answer still isn't clear. For example, if someone off the streets came up and asked, "I want to learn programming. So-and-so said I should use language X. But such-and-such say I should use language Y. Which language should I learn?" You can't just answer this question on its own---you need context. What problem do they want to solve? Do they have a mentor that they can reach out to for help and guidance? How much time/effort do they expect to spend? Will they want to pursue other programming projects in the future? I think the first question is the most important: *what problem do you want to solve?* If you can push people toward the things they care about, then in my experience, they're going to have a much richer educational experience. It's definitely possible that Rust is an answer that pops out after all those questions are answered, but I think I would only say it if, *at least*, the person has a structured learning environment available to them. OK, with that incoherent semi-rant out of the way, I've like to contribute something more useful. Or rather, point you to good work done by my friend/mentor/teacher. It's an experience report published in ICFP on teaching first year CS majors with Racket: http://www.cs.tufts.edu/~nr/pubs/htdp.pdf
LLVM Optimization may reorder, change and otherwise mangle their internal code representation. However, the profilers can use the DWARF information emitted by LLVM to find the original line of code in most cases.
That'll land in the default target, debug.
I've been looking into trying to drive a quadcopter with a Pi as the flight controller running Rust for a couple of weeks now, things are looking good (though my job does get in the way). The initial cost of getting Rust running on other platforms (in terms of hours) is pretty high (takes a while to either cross compile or find the right binaries lying around on the web and you have to install Rust and cargo yourself) but once your up and running it's easy enough to access GPIO etc (exposed through files on Linux, so don't even need the FFI).
I think Rust would be good to teach systems programming, but not programming in general. Rust is not a simple, nor an easy language. It is clean, it is well thought out, but it is not simple. There are many languages that are better suited for teaching early/beginner programmers such as Lisp or Python.
Also, if you want to record callstack with`perf`, using `--callgraph dwarf` often (but not always?) works a bit better than plain `-g`.
Null Missile Exception
If you want to use a function to initialize a variable, the function has to return a value. I.e. `let new_symbol : T = symbol(name)`. You cannot simply pass the variable by reference because the compiler cannot prove that it gets initialized. If you are absolutely sure that `symbol` will initialize the memory you can use `unsafe { unitialized() }` to mark the variable as initialized but you have to be careful with this. It also seems to me that you are trying to do something strange. Especially since symbol does not take a mutable reference it must not initialize the variable. What is `symbol`doing and why does it need to take a reference instead of simply returning a value?
Cool article. In our example, the callgraph isn't too important, because it's pretty obvious from the code. I'll give kcachegrind a test run anyway.
Sorry, my question was a bit unclear. The Default trait which SirOgeon mentioned gives me what I need. For reference symbol invokes a C method which marks the memory region as symbolic, the type doesn't and contents of the data don't matter as long as size_of works on it and the memory is allocated. (In C it looks like void klee_mark_symbolic(void* mem, size_t len, char const* name)) I'm trying to build wrappers for Rust so that I don't need to use the ugly C syntax or unsafe blocks everywhere, and I want the signature to be as succinct as possible.
I know people like to complain about syntax, but there is no point, once you program in ***anything*** for a few months, you realize that you get used to it. Its basically a bad meme nowadays to talk about too many parens. Lisp has loops and classes too, they are just not used as much. Read SICP, it jumps right into code. Thats why I love LISP, you dont need to actually *know* LISP, because it is so simple, that you can build up from nothing super easily. Oh, and your fake code is actually more valid Python than it is LISP. Also, for the record, I did not downvote you. I just really think that arguing syntax is completely useless.
Most classes are a few months long. (At best 6, if you consider the whole semester.) Sure, learning functional programming (referential transparency, side effects, pattern matching, monads and applicative whatevers) is great, but hardly cover the whole of programming. I think few months of programming is just not enough anyhow. Nor just one language. (Because every language is a trade-off between different dimensions of the design space of programming languages.) And yeah, you can build up, if you know what stuff there is usually to build. If you have not spent years thinking about programming problems, that possibility to build up will just look like the endless blue sky, at an ever astonishing distance, far and completely unreachable. Syntax is interesting, and I quite like the hard requirement in Python to produce some sort of readability, and make complexity sort of linear with number of lines. Lisps usually have many simpler constructs (expressions) batched together. The parens are largely irrelevant.
I am brand new to Rust, but I have been programming for about 5 years in languages like Ruby, php, python, JS...and I feel like Rust is seriously teaching me how to program. At this point I am learning Rust mainly for my education, because I feel like it teaches so much. 
This is too general a question to get a meaningful answer. I mostly agree with /u/burntsushi on this. I'm still a student myself, but I've TAd an introductory course and have taught/helped teach programming in many different contexts. Here's what I've learned: It really depends on how much individual focus you can give. If you're teaching a single student, any language will do. But, if you're teaching a lot of students, say, a class of 150, languages like C++ start falling behind. The reason behind this is that languages like C++ have complexities that one has to deal with off the bat. These complexities don't have to do with core ideas of designing algorithms, but rather deal with the specific language. Pointers are a good example -- I've seen students struggle with them, not really understand the topic, and then be unable to proceed because the subsequent topics are taught using pointers. On the other hand, in a language like Python, you can write quite a bit of code as a newbie and understand how to convert an idea into code (which IMO is the first priority for such intro-to-programming courses) before you have to deal with confusing concepts. Whereas in C++ one often has to understand pointers and the heap before using abstractions like a simple vector. To some extent this is a result of how the language is _taught_, not how the language _is_. Really, you can get far in C++ avoiding pointers and other concepts, it's just that often the course may be planned so that pointers come immediately after basic control structures, without a breather in between. Still, there probably is an objective advantage of languages like Python over languages like C++ from the retention point of view for large courses. On the other hand, if you pick up a language like Python you end up missing out on all of the systems-y concepts (stack, heap, pointers, etc) that are needed to understand what's going on and structure programs around that. It's work to learn C++ after learning Python, it's a breeze to go the other way around. Rust is actually worse than C++ in this aspect. C++ _asks_ you to think about memory. Rust holds a gun to your head and forces you to think about memory. Those concepts of ownership and borrowing? Guess what, you have to think of memory in the same (or equivalent) way when writing C++ code for actual codebases (or refcount all the things). Rust just forces you to think of these from the start. C++ programmers think `unique_ptr` is their ally. But they merely adopted it. We were born in it, molded by it. We didn't see the the `unsafe` till we were already proficient, and by then it became nothing but *blinding*. ..... Anyway, back on point: Just like C++ has some (not-central-to-learning-programming) concepts that need to be absorbed before you can write stuff, Rust has _more_ of this. Unless you have the resources/time to give individual focus to students, you're likely to have a lot of people struggling with ownership before they even get to writing a simple prime number program. If you _do_ have the resources, this is totally okay -- actually, it might be beneficial to inculcate the idea of ownership from the start^1. But if you don't, you have a risk of someone being left behind. So take a look at _who_ you're teaching, _what_ you want to teach, and _how_ many resources (time, human, etc) you have, and pick the language that best suits you. ^1: Bear in mind, this can make it hard for folks to interface with other programmers. Whenever talking about C++ code I end up framing my words from the perspective of ownership because I often look at C++ code from a Rust POV (even though I learned C++ first); but these questions don't directly map to the model that C++ programmers have, so I end up confusing everyone. I don't do this anymore though :)
Python (an especially the setup) is not terribly easy either. The same thing is said about Ruby. I think Rust can be very well practiced at a low level.
A note, if function that operates on the pointer, then this (as written) is unsafe. The pointer passed to that function points into the stack frame of `some`, and is invalidated as soon as `some` returns.
I only recently came around to fully read the article. This is *very* helpful information for newbies, and the style makes it quite easy to follow. A small nitpuck: The **Inside Traits** section should have been called **Inside Impls**, because your example is an impl item, not a trait impl item (though it works the same for traits).
I think Rust is a good learning language. First of all, the base language isn't as hard as people make it to be. The tooling is very simple. It works well in userspace and is rather well documented. It has a very explicit way of speaking about concepts in relation to memory (it has `Sized` and non-`Sized` types for example, which makes it terribly easy to speak about why certain things works through pointers and others do not). It speaks about Lifetimes of values _explicitly_ (something I spoke about at UNI only in implicit terms). Actually, at some points, I find it so boring and standard at the beginning that I do struggle with explaining people the awesome parts without jumping ahead very extremely (towards Send/Sync and type system things). It's a very good beginners language that are actually interested in the machine. Better then C, I'd even say. Python/Ruby are good languages for people working at a very high abstraction level and that's fine. But Rust without the bells and whistles is a very good language with a very instructive compiler as well, even now. The problem is that we advance people struggle seeing that core language, as we usually work with the bells and whistles.
Hmm, I'm not quite sure what your saying. I'm calling a function which operates on the value (definitely unsafe, but symbol wraps the FFI call in a unsafe block already) straight away. The library doesn't store the pointer (though the interpreter can track where the symbol goes later that happens outside of the program) so some's stack frame being freed shouldn't be an issue. in C it would be something like int symbol() { int n; klee_make_symbolic(&amp;n, sizeof(int), "n"); return n; } The code for reference https://github.com/jawline/klee-rust/blob/master/src/lib.rs
&gt; That would bring down the entire web service, then thousands customers have to log in again to find their shopping carts empty. It shouldn't, it should only bring down one particular thread of the service. If panics were turned into aborts, it would bring down the entire service.
Where are you defining `'a`?
&gt; you're likely to have a lot of people struggling with ownership before they even get to writing a simple prime number program. Only if you are using bigints without copy semantics.
I tried learning programming with python because I was told that it is a modern, easy and relevant language and I failed. It has too many builtin things and a strong but dynamic type system which gave me errors that I could not understand. I could look into the documentation but there were so many things that went far beyond my knwoledge. Then I continued with AutoIt (basic like scripting language) and C++. It was easier because AutoIt has just few autoconverting numeric types, strings and arrays and although not being very powerful it was very easy to learn programming in general. In C++ pointers, RAII, iterators, exceptions and virtual functions were hard to understand but I could look at the code and say "function foo accepts a reference to CustomClassA and returns an integer". That made it a lot easier for me. long story short: I don't recommend python because it is too complex for a new programmer. I recommend starting with lua because it is easy and you don't have to think about types too much and then learn a good language that is made for your field of interest which is Rust for systems programming, etc.
`let&lt;'a&gt; y: &amp;'a i32 = &amp;x;` :P
Yeah people say that a lot, but a race condition is also caused by long enough. not holding a lock for
I see. I missed that `x`'s lifetime is already restricted to current scope, and as y borrows x, it may not outlive it. Thus any lifetime declaration would be either unnecessary or wrong by definition.
Yeah, that only matters if you care about 'pure' fp though
Pythons type system is quite good, and you can care as much about types as you would like. You chose to act like it had a type system like Luas, but expected one like C++. You should treat it like C++'s, and then you will get it.
Good idea. Opened [#46](https://github.com/cyderize/rust-websocket/issues/46).
I may have a rather unique viewpoint on this, but I disagree. In fact, I programmed a rather large bag of rust code before I even had to touch the concept of lifetimes or ownership. In teaching, one could conceivably give students enough structure to base their solutions on in a way that ownership simply doesn't come up. When it finally does come up, I think Rust is in a somewhat better shape than C++ to teach the concept, because the compiler diagnostics tend to be quite helpful. Once IDEs have matured a bit more, this effect will likely be amplified. That said, I have taught students in Java and Scheme. I think the latter makes for a better teaching language, but it very much depends on the material and the learners. If I had to choose, I would probably teach assembly first and work my way up from there.
Can you annotate the other forms of race conditions so the compiler can help prevent those as well?
Is there a way to name the lifetime of the outer scope? Is my question even valid?
.. I clicked this thread expecting to link to `A-community-library` and `A-wishlist`. I do think the idea would be a cool one, though, to have 'suggested starter projects'
Not really, as if you've made the mistake in the code, you'd probably make the mistake in the annotation. It's the same problem: it's a logcal bug.
I personally don't think the adoption should matter too much when you're still a student. Rust has a lot to teach that applies to other languages as well. Even if your first jobs won't be in Rust shops, the knowledge and experience still counts.
The problem comes down to definitions: you can't really define a race condition, but you can define a data race. This is because general 'race conditions' are an application-level error: they require understanding of what you're actually doing. It's like if you needed `x += 1` but you wrote `x += 2`, in the general case, you can't tell if this is an error or not.
As far as I can tell, Rust is lacking a good set of tools for on-line monitoring and debugging of applications and services. This is understandable since few production services are written in Rust right now, but if you want to be on the forefront there are some opportunities. Ideas from narrow to broad scope: * A solid [HDR Histogram](https://github.com/HdrHistogram/HdrHistogram) Rust implementation. This is the state-of-the-art for tracking latencies in high fidelity with constant-space overhead. A lot of high-performance systems use reservoir sampling for their histograms, but the HDR author makes a convincing case for non-sampled histograms. The theories and math behind it are quite interesting, if you aren't familiar with coordinated omission definitely check it out. * A metrics facade a la Java's [metrics](https://github.com/dropwizard/metrics). Much like the existing log facade, this would allow you to register and update metrics, but be flexible about the reporting backend. Metrics should include counters, gauges, and histograms (hopefully built on HDR). Reporting backends should eventually include console, graphite, open TSDB, influx, etc. * A tracing framework that can output traces to Chromium's [Trace Event](https://www.chromium.org/developers/how-tos/trace-event-profiling-tool) viewer. I'm unaware if Firefox has anything similar, perhaps a Mozillian could weigh in? This is an excellent tool for digging into performance traces and debugging performance issues. It would be great if there were a Rust library that you could instrument your service with to output traces in the correct format.
I think it is a great language to use to teach and learn about how to structure computer programs. It is not (yet) a great language to use if you want a room of 100 people to feel good about themselves and their programming skills in a minimal amount of time. I find Rust does a great job of introducing complexity only as you need it. You can do a lot with just owned data (no references), and even more with just references (no lifetimes), and once you actually get to lifetimes you understand why you need them. You come out knowing a lot more than with other languages, but not "because it was hard", but because you understand why each part is hard.
Here's one that I've done in the past (not in Rust): build something that downloads [IMDb's raw data files](ftp://ftp.fu-berlin.de/pub/misc/movies/database) and do something with it. Maybe load it into a PostgreSQL database. It is quite challenging, but will exercise lots of areas of the language: * File IO. * Downloading data from an FTP server. (Does a lib exist? If not, build one!) * Arcane parsing (there is no consistent format and invariants must be reverse engineered). * Maybe database access if you want to store the data somewhere. * Depending on how much data you want to load, there is room to benefit from parallelism. * Expose a CLI interface. * The potential for an error to occur is just about *everywhere*, so you will be forced to deal with that. I know your post kind of asked for libraries, but libraries are what we build so that applications can be built. The surest way to discover a need for a library is to go out and build an application. What libraries will you need? Are existing libraries serviceable? etc...
Got ya hehe ;p More seriously, I scrapped the previous similar threads and most of the time redditors have ideas not mentioned in `A-community-library` and `A-wishlist`, if there was a way to have something to regroup them (whether on github or reddit) would be a good start. Then again I realize that it can be a lot of work (between collection and triaging).
The difficulty I'm finding with Rust is while the sugar is good for consistent and readable code, one really needs to understand what is behind the scenes. In some ways I find typical hello world examples to be more harmful than helpful. It is often a mystery what are semantics of the language and what are implementation details of the standard library. I agree with /u/Sinistersnare that Rust could be an excellent language for teaching systems programming because things that seem annoying are real concerns to consider, and through best practices the programming doesn't have to be arduous. Here are a few examples of things that I have found misleading while learning rust. Some of it is now better explained in docs, some of it is not. Maybe this will help some other Rust novices. **The Prelude** Types like Box/Option/Result are implemented fully in standard library and don't use compiler magic. There is just an invisible "use std::prelude::v1::*;" assumed at the beginning of each file. Relying on the prelude hides the fact that a Trait must be brought in scope before it's methods can be used. This is different than simply bringing the object's type into scope. This makes it look like something such as ".clone()" is special or there is an Object-like base class, rather than the truth that it is no more special than Traits you can write yourself. **For-Loops and IntoIterator** Until you dig into the API docs for std::iter, it is not clear that "for x in expr" takes an expression that implements std::iter::IntoIterator, rather than std::iter::Iterator. A trick here is that a default implementation of IntoIterator for Iterator exists returning self. This lets an iterator be using directly by a for loop. The reason this distinction is important is the Vec type is *not* an iterator, but instead implements IntoIterator. This is why you can use Vec in a for loop, but are unable to call iterator functions such as "map" directly. Another way in which Vec is deceptive is that "for x in &amp;vec" works because there is a custom implementation of IntoIterator for "&amp;'a Vec&lt;T&gt;". This uses an iterator that yields references to elements. Your own iterator would not behave the same unless you implemented that. This should all be contrasted to how "0..10" range expressions behave. They *are* iterators and can be used in for loops as well as directly calling things like "map". **Copy / Move** Each type has either Copy or Move semantics. The default for new types is Move, but you can use implement the Copy trait (manually or with #[derive(Copy,Clone)]) to change. The integer types are Copy which is why they behave as expected in sample programs. When we say "let x = y;" it will either copy or move 'y' depending on the type of 'y'. Note that these semantics apply to pattern matching too. In "let (a,b) = y;" the components of y will be copied *or* moved into a and b. If any components are Move, the whole expression is a Move. **The 'ref' Keyword** This is something that is encountered quite quickly and is easy to misinterpret. This "let ref x = y" is equivalent to "let x = &amp;y". Remember that "let Some(x) = Some(3)" works like "let x = 3". You can think of "let ref x = y" as "let ref x = * &amp;y". Then 'ref' matches ' * ', ending up with "let x = &amp;y". In this case there is a borrow to y. **Copy / Move / Borrow** Consider again pattern matching and let expressions. The compiler identifies a match as one of: Copy (leaving source alone), Move (consuming source), or Borrow (borrowing from source with lifetime constraints). Writing "let (ref a, b) = x" will be identified by compiler as a Borrow match. It is important to note that if 'b' is a Move type, this will result in a compiler error. The borrow type expression will leave 'x' alive and take a reference to satisfy 'a', but will then be unable to consume 'x' to satisfy a move to 'b'. Mind you that if b was a Copy type, it would be copied and 'x' would remain alive, making the statement valid.
Here are so many great answers (and I haven't read them all) -- I just wanted to add a little bit of my experience: I've been teaching programming a bit (mostly as PA at my university). And I've seen people struggle with nearly everything. Some programmer say OOP is super realistic and easy to understand because one can map the whole world as a collection of objects. No: Many beginners have problems with understanding OOP. Some programmer say that language X/feature Y or paradigm Z is the easiest to understand. No. Some beginners understand certain things better than other things. So in my experience it doesn't matter too much what the first language is as long as the beginner is motivated. And getting enough motivation is IMO the most important part. To get less meta: I think Rust is suitable as first language. As is pretty much everything else.
That is a very exciting idea. I've been a bit trepidatious about contributing to Rust, but this may be the perfect place to start. I will definitely, at the least, explore it.
Does this count as a meme, or is that only image macros?
That sounds awesome 🙆. You could do the world a big favor by documenting your efforts.
Yeah, the rule is about image macros. Off topic threads that go on for pages with just inside jokes might also be unwanted, but IMO the above is okay.
nice! I figured there must be something out there like this. This is super cool, thanks!
I wonder if Lua being 1-based would cause an issue. I remember picking Lua easily when I needed to, but stumbled on array indexes regularly because of this oddity, and I imagine it might be an unnecessary hurdle for students who first experience another language (either coming to or going from Lua).
You can define a named struct that implements the desired Fn[Mut/Once] trait and use that instead of a closure. To keep it clean, wrap the abomination in a single-element tuple struct when you specify the return value.
That's a very interesting perspective. I learned python many years after C, C++, Assembly, etc. After getting over my "this looks too much like Visual Basic" hangup, I found it liberating, and super-fast to develop with. For BIG programs, I do find the lack of typing in python scary, though. It had never occurred to me that beginners using the language might run into the same problems for smaller programs, but that seems quite likely, now that you mention it.
Yea, I started looking into that but even just a chain of 3 flatmaps gets so nasty. I guess I'll take the overhead of the boxed iterator for now. looking forward to unboxed abstract return types if/when they ever make it out.
For a dynamicly typed language it is. It can be quite expressive with its types. There are even [abstract base classes!](https://docs.python.org/2/library/abc.html)
Oh we got most of our hits on hackernews last time so if you can upvote it... But for serious, so much has happened since we recorded the interview part of the show. We will need a followup
This is a really really good answer. People are too complicated to neatly fit into the boxes using the easiest language suggests. Some like self study, some don't. Some like reading, some like it taught verbally. Some have experience, some don't. *Everything* is always different. A better question might be: "Can someone learn said language fairly directly without getting completely stumped at every step?" By that metric, any language with decent (decent, not awesome) docs and a really helpful (and well advertised) IRC channel *might* be easy. There may be exceptions though but IRC goes a long long way to making everything easier.
This is a big problem for languages like Matlab which caters to Engineers who never claim to know how to program but want to get things done. As such, Matlab avoided a lot of concepts a C/C++ programmer might need to know in order to be expedient. For example, Matlab uses (maybe it has changed since then. it's been a few years...) a path based namespace system which is polluted with names; no such thing as `from::this::to::that` annotation. This can make more experienced people look for ways to do more complicated things which aren't handled or taught well by Matlab. So, long term simplistic can be a double edged sword. Easy isn't necessarily the best thing.
Even in fairly simple languages like python a learning tool like Karel the Robot is usually used for first semesters – which might be a neat project to `repl`icate in Rust, BTW.
As a recent example, someone on HN was _furious_ with me the other day, because the book explains what a loop is. They would argue that since Rust isn't for just new programmers, everyone already knows what a loop is, and therefore, I am wasting their time. I also get tons of compliments about how the book explains things thoroughly. Different people are different.
Do you know of a language which does specialization without the confusion you're citing here? I can see your example as confusing but I don't know if there is an alternative which would avoid it.
There was an OO research language called beta that required the programmer to define explicit extension points for classes, which subclasses could then implement. It kinda turned the OO paradigm we know now on its head. Today it's largely forgotten. I'm on mobile, else I'd search a link.
The lifetime does not need a name anyway, it's fully defined by its scope.
Right, I guess what I mean is, stating the invariants has the same problem as the condition itself: human error when describing it.
I don't think it's possible to give too simple of explanations. You can give badly organized explanations, or ones that lack detail, but even the basics are useful in a reference.
Understandably HN will detect voting rings and demote posts they target. Direct links to posts look a lot like that sort of vote abuse (a lot of incoming visitors with the same referer), so the post appears to have been killed. (They kind-of are exactly that sort of vote abuse...)
From looking at the [gBETA tutorial chapter 7](http://www.daimi.au.dk/~eernst/gbeta/) which covers specialization, it doesn't actually look that different. It does state that there is a different philosophy regarding how subclasses modify superclass behaviour though I'm not sure what specifically that means. They state subclasses should never have to adjust superclasses (maybe meaning the subclass cannot call through the superclass? Is calling through the superclass something that is typically done? Does this new RFC support this? I didn't notice it.). This is supposed to force superclasses to be more general so they don't require respecification later. 
It's true. This is compiling fine for me too. I have not updated the compiler. I am not sure what happened. Anyway, I will update the blog post soon to reflect this. Overall a good news.
There's scope for representing `Option&lt;bool&gt;` as 1 byte: enum representation is defined as unspecified (except for some specific cases) for exactly that reason. However, doing more compaction that that is difficult/less guaranteed: Rust allows essentially arbitrary interior references, so any value needs to be able to have a `&amp;` taken to it and it's not possible to address individual bits. Specifically, given an `Option&lt;T&gt;` it's possible to create a `&amp;T`, and it has to behave exactly like a normal `&amp;T`, i.e. the thing it points to has to have the same representation as a "normal" `T`. There's a few proposals for allowing more control e.g. [RFC 311](https://github.com/rust-lang/rfcs/issues/311), or a `Cell`-like data structure that disallows interior references. (To be clear `Option&lt;bool&gt;` can avoid the reference problem, by ensuring that the `Option` wrapper leaves the `bool` as is: `Some(false)` == `0u8`, `Some(true) == 1u8`, `None == 2u8`.)
Good work! :) I hope this isn't discouraging... there's also [maplit](https://bluss.github.io/maplit/doc/maplit/index.html)!
But Wikipedia usually gives you the choice to go directly to the source - this page didn't when I visited.
sorry :(
&gt; The unit of compilation is a crate That's good to know too, thanks again! 
At my university, every engineering student needs to take a programming class. It's usually C++. Or rather, it's mostly C, but C strings, malloc, scanf, etc. kind of sucks, so what's taught is a C++ that's a lot like C. [Like this](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s096-introduction-to-c-and-c-january-iap-2013/lectures-and-assignments/core-c-control-structures-variables-scope-and-uninitialized-memory/). Now, it seems a pretty limited language, but consider the topics that are seen: * Variables and scopes * Control structures: if, while * Primitive types: ints, floats * I/O, both to read and write numbers and strings * Defining functions * Pointers and arrays * Structs, unions, typedef * Allocating memory on the heap: new/delete * Header files Rust doesn't really have much to offer on most of those topics. The strengths of Rust come to play when you start dealing with complex data structures that must be heap allocated - then you need to track their lifetimes to properly deallocate. But what happens is that the kind of programs the students write in the course are short running, so it's a perfectly viable memory management strategy to deallocate when the program ends (and indeed, it puzzles students that the "delete" don't appear to actually do anything important, and if removed the program continues working). If proper memory management were an important topic in this kind of introductory class, the C++ instructor would tell people to use unique_ptr and not bother with new/delete or malloc/free. And then we would have some of the same "complexity" of concepts that Rust introduces, except without proper compile-time checking. But as you see, even in that MIT introduction of *"C and C++"*, the instructions go into C++ topics up to generics and inheritance, but don't bother mentioning smart pointers. Smart pointers are considered an "advanced" topic. The only way for Rust to add something to this kind of introductory class is if we consider owned values a core programming concept. Rust would naturally lead the professor to discuss ownership and move semantics, in a class that he would probably not bother with this subject.
Great write up! very nice work. I do think that senders should return a result on send, and err when there are no receivers -- I feel like that is a common flow in rust and simply blocking seems like a pain to handle manually. ~~How did you handle select? Does it consume cpu or extra threads?~~ I wrote a simple spmc in rust and found it rather slow too, I'm curious how to optimize it; do you plan on optimizing chan? edit: I understand select a bit better, really like that solution! I did something similar in [promise](https://github.com/viperscape/rust-promise/blob/master/src/promise.rs#L113) but moved away from using condvar directly
Cool! I think I didn't understand that proposal (and I didn't see any other), could it result in the compiler interleaving the components of types to preserve alignment of their fields? Also, does Rust have something like `#[align(N)]` to force alignment the overall type to a N bytes boundary?
&gt; I think I didn't understand that proposal (and I didn't see any other), could it result in the compiler interleaving the components of types to preserve alignment of their fields? As soon as the compiler knows there's not going to be any internal references it can mangle/mix-up fields as much as it likes. &gt; Also, does Rust have something like #[align(N)] to force alignment the overall type to a N bytes boundary? Not yet.
Wonderful, thanks.
I think this is really cool and I'm going to use this in my project now! I feel like the type system could be leveraged more so that we're not performing certain forms of synchronization we don't need to at runtime. I'm excited to see what other APIs people come up with.
Oh, also the Rust book chapter on [traits](https://doc.rust-lang.org/book/traits.html) and [trait objects](https://doc.rust-lang.org/book/trait-objects.html). Traits sometimes feel like vanilla class-based OOP: when a type define a trait, you can call its trait methods (after you import the trait). Trait objects are even more OOP-like, because it lets you treat different types that implement a trait as if they were the same. But the mechanism used in Rust is trait implementation instead of inheritance. The biggest difference is that you would define inheritance at the same place you're defining your type, but you implement traits after you defined your type. When you export a type through a library, the library users may implement further traits (provided they define the trait themselves). That's how traits are "composable": you define traits for each aspect you want to model, and the types may implement any subset of them. You don't need to fit the types into a rigid hierarchy. But another thing that traits don't do is to define implicit storage. For example, in C++ you could inherit from a parent class that stores an int, and in your child class read and write to it. In Rust, every type that implements the trait must specify where to store this number themselves (and they could make different decisions, like to put it behind an Rc and such). My understanding is that this is what some kind of Rust inheritance would be about: providing some kind of "virtual struct" so that child types could implicitly contain it. Traits also provide a loose analog to mixin / multiple inheritance because a type may implement many traits, and of course you can call the different trait methods in a type. To disambiguate between methods with the same name, one must use the [universal function call syntax](https://doc.rust-lang.org/book/ufcs.html): Rust doesn't do overloading. Anyway: I think it would be unfair to say that Rust doesn't have OOP. The C++-style class-based OOP is itself very different than the prototype-based OOP of Javascript and Self, for example. I think Rust has a kind of OOP, but with its own flavor.
I've had mad fever dreams in the past of having the compiler track "dead bits" and "dead values". So, `bool`, for example, would have bits 1...7 be marked as "dead", meaning that when the compiler sees `Option&lt;bool&gt;`, it can just steal bit 1 for its own discriminant. Of course, `Option&lt;bool&gt;` *thus* has bits 2...7 marked as "dead", so `Option&lt;Option&lt;bool&gt;&gt;` just uses bit 2... and so on. Dead values are the fun ones, though; they would let you say things like "all these NaN values with non-zero payloads are all dead to me". The compiler could then use the dead values as tags, so that `enum X { A(f64), B(i32) }` wouldn't even *need* a tag; it could just re-use the NaN payload automatically. ... but it's all *probably* way too overcomplicated for the moment. :P **Edit**: Also, the above would motivate the introduction of arbitrarily-sized integers :P
What happened to the "no trailing commas" version of the select macro? :P
I was meaning more forcefully: a wrapper/annotation that ensures there can never be any (ala RFC 311 linked above).
The compiler has some infrastructure for doing this sort of thing in general; the null pointer optimisation tracks non-null-ness through layers of data structures, but I don't think even that uses all the info it could yet. (Incidentally, `Option&lt;Option&lt;...Option&lt;bool&gt;...&gt;&gt;` can do better than dead bits: using just values, it can be a single byte all the way out to 254 `Option`s.)
Yes, it's possible (and this is probably the nicest solution), but it's still not the same as `Option&lt;bool&gt;`. If I, for example, expose this type in a public API, every user is confused and needs to look up that type. `Option&lt;bool&gt;` is more understandable.
We are talking about a _teaching_ language, not if it is an extremely simple language you can pick up on your own. Making all these concepts explicit makes rust a _very good_ teaching language, not necessarily a simple one. (I started learning programming with ML, which a professor tough me - it's been a very good choice)
Storing `Option&lt;bool&gt;` on the stack isn't too costly, so you can *store* an `OptionBool` internally and return `Option&lt;bool&gt;` to the outside.
I switched a networks project from Go to Rust and one of my favorite features of rust stdlib channels was that senders could tell if they were sending on a closed channel. A lot of my programs have threads that produce, and threads that consume. It's pretty nice to be able to tell if a producer thread is basically useless.
As others have said, ipc-channel and gaol. Note that gaol is bitrotted at present; PRs to fix it on nightly/stable are more than welcome.
If the discriminator is tacked on at the end everything would still be aligned, no?
Well, it works on Rust stable for one. Also uses no unsafe. Comm's select api is also quite different.
Be aware when working with boxes that the compiler often has trouble with deciding whether to make the deref'd object mutable or not, so you might have to use the Deref and DerefMut traits explicitly. I had this problem in my last project, hopefully it gets fixed for the next stable release at least.
&gt; I mean I myself tried to teach someone python, because they were not very familiar with programming and wanted to learn. However, they just found it confusing and didn't have much drive to try and use it. Then I introduced them to C# in Unity3D and they took off like crazy. Spending whole weekends in front of their computer trying to learn to use it on their own. Yup! I maintain a couple of Python projects that attract a lot of people who have never programmed before. But the projects are specifically about a domain they *really* care about, so they spend all manner of time figuring out how to get their Python setup working to solve the problem they care about.
Haha! :) The exact same thing happened to me. I wanted to cover how ownership/borrowing/lifetimes ensure memory safety without relying on GC and how concurrency works in a 60 minutes slot. But I somehow ended up only talking about the first topic. I liked the audience, too. Smart questions.
One way to do it is to use RC and RefCell [here](http://is.gd/l4mYEW), the parent references are what's making the whole thing more difficult, so by using Weak&lt;T&gt; you can store the reference without having to fight the borrow checker. The important change to the Row struct #[derive(Debug)] pub struct Row { data: Vec&lt;Vec&lt;u8&gt;&gt;, rowset : Weak&lt;RefCell&lt;RowSet&gt;&gt;, } The RC will prevent mutability of the interior struct, so the RefCell is used to allow the mutability again. Weak&lt;T&gt; requires nightly, RC&lt;T&gt; would also work, but it creates infinite loops in the debug output. PS. This is just how I'd do it, not sure if there is a more idiomatic way to do it while maintaining parent references.
&gt; Rust allows essentially arbitrary interior references, so any value needs to be able to have a &amp; taken to it and it's not possible to address individual bits. Specifically, given an Option&lt;T&gt; it's possible to create a &amp;T, and it has to behave exactly like a normal &amp;T, i.e. the thing it points to has to have the same representation as a "normal" T. To further illustrate why this is important, you can look up `std::vector&lt;bool&gt;` fiasco in C++.
I think you want the `filetime` crate: http://alexcrichton.com/filetime/filetime/index.html
Eh I don't mind downvotes, but based on my experiences as a freshy in intro to cs (I just graduated in may so no too far back...) lisp was alien in multiple ways. We followed a sequence in programming labs that followed machine abstraction write some code in binary for simulator -&gt; assembly for the sim -&gt; Java for a high level language (and of course a ton of labs on algorithms as well, but those are not so relavent) For one lab near the end we had to use Scheme or Racket (lisp dialects iirc) And no one really got them, basically our whole class thought &gt;This language is terrible, lets get back to java please? It was not easy to understand at all back then, and I doubt it would be now. java/python/c/rust all feel better for intro to me because they follow the machine architecture pretty well. If I had to teach folks intro CS I'd probably pick C its dead simple, and a good intro to footguns too.
I was thinking of using something like: struct { opt: i1, inner: i1, } And then you could obviously fit that into an i8.
I think your problem is that you are thinking in references, where in Rust the "&amp;" symbol is a borrow. Every object you have a borrow to you are the current owner of. So circular borrows rarely makes sense. You can get around this issue by using a refence counted wrapper Rc&lt;T&gt; or you can ask your self if you really need a reference to the rowset in the row struct. Is it ever really needed, and what happens if someone were to break your internal consistency?
&gt;The only problem is that the online docs are built for linux and so they don't list anything that is Windows specific. This sounds like a bug.
https://github.com/rust-lang/rust/issues/1998
Shouldn't that be `FileNotFound`? http://thedailywtf.com/articles/What_Is_Truth_0x3f_
https://github.com/kud1ing/awesome-rust, but for things to write
Since Rust has interfaces it also has dynamic dispatch and this is what you need to implement method overwriting (aka inheritance). Again this blog describes how to do this in Go: http://objectscape.blogspot.de/2013/09/inner-pattern-to-mimic-method.html I believe this can be done the same way in Rust.
Good talk. I have found that many Rust talks open with background and motivation for the language and yet they usually -- and contrary to what I fear every time -- don't waste much time on things I've already heard.
Drop `collect().chars()`. Just use `s.chars().rev()`.
I don't have a woking Windows machine to try this out. Do you know if this is using cargo? The directory structure is tantalizingly close to cargo: * src/main.rs * target/release
AFAIK it does not yet use cargo, but that's planned for the next major version: https://github.com/PistonDevelopers/VisualRust/issues/3
&gt; The Visual Studio team recently added an extension called Visual Rust, ... Wait... is this... is this official? */excite* &gt; Unlike Java or C#, in Rust the programmer cannot assign a specific data type even if he wanted to. Uhhh, wat? fn main() { println!(“#### 60 second countdown ####"); start_count_down(); } I'm sorry, but *smart quotes* in a code example? I'm hoping this is just some egregiously stupid auto-formatting thing in their CMS. *goes digging* I *think* the author is talking about [this Visual Rust](https://visualstudiogallery.msdn.microsoft.com/c6075d2f-8864-47c0-8333-92f183d3e640), which is [this Visual Rust](https://github.com/PistonDevelopers/VisualRust), which is under the Piston umbrella... which makes most of this article either wrong or *deeply* misleading. Also, if it *is*, then it also [doesn't have Cargo support](https://github.com/PistonDevelopers/VisualRust/issues/3) which radically limits its usefulness, given the current state of the ecosystem. ... Still, it's cool to see this getting some press, and it's further along than I realised. So, to the people working on Visual Rust, I believe I can say on behalf of the Windows Rust devs: keep up the good work!
You disable it at compile time, I think.
Are there plans to stabilize arenas?
I have an `optional` crate that contains the next best thing: `OptionBool` Edit: And now also an `Optioned&lt;T&gt;` type with impls for u8..64/size, i8..64/size and f32/64.
There's a conditional in the log!() macro that first tests for cfg!(log_level = "off"). Because cfg!(log_level = "off") compiles down to a bool, LLVM can optimize out everything in the macro.
See [this previous thread](https://www.reddit.com/r/rust/comments/33tk1a/crossreferences/) for some more discussion.
&gt; reversing all chars is not the same as reversing a string in UTF8 due to combining characters UTF8 isn't relevant, `chars()` works codepoint-wise. Reversing in UTF8 would break even more (e.g. `0xC3A9` "é" would reverse bytewise as `0xA9C3` which isn't even valid UTF-8). You're correct that a codepoint-wise reversal has large chances to break, though it's usually not going to be noticeable in western languages and NFC.
Use https://doc.rust-lang.org/stable/std/iter/trait.Iterator.html#method.by_ref let area_code: String = raw.by_ref().take(3).collect(); let exchange: String = raw.by_ref().take(3).collect();
I admit I had to search the docs to find that because I was curious to know answer. I looked for Iterator methods that took &amp;self as argument vs self like the take method does.
That's actually really good to know. I didn't think of searching based on the self parameter. I mean, it's obvious if you think about it, but totally not something I've learned to think about yet.
Actually, in the past a lot of the Russian physicists would learn physics in Russian and sometimes not know much English. It was the English/American physicists who would learn Russian. Physics was published in both languages, but it was realistically possible to get by with either. English took over relatively recently. Though I agree with your point otherwise :)
&gt; The next big thing, aside from the usual fixing, polishing and documentation work, will probably be to implement support for `multipart/form-data`. *cough*^(https://github.com/cybergeek94/multipart)*cough*
`T` can be any type that implements `Wrapper`, including `Vec&lt;T&gt;`, so the two implementations are conflicting, and the compiler does not know whether to pick the implementation for `T` or `Vec&lt;T&gt;` when you supply it a `Vec&lt;T&gt;` With impl specialization it would become possible to override more general implementations, like `impl ... for T`, with more special implementations, like `impl ... for Vec&lt;T&gt;`. [The RFC for impl specialization can be found here](https://www.reddit.com/r/rust/comments/3d6ivg/rfc_impl_specialization/) The workaround, for now, would be to simply not implement `Lock` for `Vec&lt;T&gt;`, since it's not actually needed, at least according to your implementation of `Lock` for `T: Wrapper`. Edit: I realized my reply might be inaccurate if `Vec&lt;T&gt;` doesn't implement `Wrapper`. I'm unsure.
Yay!
:D
I think the comments are doing a pretty good job of pointing them out. And wow, that is a lot of misinformation.
Vec&lt;T&gt; does in fact not implement wrapper. That is exactly what is throwing me offguard
If anyone other than me was curious *how* by_ref works, I've put together a little experiment https://play.rust-lang.org/?gist=86ebcb53dafa675bf269&amp;version=stable The by_ref() trick relies on the fact that there in an implementation of Iterator for &amp;Iterator which uses the original iterator without consuming it entirely. ~~A reference to an iterator is a copy type, while the original iterator may be a move type causing it to be consumed by take().~~ EDIT: Here is an updated example distinguishing mutable vs immutable references https://play.rust-lang.org/?gist=e013f016388b39e8de5a&amp;version=stable . Thanks /u/Kimundi
It’s just a [known bug in `#[derive]`](https://github.com/rust-lang/rust/issues/25022). There’s no actual difference between `T: Copy` and `T: Copy + Copy` (or for any other trait).
Huh, didn't think about that possibility. Thanks for the quick answer!
It doesn't need to – it suffices that Vec's components implement Wrapper.
If the author reads this: 'and than' -&gt; 'and then'
Yeah, the newer community versions of visual studio all support plugins now.
You can’t. After thinking more about it, it's probably better to use Iterator instead.
Quite a bit has been written here, most of which I agree with. If you're willing to humor me, I've had some thoughts recently that roughly pertain to this, though it's much more "looking forward" than currently descriptive. Pedagogy is extremely interesting, especially as it relates to the topic being taught. I've found that the teaching style used in references tends to be different between target subjects, but very uniform within them. For example, a book on cooking is likely to take roughly the same approach as most other books on cooking, not in terms of _what_ is taught but how instruction is given. I believe that this is due to the amount of process inherent to the field, whether institutional or simply procedural. Programming is interesting because it tends to be taught in two very separate ways, a more systematic or mathematical way and from a more hands on "explore and break things!" way. Each approach has it's pros and cons, but they tend to exist fairly separately. A spectrum between the two methods exists, but rarely is there a well written guide that blends the two effectively. This mainly pertains to static references like books, articles, videos, interactive tutorials, etc. as a good teacher can actively switch according to the student's needs. The systematic method explains things very clearly and gives reproducible processes that help people grab on quickly. It tends to stop being helpful, however, when you are presented with a new scenario that requires adaptation. This is a large part of programming, but to a beginner the off-the-cuff mutations and variations that can be made are not immediately obvious and it seems that people get lost very quickly. This frustration is huge. The hands on approach tends to give small problems and let people work through them themselves. This is great for building intuition and knowledge of the process, and aims to bolster improvisation and creative problem solving. Unfortunately, it also leaves so much open that plateaus and road blocks occur very frequently, and there is no guarantee that the optimal solution will be arrived at when it could easily be demonstrated. Large stretches of "well, what now?" occur that lead to great frustration and aimlessness in the student's practice. In general, leaning on one method takes away from the other. More direct step-wise instruction? Then it will be more specific to one use case. Exploratory "exercises left to the reader"? Sure it'll lead to understanding, but at a large risk of sending the student down a maze they may not return from. So what does this have to do with Rust? I believe that, with careful writing and attention to structure, Rust instructional material is in a particularly good spot to lead beginners with both methods without compromising either one. This is due to Rust's strong type system, ability to jump between imperative and functional styles, and strong opinions on lifetime and ownership. The compiler is so specific about what it will accept, that the systematic approach is inherent to the actual practice of programming. What this means is that (on a beginner level) there is more of a "right answer" than there is in other languages. This can be incredibly comforting. The compiler's ability to check your work can act as a guide to furthering your program and lead you through ruts. Yes, there will be fights with the borrow checker, but if exercises are carefully structured with increasing difficulty, getting used to the borrow checker doesn't necessarily have to be any harder than gaining the feel of how hard to hit the strings when learning guitar, or how much difference changes to the amount of sugar in a cake makes. It allows you to _explore_ in a way that is also systematic and led, and the only crutches that are there are also present when you are just casually programming in Rust. High level concepts like code structure and design can be thought of on a hands-on level, but still be led systematically by the compiler. While I think this is enabled by the language, it is not a given that instructional material would use this approach. Care would need to be taken to talk about "programming" concepts in an exploratory way and "language" concepts in a very concrete way, and let the exercises/discussion blend the two. It would be very interesting to see how well people understand the lifetime and borrow system when learned organically, which I believe is happening more than people realize. /u/llogiq has mentioned that he wrote a large amount of Rust before really needing to do anything fancy with lifetimes, and that happened with me as well. I think if introduced in a way of "the compiler is right," then pair-programming-with-the-compiler while working on a high-level program you'd have to figure out how to build, would be a very good way to learn. Anyway, this was sort of a rambly brain-dump of some stuff that's been floating through my head recently...hope it's at least sort of relevant :)
FWIW, `map_err` can be used for quick-n-dirty error handling: try!(reader.read(&amp;mut buf) .map_err(|e| /* Convert e to desired error class */ ); This also works well with `and_then`: reader.read(&amp;mut buf) .map_err(|e| /* convert */) .and_then(String::from_utf8(buf)) .map_err(|e| /* convert */) A separate error class might be more elegant though.
Thanks buddy. I also added some issues that would improve the implementation drastically. (e.g. allow resizing etc...)
You are basically right, but note that Iterators use mutable references, which are not Copy types.
I opened an Issue for this problem. Maybe you can help out with the implementation? https://github.com/seiflotfy/rust-cuckoofilter/issues/2
it can wrap `std::str::from_utf8`. That's currently the way to make sure the input is correct UTF-8 data.
Example taking &amp;[u8] and decomposing it into mutiple &amp;str: [INI parser](https://github.com/Geal/nom/blob/master/tests/ini.rs)
Is it because `Path` (in Operation) is like `str` / `[u8]`? So you need to use some sort of a reference (depending whether you want it borrowed / owned / shared...). And if it's not borrowed it would probably be PathBuf instead.
Thanks :) The project is hosted here https://github.com/jawline/PiFC The physical library covers GPIO stuff. The rest of it is is just an abstract flight controller 'core' and web monitor as well as config readers but none of it connects to anything physical yet (Soon hopefully! if I ever get time to work on it)
I was just using UTF8 because that's the encoding we're using in Rust for the most part. And because UTF8 is unicode, codepoint-wise reversal is just as relevant for it as for any other unicode encoding.
What would be the advantages of nom vs rust peg https://github.com/kevinmehall/rust-peg ?
I had started this [wiki](https://github.com/hyperium/hyper/wiki/Related-Projects) for that. It's not really been kept up to date... 
I have not used rust-peg, but since I wrote nom, I can give you some advantages I see right now: rust-peg uses a syntax extension, while nom uses macros. Syntax extensions are more flexible, but, as far as I know, still not supported in rust stable. nom comes with a lot of existing combinators, and tooling to debug parsers, manage errors, and building streaming parsers. Basically, I wanted a real toolkit to write parsers. BTW, if you want to try writing a parser, there's a list of wanted formats [here](https://github.com/Geal/nom/issues/14) :)
Yeah if you look at the paper attached to it it compares it to several types of bloom filters!
Ah yes, I missed that distinction
Oh, it landed here as well. Nice. :D I hope I got ISO8601 parsed right. /u/geaal was really helpful, discussing with me and proof-reading that post. nom is really quite nice and I hope I get my rdb parser done in the next weeks. 
I would particularly interested in that as well. I have used rust-peg and it seems much easier to use (at least for me). Is nom faster?
We tried that too, however we ended up having a RaftError::io and a RaftError::capnp, and decided we'd rather compose them.
I've been using parser-combinators for a toy project to learn rust and found it really pleasant to work with, so I'm looking forward to combine! My only complaint and it's not much of one, is that the documentation is lacking, so I just read the source (which is very readable!). Having an overview of all the parsers and examples of their use would be all I could really ask for. Thanks for creating this!
Thanks for the kind words! I have a short example on most of the parsers ([Example](https://marwes.github.io/combine/combine/fn.between.html)), do you want an overview of all parsers or do you want better/more complete documention like in the example?
Ah, so that's exactly the compiler-internal dead bit idea Quxxy was saying?
`A-wishlist` and `A-community-library` were mentioned above, those take care of the first two. Unsure about help wanted.
No idea, to be honest.
Kinda, but said in a nicer and easier way to understand (and also probably far easier to implement than "dead bit analysis")
Basically, you want to tie `Player`'s lifetime parameter to lifetime of world in these two functions: fn exec&lt;'w, F&gt;(p: &amp;mut Player&lt;'w&gt;, f: F) where F: FnOnce(&amp;mut Player&lt;'w&gt;) fn move_player&lt;'world&gt;(player: &amp;mut Player&lt;'world&gt;, world: &amp;'world World) If you don't annotate an input lifetime, it means for Rust "for any lifetime", which doesn't work for your case (you don't want any Player to be argument. You want a Player which contains references with the same lifetime as the world). Besides, you wrote: fn move_player&lt;'world&gt;(player: &amp;Player, world: &amp;'world World) That `'world` is not connected to the `'world` you've defined in impl before and is totally redundant here. Your signature is equivalent to any of these two: fn move_player(player: &amp;Player, world: &amp;World) fn move_player&lt;'a,'b,'c&gt;(player: &amp;'a Player&lt;'b&gt;, world: &amp;'c World) (specifying an explicit lifetime is only useful if you want to tie more than one together). Another thing is that you need to add `mut` in few places (or create `Cell`/`RefCell`/atomic/mutexed etc. types). Full code [here](https://play.rust-lang.org/?gist=5c2cd5d0b22af73464c6&amp;version=stable).
I don't see why you need polymorphism for this. Your xml parser could use its own state for determining whether it is currently parsing an object's property or a tile's property. I'd do it like this: create a function that parses a tileset. It returns something like a Result&lt;TileSet, ErrorStuff&gt;. It literally does everything that's needed to parse a tileset, including parsing all the tiles. Obviously the parsing tile stuff is delegated to another function, that knows that it is parsing a Tile. This another function can return something like Result&lt;Tile, ErrorStuff&gt;. Then inside the parse_tile function you can call the parse_properties function that just returns a Result&lt;Vec&lt;Property&gt;, ErrorStuff&gt; and you can put Vec&lt;Property&gt; into your newly created Tile. Your parse_object function can reuse the same parse_properties function and just put the returned Vec&lt;Property&gt; into the Object struct before returning it. That is all. 
About the `Result` thing: I looked into it a bit and I don't really think `Result` is fitting here (though, you're right in principle of course!). Currently the data structure does not resize itself, so `add` could fail. This will change in the future, so `add` won't return anything. `delete` on the other hand may fail, because the element was not in the set before. But I think a bool is what you want here, considering how `delete` will be used. Like I said above, I'm currently working on a complete cleanup. Will link my PR soon, so everybody can comment on that.
Someone please put a Ladder with a Rust sticker on it right next to that sign.
Good catch, clarified.
If you're gonna be faffing around with bit-level parsing, may I suggest taking a look at [pnet_macros](http://octarineparrot.com/assets/libpnet/doc/pnet_macros/index.html)? (see the `pnet_macros/` directory in https://github.com/libpnet/libpnet) I've written a ton of code for parsing out arbitrary chunks of bits from an array of bytes, it could be useful for you. If the code is useful I could split it out into a separate crate. I've only tested it on nightly, but all the actual parsing stuff should work on stable (I only test on nightly since it's part of a syntax extension...).
I don't think the sign is still up anymore :(
I've had the same experience. I eventually want to do some snmp work with rust but every time I look into implementing asn.1 I just quit. I hope someone eventually follows through.
Why are people downvoting this? It's critical, but that doesn't make it bad.
I was with you up until you said C++ was the wrong tool for the job. C++ concurrency is decent in C++11 and there are libraries out there to take a higher level approach. OpenMP parallelism is not bad either. For a large system Erlang might work better, but C++ has a lot of solid fundamentals. 
Yes, you need multiple loops. I guess it could be done with a single loop and a state machine in a single function but I don't like that idea. 
I came down a bit critical on this, too, but for a far easier reason: I'm not a fan of making practices that are worthy of aspiration equivalent to unobtainable. I've heard more than once that otherwise-good programmers are afraid to study the nuances of cryptography, multithreading, and so on, because there's a cult of "it's completely impossible to do right," which puts a lot of people on the "why even try?" path. There's a very, very, very thin line between acknowledging these areas of programming as extremely difficult to get correct and implying that they're out of reach of all but the most "elite" programmers. One of them stymies people from becoming better engineers. The other doesn't. This joke is fun, but also kind of falls into the negative aspect a bit, implying nobody is worthy of doing it right.
indeed lambdas have helped quite a bit and for *paralellism*; the last time I looked rust &amp; C++ aren't so different for the parallelism case I was interested in- it still required some unsafe code, for things the checker couldn't handle. But I do appreciate a lot of what rust does (immutable default, and globals requiring unsafe is one of its restrictions that I really do like) 
Sorry, I was trying to make an analogy. If `BitVec` is plausible then `str` doesn't seem out of the question. I'm surprised that `BitVec` is in demand, it seems like it'd mainly be for compression like gzip.
Combinators do not care about the input or output types, that's the reason I used macros. Most of the parsers only care about an input that can be sliced, so `&amp;str` should be fine.
This iterates over the code points in reverse, it'll change something like "aé" to "eá", which is arguably wrong.
A lot of the verbosity of it is due to the lack of available type information, for example, the `#[construct_with]` macro wouldn't be necessary at all if type info was available. I also can't do things like check that a given function exists, so if you do something like `#[lengh_fn = "some_fn"]`, then a call gets inserted regardless of whether the function exists/takes the right parameters.
The points from @geaal regarding stability are of course valid, debugging is so so in rust-peg... but ease of use is really great
thanks , I misunderstood it.
thanks for your suggestion, I will try RefCell and Weak .
I'm also looking forward to rust and don't doubt it will be better than C++. I'm just impressed with C++ and have felt like its threading tools are enough to build up decent concurrent data structures. 
My concern there is it sounds inefficient to re-read the whole file each time for each loop.
What about ADA?
Just because you (not you in particular, the generic you) think you can tackle one of those difficult topics doesn't mean you should when writing software for your boss or customer which is what those deterrences are about I think. If you get paid to do so because you are in academia or R&amp;D or do so in your spare time, great! Because those problems are very interesting. But especially cryptography compares pretty well to actual rocket science: You have to invest a lot of time and a lot of money and make many expensive mistakes until you get to a point where everything runs reasonably well. Even then occasional catastrophes will occur.
The events() function returns an iterator, you do not have to use it in a for loop. It is an instance of a struct and you can manually call .next() on it then match on the resulted event and process it. When you encounter an element that should be processed by a lower level function (e.g: parsing the tile) just pass a reference of the iterator object to that next function. This way the next parser function can continue parsing at the same event because it uses the same iterator object. When that lower level parser is done it just returns and the caller can continue the iteration over the very same iterator like nothing happened. Just try it and you'll see that it is not that complicated. 
Whenever you dereference a pointer, you move ownership out of where the pointer points to, into your local scope. This is no different to what happens when you call a function or create a new binding. Imagine `v` were a pointer to a `Vec`, then after `let my: Vec&lt;_&gt; = *v;` the you actually own the vector in `my`, so it is crucial that is has been moved away from wherever it was. However, that would be a problem, since `v` is only borrowed, so you cannot move it away - after all, whoever owns it and borrowed it to you, expects it to still be there when you are done! Why don't you want to make `Allergen` `Copy`, which is absolutely appropriate for a simple type like that?
&gt; That can only be done by either moving or copying the value. Ok, thanks, that's where my confusion was. I thought that you could use a value without copying or moving. In my mental model, this would work too ([playground](https://play.rust-lang.org/?gist=eadbc853f426324e1944&amp;version=stable)): let allergen = Allergen::Eggs; let allergic = (allergen as u8) &amp; 255 != 0; println!("Allergic?: {}", allergic); println!("Allergen?: {:?}", allergen); Everything makes sense now.
It's not the dereference itself, else `&amp;*reference` would also move. As far as I can tell, dereferencing basically acts like you directly have the referenced value (the one being pointed to). Doing `let x = y` is a move, so we're moving from the referenced location to a stack location. This moves out the pointed-to value. 
https://play.rust-lang.org/?gist=6b0926366bddd05b9ef7&amp;version=stable Why wouldn't that work? Am I missing context?
Not really relevant to the rest of the story but isn't `&amp;255` on a `u8` a noop?
&gt; Why was Gecko written in C++? Because it was 1997. We're starting to fix that now. There was also an attempt to write Netscape in Java (the Javagator). It failed.
Ada would be better in some ways, but it doesn't prevent UAF among other considerations (unless you run it with a GC). Not worth it for the lack of industry support. You've got to remember that in 1997 software was written very differently. People didn't think heap corruption was exploitable. W xor X was going to make it impossible for attackers to execute code, because nobody had thought of ROP gadgets. Reference counting was simple and fast, because multicore CPUs were not commonplace. And so on.
Maybe they've spent some time with asm.js :P
I was just making some quick modifications to (*allergen as u8) &amp; (mask as u8) != 0 while I experimenting in the playground. I probably should have deleted that part it before copying it.
Until impl specialization lands, there is a cumbersome work-around using the `optin_default_traits` feature: pub trait NotVec {} impl NotVec for .. {} impl&lt;T&gt; !NotVec for Vec&lt;T&gt; {} impl&lt;T: Wrapper&gt; Lock for T { ... } impl&lt;T: Wrapper&gt; Lock for Vec&lt;T&gt; { ... } [Playpen link.](http://is.gd/PJji3p) You create a marker trait for everything that is not `Vec`, then tell the compiler that `Vec` cannot implement it. This lets you do both implementations. Edit: I'm actually not sure that this is needed in this case. This compiles fine: pub trait Lock {} pub trait Wrapper {} impl&lt;T: Wrapper&gt; Lock for T {} impl&lt;T: Wrapper&gt; Lock for Vec&lt;T&gt; {} fn main() { } Maybe there is something else that is causing a conflicting implementation?
&gt; I'm not a fan of making practices that are worthy of aspiration equivalent to unobtainable. Exactly. 
&gt; indeed lambdas have helped quite a bit Lambdas without garbage collection is just a car crash in slow motion. Its the funarg problem from Lisp all over again... 
Take it easy, no one is reading rust forums because they think don't think it's good.
I don't think any networking program that high-level has been built or exists in rust yet. I'm not sure what a library similar to Netty would look like in rust, but it's probably possible to cobble together a high-throughput server using the libraries currently available.
&gt;&gt;"As I understand it, C++11 offers none of those features" I dont confuse the standard library with the language. C++11,14 gives you tools with which you can write libraries. Rust' salient features are extra compile time checks &amp; ADT's making some tasks more convenient. (they can still be done in C++, its' just verbose and error prone)
&gt; I dont confuse the standard library with the language. C++11,14 gives you tools with which you can write libraries. How is that different from any other language? 
I can add to this from some experience writing a compiler for a sizeable subset of C (in Rust, incidentally): C is not actually context free. Attempts to parse it with CFG parsers (e.g. LR parsers) tend to get around this by passing state back to the lexer from the parser, which allows it to use contextual information to infer the types of certain tokens (this is known as ["the lexer hack"](https://en.wikipedia.org/wiki/The_lexer_hack)). I can't find any reference however for what class C falls into if you discount this small bit of context sensitivity. I do however know that the grammar as written in the standard is not LL. Whether it can be transformed into something that is is another matter. Regarding the C preprocessor, it's best to think of it as a peculiar state machine that operates on text, than an actual language. It manipulates and reinterprets the text it operates on as it goes along. Its behaviour is specified in the standard based on the behaviour of existing preprocessors cobbled together before the first standard was compiled. As a result it doesn't conform to any formal grammar notions. https://gcc.gnu.org/onlinedocs/cppinternals/ goes into great detail on working with these quirks of the specification. Having said all this, include directives by themselves are fairly well behaved.
That sign up says the deadline is June 25th. Should it say July instead?
Out of curiosity, what features of Netty do you expect to be available in an IO framework in Rust?
I'm sure other people will mention some books in this thread, but I'd say none. Rust is useful not because it comes with great ideas (even though it does), but because it addresses common pain points of low-level programming. Knowing what those are, how painful they are and why comes from practice. Write larger piece of software in c, watch it being used and abused, maintain it for couple of years and you'll get an idea. I am not sure if there are books that would summarize such knowledge better then experience. So in short, rust is more practical then theoretical.
i'm not saying it handles every case. I'm saying there are useful cases I personally need, for data-parallel (leveraging multicore processors, not concurency) where non-garbage collected, RAII-based deterministic memory management is perfect - and C++11, Rust handle it perfectly well. e.g. here's one loop, I know its' iterations can safely be done in parallel. Here's another , the same - and I also know it can be done in parallel with the first. So I want a construct to spawn both of them, in parallel. `do_in_parallel([](){ .. parallel_foreach(...[](auto&amp; x){ .. stuff from my first loop})}, [](){ parallel_foreach(...[](auto&amp; x){... stuff from my second loop})})`; done! (*parallel* are high order functions dealing with thread pools) now I can leverage a multicore CPU easily, and I don't need a garbage collector to do it. Did I make a mistake with my assumptions? use some #ifdefs in the definitions of parallel_* to compile a serial version, check it produces the same result. What Rust should be able to do (eventually) is do all those checks at compile time; the information is there , based on the scope blocks.. so long as the first &amp; second loop bodies don't modify the same resources, you're good to go.
your claim is "C++ is still thinking at the level of OS threads and locks". Thats not how I write parallelized code in C++. I make parallel abstractions (high order functions), then the rest of the code using them is much easier to write. 'par_foreach', that sort of thing*. C++ is the only language that does this without imposing garbage collection. When I checked, rust still needed unsafe hacks to do the exact case I'm thinking about, so its' on a par with C++. (i really miss Rusts' original do notation , which made that more natural, still thats a relatively minor syntax issue)
I was fully expecting the first answer to be "Learn Haskell" ;) I might give it a go, I remember looking at the book quite some time ago and finding it quite difficult, but looking at the first few chapters I think I should be able to do just fine now (In part from messing with Rust, heh, chicken meet egg). I also have a long journey coming up soon, I'll see if I can grab one of those books beforehand and read them on the road. Thanks! 
I agree, some high level explanations and reasoning behind the language would be very useful. For example, something like "Thinking in C++" but for Rust. Currently materials on Rust are mostly reference like, with some high level explanations scattered here and there. Hopefully this gap will be filled soon.
&gt;Before Rust I had not heard about Composition vs. Inheritance This makes me think that you have limited experience? If you've written less than, say, 20k lines of code in your entire life, go write more. [Watch the first three minutes of this video](https://www.youtube.com/watch?v=FiflFiZ6pPI); theory is your safety blanket when facing the scary and confusing world of programming. But you need to face that world first, to even understand *why* theory is useful. If you have experience, or if you're curious about theory for its own sake, then what you want to be looking into is programming language theory. I then recommend that you take an online class on programming paradigms, such as [this one](https://www.edx.org/course/paradigms-computer-programming-louvainx-louv1-1x-0) with Peter Van Roy.
&gt; Thats not how I write parallelized code in C++. That's interesting stuff but we were talking about concurrent programming. 
title: "Must be This Tall to Write Multi-Threaded Code" multi-threaded code is used for 2 purposes - concurrency and multi-core parallelism.
Since it is mentioned: How does combine compare to nom?
It's not parsing, but I spent this weekend writing an ASN.1 (DER) serializer: https://github.com/alex/rust-asn1
Turn the question around, what have you read for C++ and the concurrency course, maybe we could relate it to that material. "Concurrency" courses cover low level mutexes, locks, semaphores OR no-shared memory/green threads environments (erlang/BEAM), OR they should really be called parallel, SIMD strategies for graphics/linear algebra/difeq (MPI, cuda etc). ( for C++, Scott Meyers last 2, *Effective Modern c++* and *Effective 3rd ed* are alwas recommended for *is-a*, **has-a**, *is implemented in terms of* etc.) ------ For type systems, people generally go with Pierce's TAPL, or Harper's Software Foundations https://mitpress.mit.edu/books/types-and-programming-languages http://www.cs.cmu.edu/~rwh/plbook/book.pdf --------- there's also some good langA vs langB threads you can google, e.g. https://www.reddit.com/r/rust/comments/2mwpie/what_are_the_advantages_of_rust_over_modern_c/ https://www.reddit.com/r/rust/comments/2ifgsx/what_can_c_do_that_rust_cant/ https://www.reddit.com/r/rust/comments/2pi3ju/rust_ruined_c_for_me/ ----- also there are some good design rationale papers for Scala, clojure, d, golang, haskell, ML family and others, i could recommend if you want.
I have not written anything with nom so take this with a grain of salt and please correct me if I am wrong. # combine benefits * Better error messages (EDIT: Better as in they can express more types of errors in an easy way. Though this is at the expense of being a bit slower) * More generic (nom is specialized to [u8] as input while combine can handle any type of input conforming to the [Stream](https://marwes.github.io/combine/combine/primitives/trait.Stream.html) trait * No macros (granted this is a bit subjective, personally I find it easier to stick within the language as far as possible) # nom benefits * faster (combine chooses to have better and more flexible errors at the cost of speed, I don't think combine will ever be as fast as nom due to this tough I hope to reduce the gap a bit more with the above mentioned ranged streams) * streaming parser (nom in general seems to be more flexible in how input is provided) * macros to define parsers (combine has some problems with long compile times currently due to its use of separate types for each parser so if nothing else using macros to avoid that is a plus) All in all the libraries target quite different needs.
Yeah, but it's HTTP, I'm really looking for a networking library that would help me build any application protocol.
So, here's what I'm mostly looking for: * Connection and event loop management for clients and servers - in netty you kind of forget about this because the framework handles it all uses optional callbacks to let you know about stuff that's going on if you want more info; * Channels and channel pipelines - building networking protocol parsers is kind of a repetitive job and the channel pipeline that netty provides offers both great building blocks (with the base decoders) and a _place_ for you to place your own encoders/decoders. The structure Netty provides to projects is great as well since there are well known extension points you need to provide to actually implement your protocols so you can basically just write the encoders/decoders and you'll have a client/server ready to use.
Buy these books and read them: * [SICP](http://smile.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871) * [Understanding Computation](http://shop.oreilly.com/product/0636920025481.do) * [The Design of Design: Essays from a Computer Scientist](http://www.informit.com/store/design-of-design-essays-from-a-computer-scientist-9780201362985) Keep in mind that you should *not* feel *bad* about *not being able to absorb everything* in these books. Even only a *fraction* of the material *will* make you a better software developer. A book which I've only just started reading but which I like so far is [Expert C Programming](http://smile.amazon.com/Expert-Programming-Peter-van-Linden/dp/0131774298).
I don't see how the funarg problem applies to Rust's closures (i.e. closures/lambdas without GC) at all, could you expand? Rust's borrow and move checker is strict about how things are shared/escape, and this reasoning of course applies to closures.
The problem is that `Result` is a generic type that depends on the *two* arguments it requires. When you see [the type signature](http://doc.rust-lang.org/nightly/std/fmt/trait.Display.html) in the doc, it reads `fn fmt(&amp;self, &amp;mut Formatter) -&gt; Result&lt;(), Error&gt;`. So your type implementation should also follow `Result&lt;(), Error&gt;` instead of just `Result`. That's why the compiler complains about the lack of the arguments ("expected 2, found 0"). Also, not related to your original question, but there are more things to consider. Firstly, the types of the patterns in a `match` block must be matched against the target. As `self` is a reference (because `&amp;self`), you should write `match *self` instead of just `match self`, or prefix `&amp;` before all of the variants (`&amp;One =&gt; ..., &amp;Two =&gt; ..., &amp;Three =&gt; ...`). I believe the former is easier. Secondly, each variant of an enum is namespaced by default in Rust, which means, you should use `SuitNumber::One`, `SuitNumber::Two`, `SuitNumber::Three` instead. If you find this tedious, you can import those names into the current namespace with `use SuitNumber::*;`. [A working example.](http://is.gd/D2rPaO)
Also note, the example in the documentation uses `fmt::Result` because the `fmt` module defines a type alias `Result` as `result::Result&lt;(), fmt::Error&gt;`.
they are certainly transferable, we've done it once already. IIRC, there is a link in the email that lets you release a ticket. You should get a refund and it goes to the first person on the wait list. Of course, if you have a specific person in mind, that can be done too. If that link doesn't exist or you're not sure, pm me your email and I can take care of it.
Great, thanks so much, Steve!
Hmm, so is the link I gave for the Display trait wrong?
As /u/stebalien said (and I was reminded again), there are two different `Result`s there. One is a generic type `Result&lt;T, E&gt;`, which I explained in the parent comment, and the other is an alias to `Result&lt;(), fmt::Error&gt;` which is named `fmt::Result`. If you know C, the latter can be thought of `typedef`. What the doc you linked explains is that the `fmt` method returns the latter. Actually, if you click `Result` in the page, you will be guided to [this page](http://doc.rust-lang.org/core/fmt/type.Result.html), which says the same thing. So, `Result&lt;(), Error&gt;` in my example can be substituted with `fmt::Result`, or with just `Result` if you `use std::fmt::Result`. The reason why your original code didn't work is when you type `Result` it defaults to the generic type unless you explicitly `use std::fmt::Result`. Many modules define their own `Result` synonyms for your convenience. But yeah, this can be quite confusing. :)
As a bit of a lesson, I changed my using to `use std::fmt::*;` so that I pull in all of these. I'm a little surprised that the signature mismatch didn't create an error (between my implementation function signature, and the trait function signature). My compiling implementation: use std::fmt::*; pub enum SuitNumber { One, Two, Three, Four, Five, Six, Seven, Eight, Nine } impl Display for SuitNumber { fn fmt(&amp;self, f:&amp;mut Formatter) -&gt; Result{ use tile::SuitNumber::*; match *self { One =&gt; write!(f, "1"), Two =&gt; write!(f, "2"), Three =&gt; write!(f, "3"), Four =&gt; write!(f, "4"), Five =&gt; write!(f, "5"), Six =&gt; write!(f, "6"), Seven =&gt; write!(f, "7"), Eight =&gt; write!(f, "8"), Nine =&gt; write!(f, "9"), } } } 
What if the argument to index wasn't a ref. Then it would be up to the implementer to define Index for ref-types. Doesn't that fix the problem without HKT's? ie trait Index&lt;I&gt; { type Output; fn index(self, idx: I) -&gt; Output; } impl &lt;'a, E&gt; Index&lt;usize&gt; for &amp;'a MyContainer&lt;E&gt; { type Output = &amp;'a E; fn index(self, idx: usize) -&gt; &amp;'a E { ... } }
That could well be true. I've always assumed the subtext of "in production", but I can see that some people might not, or be too wary of it to experiment. In the crypto case, I like to point people to the [cryptopals](http://cryptopals.com/) site - you write your own crypto, and you learn why it's such a bad idea at the same time.
Nice! /u/kibwen has an idea for a way to do this that can work cross-crate (by only generating crate metadata and using it for the next crate). 
Never battle-tested, but I switched to mio in my [toy IRC server](https://github.com/nwin/rauta/blob/master/src/client_io.rs) a while a go. Not sure if it is a good educational example though (the first doc comment is actually a lie, it is not multi-threaded yet ;) ). On the plus side I should state that almost everything related to mio is limited to that module (just search for mio in the repo).
&gt; If A calls B as the very last line of its function body, then it should be safe for B to call back into and modify A. If that is the case, then all you have to do is to manually drop the `Ref` to `child` in `A` before calling a method of `B`.
Just explicitly forward the mutable reference to `A` into `B`'s method. That's the way of explicitly requiring it to happen. You tell me the types but not the values. Say that we have a struct type `A` called `a1` and it contains a child reference to a struct type `B` called `b1`, and then `b1` has a weak reference to another type `A` struct called `a2`. Is this possible? If it isn't then don't make the child be `Rc&lt;B&gt;` make it just `B` to make it clear that each `B` belongs to a specific `A`. impl A { fn first_call(&amp;mut self) { child.b_call(self) } fn second_call(&amp;mut self) { // bla bla } } impl B { fn b_call(&amp;mut self, parent: &amp;mut A) { parent.second_call() } } Also how did you get a `&amp;mut B` from an `Rc&lt;B&gt;`? You can only get a `&amp;B` from a `Rc&lt;B&gt;` the only way to get a mutable handle is using `make_unique` which will copy unless you have a single `Rc` which only matters when `A` is the only owner. Notice that altering this value will not affect the other existing ones. If this wouldn't work please tell me more about what you are trying to do to solve your problem.
No, I meant dropping the `Ref` you got by borrowing your `RefCell`. You also cannot afford to borrow `A` mutable in `B`. impl A { fn call_b(&amp;self) { // This cannot be `&amp;mut self`. Thus you need to put everything you want to // change into a RefCell. let ref = self.other_data.borrow_mut(); // .... drop(ref); self.child.do_something() // Do not call self.parent.borrow_mut() in do_something() this will crash. Just use RefCells } }
&gt; Could you elaborate on why you think error messages are better in combine? Because this might give me some ideas. I care a lot about error management :) Maybe I am a bit biased with thinking combine's error handling is easier but I had a difficult time grasping what information you could save in nom's error handling. I guess you can encode any information that you want in a `&amp;'static str` making it powerful enough for static errors?. I suppose that there shouldn't be any dynamic errors in a parser but I opted for what I feel is any easier approach, just store each error in a Vec as it is found. Unfortunately, while this is really easy to add errors to it does not come for free since there can be a lot of Vec's allocated and deallocated while an acceptable parser is searched for. ([ParseError](https://marwes.github.io/combine/combine/struct.ParseError.html) you can click through to see `Error` and `Info` as well. I added a way of avoiding error creation recently for LL(1) parsers but even with that change I still see a 30% overhead just from needing to run destructors for empty Vec's (for the JSON benchmark). &gt; About genericity, only a few parsers handle &amp;[u8]. I was under the impression that only `&amp;[u8]` but looking at the macros I can see that you can actually put your own types in there but doesn't [Consumer](http://rust.unhandledexpression.com/nom/trait.Consumer.html) need to be over `&amp;[T]` as well? In any case it seems to me that you are bound to keeping input in an array which makes it efficient while combine takes the more general approach of just needing an [uncons method](https://marwes.github.io/combine/combine/primitives/trait.Stream.html).
I'm also new to Rust and struggling with various of the concepts I'm not yet familiar with, asking myself: Why did they do it this way? And while much of the advice here's inspiring, I'd much rather have a book on Rust that explains the concepts people are likely to be new to rather than having to learn Haskell or typing systems theory+practice etc. upfront. Plus, the ownership/borrowing/lifetimes model for example is unique to Rust if I'm not mistaken. I'm only aware of this one (which hasn't received great reviews) being currently available: http://www.goodreads.com/book/show/25644753-rust-essentials Then a book project on Rust, aimed to be released November this year: https://www.kickstarter.com/projects/1712125778/rust-programming-concepts-book I've paid up and am looking forward to reading draft chapters early on. Also to be released in November, and I'm hoping soon available in draft as well: http://shop.oreilly.com/product/0636920040385.do Anyone aware of anything else available or in the works? In the meantime I'm trying to get by with the official online docs, blog posts, help on IRC and reading up here. For you as a C/C++ programmer this may be of interest, too, but I can't say how up-to-date it is (looks like changes such as i32-&gt;int have been made) or how deeply it goes into the core subjects you want to learn, perhaps worth a look: https://github.com/nrc/r4cppp 
You can't use `ref` as variable name since it's also a keyword.
A tiny bit of feedback on the bits shown (and please show more, it’s hard to judge without it): `T: Borrow&lt;T&gt;` is a completely meaningless bound; there exists `impl&lt;T&gt; Borrow&lt;T&gt; for T where T: ?Sized`, so all types implement `Borrow&lt;T&gt;`. Also consider [`Borrow` vs. `AsRef`](http://doc.rust-lang.org/book/borrow-and-asref.html); on the code shown, it looks like `AsRef` may be more appropriate. (If not, you may well want to mesh `ToOwned` stuff in there instead of `Clone` for `T`. Hard to judge without more code.)
Sure: let's say I want a crossover operator that takes 3 parents. From the `parents` value I passed in I can extract this in a few ways: let parents_iter = parents.into_iter(); let parents = (parents.next().unwrap().borrow(), parents.next().unwrap().borrow(), parents.next().unwrap().borrow()); // Now use tuple indexing to access the parents, e.g: parents.0.genes() The one problem with this is (apart from it being really ugly) is that I can't lift the `.borrow()` calls into a macro, because the borrow will not outlive the scope of the macro. Now there are a few alternatives to this: I made a `pairs!`, `triples!`, etc. macro with [itertools's batching iterator](http://bluss.github.io/rust-itertools/doc/itertools/trait.Itertools.html#method.batching) to extract the parents: let children = triples!(parents.into_iter()) .map(|(p1, p2, p3)| { // Produce a children tuple }).next().unwrap(); The problem with this is that writing the batching macro (even for 3 elements) is really cumbersome. An other alternative would be collecting the iterator into a `Vec`, but I really don't want to allocate one for every single cross call. EDIT: Added a stripped down example to OP. 
Thanks! It's a start at least :D
I made a thin wrapper around Mio called [Reactor](https://github.com/rrichardson/reactor) which does a bit of what you are after. It manages connections for you for both connect and listen, and has basic support for protocols (and callingbacks for the protocol level completions) Unfortunately I have yet to update it for the latest version of Mio. If there is interest, I'll happily maintain it. I wrote it with the intent of writing an async websockets server with it. (It is still on my list of things to do, its just like, 3rd or 4th :) )
What other things? Curious because I've been off of IDE's for so long.
I would like snippet expansion of impl signatures and struct instantiation (?) where the compiler knows the names/signatures of what it wants and I shouldn't have to find or remember them.
Now that I have logged in and toggled the switch for one of my repos, what does this website actually do?
Yeah, "like goimports" seems to cover what I'm after pretty well. Definitely a separate command line tool, the use case doesn't really fit how you use either cargo or rustc, but it would need to be pretty cozy with the machinery used by both of them.
Every hour, if your project hasn't had a build in the last day it will trigger one.
Big deal features that I use in Java IDEs are things like 'find me all the places this method gets called'. You can do a poor man's equivalent using grep, but it's nowhere near as good - being able to traverse up the call hierarchy, for example, is wondrous when you're working on a big gnarly codebase.
Or the go to method/field declaration feature. Always saves a lot of time.
"Last" day?
This thread is about your assertion that "C++ concurrency is decent in C++11". 
Dropbox has an internal project called Rufio that we'll be open sourcing within a month or two.
Does anyone know what happened to Rust CI? It stopped triggering builds a long time ago indeed. I think a site like Rust CI would be a great way to increase Crater coverage, because both programs and libraries use CI. Uploading programs to crates.io without any purpose except to increase Crater coverage feels like a hack to me.
&gt; Erlang is highly scalable but it is never used for parallel programming I want to emphasize this by pointing out that Erlang did not support symmetric multicore processing until well past the year 2000.
I didn't downvote you until the last sentence, that's _extremely_ inappropriate.
For reference, there's already a crate called cpuid, but it's a wrapper for libcpuid, the C library. There are some [other crates](https://crates.io/search?q=cpuid) that also allow access to the same information. Unfortunately for me, it looks like [another crate](https://github.com/gz/rust-cpuid) even sprang into existence between when I surveyed the landscape a few weeks ago and when I got far enough to publish about it... It [used to be](https://github.com/gz/rust-cpuid/blob/5ad87ef92b60f5c07e4a92ebac110bd7b0df1e14/README.md) marked as deprecated, so I wrote my own. I guess that author decided to resurrect the project.
Yes, I was eliding many of the details, thanks for the catch! I was thinking ahead to my second example with `RefCell` when getting the `&amp;mut` from the `Rc`. Forwarding the parent pointer doesn't scale. For instance, if A calls B calls C calls B calls A, we have to plumb the pointers very far down the call stack. Assuming C doesn't even know what an A is, it doesn't make sense for it to take `&amp;mut A` in its signature. Would it help if I provided some C++ code of this pattern? It is fairly common in async code.
So, that would compile but it violates the surprise factor. Someone calling `call_b` would be very surprised to find out it mutated data, if they were looking at the type signature! Maybe this is the best we can do in Rust? It seems really ugly to put everything inside a nested struct though.
That helps me, too. And probably most people. That's one of the reasons I mentioned SICP above. The example programs helped me a lot.
The Rust Programming Language specifically does not assume you know C++. If anything is assumes too much still, please let me know. :)
Not the language, a few tutorials I've attempted to read. Please note that I'm interested in rust only out of curiosity. Which means I haven't tried very hard to get through the tutorials. I do realize that pointers, references, copy constructors and the like are freshman level topics. However, freshman year was a LONG time ago. I've simply forgotten the difference between *var and &amp;var. I'm sure there are plenty of others in a similar situation. I'm sure there are plenty of c++ programmers looking to keep their performance gains, yet take advantage of greater type safety. I'm in the camp where I'm looking to experiment with code much closer to the machine, but have been put off by c and c++ (the languages, not their lowlevelness). 
Yeah, I thought about whether this should be a cargo command, but it feels ergonomically odd when all of the current cargo operations apply to the whole project and I'd want `imports` (`uses`?) to always target a single source file because of the potential user interaction required. Might still be worth a shot just adding it to cargo since it would need to do all sorts of cargo-integrated thing like crawling the linked packages anyway.
No, I mean http://doc.rust-lang.org/stable/book/ . You will probably want to go the "Syntax and Semantics" route, I'd bet.
Yeah, but `concat_idents!` is *practically useless*. I'm yet to find a practical case where you really need it. The only one I can think of is something like this: #![feature(concat_idents)] macro_rules! call_foo_with_4 { ($suffix:ident) =&gt; {(concat_idents!(foo_, $suffix))(4)}; } fn main() { call_foo_with_4!(bar); } fn foo_bar(v: i32) { println!("{}", v); } But I've never actually needed to be able to do that. I *have* needed to *generate* symbols with constructed names, but `concat_idents!` can't be used for that. The "stuff into a `tt*` then extract again" trick doesn't help because you just end up with the original macro expression again. So far as I know, the solution to this is: (macros have to change to let you expand to idents &amp;&amp; (the AST has to change to allow expansions in ident position || we get a way to eagerly expand a macro (maybe double `!`s?))) || we get a native syntax for concatenating idents.
&gt; Would it help if I provided some C++ code of this pattern? It is fairly common in async code. Not just the pattern, but also the exact context you want to use it. I can think of a few ways of mapping the pattern, but the best one would depend on what exactly you are trying to use.
The simplest solution I've found for cases like this is to [`replace`](http://doc.rust-lang.org/std/mem/fn.replace.html) one of the values in question, make the change, then replace back. If having dummy values floating around makes you nervous (like it does for me), stick this logic in a function that does all of what I've just described, and invokes a closure with the two `&amp;mut`s. That way, it's impossible to access the "dummied-out" collection accidentally.
Context: async IO processing. Top level handlers receive events and pass messages to listeners. The listeners may want to immediately pass another message back up the callback chain to update state. Contrived quick example I threw together ( http://cpp.sh/6kxv ): #include &lt;memory&gt; class CodecParent { public: virtual void codecFinished() = 0; }; class Codec { public: Codec(CodecParent&amp; parent): parent(parent) {} void onIngressBytes(char* buf, size_t len) { // process bytes, update any listeners, etc. parent.codecFinished(); } private: CodecParent&amp; parent; }; class Reader: public CodecParent { public: void onBytesRead(char* buf, size_t len) { numReads++; codec-&gt;onIngressBytes(buf, len); } void codecFinished() override { codec = nullptr; // delete the codec } private: size_t numReads{0}; std::unique_ptr&lt;Codec&gt; codec{std::make_unique&lt;Codec&gt;(*this)}; }; int main() { // Pretend main() is dispatching ala epoll Reader reader; reader.onBytesRead(nullptr, 0); }
The problem is that macros aren't a separate pass like they are in C. They are *explicitly* represented as nodes in the AST. Thus, they can *only* appear in positions that explicitly support them. When it comes to names of things, those are represented by an `Ident` which is basically just an interned string; no other variants. If you changed it so that you could have either an `Ident` or macro invocation, a *huge* pile of code that relies on names always just being strings would suddenly break. I don't know how big the fallout would be, but *I* certainly wouldn't want to be the poor sod trying to implement it.
Basically I see value in persistent data structures for their avoidance of race conditions under shared-memory parallelism. Unfortunately, without the parallelism scheduling that bolsters functional languages, lots of these benefits may not manifest in rust, other than hand-coded stuff or other libraries being written... or atomicity of some sort built into the methods on the persistent data structures. I'm currently actually doing my research on using algebraic information to avoid race conditions under write contention, and I'm using lots of ideas from persistent data structures to do it. Basically I'm implementing transients that are performance-tuned for multiple simultaneous write situations. Any stale reads performed by a process due to a data race will be pre-empted or retroactively addressed, depending on the exact ordering of operations. The trick is to "register" the operations that are going to performed on the locations storing data that might experience a data race. Properties of those operations (mostly combinations of basic things like transitivity/associativity/commutativity/distributivity intended to address certain kinds of staleness) allow this to happen. You can also do operation regrouping to try to draw out nicer properties -- subgrouping operations cleverly might allow those subgroups to have nice properties, where the individual ones themselves might not. Sometimes things do really have to happen in a prescribed order. The trick is to avoid wasting space if possible. And knowing how to write code and redesign algorithms to draw out parallelism in the first place. Parallelism isn't easy, not even in 2015.
Really wonky ATM :/
It is only surprising if you have the wrong expectations. It is a wrong deduction that you cannot mutate objects through shared references just because unique references are expressed by `&amp;mut`. There was a longly discussion to call them `&amp;uniq` instead of `&amp;mut` for exactly that reason. It is an established pattern in Rust to mutate objects though shared references which is called [“internal mutability“](http://doc.rust-lang.org/nightly/book/mutability.html#interior-vs.-exterior-mutability) and for example used in `RefCell`, all synchronization structures and rc pointers.
* Directly highlight errors in the source * Quick fix for methods, struct fields, enum variants – you write a call to a nonexistent method and the IDE inserts a stub (including inferred types of your arguments) on hotkey, or you access a nonexistent field on a struct or match a nonexistent variant on an enum, and the compiler inserts the field/variant. * "widen type" hotkey that e.g. widens Vecs to slices, Iterators to Into&lt;Iterator&gt;s, etc. in function arguments * Search references to a fn/struct/etc. * Refactorings, e.g. rename fn (and all call sites), extract method, move variable to field (only in struct impl) and vice versa, etc. * Lint &amp; run unit tests while you type (e.g. FindBugs &amp; Infinitest for Java)
I would add https://github.com/nikomatsakis/dogged
This is a bit more complicated and I'd have to sit down to think on this hard. My instinct (and advice for now) is to look at [mio](https://github.com/carllerche/mio) and libraries that use it, since they use the event loop method. To answer your question. What you propose is very dangerous, and problematic. It makes it hard to reason about code. This doesn't mean that it's impossible to make things efficient, there might be solutions that are being ignored. To put in your example, you are deleting an object before you exit the method, which leaves a `this` pointer dangling. The pointer could be used in unpredictable manners (for example a destructor to some object on the stack in the previous lines might depend on this reference). It becomes impossible to be certain that it works without observing it, which makes it a dangerous pattern. At this point we might be mutating data after getting a non mutable reference, since the code is already doing more than is immediately obvious.
Yup! You've exactly identified the danger! However, this code actually uses the pattern correctly and safely. It is perfectly fine for `this` to be deleted while its method is on the stack *as long as the rest of that function doesn't touch any member variables*. This seems right up the alley for Rust's lifetime system, however, I haven't been able to express this with Rust's borrow checker (yet). I have firsthand experience with C++ programs where immediately calling back enables lower latency and higher throughput in high performance proxies. Originally, the code delayed the update until the next event loop, which was less efficient.
Losing cargo is undoubtedly the hardest thing to me about going back to writing Python when I'm coding for fun.
&gt; So all my knowledge of best-practices comes from reading blog-posts and the likes, and my theoretical knowledge comes from Wikipedia. This is the pretty much the same for me (I read a lot of PL blogs too) and I've been a professional dev for years. It's not a bad way to be.
While Rust has many amenities, like cargo, racer (+integration), *very* good compiler diagnostics (that nonetheless get improved with each version, kudos to the responsibles!), lints, etc., there is still much room for improvements and integration. I have said it before on numerous occasions, a full-featured Rust IDE would make Rust programming both very powerful and very ergonomic. But I trust that it will come in time.
You could mention that it only runs on nightly. Also, it looks like every method on cupid::Master consumes the struct? Doesn't that make it impossible to run more than one query? Edit: [Yup](http://i.cubeupload.com/Wxd0eQ.png).
This will definitively kill mods if it becomes popular. Mojang still hasn't released a modding api for the java verison. I doubt the Pocket Edition will get one.
Personally I use a self-made shell-script which basically switch to `gh-pages` branch, build doc, remove unnecessary folders and commit it (it doesn't automatically upload it because it doesn't necessarily know the remote name). If you want to see the script: https://gist.github.com/KokaKiwi/13bec50d1f929b1a2e11 
Thanks! I think I will do this manually for now until I'm somewhat comfortable with it.
I use https://github.com/huonw/travis-cargo, eg https://github.com/cmr/evdev/blob/master/.travis.yml
From someone just beginning to look at Rust, what steps could s/he take to make sure the struct isn't moved? 
I gotta take better care of my git config.
Thanks for noticing it! A pull request has already fixed it, so that's nice. The short version is that the flags on each of the sub components all consume self, so when I added the delegation I did the same thing. Of course, that's a bad idea because the master struct isn't copyable like the sub components are! I'll definitely update the readme and docs to point out that it only works on nightly; that's just an an artifact of me writing more assembly in Rust (like jetscii) and forgetting the important basics!
http://is.gd/hUXCtn See comments in there.
It's a bit difficult to follow the macro code but from what I can tell you don't store the error from each alternative but instead there is a single error for the whole [alt!](http://rust.unhandledexpression.com/nom/macro.alt_parser!.html)? Otherwise I would think that you'd encounter the same problem as me in the `choice` parser in that if a parser at the end matches, the previous parsers have already created errors completely unnecessarily. I'd like to store more of the input directly in the error similiar to in nom though since iterators can't be pretty printed I'm not sure how to implement it. In the beginning it's all bytes true, but in combine I wanted to have the ability to plug in say, an existing lexer which would output tokens one by one instead of producing a slice of tokens immediately. 
Here's one that doesn't mess with the working tree: https://github.com/Stebalien/rust-git-hooks/blob/master/update-docs/post-commit
I prefer the former because 1) the patterns are visually clearer. 2) if I change the function to get `x` as a value, simply removing `*` from the head expression is sufficient. I'm not sure of the convention, though.
Having come from Haskell, I feel like I take things like cargo for granted when I read articles like this.
You can call Rust applications from Java the same way you can call C/C++ applications from Java - though I guess you would have to still write a small C-Rust adaptor because Rust on its own won't understand the Java-C FFI header files.
Source: [Wearing the hair shirt: a retrospective on Haskell \(2003\) ](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective?sess=ddab0ffdfdfcb974595c06a0c3d3492e)Simon Peyton Jones; the links seem to be broken, so [\[PDF\]](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/HaskellRetrospective.pdf).
Afaik thats also the convention, for exactly those reasons.
The important part from Haskell that I see is traits. 
Experience is a great teacher, but it's not fast and it's not pleasant. Learning from other people's mistakes, while not as actually useful, is considerably easier and a bigger bang for your time.
http://steveklabnik.github.io/automatically_update_github_pages_with_travis_example/
&gt; Java-C FFI Is this an external library that hooks into C?
There are several techniques for calling Rust from Java, you should take a look at these as a starting point: * https://en.wikipedia.org/wiki/Java_Native_Interface - This technique involves writing code specifically for interacting with Java from Rust * https://en.wikipedia.org/wiki/Java_Native_Access - This enables you to simply expose your Rust functions directly to Java Both of those specifically talk about C and C++, but the same techniques still apply. Those, in combination with the [FFI chapter of the book](http://doc.rust-lang.org/book/ffi.html), should get you most of the way. The scenarios where you might want to do this include: * Any time you'd want to write C/C++ code to be called from Java, but would rather use Rust * If you have some performance critical code which would benefit from being written in Rust rather than Java (note that it may end up slower, due to the overhead of entering/leaving the VM, marshalling/un-marshalling values, the fact the VM can't do any fancy optimisations the second the native code is touched) * If there's a cool Rust library you want to interact with, for which there is no Java equivalent * If you have an existing Java application which you're gradually migrating to Rust Hope this helps.
BTW I just realized that you are the author of mio, sorry I didn't realize before. This is a harder problem, because it is unsafe, and doubly so in Rust. In C++ everyone expects things to be crazy so there's a lot of limitation in what you can do, in Rust those limitations are changed by the borrowing requests. I think that the best solution is to use runtime borrow checking, `RefCell` from the start (or inject it last second). I don't have a lot of time right now, but I will come back with some code later on to show what I think could work.
Yes this totally lays it out. Thank you, this helps me out. I'm going through the book now, but haven't gotten to the FFI part. It's a decent guide. Hypothetically, do you think I would have better performance using Rust from python vs java since it doesn't run on a vm?
The official tool to link Java with C code is [JNI](https://www3.ntu.edu.sg/home/ehchua/programming/java/JavaNativeInterface.html). There are libraries ([JNA](http://www.javaworld.com/article/2077828/java-se/open-source-java-projects-java-native-access.html?page=2) for instance) to do it too without writing wrappers in C code
Python does run on a VM, so would have similar problems. They are likely to be less noticeable though, since the VM is no way near as advanced as the JVM. You'll notice it's very common to write performance critical code for Python in C or C++, whereas it is typically written with Java, for Java (see scipy/numpy, for example). There are actually examples of using Rust from Python in the book, if you're interested in that.
[`Result::expect`](http://doc.rust-lang.org/nightly/std/result/enum.Result.html#method.expect) is probably what they were referring to.
More macro freedoms! Baby steps :)
I closed the issue just because the repo is defunct in favour of https://github.com/contain-rs/
On the plus side, Java is pretty close to Rust, raw performance-wise.
I think I got a potential solution [right here](http://is.gd/ShhDtq) it compiles with a few warnings of unused stuff, but the borrow checks out and it doesn't panic. The code looks uglish, but it's doing ugly stuff and should feel dangerous and not elegant IMHO. There's a minor cost and the code could panic if at any time you do something you shouldn't. In the C++ code there's a minor cost of virtual functions which I've repeated here (in an attempt to look as much as the code you gave). I'm still doing the forwarding oneself at call (but I'm willing to later offer a solution that doesn't need this if you still need it) but I've made it now happen by extending the struct itself. The main reason is to avoid potential `Rc` cycles which would lead to memory leaks. Probably more clever solutions can be built on this, but I hope it's a start.
That book seems to have it all. I'm gonna check it out. Thanks again.
The last method I put in there does not do anything unsafe at all, and that is the only one I was recommending... But it depends on context, if it can be encapsulated properly then using raw pointers is not an issue. Just for fun, because I am still learning this language, and I was wondering how hard it is to properly do this encapsulation thing, I have upgraded the UnsafeGetter into one that gives an error whenever a duplicate is requested, and does not compile if there is a external borrowing conflict. http://is.gd/EbyvPT I doubt this thing is actually sufficient to be totally safe, since if BTreeMap moves any of those pointers around it stops working, although usage of the BTreeMap is blocked so long as the MutGetter is borrowing it...
Sounds like Mirage!
I had a similar experience with Cabal... Loved Haskell, but I can't stand Cabal.
You might get some milage out of https://github.com/Hoverbear/raft
Full ack – I just tested it with great success. Now if only I could get my build to include the unstable features only on nightly...
I wrote a paper about this! https://hoverbear.github.io/rust-education-paper/paper.pdf
I enjoyed the posts. All these &amp;str to String (like: elements[1].to_string()) can use to_owned() instead
they waste time. learn to program in Rust...slacker =p
This is how I do it- with custom features https://github.com/shepmaster/sxd-document/blob/master/.travis.yml
Yeah, that's definitely happened. I suppose you read my comment history. Thanks :)
Not really. I had seen a decent blog post, but now I can't find it. This might be good, but I've only given it a really quick look http://www.lucacardelli.name/Papers/TypefulProg.pdf I think this lecture goes over it a little bit too, but I don't have time to watch the whole thing right now: https://vimeo.com/14313378
Hotspot is absolutely insane. I wouldn't expect raw performance to be extremely different between the two, but you definitely are correct about RAM usage :)
Thanks – that should solve my features problem nicely.
Nope, didn't read it. I've just seen it a million times before.
Yea, I meant to post this to /r/rust :) This is Evan Czaplicki discussing the design of a programming language to be friendly towards newcomers. The subtext being how to avoid the pitfalls that seem to have encumbered existing functional languages. IMO, Evan has done an excellent job with API design, especially as it relates to naming, in order to appeal to its target audience, which is web devs. 
Use the slicing syntax (don't forget the &amp;) let date = "20150721"; let year = &amp;date[0..4]; you can try it here http://is.gd/WVaYX7
That seems really dangerous to me. Why do you want this, just for performance? Context switching and memory safety seem totally unrelated - rust provides memory protection**, ring separation provides isolation. ** And not perfect memory protection. It makes promises (not guarantees) about specific types of vulnerability classes.
Are you sure a regular expression wouldn't be better? The [regex crate documentation](http://doc.rust-lang.org/regex/regex/index.html#example:-iterating-over-capture-groups) shows an example that is similar to what you're doing.
This is what I imagine for Rust in the web sphere. I will never expect it to overtake any of the popular scripting languages, however, when you know that you could save some money on something like AWS or DO, then that is where Rust will shine. I have been having the same idea as you in regards to a web service that I maintain.
I think go-kit is a bit more high level than that. It provides common interfaces for things like logging, rate limiting, load balancing, tracing, RPC with pluggable transports, metrics, circut breaker, logging.. There's a changelog podcast episode about it here http://5by5.tv/changelog/163 Maybe we should start a rust-kit org, and start porting the go code to rust :) 
That or nom are the Right Answers for sure.
The reason why not to contort the design of the kernel is so that you can run on stock Linux distributions, with a native TCP stack. I think that you're setting the bar way too high for wanting formal modeling before you trust that Rust is memory-safe. Formal modeling is primarily going to prove whether a very small subset of Rust is safe, and I'm pretty confident that that small subset is ironclad. Even if memory safety bugs were found, they'd probably be in corner cases that you'd have to intentionally write bad code to exploit (like the `Rc` overflow). The probability that actual attackers who can't write Rust code of their own but can only *interact* with a Rust program can exploit such a bug is incredibly small. Golang, for example, has known, wildly unsafe and totally exploitable in the right conditions, use-after-free holes around slices and maps, but Google properly understands that attackers can't practically weaponize these without writing Go code of their own, so they aren't a problem in practice. And the solution to the unsafe problem is to not use unsafe code. Put the unsafe code into userland and use context switches whenever you want to call *it*, or just write the unsafe code in safe Rust.
Why is that dangerous? &gt; rust provides memory protection**, ring separation provides isolation. Is the difference, in your view, that "memory protection" means that there's no undefined behavior, and "isolation" means that you can't access data of another process and you can't call functions belonging to certain processes? If so, Rust can effectively provide both by simply restricting which libraries your application is allowed to link to (along with `-F unsafe_code` of course). You can create a compile-time sandbox in this way. &gt; ** And not perfect memory protection. It makes promises (not guarantees) about specific types of vulnerability classes. What is the difference between a "promise" and a "guarantee"? Formal modeling? If so, I think your standards are unrealistically high, for reasons outlined in the post above.
What I mean to say is that the separation between the kernel and user is not about process isolation, it's about privilege at a hardware level both for security and safety of the devices. That is why we need to change contexts, so that at a hardware level things can happen and that's a lot more than just memory safety. So, yes, separate address spaces can provide some nice memory isolation and safety, but I'm talking about why we need a separation between kernel and user. And yeah, it's sort of a moot point because rust security is not infallible and I think it's important to make that really really clear to people who might think that security can be accomplished entirely at the language level. Which brings me to my original surprise at wanting to put a web server in the kernel - there are better ways to increase performance without putting the entire system's security at the hands of the borrow checker.
And include overflow checks in release builds ;) 
I think I wasn't clear since both you and the other redditor think I'm talking about process isolation, which is a function of the operating system, when I'm talking about isolating the kernel from the user, which is a function of hardware. &gt;What is the difference between a "promise" and a "guarantee"? Formal modeling? If so, I think your standards are unrealistically high, for reasons outlined in the post above. A promise is, in my mind, weaker. The compiler promises that there are no bugs, but it can only know so much, it's not perfect. Formal standards are not what I'm thinking, I'm not telling you rust isn't good enough, huge improvement, but rust can not guarantee bugless, unexploitable programs. It can make some 'promises' about specific classes of memory corruption vulnerabilities. As I said to the other user, you're putting the entire system's security all on the borrow checker. 
What could happen at the hardware level if the process can't access resources that don't belong to it? &gt; there are better ways to increase performance without putting the entire system's security at the hands of the borrow checker. This is where I disagree. A hypothetical provably correct static memory analysis would be inherently better from a performance perspective than runtime checks, because it would be zero cost at runtime. I don't think Rust provides that, but it would be the best solution.
cool, I'll keep my eyes posted
I've not had a single issue (other than soul crushing compile times) since moving everything to sandboxes.
Feedback welcome! I'm hoping to use syntex to bring this entirely to stable, without the use of OIBIT. `nue-macros`is quite a mess right now, but does enable some (excessive) cool things. Also, there's a very interesting related discussion over at this [POD pre-RFC](https://internals.rust-lang.org/t/pre-rfc-explicit-opt-in-oibit-for-truly-pod-data-and-safe-transmutes/2361), which I only found out about last night thanks to TWiR ^(P.S. I missed specialization a lot when writing some of this...)
It wouldn't need any integration with cargo. All the dependent crates are already listed in the source as `extern crate` lines. Integration with rustc is enough, since it can just search the available names in the current and linked crates.
I agree with you for what it's worth. I think it may be counterproductive to try to force changes to the existing kernel since the politics are probably harder to cope with than the problem we're trying to solve. If we are really being serious about a total rust kernel we should try to design it in such a way that allows for things like decentralization and inherent encryption. I don't think we should try to replace today's kernel we should look at what tomorrows kernel will need. Maybe even hire some mathematicians to produce a 0 knowledge model. 
Thanks!
What if instead we absolutely diminish the separation of kernel and user and use encryption schemes to enforce separation/isolation instead. I think we need to learn that given enough time, humans will find a way. Instead of putting the eggs in the basket of impenetrable fortress which can only be proven via test of time. We should figure out how to utilize physics itself (rng,crypto) to do these tasks for us.
I was referring to the lack of isolation within the kernel. That's cool that rust can do all of that. edit: What I'm saying is that there is nothing special about a rust process that would limit it, should someone desire so, to access resources - the OS will not do anything to stop it once it's in the kernel. Whether you have to use unsafe or anything else is totally irrelevant because all of that is compile time.
&gt;I don't see it that way. I think a more accurate way to phrase it is that we're putting the entire system's security on the likelihood that bugs in the borrow checker will result in programmers accidentally writing applications that are remotely exploitable. That is a much stronger security statement than "the entire system's security depends on the borrow checker having no bugs". You can break it out into a longer sentence, I don't disagree - it is assuming bugs that allow for vulnerabilities in the borrow checker, libraries, or other areas of the build process. &gt;Virtually all of the borrow checker bugs (once the system matured) have been found not in real applications but in people explicitly trying to find holes in the borrow checker. Why wouldn't that include a motivated attacker? &gt;To make such a bug dangerous, there are many other steps that have to happen. The bug in the borrow checker has to be found. The bug has to be shown to be accidentally relied upon in production code. That is, code that should have been rejected was written and deployed. The issue was not a false positive on the part of the borrow checker; it actually constituted memory unsafety. The memory safety issue has to be remotely exploitable. I disagree with 4 - local exploitation is still important. Especially in a kernel service that would give full privileges to a local attacker. Otherwise, yes, agreed. Those things must be true, never said otherwise. I think I've explicitly stated as much, though perhaps that was another topic or just my imagination. Of course this is just for attacks involving flaws in the build process. Logical flaws in rust or flaws outside of the scope of its current memory protection won't require bypassing the borrow checker. &gt;All four of these steps have to succeed to actually construct an exploit out of a borrow checker bug. Now I'm not saying it's impossible, or that it never will happen. But I think there's a good argument to be made that it's going to be a rare security vulnerability, and going to be much less of a problem than straightforward logic bugs. I think it'll be rare until it starts providing an actual barrier for attackers, in which case it'll become less rare. To what degree I am curious, but I don't think we'll ever get there, the borrow checker is likely not going to be the weak link in the system in my opinion. &gt;Few people think Golang is insecure due to the Off to the Races vulnerability, and that's much worse than almost any of the other borrow check bugs that have been found that I can think of. That's because Google correctly expects that getting the bug through the (2), (3), (4) chain is infeasible in practice. That's cool. I honestly don't have much interest in Go, I'm not that big of a fan of the language and it's not something I've needed to look into much. I'm a lot more interested in rust. Rust is cool, I think you're overselling the security benefits, and you're advocating for dangerous architectural choices, using rust as a safety net for otherwise bad behavior. If you want to build that kernel web server though, I think that's great. I just don't think it's going to be as safe as you think. And perhaps we may just have to disagree, at least for tonight. edit: I've actually wanted to ask for a while but haven't remembered. You've mentioned bugs in the borrow checker, what are they? What is rust's history in terms of potentially dangerous flaws?
&gt;Yeah, by "remote" I mean "remote to the kernel"—i.e. local—in this context. Apologies for the imprecise wording :) Ah, I see. No problem, and in that case we agree. &gt;You also need a very careful whitelist of allowed functionality that client applications are allowed to touch and interact with, and this is the hard part. Right, the thing is, I feel like we agree on this, but then you say you want to run with the highest rights possible on the system. You're trying to jail a process that resides within the heart of the system. You're simultaneously assuming that the rust code is safe, while advocating sandboxing it, while advocating building it into the kernel.
This video got me interested in Elm so I looked it up and I can't find anywhere how getting started works or what Elm actually does / is. So, do I write this functional Elm code and then compile it to `index.html`? Or does it work like PHP in that it has its own server and I can have an `index.elm` and an ELM listener serving HTML / CSS / JS? So if it's the first one, it's an alternative for CoffeeScript / TypeScript etc.? What is Elm's role in the bigger picture... As a total newcomer, I am kinda sad that he emphasizes communication and noob-friendliness but these basic questions are left unanswered.
&gt;Well, what I'm saying is that it'd be interesting to semantically sandbox the program at compile time instead of doing so at runtime. I'm not talking about giving the program more privileges than it has; I'm just talking about enforcing the privileges it does have in a different way. How? You said you wanted to run a web server in the kernel to avoid context switches. Sandboxing came In halfway through the conversation. I agree that it would be really cool but I guess that's just not what I'm talking about. I'm saying rust isn't a silver bullet and the borrow checker is not a replacement for kernel separation or architectural security or least privilege or really a ton of things. Anyways, it is time for sleep. I enjoyed the conversation, I hope you can possibly see some merit to what I'm saying, if you want to discuss it further we can pick it up some other time :)
Haha I'm not the author of mio! I have worked extensively in that space however. I'm a rust newcomer with extensive c++ async networking experience.
Exporting a C compatible interface from a Rust lib and calling it from JNA should be straight forward, it would be my first choice.
I think Rust can be used to create an execution environment safe enough so that Web applications written in the kernel won't be appreciably worse off from a security standpoint.
Thanks. I changed the map to store RefCell and everything worked. This is, to me, a pretty clean way to solve the problem I faced. 
Yeah, you do lose defense in depth. Specifically, data leakage due to wild memory reads have larger scope, and RCE leads to kernel compromise instead of user-level compromise. (Are there any others?) The thing is, wild memory reads and RCE are already pretty much game-over vulnerabilities on servers even if it's only the HTTP server that's compromised. So the most that running in ring 3 is buying you is that very bad situations turn into very very bad situations. Assuming, of course, that the apps you're running are not intentionally written to be malicious. I think the reduction of userspace and driver copies probably helps more than the context switches.
Huh, for some reason I though you were someone that had submitted patches. Ah well maybe my memory was getting mixed up. Either way the above is a nice way of doing it. The only thing is that it's putting everything on runtime, so it could panic on a weird scenario which is not good.
This sounds like singularity, a MS research kernel with a single address space, implemented in a safe subset of c#. Instead of separated processes, it had tasks that run all in the same address space and they are isolated by strict api boundaries and compile time guarantees. In general, modern kernel design means minimizing the kernel. Singularity effectively has no kernel, just a language runtime. Other modern models are micro kernels where add others have pointed out the aim is to move as much as possible to user space and the kernel is reduced to a minimal ipc system. Also there are exokernels where the kernel functionality is provided as user space libraries. And of course there are hybrids of these two main approaches. 
For interested people, the SPIN operating system at UW did roughly this (link Modula 3 code directly into the kernel). I'm not sure what their conclusions were, or why it never took off (if it was only the lack of a performant memory-safe language more popular than Modula 3, sounds promising). https://en.wikipedia.org/wiki/SPIN_(operating_system)
The ability to read private fields doesn't mean the ability to read arbitrary memory addresses. OOM would take some care to handle safely. Actually, panics in general are something that would take some care to handle properly. There needs to be some way to safely kill a malfunctioning Rust thread, without aborting the entire process. Shouldn't be an insurmountable problem.
great! Sign up on meetup.com, I'll email out details when I make a plan
What about simply using Lines&lt;T&gt; in your implementation? It's basically an iterator over BufRead lines. See https://doc.rust-lang.org/stable/std/io/trait.BufRead.html#method.lines https://doc.rust-lang.org/stable/std/io/struct.Lines.html
&gt; We should really steal their semantic `diff`. Cargo `semantic-diff` would be a blessing. Do this. Please. Yes! There is a [Cargo issue](https://github.com/rust-lang/cargo/issues/374), but i don't think anybody has started work on this. (BTW, `elm-package bump` shows what the next version's number should be.)
So I guess the safe version would be let year: String = date.chars().take(4).collect(); Probably not too relevant for date strings.
In that vision I expect Rust would take OCaml's place so you'd get Rust code running directly on top of Xen (or something else, the point of the hypervisor in that case is just so you don't have to develop hardware drivers and can run on top of a guest OS for easier development).
&gt; I would want to read every line into a file. Did you mean "of" instead of "into"? &gt; Could you write an example to implent BufRead (or Read) trait. Did you mean "use" instead of "implement"? The `BufRead` trait is already implemented for lots of types, including * `BufReader&lt;R&gt;` where `R` could be a `File` * `Cursor&lt;&amp;[u8]&gt;` You can use the last one to feed a parser with data that's living in some `String`: use std::io::{ self, BufRead }; // parser accepts any `BufRead` fn parser&lt;B: BufRead&gt;(br: B) -&gt; io::Result&lt;()&gt; { for line in br.lines() { let s = try!(line); println!("| {} |", s); } println!("---"); Ok(()) } fn main() { match || -&gt; io::Result&lt;()&gt; { let s: String = "first line\nsecond line".into(); // Cursor&lt;&amp;[u8]&gt; implements BufRead already try!(parser(io::Cursor::new(s.as_bytes()))); Ok(()) } () { Err(x) =&gt; println!("Error: {}", x), _ =&gt; (), } } 
I dont agree that its game over. A compromised userspace web server can't start serving dhcp requests, or read arbitrary data of other applications, just to list two examples. Userspace can do zero-copy networking. It doesn't require running in the kernel.
Just beware of the numbers referring to byte positions and not character positions. `date` refers to an UTF-8 encoded string where these digits can be represented all in a single byte, so, in this case, there is no difference between byte index and character index. Also, if you allow the user to enter "xyzÄblah" (attempt to split between byte 3 and byte 4 would yield invalid UTF-8) or "xyz" (too short) and try splitting the string like that, your program will just stop. So, you probably want a better user-input-error handling.
&gt; There will be one or two beginner talks. It will be a rather informal meeting and the focus will be getting a feel about how active the Rust scene is in the Cologne area and who is working on which projects.
Sorry for the short response, but both cargo and xsv use Rust's unit testing to test the compiled binary. Both use similar code for discovering the binary name and creating scratch space. But ya, it boils down to defining lots of helper functions/macros, which looks like the trajectory you're on.
Normally I start a rust tutorial, get to the section about memory management where I feel like the author is glossing over pointer/reference syntax and move the on to something :). I want to emphasize that I don't move on out of frustration, merely because I think rust is extremely interesting and I always plan to come back to it when I can devote more time to it. More to your point, the reason I, a Java dev, is interested in rust is to learn more about stricter control over data layout, memory management, being able to use more of what the latest CPUs offer (instructions for vector calculations, concurrency instructions, etc).
This is now second time I've heard the term OIBIT and I have no clue what it actually means. Could someone ELI5 that for me?
Looks like since I looked at https://github.com/tsgates/rust.ko last week there has been an update with the removal of the `fixup.rs` - seems like I had a crack at this days before it was updated!
A compromised Web server can start making arbitrary requests to the database (allowing full dumps of username/password info in most cases) or start injecting malicious JavaScript into pages. SSL keys for the server can be stolen, allowing the attacker to start posing as the page. For most Web apps, it's hard to imagine how compromise can get worse than this. The one thing that it does prevent attacks from doing straight away is attacking other systems that happen to be inside the same network (or, likewise, VMs on the same machine). But everything I've read states that rarely does this pose a barrier to a determined attacker: after getting RCE, pivoting that RCE into full network takeover is the easy part. They've usually got arbitrary write access to the database, after all. There's a reason why Heartbleed (userspace information leaks) and the Rails YAML vulnerability (arbitrary userspace RCE) were considered fire drills. It's because running in userspace is little defense against the disastrous security compromises caused by RCE.
If the extra protection can be made truly free, then sure. But there's a large gap between "a research OS" and "what can be done inside mainstream operating systems today". I mean, that paper doesn't even implement the POSIX API (since it's too slow) and instead implements a shim on top of libevent.
I didn't know this was in the works. The presentation was really cool!
Well, hey, if it works then I'm happy to suggest that we port Hyper and MIO to that instead. I don't actually care how the implementation works internally. Mostly I'd just like to make a general point that we shouldn't shy away from dropping runtime checks (such as MMU paging, context switches, and copies) that are there to duplicate things that Rust already provides at compile time. I don't think that, once you have memory safety and the ability to provide compile-time sandboxes, that the ring 0/ring 3 divide buys you that much in terms of security.
&gt;Ah, well I wouldn't count that as an inherent problem, because anything can have bugs: the kernel, the hardware, the security measures around the physical location where the machine is located, etc. Anything can have bugs. Including rustc, including the language's entire idea that ownership provides safety, etc. All of this is potentially flawed. So jumping straight to 'we no longer have to care about least privilege or anything else, put whatever you want in the kernel' is a very dangerous thing to come out and say. &gt;As I understand it, the whole argument of pcwalton hinges on the assumption that the compiler and whitelisted libraries are sufficiently bug-free to be on equal footing with the kernel, which makes the only remaining issue that more code =&gt; more attack surface, which is a argument I could understand. I don't know why there are any assumptions that the compiler or libraries are in any way bug free when historically this has never ever been the case in any program, including formally verified programs. Besides, rust simply *doesn't* guarantee a lot of things. I can still write a web app that, when you send it "do thing", it does a thing no matter how dangerous it is - rust has no concept of malicious intent. And even without flaws in the borrow checker (or other areas of the build process like optimizations), do you honestly believe memory corruption is completely known to us? That we have found every technique, or otherwise that rust will prevent us from all future techniques? This has, to date, never been the case. It seems incredibly dangerous to tell users that rust is safe when it very certainly isn't safe, it simply promises it is safe in a very tiny context of security. Rust checks your code. The language provides semantics that make it possible to check for specific things. That is all. Code outside of 'unsafe' blocks is not safe, it is just checked. Code within 'unsafe' blocks is, itself, not necessarily unsafe but it is unchecked. I think the rust community should try to get behind this, because years ago when the language was starting to get some attention this is exactly what I and many others were worried about - that people would think rust is a solution when it is merely a language that lends itself to static analysis. Really, the 'unsafe' block couldn't be more poorly named and I think this entire topic kind of shows that.
Thanks!
Thanks!
I'll definitely check it out, thanks!
It will need to access the namespaces of the external crates, and cargo is needed to download the crate source codes if they're not available yet and to point out where the source files can be found.
OIBIT is [opt-in builtin traits](https://github.com/rust-lang/rfcs/blob/master/text/0019-opt-in-builtin-traits.md), the feature that allows `Send` and `Sync` to work and automatically be derived on types that only consist of other types that already derive those traits. POD seemed like a good fit for it too, to let the compiler ensure that the type is only made up of POD types. I've since reconsidered that stance, and am using syntax extensions and unsafe traits to make these guarantees instead.
Thanks, I've actually just finished my syntex support - it's up on crates.io now! POD differs from serialization in that its purpose is to make a safe guarantee that a type can be transmuted to/from a byte slice. It's not necessarily only for serialization to/from a Read/Write. It could actually stand alone as its own crate, which is why I named it `pod` rather than `nue-pod`. As for `NueEncode` and `NueDecode`, these are more similar to serialization, but still differ in some important aspects. Their main purpose is to provide simple parsers for arbitrary binary formats. They are not commutative, as encoding will often lose type information, and decoding needs extra information to process the types (vectors and strings are an example of this, since their length isn't necessarily encoded in the stream). Whether this can be expressed through a serde serializer, I'm actually not sure. `nue-macros` does some crazy stuff with embedded expressions in attributes to allow for "creative" expressions of binary layouts. (also the reason Encode/Decode are in the `pod` crate and not `nue-io` is because I can't `impl&lt;T: Pod&gt; Encode for T` in an external crate without hitting issues with overlapping traits. Needs impl specialization!)
How often do people ever take over servers by exploiting memory corruption? Is there a single instance of that ever happening? &gt; I also am not sure I buy that one is always able to take over the rest of the network from user mode if there's a vulnerability there; that depends quite heavily on your server's setup and is a huge reason to sandbox it in the first place (especially given the increasing prevalence of large botnets). Oh, I don't think attackers are *always* able to take over your network when they achieve RCE on your Web app. Rather my position is that (a) they're *usually* able to take over your network; (b) they often don't need to take over your network to perform catastrophic attacks (full database dumps, serving malicious JS, building botnets).
A question for one of the examples in the article: Since both "walk" implementations in the "Walk" trait are 100% identical is there a way to just write "impl Walk for Dog, Bear {}" or something? struct Dog { walked: bool, } struct Bear { walked: bool, } trait Walk { fn walk(&amp;mut self); } impl Walk for Dog { fn walk(&amp;mut self) { self.walked = true } } impl Walk for Bear { fn walk(&amp;mut self) { self.walked = true } }
I've done this primarily with Python scripts to run the compiled binary and test the output. But that has some obvious drawbacks. (Shameless plug) As of [clap](HTTPS://github.com/kbknapp/clap-rs) 1.x you can simulate command line arguments by using 'get_matches_with()` which takes an arbitrary iterator of "command line arguments" and then include all your tests via normal Rust unit and integration tests. 
https://github.com/insanitybit/Cricket/ This project is designed to act as a (non-fuzzer-specific) distributed fuzzing harness. The goal is to: 1) Provide a standard API within rust for multiple different fuzzers (currently only works with AFL, but I'd like to add Peach support) 2) Allow for those fuzzers to share their corpora in a developer-defined way 3) Use the above to implement a distributed fuzzing system It does the above, but only with AFL right now. I've had fun using it on some ec2 instances so far, but only for testing. In the future I'd really like to build some machine learning into it, so that *how* the different fuzzers share their corpora is handled by the ML. Lots of open issues. I lack formal expertise in almost every aspect of the project. I've learned a lot, which was the point, and I continue to make progress, but I'd love to get someone else to have a look if only for opinions or ideas. Places where you could help: * Documentation needs a lot of help still * I'd love to see some [test] and [bench]'s * Writing a struct that implements Fuzzer for the Peach fuzzer or any other (even a web fuzzer, doesn't matter as long as you can implement Fuzzer for it) * Building the 'worker' code into a Harness struct so that a single rust server can have multiple different harnesses, which opens some fun doors. * Improving the AFL struct - there's a ton here. Improving scoring, which I have lots of ideas about. Improving how a lot of it is handled in general, some functions always return the same value because they're just placeholders. * Generating / storing score information on the server side (currently done by my History struct, which needs a complete overhaul) * Genetic algorithms * Supervised learning algorithms **This is not an exhaustive list.** These range from pretty hard to pretty easy, from rust-heavy to rust-lite. Some of it's research. There's really a ton to do here and github issues for most of the above and some other things. And if you have ideas not mentioned in the issues, just make an issue. If you have any questions feel free to ask.
https://github.com/pistondevelopers/opengex The OpenGEX format is a game engine exchange format for 3D scenes, with plugins for Maya, 3D Max and Blender. I am working on this for the Piston project. The goal is to have a library that can extract data from OpenGEX, without depending on the rendering API. A subset of the format is parsed and validated by the piston_meta library. Would like some help to design the API for searching and extracting data from the tree structure.
There isn't, because you can't be sure that any `Walk` implementor has a `walked` member: impl Walk for i32 { fn walk(&amp;mut self) { println!("yup, you can do this"); } }
Obviously
Great work! Looking forward to reading more about this. For an aside, I took a bit of time and looked in to making a sort of concurrent style hashmap myself called [cubby](https://github.com/viperscape/cubby) that sort of filled a niche-- allowing independent access to shared data. Though, I should probably move away from using rand to uuidv4. Anyway feel free to give it a peek; I see that you're busy! On a general level, what has been your experience using Rust and building this project?
&gt; For instance, by default Rust has the mpsc channel, but not the reverse. I would like to see all 4 different versions implemented eventually, but the comm package solved that particular problem. I recently wrote [`chan`](https://github.com/BurntSushi/chan) which also solves that problem, but works on Rust stable and has a select macro that works a lot like Go's `select` statement.
The homework problem is a bit, uh, problematic, in that it tells you to write a class hierarchy in Rust. Rust really doesn't do inheritance, as you discovered when you tried to work around it. Creating a class hierarchy in the real world usually isn't an end to itself, you'll have some kind of problem to solve, and usually there's another way rather than using inheritance. A more Rustic way would have been to have traits for the different kind of attributes that animals have, for example being cold blooded, or able to fly. On a more concrete note, you really should not be using `unsafe` anywhere in this code. There's simply no need. You could have used an [AtomicUsize](https://doc.rust-lang.org/stable/std/sync/atomic/struct.AtomicUsize.html) for the increasing counter, which would have made mutating the static variable unnecessary. Also, when you have a function which needs to use `unsafe`, you can use an `unsafe` block within that function, rather than marking the function itself as unsafe, otherwise you end up with a situation like here, were `unsafe` ends up infecting all your code. But again, you shouldn't have *any* usage of `unsafe` in this code. What you're doing in the `Animal::id()` function is prone to data races when accessed from multiple threads, so it breaks Rust's safety guarantees.
If the exercise is to demonstrate that different types can offer common functionality via traits, then that certainly fits the bill. However, it relies on state that is often unnecessary for the objects to contain, and there is no semantic enforcement of the values for the different types. This, IMO, misses the real magic of traits, especially in the context of a zoo. I would make a bunch of traits such as trait Animal { fn breathe(); fn get_body_temp(); } trait Carnivore { fn eat&lt;A : Animal&gt;(other : A); } trait Bird { fn fly(); } Now you can 'mix' in functionality that is specific to the classification of the animal and get the desired results, for instance, the constraint on Carnivore::eat is that the target must be another Animal, that is it must implement the Animal trait. Again, this may not be the point of the exercise (I couldn't really understand the homework page) but I thought it was pretty cool. *edit* looking at the homework doc in more detail, I guess its asking for actual inheritance, which I am mostly happy that Rust doesn't do, I think classification systems better match the typeclass/trait approach to design. 
Looks interesting. I was interested in performance to start with, but outside of the serialized version that I wrote, that isn't an issue.
I don't think that rust is capable of hot upgrading, neither do I think that there are production ready webframeworks. http://arewewebyet.com
 walk(3i32); would be a thing then?
or `3.walk()`.
And ... why would that be bad? Aside from the fact that you have a walking number then. And you would have to implement it for the type with impl Walk for i32 { so maybe you want a walking number?
What do you need it for? If its only for development, then simple script watching for inotify changes and recompiling/restarting server automatically will do.
&gt; Rust isn't OOP. Well… "contains OO styles" fits it better, yeah. &gt; gists.github.com is far better than pastebin ;-) True, I might have been lazy.
Okay that makes sense. But why does that prevent an easier way to impl Walk for multiple structs at once? impl Walk for Dog, Bear {} for example would be totally fine or not? Instead of copy+paste it for both types in that case.
Yeah, in theory, I think that could be implemented.
Side note about JVM, not necessary to read:) : In JVM world there is `SBT` build tool for this kind of job. Since re-starting a JVM is pain (slow), the ability to watch sources/resources and restart some build tasks is built-in in SBT. In Rust we presumably don't need this, at least not that much. (Restarting `cargo` is comparably cheap.)
FWIW, it does have a speedy compiler.
If you have a web server, it's also not to hard to just `cargo doc &amp;&amp; rsync -ahP --del target/doc/ user@server:/var/www/site/static/rust/project/`. I mean, it isn't as automated as the other solutions, but just manually doing things is totally viable for small projects.
That's a pretty neat plug. I'm somewhat tempted to replace docopt with this.
&gt; a way to eagerly expand a macro A thousand votes here. Don't care if they catch me or whatever, I'm giving this a thousand votes *somehow*...
I can't find the name of the session on the website. What is it?
I find macros useful for performing essentially overriding in more oop based languages. It allows me to create and run similar functions differently depending on the input. There might be more useful reasons that I'm not aware of but that's mine. I'm interested in what the performance cost would be using functions vs macros
Thanks! :) But I also don't want my comment to seem like a `clap` vs `docopt` because I have a huge amount of respect for burntsushi's work (in many projects, not just docopt)! Both CLI parsers do a great job and each does certain things better than the other, or fits a particular niche space. I always say try them all and use the one which fits your project requirements or personal tastes! :) 
I agree, our current methods of building software are vulnerable. But, to me, this is an argument for MORE isolation not less.
To the naysayers: you *can* do hot reload with Rust, though it's *really* hacky and poorly supported. It goes about how you'd expect: watch for (or get explicit notification of) changes, serialise the program's state to a blob, load new library, deserialise state into new code, resume. Of course, this comes with a huge pile of constraints, the biggest being that you absolutely *must* have a sequence point where nothing else is running and all borrows and RCs have gone away. Still, I managed to pull it off for a simple little Glium renderer. Just before anyone asks: it's poorly supported because you can't have generic trait parameters, TypeInfo isn't stable across compilations (though it's supposed to be), and you can't move a value out from an `Arc`.
Well, being a systems programming language, I was hoping Rust would natively support reading from devices.
Aside: if you rename `cargo` to `cargo.py`, it should work on Windows, too. *Technically*, you should use `os.path.join`, but unlike *some* operating systems, Windows is magnanimous enough to accept the incorrect directory separator to make your life easier. If only *all* OSes were as accommodating. ;) (Unless you happen to be using a program that's so psychotically militant about this that it goes out of its way to *reject* forward slashes in paths. Please, no one do that.)
Oh wow. Thanks a lot! I'd love to create a crate for this. Will need some guidance along the way though.
You should program with functions as much as possible until you want to write a function that can't be written. Then, you write as few macros as possible until you can start writing normal functions again. The language you create should ultimately look like Rust code. I'll repeat that again: even if you do not realize it, writing a macro amounts to writing a parser for a regular grammar, and thus you want your macro's usage and result to be expectable, i.e. it takes some arguments and returns something.
Cool! Hopefully I get a chance to try this soon, looks like you've poured some love into it :) I'm also working on my own little rusty music/audio/dsp ecosystem ([here](https://github.com/RustAudio) if you're interested) - I'll defs be checking out / referring to your stuff while I move ahead :) I really like you're `DeviceChain` concept - it's simple but makes it look nice and intuitive when stringing together a few nodes in a row when you just want a small linear chain. Just thought I'd mention that where you've written the demo of it like this let mut chain = DeviceChain::from( engine.default_input(1).unwrap() ).into( Delay::new(1.0, 0.5, 0.5, 1) ).into( engine.default_output(1).unwrap() ); I think the more conventional way to format it would be like this: let mut chain = DeviceChain::from(engine.default_input(1).unwrap()) .into(Delay::new(1.0, 0.5, 0.5, 1)) .into(engine.default_output(1).unwrap()); Probably doesn't really matter though. Also, not sure if you're interested at all, but there's an effort to write a purely rust cross-platform audio backend going on called [cpal](https://github.com/tomaka/cpal) (like a rusty portaudio I guess) which could use some more contributors! For now it still has a long way to go, but I can imagine a nice future with a purely rust audio stack :) Thanks for sharing!
This post was written on July 1st and since then a bounty program has indeed been announced: http://blog.maidsafe.net/2015/07/08/maidsafe-code-bounty-program/ I had a look at a few of the tickets but hadn't dived deep enough into the SAFE network model to make much sense of it.
2 of my rust projects: [rustorm](https://github.com/ivanceras/rustorm) rustorm is aiming to be a full ORM for rust, main support focused on postgres, while sqlite and mysql support is catching up, with contributions from other developers who specifically uses these database platforms. If you are using oracle db in your rust projects, maybe you can help me with the projects. [codegenta](https://github.com/ivanceras/codegenta). This other project intelligently generates model code based on your database structure, which you can use to manipulate in your controller code. Saves you a lot of time coding boilerplate code based on your database tables, changes in your database models will be reflected back to the code when you re-run the generator.
@tracekill, can you elaborate on what you mean by »underlying infrastructure that supports OpenMPI«?
[mod_perl](http://perl.apache.org/) is a good choice with [Apache httpd](http://httpd.apache.org/).
&gt; I'm interested in what the performance cost would be using functions vs macros You can add `#[inline]` to a function if you’re worried about the run-time cost of a call. Switching to a macro just for that is overkill, you loose the type safety of functions. But if such performance "in the small" is really critical, you should benchmark it: http://doc.rust-lang.org/book/benchmark-tests.html
Well, I am trying to make this project more than just generated bindings by e.g. leveraging the type system to infer what MPI calls datatypes or having resources managed using RAII mechanisms. Why do you think implementing the underlying infrastructure (with the standardized C interface(?)) in Rust would have a more significant effect on usefulness than wrapping the C interface in a layer of Rust?
https://github.com/Manishearth/rust-clippy This is a collection of lints for use with Rust. There are tons of issues filed for more lints, and most of them are easy. I'm willing to mentor anyone who wants to try their hand at it. I find that writing compiler plugins is a good way to ease into hacking on the compiler itself, so this is a nice place to start :)
&gt; In his spare time (between about 2 a.m. and 5 a.m.) he started transposing one of our most complex libraries, Self-Encryption (the component that seamlessly splits data into smaller chunks and encrypts them), over to Rust, which at that time wasn’t even in Beta yet. This is how they prototype new technology? An employee volunteers to work for free?
Joke's on them, now he's the only one who knows how it works! A few restless nights for several years' worth of job security? Seems like a worthy sacrifice. /s
generally speaking, for established, longstanding C or C++ libraries, the key value-adds of a rust port are: * higher maintainability going forward through eliminating classes of bugs * cross-platform if the original is not. * easier implementation of any previously unrealized concurrency and parallelism performance gains. All this is to say, that until the port is feature complete and well-tested for the set of, or a specified subset of, the library's functionality, there is little reason for a rust developer to use it over a rust FFI wrapper that can be churned out in a tenth of a time of a port. If you're enthusiastic about it though, it can't hurt to look into it - at the least you'll get good practice with rust. 
He probably wanted to learn Rust and did so in his free time. That's how i learn new stuff, too. If it has a benefit for the company i work for, even better. 
It does type checking, so it only catches a subset of breaking changes. But it catches ALL of the breaking changes that would break builds. This is what's important - if you don't use the functionality that was changed the build still works.
It's not that big of a deal, the industry will survive despite programmers enjoying their jobs.
Absolutely! Tools like hwloc and netloc that build the foundation for OpenMPI topologies and provide the basis for distributed computation through MPI. These tools discover and map the local network and hardware infrastructure available to MPI-enabled programs. 
Could you post the error? I've got the code in front of me and I'm assuming it's a lifetime issue but it would be helpful.
For the exact same reason that the Rust community is pushing for a solely Rust kernel, coreutils, etc. Safety is the crown jewel of Rust as a language but its only guaranteed until you cross that `unsafe` barrier, which FFI's necessarily do. If you'd like to collaborate on some plans for implementing a solid HPC platform in Rust, I'd love to get that going in a serious way. I also want to mention that my intention was not to downplay the work you've done here in any way at all. I was actually commenting out of excitement over having found another soul interested in Rust as an HPC tool!
*Enjoying ones job* implies that you are doing a job, which in this case should imply that you get paid for doing it. No one has complained about people *enjoying their job* (!). I'm just suggesting that they might enjoy what they do, *and* get paid fairly for it. Wow, doesn't that sound great? The overly self-sacrificing *passionate*[1] programmer has already been a meme for a long time. And people are questioning how healthy this *meme* is, for the programming profession as a whole and for the *passionate programmers* themselves. If someone is interested in something (even *passionate*) other people can take advantage of that. And although you can say that it is as simple as "if they like what they're doing, so whaat if other people benefit from that". But I don't think it's that simple. First of all for the programmers themselves; just because you like going *the extra mile* for your company *right now*, is this the best way to spend your time for yourself, and will you feel this way forever? And when you don't, you have already put expectations at a certain point. Can you really temper your apparent enthusiasm *now*? For the programming community as a whole; it impacts the bargaining power of other programmers. What, you haven't learned this cutting edge, pretty domain-specific technology for your job on your own time out of your sheer "passion"? You left at 5pm last evening even though there was some outstanding work to be done? You have started a family with young children and so you don't have as much time to go above and beyond your 40 hours a week? You are 55 years old so you don't have the stamina to code something *between 2am and 5am* just for the hell of it? Work a lot if you want; code till your hands fall off. But if you're going to be working productively for something like 60 hours a week, *make them pay for it*. Don't sell yourself short so that they can discard you the minute a more fresh body comes on the market to be overworked. But things are relatively good right now so who gives a shit. Right. [1] Or at least *apparent* passion. People seem to get burned out pretty often, and might not even realize when they slip from just being *passionate* to becoming obsessed to the detriment of a life-balance, and eventual burn out. Or maybe they like programming, but feel like they should go that *extra mile* because they fear that they're not *passionate* enough. All of a sudden they've gone from being happy, productive programmers to self-doubting burn outs who don't see the point any more.
&gt; An employee volunteers to work for free? David Irvine is actually the founder of MaidSafe :-) In this [post](https://forum.safenetwork.io/t/rust-vs-c/3216/2?u=frabrunelle), he explains why he likes Rust.
isn't it just a big scam?
A few tips: * the compiler requires an explicit lifetime because of the call `self.server.accept()`, but `readable` cannot be annotated. You could make a `accept` method on `ServerHandler` which calls `accept` on the server. That one would be more easily annotated. * you annotated all the lifetimes in `accept` with the same lifetime `'a`. It is often too restrictive and hinders the compiler's work. Let it tell you what it needs by declaring your function like this: ``` fn accept&lt;'a,'b,'c,'d,'e&gt;( &amp;'a mut self, connections: &amp;'b mut Slab&lt;Connection&lt;'c&gt;&gt;, event_loop: &amp;'d mut EventLoop&lt;ServerHandler&lt;'e&gt;&gt;) -&gt; bool { ``` It will automatically suggest simplifications.
I can't just go around invoicing my employer for work I did at home at 2AM because I felt like experimenting with something and it just so happens that said experimentation could potentially be useful to the company. They didn't ask me to do that work and it's not on my contract to work such hours.
Someone mentioned this already but their comment has been deleted though. Here's the reference: http://linuxtv.org/downloads/v4l-dvb-apis/
Not everyone is going to find your repo through the main website. A short summary of what the repo does on your main readme would be helpful. 
Well, I just need to video capture part.
Downvoter - care to explain?
&gt; [...] just so happens that said experimentation could potentially be useful to the company. I did write "productive" work as opposed to *potentially* productive work. I don't know if it is normal/possible to do an invoice for work that you found out was productive, and so you decided to get compensated for it based on that. &gt; They didn't ask me to do that work and it's not on my contract to work such hours. So don't?
&gt; I did write "productive" work as opposed to potentially productive work. I don't know if it is normal/possible to do an invoice for work that you found out was productive, and so you decided to get compensated for it based on that. In the context of this comment thread, presumably David Irvine didn't know if porting the code to Rust was going to be a waste of time before he did it, so it was experimental and only *potentially* productive. &gt;So don't? I should just not do things I enjoy because I won't get paid for it?
Thank you so much. Unfortunately the compiler still complains about the lack of the annotation in readable. Any ideas? https://github.com/arthurprs/floki/commit/3254b2618a504dc717dab33b8e4c710c0c8dd275 src/server.rs:367:28: 367:51 error: cannot infer an appropriate lifetime for autoref due to conflicting requirements src/server.rs:367 SERVER =&gt; self.call_accept(event_loop), ~~~~~~~~~~~~~~~~~~~~~~ src/server.rs:364:5: 381:6 help: consider using an explicit lifetime parameter as shown: fn readable(&amp;'a mut self, event_loop: &amp;mut EventLoop&lt;Self&gt;, token: Token, hint: ReadHint)
Nice post, you should file a PR to include it into https://github.com/carllerche/mio/blob/master/README.md#reading
Didn't think of that, that might be a really a good fit. Racer already operates in the same interactive editing space the autouse tool would go in, and it needs to solve the exact same multicrate namespace search problem. The tool might work as just a new command to racer, though it would need quite a bit more compilation-unit wide smarts than the current racer commands do.
Using the Python installer gives me `python` not `py`. Is `py` something special for PowerShell that I've not heard of before?
I'm not the GP, but I think they'd answer "the investors." Wiki says this project started in 2006 and has been raising money (millions of dollars!) since. In that time they've produced a C++ codebase that has only delivered on this proof of concept-type testing. I know C++ is not pretty and it could be just packed with technical debt in that time but that's not a lot of result for a lot of money. I mean, for Christ's sake, they just threw out a 8-9 year old C++ codebase and rewrote it in an experimental new language. And it seems like the rewrite is about as functional as the old one! Clearly there are project management problems here, but when combined with millions of dollars of investor money this becomes a scam.
`py` gets installed along with all recent versions of Python on Windows. It's the launcher that decides which version of `python` to use based on various configuration files and the hashbang (if any) in the script. It exists primarily because Windows *doesn't* parse hashbangs, so there's no other way for Python scripts to declare which version of Python they're for. Actually, come to think of it, you *should* be able to just run the `.py` file directly as a script, since the default is to associate them with the `py` launcher. Then again, `buck.py` doesn't specify a version of Python *anyway*, so it probably won't make a difference either way: either it'll run with the default Python on the `PATH`, or it won't. I'm not entirely sure which is the best way to go, actually.
This is a plugin I love to use with my code. How would I go about getting involved? I haven't really hacked around with the compiler itself but I do feel comfortable with Rust as a language.
Didn't they just rewrote a couple of incomplete libaries in Rust, not the whole codebase?
I'm pretty sure that non-monetary compensations for hours worked are illegal where I live. And yes, I implicitly mean "work" as in "employed, paid work" and not just "some productive activity". Your mileage may vary, and all that. Maybe I'm being obtuse and you mean that compensation can be in the form of secondary and informal *rewards* on top of the formal rewards. I think that both employer and employee should stick to their (legal) contract and pay and get paid respectively for the time that is put in. And then if the employer or employee get some indirect compensation (?) beyond that, then that seems fine as long as the parties are OK with that. There could be alternative arrangements, but what is important to me is some kind of symmetry. i.e. if I can't argue that I didn't really miss two hours of work last week since I *compensated* my boss by entertaining and being such good company at our dinner on Thursday, then I won't allow him to argue that I shouldn't get paid for my overtime last Wednesday because I said to him that I had such a good time doing that particular work.
Apply [the principle of least power](http://c2.com/cgi/wiki?PrincipleOfLeastPower). I would amend the definition in that link to mean *choose the least powerful construct for the task at hand*. If using a function is the least powerful construct and yet gets the task done, then use a function.
scam confirmed. they also have their own coin, unneeded
hah, oops. And then all the gleeful upvotes to drive the shame of my mistake home, nice. If it makes people happy.
&gt; We all make our choices based on our own values and preferences. I have just presented and argued for why I might not want to do that. That's good to hear. For future reference, your other comments in this thread definitely did not come across that way to me.
Video showing the SAFE Network in action with 18 Vaults. https://www.youtube.com/watch?v=tjpl23HAGy4 NO SCAM! https://forum.safenetwork.io/t/safes-impact-speculation-anyone
&gt; (And I didn't expect that encouraging people to get compensated for the productive work they do for company would be this controversial, but people keep surprising me everyday.) Personally, I found some of your comments aggressive and condescending. If your goal is to encourage others to seek compensation for their efforts, then my advice is to give some extra care to how you deliver that encouragement.
I'm working on 2.3.1. Maybe I should write it somewhere. Also keep in mind that I have not tested much of the features, so there might be bugs.
Point taken. But I don't think I will take heed of tone arguments in this specific case.
*Choose the least "externally free" construct for the reasonably expected future evolution of the use of the task at hand.* You don't want to block upgrade paths ;)
I'm actually not sure if this is right because I'm new at this cross compiling thing but, I had a --prefix=$HOME/.. at the end of the ./configure target=... command Could that be it? It wants a new directory outside of the rust src directory?
It's kind of scary! I guess there are just some fairly canonical cases of when to use macros, but even that doesn't explain the almost exact same word choice...
Not 'unneeded', you need to have an incentive to give your resources to the network (storage, bandwidth or CPU). Also used to reward application developers and much more http://maidsafe.net/docs/Safecoin.pdf
Thats pretty cool. I have been playing with Piston a bit and I'll have to see if there is a way to shoehorn Carboxyl in (Piston's event stream is nice, but full FRP would be even better).
An employment relationship is defined by monetary payment. (If you're an employee (and subject to the Fair Labor Standards Act!) you're entitled to at least the federal minimum wage. Many unpaid internships are technically illegal for this reason.) I'm assuming that "doing a job" here means "for your employer" and isn't being used in the sense in which I might say that cleaning house is one of the jobs I have in my relationship, for instance. In that sense compensation *cannot* come "in many forms", it can only come in a limited set of forms. Unpaid, voluntary labor can be quite pernicious: it might be fine for you if you can afford it, but it's bad for people who actually need their work to be paid so they can pay for things in turn.
If you check commit history you will see the opposite, I hope. I started it and then spent a while doing whiteboard presentations and Q&amp;A's to pass libs over to maintainers, who have managed the libraries since and are responsible. You will see my commit history jumps in and out of libs when required, mostly cryptography, networking and consensus algorithm parts, very much in the overlay parts as well. I am directly responsible for none as an Engineer (I would love to be though) As for working free at night, we use agile now and practically ban folks from working on our code at night when sprints etc. are on. they can do bits and prices but not our code as we have folk in different time zones who take over tasks. Also health reasons are something people should consider for their colleagues. As a founder (don't like that, I am a team member no more or less important) then it's not a few hours of code you will do for free, I work 18-19 per day 7 days a week, no holiday over a day or so for about 5 years and completely responsible to make sure everyone is treated well and gets paid. so a few hours of rust is a pleasure not a chore :-) A big and important part of the project is a mechanism to allow folks to write code and release with automatic payments. This means less stress hassle and companies needing formed. If you write something that becomes popular the network will pay you. If it works just imagine how great it could be and how that could improve over years of improvements by other programmers. Could be great. 
That's still a gray area at worst. Again, you presume a crime without providing the requisite proof. Even if you turn out to be right, you should either clearly mark this as opinion or provide better proof.
&gt; Code generation in the item level. This is obviously impossible with functions :) What is the item level?
I'm going to press on and say that if you are an employee and feel inclined to do extra work because you find it interesting, you should insist on getting paid anyway, even if you'd do the work without being paid, just to reinforce the norm that one is to be paid for the work done for one's employers. Do it for others' sakes, even if not your own. I very much consider this something that other people also should do, and not merely what I consider in line with my preferences for my own behavior. (My preferences extend to the actions of others!) Atomism = failure.
They are apparently working in an actively researched area including cryptographic protocols. I'm only remotely knowledgeable in that area, but I would not rule out that having a watertight PoC in that domain actually *is* worth a few million $. Also reimplementing a working system in another language is *much* easier than coming up with said system in the first place.
Thanks Steve!
Keeping all of the working data encrypted in RAM doesn't even seem possible with current hardware, and if it is, the computational overhead would be unfathomably enormous, having to encrypt/decrypt (verifiably!) on every memory access/store. Cryptography is a strong defense on the network, where we're passing data around. It just isn't feasible to try to have all three of integrity, secrecy, and authenticity when your adversary can control your machine.
I filed a PR as per killercup suggestion, https://github.com/carllerche/mio/pull/218 let me know if you want to take it over.
Closures have the same return semantics as functions. You can use `return` in a closure to return from the closure itself, but never to return from the scope in which the closure is declared. fn main() { println!("{}", (|| return 1)()); }
No they couldn't use bitcoin, or an altcoin because as you know bitcoin doesn't scale. While Satoshi was inventing the blockchain, David Irvine was inventing a set of technologies that do even more, and without the limitations of centralisation risks, of the blockchain not scaling, and proof of work confirmation. The result is an entirely different technology that, if it delivers, will surprise even bitcoiners. Safecoin is just one part but it alone would be enough to justify the network because it will be energy efficient, scales with network size, has instant confirmation, massive transction rates, and zero fees. The network is being built and tested right now, and the final foundation blocks carved into shape. You can read the code, build it, test it, watch the repositories etc. I've been following this daily for 18 months, it's real, and it is going to change the world! IMHO :-)
I could! I'm lazy. (and busy...) :P ((might do this when I have some more free time)) 
I'm not interested in valuing my work and time based on what some law happens to say, sorry. &gt; Unpaid, voluntary labor can be quite pernicious: it might be fine for you if you can afford it, but it's bad for people who actually need their work to be paid so they can pay for things in turn. Do you think this is somehow inconsistent with what I said? Seems like it fits perfectly with "people value things differently."
 λ py Python 2.7.10 (default, May 23 2015, 09:40:32) [MSC v.1500 32 bit (Intel)] on win32 Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; I'll be damned. Can I make it default to Python 3?
Might I simply suggest `let path = parts.connect("/")`? Note that this will not add the leading " " or trailing "/" you are currently adding. Alternatively if you want to do it manually `path.push_str(parts[i]); path.push_str("/");` There are several issues with what you're trying to do: * String concatenation with the + operation is radically inefficient for building a string. Unlike e.g. Java Strings in Rust are not immutable and are built to be growable and changed. Use methods on String to mutate in place. * You made path an &amp;String, which is immutable, and also a borrow of a temporary (the result of " ".to_string()). * You try to do the same in the loop, but this time the temporaries are contructed and lost in each iteration, meaning it will be lost. I'm actually not sure why you're getting this particular error; Rust is getting confused trying to work out what's going on I guess. Edit: Also just consider using https://doc.rust-lang.org/std/path/struct.PathBuf.html because building paths correctly is hard.
`path` must be a `String` for this to work. A direct parallel of your code that works would be this: let mut path = String::with_capacity(1 + parts[..parts.len() - 1].iter().fold(0, |x, s| x + s.len() + 1); path.push_str(" "); for part in &amp;parts[..parts.len() - 1] { path.push_str(part); path.push_str("/"); } (The `String::with_capacity(…)` approach rather than using `" ".to_string()` is for efficiency so that it allocates an appropriately sized `String` all at once.) … but I’m dubious that this is all what you actually want—if you want to include the last element and don’t want a leading space, then `parts.connect("/")` is the right thing instead.
The reason I'm doing that is that these strings are part of a filepath, and i need to check in every iteration if this filepath exists, so IMO I've no other choice than doing this with your method (Which is btw really really nice) Thank you for providing the solution for that, it works like a charm now :)
I'm sensing an XY problem here. Looks to me you want to make a path by turning its individual parts into a string, which [PathBuf](http://doc.rust-lang.org/std/path/struct.PathBuf.html) is much better suited for. But anyway, just for fun and as a learning experience, lets get your code working. First of all, strings aren't concatenated with +s, so lets use the push_str() and push() methods to add the bits you want: let mut path = &amp;" ".to_string(); for i in 0..parts.len()-1 { path.push_str(&amp;parts[i]); path.push('/'); } However, this still won't work thanks to a little mistake made earlier; `let mut path = &amp;" ".to_string()` means `path` is going to be an immutable reference to a string, not the string itself. That's not what we want, so lets cut off the &amp;: let mut path = " ".to_string(); for i in 0..parts.len()-1 { path.push_str(&amp;parts[i]); path.push('/'); } There we go, compiling! However, still got issues! First of all, C-style array iteration isn't all that rusty, and it looks like the attempted loop even has its own bug! Let's fix that: let mut path = " ".to_string(); for part in parts { path.push_str(&amp;part); path.push('/'); } Much better! I think I'll stop fixing things there, as after this we start getting into the nitty gritty of paths, which is filled with complicated everythingness, so like I said, just stick to [PathBufs](http://doc.rust-lang.org/std/path/struct.PathBuf.html): let mut pathbuf = std::path::PathBuf::new(); for part in parts { pathbuf.push(&amp;part); } let path = pathbuf.as_path();
&gt; If a shop, restaurant or other establishment needs to be cleaned every day, but there is a culture of voluntary cleaning for such businesses by people who are so passionate about cleaning and being tidy that they do it for free, that might be a whole occupation of eligible for-pay workers that is eliminated because a culture of free cleaning has been established. Culture changes all the time, and with those changes come new jobs and the expiration of old jobs. Nobody is to blame for this. It just happens as society evolves. I really don't see anything wrong it.
Then I'd second /u/Gankro's hint towards [PathBuf](https://doc.rust-lang.org/std/path/struct.PathBuf.html). It's basically a `String` that's especially made for paths with things like non-UTF8 encodings in mind.
Word of warning. This is a drunken post. Bitcoin with Lightning Network scales fine and do handle microtransactions well. No problems here. Lets go though requirements. a) microtransactions - CHECK. Just send updated transaction to hub. b) centralization - debatable. Bitcoin may get too centralized in future and get shutdown. But this is a known thing. Unintended properties of Safecoin are unknown. c) proof of work - moot point, still cheaper than banks. And its outside of maidsafe. And we have nothing better that actually works. (PoS does not work!) d) being online at the moment of payment: both Bitcoin+LN and Safecoin are identical in this regard. Unless payer is not identical to consumer OR payee is not identical to provider. e) scales with network size - totally f) zero fees - very debatable. Bitcoin+LN might be very cheap. And you still need to sell and buy Safecoin somehow. And there are your transaction fees. Safecoin might be a better candidate then Bitcoin+LN. Problem - its a research project. Bitcoin is rock solid, LN is simple and easy to understand, with known risks. How many persons have tried to break Safecoin? EDIT: PS. I share your enthusiasm on Safe Network.
I trust you will have the integrity to be just as nonchalant and stoic about it when the changing wheels of fortune lays you beneath its hooves. Oh well.
I don't think it has anything to do with "people value things differently", which is why I said it's *bad* for people who need to be paid, not that they happen to value being paid. (They probably do value being paid, but that may simply be because they value things that you also value, like food and shelter.)
Always best to post sober ;-) I acknowledged SAFE is untested, whereas bitcoin is known, and was pointing out why bitcoin was not chosen as the reward token, and what sets Safecoin apart from bitcoin and altcoins. As for which is best, let's just see how it pans out. 
Posting drunk is double fun! ;-) I really need to dig into Safe Network sources. Great opportunity to finally do something practical with Rust.
Safecoin is more efficient in two ways: - confirmation involves 32 nodes, not the entire network. Just compare the energy cost here. - confirmation is computationally easy, not trying to solve a really hard math problem And it is even more robust to attack. Despite only 32 nodes doing the work, you would need &gt;80% of network to falsify a transaction. 
&gt; Despite only 32 nodes doing the work, you would need &gt;80% of network to falsify a transaction. This is a poor defense against Sybil attack. What is the cost of node creation? Processing power is expensive, and that's why ~~PoS~~PoW works. I'm afraid that scheme you've described puts real nodes which correspond to physical machines against attacker's nodes that can be cheaply simulated. How do you distinguish large network behind NAT from attacker's farm on one strong machine? EDIT: typo EDIT2: I just understood that STAKES are low. You can use farm to create sibils, or you can use this farm to provide services. This indeed might work. Unless tokens will be used as Money. Sniping large transactions might still be worth the effort. EDIT3: you don't really need to distinguish attacker farms from NATted networks. Just count them as single vote.
To respond further to the allegation of having pulled a trick, if half of a couple ("A") didn't give a toss about the state of the kitchen, for instance, and the other half ("B") wanted it to be kept clean, then if A kept on not cleaning, B might remonstrate, calling A self-centered, for not taking B's preferences into account. This, I think, would be just. You might say that it's different in the members of a couple since they have some kind of presumptive mutual obligations to or interest in each other, whereas there's nothing binding random person X to random person Y, but then I'll just say, well, that's your basic atomistic view innit?
Who decided that connect would be a good name for something which is called join in almost every single language?
Looks like it will be renamed to join: https://github.com/rust-lang/rfcs/pull/1102
I never said it was a good thing that people lose jobs. But like the tides, nobody can stop the shift in culture over time. Culture will always shift and jobs will always come in and out of existence.
The bitcoin network doesn't have any real work to do, so miners have to do a bunch of make-work number crunching to confirm transactions. It'd be awesome if it could be doing SETI@HOME or something instead, but that's not really feasible. If Safecoin can manage to use some sort of useful work (e.g. storing user data) as it's proof of work, that'd be a significant innovation in cryptocurrency.
&gt; Well ... yes? I'm not sure what the objection to this could possibly be, actually? My objection is that "not MY PREFERRED BEHAVIOR" doesn't necessarily translate to "self centered." &gt; and he or she knew I was burning the midnight or weekend oil after going home on work-related projects, he or she would either say I should be remunerated for that, or should take extra time off (and would insist that I took time off!), or should all things considered just not do that. None of this is exclusive to what I said. In fact, my employers *have* said just that several times. Some times I say, "Yeah, you're right." Some times they say, "Have the day off tomorrow." Some times I say, "No, I'm having fun and I want to do this." Of course, this wasn't in your original calculus. Your original statement was to insist on being paid---or not do it at all. My point here is that some people don't see their social relationships (even when it's their boss) as so crisp and would rather not treat them so coarsely. It sounds like you understand this, but it was certainly not the impression you gave in your original comment. &gt; then it seems very much to me that your continuing to say "well, to each his own" is precisely an atomic view of the world, and a denial of any community or any idea that we have reciprocal obligations to each other. The amazing thing about words is that I either vehemently agree or disagree with this based on the definitions of several key words. But, that is, I fear, veering too far off topic. Frankly, I don't see "atomism" and "community" as exclusionary concepts, which is why I took objection to your binary labeling in the first place. &gt; nevertheless, I think it's important for individuals to be aware of the significance of their choices even if no individual choice changes the system I find this very very easy to agree with. I find your initial statements *much harder* to agree with. This leaves room for the individual to maintain a delicate balancing act in their social relationships. Your original "if you are an employee and feel inclined to do extra work because you find it interesting, you should insist on getting paid anyway" is just impossible for me to get on board with.
&gt; To simply say "things change" (like the tides...) is just an empty truism. Well... yes. That's the point. You concocted a crazy example thinking I would agree that it was undesirable, but I recognize it for what it is: a shift in culture. The key point I'm trying to make is that one cannot assign blame, but your comments tell a different story.
&gt; You concocted a crazy example thinking I would agree that it was undesirable, It's not a *crazy* example as much as it is a simplified and illustrative one. Like "Hello World" is not an unrealistic and useless program, but an illustrative one. And keep telling yourself how you understand how things *really* are, namely... just that stuff changes. That just seems like a weak excuse for not caring -- and I don't care about a lot of stuff, and I couldn't if a I wanted to -- so why not just say that. Have a nice life.
None of this, please.
Does it count if I only use other peoples' generics (you will never separate me from my `Vec`s), and don't write any of my own?
You don't currently check in each iteration if they exist. Here I have some code that does, though. parts.into_iter().fold(Ok(PathBuf::new()), |result, part| result.map(|full| full.join(part)).and_then(|full| fs::metadata(&amp;full).map(|_| full)) ) This code will fold your strings into a Result&lt;PathBuf, io::Error&gt;, returning an error if the path doesn't exist or some io error occurred while checking if it exists (you may replace `metadata()` with whatever function you were already using to check if the file exists).
I do work outside of working hours and it made my employer value me more than my peers, to the point where they actually aren't my peers anymore, as my employer decided to promote me and give me a big raise. So, I did get paid, just probably not in the sense you were hoping for. Have fun w/ the struggle, I'll be over here enjoying my fat stacks, by which I mean supporting my family and social causes that actually matter, instead of sandbagging to prop up the wages of mostly white, mostly male, mostly overpaid anyways programmers who can't be bothered to keep their skills sharp.
It's a good idea to check PRs if you're struggling with something, especially if it's a small project where you can reasonably look through everything. Case in point, I had that no-fixup PR there for almost 2 months before the author merged it. It was just a matter of using the right codegen options. Doing something more useful than "hello from rust" is left as an exercise for the reader. :)
This problem is easily solved by making SubClass object safe, for instance by demanding that `Self` be `Sized` when calling `SubClass::new()`. trait SubClass{ fn new() -&gt; Self where Self: Sized; }
The `as_slice` function can be rewritten without unsafe! #[inline] pub fn as_slice&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a [T] { &amp;std::slice::ref_slice(&amp;self.value)[..self.is_some() as usize] } (unfortunately, [`ref_slice`](http://static.rust-lang.org/doc/master/std/slice/fn.ref_slice.html) is unstable). And it seems like LLVM does its job in eliding bounds checks, because I don't see any speed difference. I hope I didn't diminish the value of your achievement ;)
What if it doesn't pan out? What if the experiment is a failure? Should the company still compensate you? What if they don't give you the go-ahead, but you *really* want to try it out, and you need some project that you're already familiar with so you're not fretting over the details instead of trying the new tech?
6 hours isn't uncommon, but 7 days a week is pretty rough.
I may want to re-phrase that. Edit: Or maybe not.
&lt;https://docs.python.org/3/using/windows.html#customization-via-ini-files&gt;
You didn't. ☺ For now, it'll stay unsafe, as I want it to run on stable.
And...another one for QOTW. You folks are *so quotable*. Every time I try to say something clever, along comes...well, usually Manish, but also others, and *poof* – we have a new quote of the week.
Mine just panic!ed
Yup, thank you. I believe your solution is the best so far. Quick question though : when you add Self + Sized as the return value what actually happens to the objects in the container? Do they lose information about their actual class and get typecast to the base class? 
You can't. A lifetime that only shows up in the return type is a good sign that what you're trying to do is impossible. In order for a borrow to exist, something must own the thing being borrowed, but there's nothing that could possibly own the thing being borrowed here. Also, I think you've fundamentally not understood dynamically sized types; `[u64]` is a DST, but `Table` is not because it doesn't *contain* any DSTs; it only contains a `&amp;[u64]`. Your problem has *nothing* to do with dynamically sized types. The [Ownership](http://doc.rust-lang.org/book/ownership.html) and [References and Borrowing](http://doc.rust-lang.org/book/references-and-borrowing.html) chapters of the book might be helpful.
Lifetimes are a compile time thing. They don't determine how long something actually lives. The Vec you are creating here will only last till the end of the block no matter what, therefore any references to it will become invalid. The compiler is telling you about this but you're misunderstanding and think that the solution involves just getting the right lifetime annotation. Trying to work with `[T]` and creating your own sequence of T's is basically reinventing Vec, so the solution is actually pub struct Table { table: Vec&lt;u64&gt; }
I think using a Vec would be a better solution since I'm doing a lot of what Vec already does, thanks
That makes sense, I think what I'm trying to do has already been done in Vec, so it would best to use it. I might take a look at the way Vec is implemented.
&gt; Have fun w/ the struggle, I'll be over here enjoying my fat stacks, by which I mean supporting my family and social causes that actually matter, instead of sandbagging to prop up the wages of mostly white, mostly male, mostly overpaid anyways programmers who can't be bothered to keep their skills sharp. "Fuck you, got mine." Damn, my plan to prop up the working class white man has failed. /s
&gt; I don't think it's fair to describe your being downvoted as due to unpopular ideas. /r/rust is a civil place by convention, and comments that come across as less welcoming don't typically fair well. How do you distinguish between a community being nice and fair and balanced and a community just being *nice* to reasonably popular ideas, then acting subtly hostile[1] towards people who propose such ideas, in turn making such persons agitated. At which point the popular community opinion can say "hey, he's acting like an asshole" because he let's that agitation shine through the cracks? Of course, further defensiveness and explanation on my part just makes me seem more *confrontational*, further *proving the point* that I was confrontational to begin with. Like I'm doing here. It's like a small group subtly agitating someone (and I am easily agitated, though I try to hide it), then getting that person agitated, and saying "woah dude, you should probably work on your attitude/tone/presentation. You catch more flies with honey. Just a thought :) ". Of course they were not *that* "confrontational", and given that they reasonably agree with each other, they don't particularly see any "aggression". So they conclude that it must be *that* guy who is being confrontational and unreasonable. It's honestly very frustrating. And it's not like anyone here at this point is going to "believe" this post, in any case. Instead they'll see one unreasonable asshole and a few others who are either caught in his crossfire, or some *kind* souls who are *gently* suggesting that he should perhaps be more nice. And oh, how nice we are, to be so understanding and patient with that asshole, kindly suggesting to him that he might want to change his tone and attitude for ~~what we prefer~~ something objectively better and more pleasing... This truly is a *civil* community, my goodness. [1] This is a pretty "tone argument", which I didn't want to make about such *subtle* behaviour, instead sticking to facts. But I'll break my own rules this time.
Check the forum if you want to dive into attacks. As you can imagine, this has been suggested before and answered. The is a lot of information there to grok but I've yet to see anyone take the time to follow through and seriously try to demonstrate an attack despite being encouraged to do so, because it would help make the network more secure, obviously. Like bitcoin, the proof will be in the network operation, so I'm not going to get into this, others might, but I've read enough discussions of this to know the only proof will be in a live network. 
And you can even get paid for it - developer bounties just started. 
Not yet
My point was simply that there are already a number of under-developed generated bindings for OpenMPI. Granted, this project seems to have some value add for sure. We're all just side-stepping the inevitable however. I really have no idea why you'd buy into Rust for actual research applications at this stage, but godspeed.
This doesn't work for the `bool` case though.
Of course there is some risk to bet on Rust for such applications at this early stage (as there always is in research). But, I hope, given the excellent FFI, much of this risk is taken away. Potential problems probably can be sidestepped more or less easily, by using battle tested libraries for certain tasks. I see no problem in that. Effectively it gives me a lot nicer language than C/C++ to formulate my problem in. I don't think »nothing or all« applies here necessarily. In theory at least ;). I'll see where it takes me. No risk, no fun.
I think I will just implement conversion traits and keep the same core.
The problem, I think, is that Rust does not know that the context still exists at the time you have finished the long work. Maybe the context has been destroyed by the main thread by that time? The second problem, I think, is that there is no way the GTK bindings can prove to the compiler that the closure inside `idle_add` is actually executing the thread that `context` belongs to. You could potentially put `context` in an `Arc&lt;Mutex&lt;&gt;&gt;`, but that seems a bit silly so I would probably go with `unsafe`. Assuming you yourself are totally certain that `context` is not destroyed by the time the worker thread finishes.
`@fjh`'s answer is amazing. Such a concise example of where having different lifetimes *actually matters*. That should go into the documentation. Steve - if you're reading this - add this example into the Rust tutorial!
Idea::&lt;Good&gt;!()
It's not silly to use `Arc&lt;Mutex&lt;&gt;&gt;` here because it's definitely possible for the context to be destroyed while the idle is still pending. It's relatively common in C GTK+ code to not bother refcounting and rely on `g_source_remove` in the object's finalizer, but I have definitely written broken code that left sources attached to the main context, so yeah, Rust is right here. 
Screw it. Why even bother.
I definitely agree it should be added. Why *wasn't* something like that in there? What was the point of concentrating *primarily on redundant uses of this feature*?
Where should the memory of `c` be kept after `foo` is left?
&gt; Why wasn't something like that in there? What was the point of concentrating primarily on redundant uses of this feature? The point was to show that lifetimes are jut name for scopes. This stuff, and explaining it, is _hard_. I hope more people continue to write examples and things like this, so we can figure out effective ways of explaining it.
The bindings take care of reference counting for you. I don't see what either of `Arc` or `Mutex` would buy here, but it's not clear to me, what `context` is in this case.
Thanks for replay , but looks like that adding *--prefix* is not help. Issue appear during procedure preparing binaries for install, copying it into temp dirs, at this step script not found libs with mask *arm-xxx-linux-gnueabi/lib/libstd-*.so
Thank you :-) I also love Shepmaster's answer, which is basically the answer I was too lazy to write.
You don't actually need hot code reloading for web stuff. All you need is to keep the fd open from a wrapper that accepts requests and hands them over to another process. If the code changes you recycle that process.
Glad the message wasn't too subtle. Programming is a profession that has normalized extreme perks - unlimited vacation and work-from-home, flexible hours, drinks and snacks, in-office arcades, high salaries for relatively modest levels of work, and zero actual accountability for being fairly uniformly racist, sexist, conformist, and exclusionary. You got all those things w/ zero organizing, zero strikes, zero struggle. Ever ask yourself why? Because you're ultimately the most effective scab that's ever been created. The very act of being a programmer is saying "fuck you, got mine" to a whole mountain of people you've clawed your way on top of, but please feel free to feign moral outrage that I'm willing to say it aloud.
Thanks, I'll have a look.
You probably want to use `subprocess.call(["cargo"] + sys.argv[1::])` instead of `os.system`. Using the `subprocess` API will allow you to accept parameters with whitespace too!
Since you don't require &amp;mut access to each dog, you should be able to get rid of all the Mutexes around them (Arc already allows &amp; access). Otherwise this seems basically fine, yes. Not really sure what you're looking for? Edit wait no, there are some other things. Gimme a sec...
[Replying here mostly so it's towards the top of the thread - it's not a direct reply to you] This conversation is getting a little heated, [to all participants] *please consider taking a break* and coming back to it :)
I have no idea what I'm doing and wanted feedback :)
So this is probably what the "concurrent" example image is opining: http://is.gd/lzqB12 No synchronization (beyond spawning and joining threads), just move the data to the thread that cares and do the work. Message passing concurrency. This is my slightly cleaned up version of what you're doing: http://is.gd/AE1pJT I removed the Mutex around the Dogs (unecessary), and also change the code to *only* clone each Arc you were actually interested in. As written, you were cloning every single Arc (and allocating a whole new Vec for them) on every single iteration, which wasn't what you wanted.
TCO isn't guarnateed, but LLVM will do it sometimes. We will allow you to opt into TCO. We don't have purity, we got rid of it because it's not useful in Rust. We, and I don't think any other language, can prevent deadlocks, that seems... bad. Depending on how it's defined, 'guarantee memory deallocation' is wrong, and for just about every language as well, given that leaks are a thing.
Rust doesn't prevent deadlocks. I would write "partial" for TCO. Rust does sibling call optimization. (I'd quibble about whether TCO is really a safety concern though; it's about expressiveness more than anything.) I also think Rust is so close to functional purity as to make no difference in practice. Having a pure annotation that disallowed I/O, TLS, and cell/refcell/mutex manipulation wouldn't actually solve any problems I can think of.
According to that site, Closure can prevent deadlocks.
Tail Call Optimization is about preventing Stack Overflow errors, according to the author.
The crowdfunding was done using Mastercoin, but to suggest that because of what Mastercoin have done, that makes MaidSafe a scam is very poor thinking. You seem just to be trolling.
"Ensure list element exists" is only done during runtime, which would result in the mark "partial".
Citing low reddit activity as evidence someone needs to 'get a life', nice. While the idea of having a reddit stalker is no doubt highly appealing to you, I can assure you that my two interactions with you are really just the statistical outcome of you being the lone high-volume asshole on a sub that's otherwise pretty great. 
&gt; of you being the lone high-volume asshole on a sub that's otherwise pretty great. What a coincidence, that sounds similar to my impression of you! We truly are meant for each other.
That really makes no sense, TCO will allow significant or unbounded recursion of tail calls, but non-tail calls will still overflow the stack or run out of memory at some point.
I've opened an issue on the blog's repo to at least let the author know of this discussion :) https://github.com/steveshogren/blog-source/issues/3
&gt; As far as I know, the problem of whole-program lifetime inference isn't known to be fundamentally **im**possible. &gt; Since lifetimes involve both subtyping and polymorphism, any solution would probably not be pretty. Did you mean to say "isn't known to be fundamentally *possible*"? Also, could you expand on that?
Yes, that probably would be another good change to make. Like, I said though, it was a quick script to try out the idea. Ideally I wouldn't need a python script at all... just some settings in the `Cargo.toml` or a `build.rs` or something, but cargo scans every subdirectory for ones matching `*/src/*` and errors when they don't have a `Cargo.toml` (which occurs in `CARGO_HOME`), so I fell back on a makefile and then the python script as a way to try out the environment variables.
awesome!
By all means, explore and report those findings! I'm very curious to see where it leads. I'd take Rust's FFI over binding from any other language, for sure. It's important to note though that even the C++ bindings for MPI have been deprecated. I mean we've used boost::MPI in the past but that has stagnated almost entirely. I didn't mean for this to be interpreted as some criticism as the OP or his efforts, just wanted to point out that the HPC community needs to be shook up a bit and I think a full, end-to-end Rust framework is just the right way to do that.
Maybe I'm mistaken, but "checking at runtime" is still a safety guarantee.
This guy's actually accepting pull requests for this table. [Here's one for Rust.](https://github.com/steveshogren/blog-source/pull/4)
[You mean it isn't cute?](https://github.com/barosl/rust-rfcs/blob/rename-connect-to-join/text/0000-rename-connect-to-join.md) It's certainly cuter than the raw source code. Or do you think it should be called the rendered version, since that's what everyone else calls it?
I totally agree, you need to start with the simple examples to expose concepts and terminology 
Thanks! I'm glad that our answers could compliment each other so well. 
I knew I was missing something. Thanks! [Edit] That is exactly as beautiful and simple as I was expecting it to be. Probably why I missed it. Thanks, again!
For reference: * `mem::forget(Box::new(0))` * `let x = Box::new(0); abort!()` let x = Rc::new(RefCell::new(None)); let y = Rc::new(Refcell::new(Some(x.clone()))); *x.borrow_mut() = Some(y.clone());
Yeah, I think steve missed the question a bit. The question was 'template metaprogramming', and he did not get 'generics' from that, but 'metaprogramming'.
It sounds like you took it as an emphasis on the "generic", and I took it as an emphasis on "metaprogramming." You're right that I could have mentioned traits and connected it to Concepts, though....
I'd rather go with "yes^(*)", since you generally need to go out of your way to leak anything.
It is debatable whether C++ templates really qualify as metaprograms. In a C++ template program, what is the metalanguage and what is the object language? It might be better to described C++ templates (in concert with ‘constexpr‘) as a combination of type-level computation and staged execution. Rust is missing a lot of the expressiveness of C++ templates, e.g. type-level integers, higher-kinded generics, and trait/policy-based design. Unfortunately, C++ templates are quite useful in practice despite their haphazard design.
https://github.com/Munksgaard/session-types 
You could. It would have the following drawbacks: * `Weak` has a runtime cost of always having to upgrade the reference before you can use it. * You will have to embed a `RefCell` just for the initialization. `RefCell` also has some runtime overhead. * `Rc` and `RefCell` also both have some memory overhead * Suddenly your code ends up full of `upgrade().unwrap().borrow()` where it would have been okay to use references. Edit: Now I also have a version working on stable Rust - which is also an advantage over `Weak`.
In case you find it relevant - a few months ago I [came up with and posted](https://users.rust-lang.org/t/an-approach-to-inner-pointers-and-parent-pointers-is-it-safe/1532/1) a macro that theoretically provides a safe interface to this kind of thing, with some syntactic overhead, plus the overhead of `RefCell`, depending on the use case, but without the overhead of `Rc`/`Weak`. I say theoretically because as I wrote then, I'm not sure whether it's actually safe, and nobody else looked at it - probably is, though. 
Right, for this simple example it works, but once you start to have more complex relationships between objects it gets worse. 
Oh, definitely interesting! I guess I'm not confident enough to say that your version is safe, because lifetimes are complex and I'm not sure I understand everything in depth just yet, but I did try a few simple ways to poke holes in it and I failed, so that's at least something :-) ...whereas my version is more along the lines of "here's an unsafe reference to shoot yourself in the foot with, so be careful with that". That includes trying to reference the parent from the child in a destructor (unless you manually clear the children array in the parent's destructor, then it should work).
In fact, my answer contains an example where the function returns a ref to something that will outlive the program – because, as Rusky said, it's 'static.
I think you want /r/playrust
&gt;Turing completeness doesn't imply that something is a form of metaprogramming. Metaprogramming means writing programs that themselves manipulate programs. In the case of C++ templates, there is no obvious candidate for the program being manipulated. But C++ templates *are* used to create specialized instances of code. &gt;What definition of generics are you using that excludes C++ templates? They allow types parameterized by other types. They delay type checking to the actual use of an instantiated item, but they still do it at compile time. Rust's definition of Generics don't need templates. Nor do Java's, C#'s, Haskell's or others.
http://is.gd/pg2gvR This works, but Table is not a DST, it just contains a pointer. The slice must be declared outside of the function call, some variable needs to own the array before it can be borrowed. You cannot create an array with a dynamic size, all arrays must have a size known at compile time. The syntax is ugly, and as far as I know macro_rules! cannot currently help with that in this particular case. The advantage of this over a Vec&lt;&gt; is that those are heap allocated, but arrays having a size known at compile time can be put on the stack. I made the Table borrow a mutable slice so that it can make internal changes, but it can never change the size of the slice it contains. For that you would again need heap allocation and then you should just use a Vec&lt;&gt;. edit: it can be made slightly prettier with two macros: http://is.gd/d6hpeX For now those two cannot be combined using any macro_rules.
Without allowing scripts from the discourse.org domain a blank page is shown. :(
Discourse is written in Ember, and FastBoot isn't done yet, so...
We actually looked into session types with Rust three years ago. There was even a protocol generator (`pipec`) for them in the main tree! They were removed because they could be built as a library.
Wow, stable no_std and allocators! Time to work on the OS again.
This is mostly a stylistic change, but you have a loop with a match block inside of it, Rust has a special syntax just for that: fn start_serving(rx: Receiver&lt;Request&gt;) { thread::spawn(move || { let mut forks = [ true, true, true, true, true ]; let mut pended = Vec::new(); while let Ok(rq) = rx.recv(){ //&lt;------------------------------while let match rq.request { ForkRequest::Take =&gt; if forks[rq.phil.left] &amp;&amp; forks[rq.phil.right] { forks[rq.phil.left] = false; forks[rq.phil.right] = false; rq.resp.send(()).ok().expect("Failed to send response!"); } else { println!("{} is waiting.", rq.phil.name); pended.push(rq); }, ForkRequest::Return =&gt; { forks[rq.phil.left] = true; forks[rq.phil.right] = true; rq.resp.send(()).ok().expect("Failed to send response!"); } } pended.retain(|rq| { if forks[rq.phil.left] &amp;&amp; forks[rq.phil.right]{ forks[rq.phil.left] = false; forks[rq.phil.right] = false; rq.resp.send(()).ok().expect("Failed to send response!"); false } else { true } }); }; }); } (could be some wrong semicolons in there, did not debug) edit: just wanted to add that while let is completely ignoring any possible errors, and in a rather subtle way. If you wanted to handle those errors, here is one way to do it: enum Errors{ Bad, Meh, } fn res()-&gt;Result&lt;i32,Errors&gt;{ unimplemented!() } fn start_serving() { while let Ok(rq) = res().map_err(|e| if let Errors::Bad = e {panic!("unexpected error!")} ){ //do stuff } }
I started this as a sort-of joke a few months back when I got frustrated with people asking about linked lists in Rust. A flamewar over linked lists in proggit reminded me that I should probably finish it. So here's the first chapter!
I love to see guides/tutorials on implementing data structures in Rust. Great! I like the touch of showing compiler errors as opposed to just giving the finished, flawless (as far as the compiler is concerned) code. People will always run into compiler errors, even if they were just copy-pasting all the code. Might as well discuss flawed code, and the compiler errors that they provoke.
This is a guide I've been waiting for. A linked list was in fact the very first thing I tried to implement after reading the Rust book and, judging from a lot of questions I found on the Internet, I wasn't alone. I failed (or rather, I let myself fail for the time being). The point is not that we need a gazillion linked list implementations. A linked list is a (usually) easy thing to get your feet wet when evaluating a new language. When I compared languages I did the same thing in Go and it took me five minutes tops to have a working implementation with a few basic operations. This does not make either language better or worse than the other; it's just how it is and maybe a bit unfortunate for Rust. I'm still a beginner and I still have absolutely no clue when to transfer ownership vs using cells, ref cells or boxes. How to create efficient and idiomatic data structures and algorithms is sorely missing from the Rust book so far. Maybe something like this guide in a compacted form could be a starting point.
Linked lists of small objects are basically exotic in modern programming. Pointer chasing and heap allocation are too slow relative to everything else. They should only be used with chunks of larger non trivial data.
Note that Servo uses std::collections::LinkedList for its Display Lists, because it needs to concatenate tons of them together.
I don't believe you need to put the Philosophers in Arcs. You never call clone on it, so you only ever have 1 reference. Even if you did, your think and eat functions could accept &amp;Philosopher instead an Arc. 
It'd be nice to get this style list for *everything* currently marked unstable. Like the OP I'm sure quite a few people would like to know where a particular item falls on the stabilization calendar, or what can be done to assist the Rust team. Although highly capable and dedicated, I'd assume they are also somewhat physically constrained by manpower. 
Unfortunately `DList` can't really be used in that situation.
That was not my point. Imagine you have a new functional programming language. A reasonable start would be creating a function that recursively calculates the Fibonacci sequence. Or a faculty. Yes, or a linked list. That doesn't mean that calculating Fibonacci numbers is inherently useful. It's a way of getting to know the language. Maybe there is a functional language that is for the Fibonacci sequence as Rust is to linked lists. That language deserves a guide on how to do it.
Darling, why didn't you just ask me? I'm sitting right across the yard ;) &lt;3 I was just reading [this thread on internals](https://internals.rust-lang.org/t/the-life-and-death-of-an-api/2087/20?u=carols10cents) the other day, and /u/acrichto said this which I found interesting: &gt; Another overall point (which /u/aturon mentioned as well) is that I think it'll be super important for us to get some form of dashboard for all the unstable APIs we have. I think we have an alarmingly large number of unstable APIs today under std_misc and core still, and I've just been losing track over time what's there and what their status should be. I immediately started brainstorming how such a thing could be built using git-- find all the `#[unstable(feature = "")]`s, group by feature name (but link back to all occurrences), figure out how long each feature has been in the codebase, etc. Feel free to steal my idea, anyone! It sounds like it could be useful to the core team :) I see that the subteam reports now include a "dashboard" section ([example](https://github.com/rust-lang/subteams/blob/master/libs/reports/2015-07-24.md#dashboard)), but that seems to be used for things currently in progress, not keeping track of everything unstable still outstanding (I could be wrong though) Also, /u/gankro eventually turned that internals thread into [an RFC](https://github.com/rust-lang/rfcs/pull/1213) that's still open that you might want to comment on.
not necessarily, especially with intrusive linked lists. For example, the implementation of `select()` in lwip (a TCP/IP stack for embedded devices) works by constructing a linked list of objects across the stacks of each thread which is blocking on a `select()` call.
This is amazing! It made me laugh AND taught me a bunch-- my favorite kind of tutorial :) Can't wait for the rest of the chapters!!
Even though this tutorial starts with the absolute basics, I still managed to learn a thing or two. Never knew about `DerefMove`, for example. Nice!
yeah, the code took great care to remove the node from the list before the function returned.
Maybe a guide would be interesting, but to start at a linked list is a bit silly, especially in Rust. The whole language is designed around minimizing the use of raw pointers, so it is an advanced topic to actually use them.
The fact that it's not suited for it might actually be an advantage. You run across a lot of the more confusing/exotic areas of rust with a data structure that, presumably, you already understand well. I liked this article, but I didn't have time to fully digest it. Thumbs up from me.
`T` might actually be `&amp;'b U`, or a struct with an `&amp;'b U` field, etc... If `'b` doesn't live at least as long as `'a`, then the type won't be well formed, since you'd have a reference (i.e. the field `foo` in `Foo`) that lives longer than what it references (i.e. the `&amp;'b U`), which is a Bad Thing™. The type parameter bound `T: 'a` says that, whatever type `T` is, any lifetimes it contains have to live at least as long as `'a`, so you won't have a problem.
Thanks, I'll look into those. I may also add Hash. And Bounded. And FromIterator/Extend. And Ord/Eq and their partial cousins. And IntoCow. Did I mention Read/Write?
Thanks!
I've been checking this repo for a while and I must say this is a great start. Edit: also, we need a way to interact with the blocking_world/channels/threads.
You don't even need the destructuring! fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.next.take().map(|node| { self.next = node.next.as_mut().map(|node| &amp;mut **node); &amp;mut node.elem }) }
As /u/lurgi already pointed out, pushing something not designed for the task can make one learn the intricacies much more rapidly. If the problem was on the happy path, not much would be learned.
Thanks. I'm happy how the initial API turned out, but I have been very narrow in what I'm aiming at. Interaction with "outside blocking world" has been out of scope so far. I will get to it at some point. I welcome comments, suggestions and ideas.
The API stabilization metabug, as far as I know, is basically that - everything which is planning on being stabilized in the near future, or has some specific known thing which needs to be changed. A lot of things which are unstable aren't really figured out yet, or may never be stable, so listing *everything* unstable in one place really wouldn't be as helpful afaik. The issue doesn't have everything that will become stable in it, but I'm pretty sure it has everything which currently has plans to become stabilized.
http://cglab.ca/~abeinges/blah/too-many-lists/indy.gif is 404ing for me.
Fixed.
In case it's not clear, `DerefMove` is hypothetical: it's not possible to overload `*` to move out at the moment.
You can use a ring-buffer for O(1) amortized concatenation.
Wow this tutorial five a lot of thing by known and proceed fast. As someone who NEVER wrote rust and is reading its first tutorial lying in the bed.. I like it. But really I can't understand why next: self.head, Does not work because, afaik next can be modified if we have to move the next element in the list; should work if next is constant. You dove with next: mem::replace(&amp;mut self.head, Link::Empty), But what about doing just? next: mem::replace(&amp;mut self.head),
Good write-up, thanks. I would add another use case where linked lists are necessary, though. Linked lists have the nice and unique property that its iterators are never invalidated. If you need to store iterators, or if you know the list can be mutated while you are iterating on it, then a Linked list is the perfect solution. I'm mostly a desktop application developer, and I'd say I needed a linked list twice in my carreer, only for this exact property.
I don't get why you are misunderstood so much on this point. If I implement bubble sort in some language, is the first thought that comes to people's minds that 1) I intend to use *my own* sorting function when there probably is a much more robust one in the standard library. 2) I intend to use *bubble sort* to solve all my sorting needs. Hopefully not. Implementing classic data structures and algorithms (even though they're "bad"!) is a very common learning tactic.
One nit: you can *totally* implement `IterMut` for a singly-linked list in entirely safe code: http://is.gd/JqxAzQ - lvalue references are awesome.
Also, the `make_unique` code you presented (for persistent `Drop`) is totally wrong - for example, it doesn't actually use `make_unique`. Also, if you do it right, you will only recurse from an element when it is dropped - that's only `O(n)` overhead.
Nice, thank you very much. The tests directory may be just what I need.
Hm… this is interesting. I've been wanting to enhance carboxyl with green threading support for a while and this looks pretty convenient to use. I have to play around with it a bit though.
You should add this to esolangs. https://esolangs.org/wiki/Main_Page Unlike emojili people could understand these programs/scripts. Tom Scott did a presentation of how he built emojili and the troubles he had with Unicode and SQL databases. Did you have any trouble with unicode? 
Yeah, as someone who's new to Rust, I'm genuinely confused by some of the responses here. Data structures are a fundamental part of computer science and programming. Like, how would you implement a binary search tree if you can't make a linked list? You have to start somewhere.
It doesn't seem that ridiculous. When I came to rust most of what I knew was that it was like a safer C++. C++ makes writing something like a linked list incredibly simple - the language really facilitates making generic libraries. So I figured, I know linked lists, I know how they should behave, what an interface would look like, etc. I'll build one in rust, which should teach me how memory allocation works, how generics work, etc. This seems reasonable.
It would have been useful for you because you are trying to push a square peg through a round hole. Of course it's hard to do a linked list in rust when you are starting out. Why don't you learn the language and work up to it? The idea of trying to teach people how to write unsafe code as one of their first programs is insanity, it goes against the whole point of the language. The problem isn't rust's begginers tutorials, if anything it is mismanaged expectations that hasn't been clear that a linked list is the antithesis of a beginner rust program.
 fn new() -&gt; Self should be pub fn new() -&gt; Self in chapter 2.2
&gt; There's no Send required for mioco handlers Nice! Now what I want is a mioco-friendly version of ::std::sync::mpsc::sync_channel() that would allow me to pass messages between coroutines. Also: do you think it would make sense for there be a way to recover from a coroutine panic without panicking the entire thread?
Ah, yes. I have 1.1.0 stable.
This is a great tutorial! I really wish I had read this earlier; I, too, tried to implement a linked list and failed pretty quickly. I've only worked through the first book, but it looks like `mem::replace` is used a lot. Is this idiomatic Rust and isn't this basically a way of cheating the borrow checker?
This looks really good! I ended up figuring out mio after a while, but it was too low-level to be appealing at first. As an aside, I have been playing with a [hand-written desugaring](https://gist.github.com/eddyb/822c658190ccf18058db) of generators that is capable of supporting async/await and [runs on stable](https://play.rust-lang.org/?gist=a5c2d92666cd3b39dac9&amp;run=1). It's not really implementable with just a syntax extension, which is one of the reasons I am really excited for the upcoming [MIR](https://github.com/rust-lang/rfcs/pull/1211). Perhaps I could do the same thing I did for `copy`, with a TCP echo server instead (in retrospective, I should've done that in the first place). And then we can benchmark! I'm really curious if these state-machine "async functions" bring anything to the table, or a more expensive solution works just as well.
&gt; Is this idiomatic Rust it's used often in this kind of code, but I wouldn't say in Rust generally. &gt; isn't this basically a way of cheating the borrow checker? What's cheating about it?
You're welcome :) It should be included with all future posts, and I'll fix the previous ones at some point.
| If something looks like a duck, but you haven't heard its sound yet, you shouldn't assume it's a duck. Not sure what you mean? I saw something that looked like a duck, it sounded like a duck. Then I gave the duck a hat, and it stopped sounding like a duck.
I didn't provide the make_unique code, I provided the O(n^2) code.
It works here because there's a well-defined `None` case. What if I have a type without a well-defined `None` but I still want to do something where I move `self`, and re-assign it later?
Actually a great tutorial would show how to port a C program to Rust in various phases. 1. Literal translation but Rust syntax. Everything is mut Box and unsafe. 2. Iteratively clean the code until it is min(mut), min(Box) and zero(unsafe). The ending segment would describe some Rust design philosophy and that doing a direct translation is probably not the most efficient.
The concept you're looking for is "anonymised types", and a little bird told me that it's apparently up for discussion sometime in the next week. There's also a prototype implementation lying around just waiting to be worked on thoroughly for who knows how long before being rigorously tested, worked on some more, tested again, picked apart in the review process, worked on some more, two week vacation in the woods with no internet access... It's coming, but I wouldn't, like, hold your breath or anything. Give it time. It needs time to mature. Like a good wine. Supposedly, anyway; I don't drink, so I wouldn't know.
Could you ELI12?
Now that I think about it, maybe `UnixPipe` or `UnixSocket` would be good enough workaround. Coroutines can block/select on them, and you can use them just as notification mechanism, putting the actual data in any data structure shared with `Rc&lt;RefCell&lt;_&gt;&gt;`. Actually, maybe this is not a workaround. I think I can wrap it into proper types, and make a proper signaling between mioco handlers and blocking outside world.
Exciting! I see that the RC discussion turned into something a couple of steps more general and powerful.
Thanks!
If you care, the String (= [Char]) type in Haskell is frowned upon (you should use Data.Text), and the actual common use case for linked lists in the language is for control flow, not for storage.
Aha, sorry, I hadn't even considered that the explanation might be new! You can see it here: https://doc.rust-lang.org/nightly/error-index.html#E0072
It has no relation to any code written outside libstd, it's an entirely self-contained example. The first file in the gist is the fully sugary version, what the user would write (shouldn't be much worse than `mioco`. The last file contains what would presumably go in libcore (the `gen` module) and very few simple I/O actions (read from file, write to file, get file metadata), with a synchronous runner example, `run_sync` (obviously you'd want this on top of `mio` or some other framework, but this was an experiment and I can't demo `mio` code on the playpen).
A local channel could be just "put a value on the receivers stack and switch to it" or something similarly lightweight. The low overhead of that enables some interesting concurrency patterns that I would hesitate to use over pipes.
You're assuming allocating space is O(1), of course.
When we get actually-working `slice_patterns`, you should be able to write a safe `IterMut` for slices too.
Also, you can *totally* do it with binary trees: http://is.gd/oz4gu8 - basically the only case you need unsafe code is with slices, and that's because of the lack of `slice_patterns`.
@jroesch here, just as a note this landed behind a feature gate. We did this to allow the community to provide feedback on the current fallback priority, and if necessary tweak the algorithm accordingly. You can turn it on with: #![feature(default_type_parameter_fallback)]
Huh? Why cross-post there?
cool
Now I wonder if we could get `Iterator::collect` to default to `Vec`...
That's... actually a really cool idea. edit: Iterator is defined in libcore so it can't work, though :(
I think some of the reactions to this are unfairly dismissive; Ruby has extremely similar closure syntax to Rust &amp; this error is not at all clear if you don't understand the ins and outs of Rust's syntax. I don't know what the best solution is, but I am fairly sure that not adding a better error message is not it.
The solution is obvious: add a simple linked list to libcore and default to that! Everyone loves linked lists! :D
With the standard Rust library you can't have a structure that simultaneously contains a boxed data structure and a pointer into that structure. This library lets you do that. As a minimal example: `(Box&lt;Vec&lt;int&gt;&gt;, &amp;int)`, where the reference points to an element of the vector.
Not yet, but it is a desired feature. [This is a tracking issue for it.](https://github.com/rust-lang/rfcs/issues/1038)
Ooh interesting. Thanks!
Obviously default to `Option&lt;T&gt;`.
Unless you're thinking of a different sort of ring-buffer, concatenating two ring-buffers implies copying the contents of one into the other, i.e. O(n)?
Wrong subreddit
you are right. my apologies
This is what the notify queue in mio is for, it's implemented with a lock free mpmc queue and a self-pipe or eventfd depending on the system for waking the event loop when events are ready to be processed.
I thought this already was in stable. I've defined a few types with default types... What exactly is the difference with this? 
Before, all default types did was `Foo&lt;T=U&gt;` could be written as `Foo` in some places as a shorthand for `Foo&lt;U&gt;`. Now, if T is ever ambiguous *inside* a function, U will be chosen if applicable. So for instance if we set `enum Option&lt;T=bool&gt; { ... }`, and then you just had `println!("{}", None)` it would actually compile and be a `None::&lt;bool&gt;`. I'm excited for this in particular because it means we can remove some hardcoded type params in favour of fallbacks. For instance, HashMap::new hardcodes the State to be RandomState (SipHash). Instead we can just change that to require S: Default.
Yeah, it's listed on https://doc.rust-lang.org/nightly/error-index.html#E0072.
When I was young the side-effects were perhaps the most interesting part of C. Knowing that if you do *x* in situation *y* then you would get effect *z*. And realising that if you do *x* in situation *v* you would get a very different result. Then having to learn underlying architectures and such. Now I'm older and feel outrun by an onslaught of new architectures I'm quite happy and content keeping to the straight and narrow - following the rules.
I don't disagree with that assertion, my point is that "straightforward" TCO only fixes a very small set of recursion cases. Heavy optimisations may convert non-tail recursion to tail-recursion then apply TCO but that's a risky bet as it's fairly unstable (small chances can defeat the transformation). You won't generally prevent stack overflow errors like that.
From the crates.io doc: http://doc.crates.io/crates-io.html#publishing-crates
Hey, I am also building an ORM for rust https://github.com/ivanceras/rustorm I took a more practical route of doing things which is more applicable to my use case. The unarguable standard way of defining models/database tables is expressed in the underlying database native SQL language and leveraged by some mature GUI tools such as PgAdmin, Worbench, SqlMaestor, Toad, etc. So I am taking the approach of [generating model codes](https://github.com/ivanceras/codegenta) based on the database tables and the developer can manipulate the model codes in their controller codes. The generated model and meta table codes can then be used in building the SQL queries. This approach is also practical to porting existing application which already has a database models. Your approach is however much applicable to building new apps, and the programmer don't need to know about the underlying database. I may be also supporting this approach later on as the project matures. I'll be eyeing on these project as well, keep it up. 
Invent a better name. This can be annoying or not, but this is a common policy among package managers so [crates.io follows that too](https://internals.rust-lang.org/t/crates-io-package-policies/1041). You can also try to reach to the original maintainer for giving the name up, but your mileage may vary.
Right now, what you can do is a macro that creates a type `struct GridN&lt;T&gt; { data: [[T; N]; N] }` for some number N and implements for it a trait `Grid` with an associated constant set to N. This means creating a new type for every grid size, but you can treat those types uniformly by having an `fn f&lt;T, G: Grid&lt;T&gt;&gt;(..)`.
Linear Algebra
Hey, I saw you project some time ago, good work! I started with little different approach than you, since I also wanted to see how macro system with syntax extensions work. I am still in the beginning, someone can say experimental phase but I think Treasure orm can succeed. For systems that already have defined database structure I will provide tool that will generate models from database. Anyway I will watch rustorm and Keep up the good work!!!
I wrote [scid](https://github.com/cristicbz/scid) for D, so I have some experience, but Rust's lack of specialisation is a big roadblock for an expression template style library (like Eigen or scid). Once we get that I'd be interested to work on this.
Because the language is primarily about pattern matching. See http://zohaib.me/binary-pattern-matching-in-elixi for an example.
Do you need CGI for legacy support? Because that's quite outdated stuff.
This looks interesting, but I could really use a simpler explanation. Anyone? Please?
I'm curious what situations you end up in, that require specialization. Could you give a minimal example? (if there even are simple cases) There was one specific situation in which I needed specialization, and that was when I was playing with UI code and I had event types and event targets. I wanted to have `Handle&lt;E&gt;` implemented by all event targets, for all event types `E`, but only writing the implementations that actually did something. I managed to get some specialization by abusing OIBIT (i.e. `impl Trait for .. {} impl !Trait for Foo {}`), but some bugs prevented the `Event` trait from working when mixed with OIBIT. That scheme might be fine now, I haven't checked since the bug was fixed. The specialization RFC worries me a bit, but I am willing to embrace it if it's what we need for expressiveness in certain situations.
- ZIP/Unzip with compression. Ability to read/write compressed files using the usual Rust file i/o API. - Hashing with up to date approaches like PBKDF2. - Encryption. - Scanner (scanf, fscanf equivalent).
Either SOAP or SAP NW RFC. Would love to connect SAP-Systems with Rust.
It would default to what allocation method?
What is it you want to know? The idea of session types is that you define a protocol for your communications, eg "A sends a message to B" etc. The messages can refer to inter-process communication, threading, networking, whatever you like. Using this protocol definition, it's then possible to verify that all communications abide to that protocol. This lets you do cool things like guarantee deadlock and livelock freedom, and ensure that all possible messages are handled in the correct order, with the correct responses. I haven't read the paper above yet to know exactly what it does, or how it approaches the issue, there's lots of further reading if you want to know more though (I can find some links depending on what you want to know).
Yeah it's a good idea in general (all of libcollections can do this, in fact). It just doesn't work for this.
Full text search. Red black trees. 
Bindings for Monetdb
Thanks for the link to the issue. I'm not sure whether the workaround would be enough in my case, as I need the type inference to work so that two different HLists are compatible in a certain way. With conversion methods I don't think that would work. I'll have a look into it. What about the other problem, when the `impl` is uncommented? As mentioned in the comments, that causes an overflow in the compiler. Do you know if there's an issue for that?
I think linear types could solve that problem. Until then the #[must_use] directive can at least catch many of these bugs at least as warnings. If I understand the paper correctly.
I'd drop C++ like a House beat if Rust had something with the power/elegance of Eigen available. If a project like that ever gets started I'd be interested in contributing too.
I can think of quite a few libs. But the simple fact is that the basics are *wrong*. With that I mean: BSD sockets (plan-9 and Go fixed that), monolithic kernels and the networking protocols where security is optional. This is not fixed with creating new libs because it is a fundamental problem. And to be honest, I don't see this problem fixed until the bridges collapse. Sorry for being pessimistic here.
I don't get the point of having OpenSSL bindings for Rust. Why would anyone pick a language where safety is one of the biggest reasons to use it then use one of the most insecure libraries in existence (25 vulnerabilities so far this year according to NVD).
The sequence can branch and loop, you just have to encode the fact that it may branch or loop (and how) in the type.
Can you be specific in how Go's socket library is fundamentally better than Rust's?
We need a thread where we discuss "I will drop XYZ in a heart beat if Rust had ...". Personally, for small projects I will drop Java and Scala if Rust had reactor based web service and http client. For larger projects I will need hashing, encryption and other things that I posted separately in this thread.
I don't understand how it could work if the events have an external source and their order cannot be statically checked. Let's say that I have a very simple state macine, it starts in state A and after receiving event a, it goes into state B where only event b is allowed and that is it. How can I encode this information in types? I instantiate the machine as type A (whatever that is). Then when I receive event a then I call handleA on type A (which is type safe) and it gives me a type B. This is all good. But how the hell can the compiler statically ensure that the state machine is in fact in state A when handleA is called? What magic is written in that message handler function? I cannot see how I can do that runtime check at compile time. 
Could you explain how that specialization worked? I've been pretty unable to get OIBIT helping with any sort of specialization. I might just be running into the bug you mentioned, though. I do a lot of I/O fanciness with wrapper types, and am sorely missing specialization. There are a lot of instances where I'd like Read and other wrappers to be able to specialize if the type also implements Seek, for instance. I just feel like I end up exposing a lot of specialized types instead, where the burden is on the user to figure out which exact specialization they need to use (and making sure those requirements cascading down through generic types is a pain).
&gt; Networking in Go and Plan-9 is fundamentally better than BSD sockets because of dial() and read()/write() where everything is a stream of bytes. It's about abstraction and the resulting simplicity. But I am not speaking about Rust. Can't you `read` and `write` on BSD sockets too? The ugly part of BSD sockets is addressing - `sockaddr` and everything that deals with it - but higher-level languages handle that nicely (except for `socket`/`bind`/`listen`, but that *does* have a decent reason behind it - you want to be able to `setsockopt` *before* you `listen`).
`where` clauses *are* enforced on use (this area of the compiler is somewhat buggy, but that's not the problem here). The problem is that non-supertrait bounds aren't elaborated. The reason for this is that otherwise you may need to elaborate infinitely many bounds (this can't happen with supertraits as they must be acyclic). Significant research work would be required to remove that limitation. 
The paper I linked above (http://www.doc.ic.ac.uk/~cn06/pub/2012/heart/paper.pdf) shows how you can verify logic like that, admittedly it skips a lot of the interesting details, since it gets very math heavy. I haven't read the paper yet to be able to answer your specific question, but perhaps someone else could.
Your type would be something like `Recv&lt;A, Recv&lt;B, ()&gt;&gt;` (the dual type on the sender end is `Send&lt;A, Send&lt;B, ()&gt;&gt;`, and when you `.handlea()`, you get an `A` and the type statically becomes `Recv&lt;B, ()&gt;` (and so on). More complicated compositions can handle branching and looping (see the paper)
Hi phonkee, I am currently rebuilding a little rust web-application prototype, that uses in-memory-db, into a more sophisticated, persistent and scaling program. Of course I want to use some kind of DB-Backend for that kind of application and I was quite surprised, that any ORM libraries in Rust do exist. I had a look at rustorm and deuterium-orm, also checked how difficult it would be to use some a postgres-driver directly, but your library looks most promising in terms of usability. I will probably decide against using it right now, because it is so incomplete, but will definitely keep an eye on it. If you got any db-backend working and a little core-set of features available, feel free to drop me a comment or direct message. I would be more then happy to try your library and give direct feedback. As long as this library is in active development, I could imagine using it in the development process and just see how it goes. I expect instabilities and api breakage, but as long as my application is also in active development (which will be the case for quite some time), I do not require production-ready code. Greetings and keep up the good work. Rust needs libraries like these. - Drakulix
Anything related to parallellism and concurrency, like Intel TBB or task/async of C#. 
The current package policy is here: https://internals.rust-lang.org/t/crates-io-package-policies/1041 TL;DR: case by case only.
Well probably or at least something related. The only reason I knew that is because I [rewrote](https://github.com/rust-lang/rust-by-example/pull/607) the lifetime section in rustbyexample and I tried to flesh out some of those details better (writing involves learning the topic a bit better).
To clarify: Session types provides deadlock-freedom between two processes communicating over a single session channel. However, simple session types cannot ensure that multiple processes communicating over multiple channels are deadlock-free. Multi-party session types tries to address this, and in particular we found the [Multiparty asynchronous session types](http://dl.acm.org/citation.cfm?id=1328472) paper interesting in this aspect. The session-types library however, does not attempt any of this. Also, concerning communication-safety, it must be clarified that we only guarantee that any communication taking place through a channel is correct, not that communication actually takes place (halting problem and what-not).
There are two kinds of "branches" in the session-types library (with corresponding constructs in general session type theory): `Offer` and `Choose`. Imagine two processes that are communicating over a session channel, process 1 has a channel with type `Chan&lt;Offer&lt;A, B&gt;&gt;` while process 2 has a channel with type `Chan&lt;Choose&lt;A, B&gt;&gt;`. Process 2 is the party that performs the active choice, and gets to decide whether path A or path B is taken. In session-types this is done via the `sel1()` and `sel2()` methods. The result is simply that the protocol is stepped in the corresponding direction. In other words, the branch chosen is "known" at compile time (disregarding the fact that it may be chosen at random or through user input). Process 1, however, has to wait for process 2 to decide which branch to take. The only method available for process 1 is `offer()`, which returns a `Branch` type. A `Branch` is similar to a `Result`, in that it is an enum that is either `Left` or `Right`. So in this case, if process 2 selects branch A, process 1 gets a `Left(Chan&lt;A&gt;)` otherwise process 1 gets a `Right(Chan&lt;B&gt;)`. Then, a simple `match` statement will allow it to procede depending on which branch is taken. Edit: Perhaps I should clarify that channels are explicit values, so process 1 will have something like: match c.offer() { Left(c_a) =&gt; { // Do something with `c_a`, which is a channel of type A } Right(c_b) =&gt; { // Do something with `c_b`, which is a channel of type B } } 
&gt; Invent a better name Or in most cases, a different but worse name. I really wish we had namespacing.
Hi Drakulix, Thank you for kind words, I am working heavily on this to bring query api right now. I will definitely ping you in case that querying will be at least little bit usable, however if you set watch on github repo you'll be notified with updates.. Thank you, bye! 
For reference, I believe the general name for this idea is "[dependent types](https://en.wikipedia.org/wiki/Dependent_type)."
How would you encode a constraint like the fact that you can only send a Close(n) message if you sent an Open(n) message before for the same value of n? Overall, it seems that this only works for protocols expressible, after some abstraction, as a regular expression over the sequence of messages sent, and is only useful if that regular expression is nontrivial (i.e. something else than ".*"). 
Interesting. It's a little hard for me to grok the consequences of this--would this rule out C++-style template metaprogramming for Rust?
Not a fan of these hyped-up named vulnerabilities like "stagefright", "shellshock", "heartbleed", and so on. Sure, it helps these particular vulnerabilities get some extra exposure, but at the expense of people simply keeping up to date on all patches. There are probably dozens of vulnerabilities, some with patches, some without, that get much less exposure than these with big PR campaigns behind them, but these kinds of big media exposure vulns make it seem like you can just watch the news, make sure you react to the big ones, and not have a comprehensive security story the rest of the time. This one is especially frustrating because the PR campaign comes without a way to test for the vulnerability or patch it. At least with shellshock and hearbleed, enough people got interested to look for followup vulnerabilities, write test programs to test for vulnerability and proof of concepts to demonstrate the level of exposure. And this just boils my blood: &gt; Zimperium’s advanced Enterprise Mobile Threat Protection solution, zIPS, protects its enterprise customers from Stagefright vulnerability. So, they're just doing this as a big ad campaign. Announce vague, scary sounding vulnerabilities with no details. Talk about how it's unlikely carriers will get out patches in a timely manner. Offer your product for sale that "protects enterprise customers."
I haven't watched the whole thing yet, but I'm going to say that in Gary's scheme, Rust is mostly a "suitability" response to C++ rather than a "capability" one: like Java, it removes a lot of the wilder parts in favour of a stronger, more reliable core.
Just to clarify a little: &gt; Not a fan of these hyped-up named vulnerabilities like "stagefright" ... From the article: &gt; The attack is called Stagefright, named after Android's [system-wide media playback component](http://source.android.com/devices/media.html), where the vulnerabilities lie The first line on the page they linked documenting the Android media playback engine: &gt; Android provides a media playback engine at the native level called Stagefright 
That's exactly why I posted it, it's a retraction from "you can do everything and shoot yourself in the foot" of C++ to "you can do these specific common use cases very well and it's hard to mess up" However, compared to Java it's a "capability" language because it's able to do more things than Java, but it's less suitable for things that require heavy garbage collection (it's easier to make reference cycles that are not accounted for)
I don't know if they are a majority or not, but there are many many programmers doing embedded, kernels, drivers, system libraries, graphics, DSP, &amp; networking that deal with spaghetti messes of inter-linked intrusive data structures all day long. Mostly in C. I look at Rust hoping that it can be a useful tool for implementing router code because building complex concurrent systems in C is such a pain. But boy is C incredibly flexible for implementing data structures.
EDIT: HAVE NOT TRIED THIS CODE. To give a concrete example. Say that we need a "swap" of values between two threads. We decide to use channels: use std::sync::mpsc::{SyncSender, Receiver} [#derive(Send)] struct ThreadSwapper&lt;T&gt;(SyncSender&lt;T&gt;, Receiver&lt;T&gt;) where T: Send impl ThreadSwapper&lt;T&gt; { fn swap_with_thread(self, t: T) -&gt; T { let sender, receiver = self; sender.send(t); return receiver.recv().unwrap() } } Can you spot the problem? If both threads have a Swapper they'll deadlock! Even using `try_send` could still cause this (if both fail at sending at the same time). You could use a `Sender` but that would be problems, especially if you want the swap points to be sync (more over it might be more efficient). There's another issue: a Swapper could accidentally swap with itself (which always would cause a deadlock)! We'd have to be very careful and make sure not to break it whenever we construct it. To make it worse it's very hard to test, threads are, by their nature, unpredictable in the order they'll run and tests that can give a false negative means that errors can slip by. In other words it's really hard to make a test that will always fail if there's an error. Lets instead make a type system that guarantees that the above problems are solved. That way if we screw any step the program simply won't compile! We can then prove through the type system that we can't have a deadlock (and focus on the other stuff). use std::sync::mpsc::{SyncSender, Receiver, Channel}; use std::sync::mpsc::{Send Error, RecvError}; [#derive(Send)] struct A; [#derive(Send)] struct B; trait Swap { type Other; // where Other: Swap } impl Swap A { type Other = B; } impl Swap B { type Other = A; } trait ThreadSwap&lt;T: Send&gt; { fn swap(self, T) -&gt; T } [#derive(Send)] struct SwapSender&lt;S: Swap, T:Send&gt; (PhantomData&lt;S&gt;, SyncSender&lt;T&gt;) impl&lt;S:Swap, T:Send&gt; SwapSender&lt;S, T&gt; { fn send(&amp;self, t: T) -&gt; Result&lt;(), SendError&lt;T&gt;&gt; { let (_, sender) = self; sender.send(t) } } [#derive(Send)] struct SwapReceiver&lt;S: Swap, T:Send&gt; (PhantomData&lt;S&gt;, Receiver&lt;T&gt;) impl&lt;S:Swap, T:Send&gt; SwapReceiver&lt;S, T&gt; { fn send(&amp;self, t: T) -&gt; Result&lt;T, RecvError&lt;T&gt;&gt; { let (_, receiver) = self; sender.recv(t) } } [#derive(Send)] struct ThreadSwapper&lt;S: Swap, T: Send&gt;(SwapSender&lt;S, T&gt;, SwapReceiver&lt;S::Other, T&gt;) where T: Send; impl&lt;T&gt; ThreadSwap for ThreadSwapper&lt;A, T&gt; { fn swap_with_thread(self, t: T) -&gt; T { let sender, receiver = self; sender.send(t); return receiver.recv().unwrap() } } impl&lt;T&gt; ThreadSwap for ThreadSwapper&lt;B, T&gt; { fn swap_with_thread(self, t: T) -&gt; T { let sender, receiver = self; let recv_val = receiver.recv().unwrap; sender.send(t); return recv_val; } } fn swap_channel&lt;S: Swap, T:Send&gt;() -&gt; (SwapSender&lt;S, T&gt;, SwapReceiver&lt;S, T&gt;) { let (sender, receiver) = sync_channel::&lt;T&gt;(); return (SwapSender(S, sender), SwapReceiver(S, recevier)) } fn thread_swapper&lt;T: Send&gt;() -&gt; (ThreadSwapper&lt;A, T&gt;, ThreadSwapper&lt;B, T&gt;) { (senderA, receiverA) = swap_channel::&lt;A, T&gt;(); (senderB, receiverB) = swap_channel::&lt;B, T&gt;(); // Try changing the parameters to each ThreadSwapper // below. ThreadSwapper requires that if the first one is of A, // the second one is of B. return (ThreadSwapper(senderA, receiverB), ThreadSwapper(senderB, receiverA)) Now we can see how the types work. If you want things to be symetrical (that is they don't care who they are) you simply use `ThreadSwap&lt;T&gt;`, if you want the function to be aware of who it is, then you specify `ThreadSwapper&lt;MarkType, T&gt;`. Since Rust doesn't allow abstract return types yet you have to specify them on the return type, but in a near future we might have the last function be something like `fn thread_swapper&lt;T: Send&gt;() -&gt; (impl ThreadSwap&lt;T&gt;, impl ThreadSwap&lt;T&gt;)` so people wouldn't even know the real type (but forwarding would still be required). Notice the compiler enforces that we handle the swap pairs correctly, and the compiler also enforces that we keep the swap asymmetric to prevent a deadlock. Finally notice that, because the swap consumes the swapper you can only do it once, if you want to do another swap you need to make more swappers. We could solve it by having the thread swapper return itself after each trade, which would allow us to use it. With a smart use of tuples you could also limit the number of swaps, but I'll leave that as an exercise to the reader. Notice there's one case that still can cause deadlock in the latter example: if you try to swap two values in the same thread.
The thing is: can you mount it? Can you treat it as a regular file system where all the system tools can operate on as if it was local? (of course you can, with FUSE/9P, but I mean it isn't a multi platform standard approach)
Rust definitely feels like a suitability retraction in a lot of ways; not only from C++, but also from the way that it sort of retracts from crazy duck typing &amp; reflection of say Python and Ruby into a type system that's more expressive than C but statically analyzable. But at the same time its sort of a capability expansion because of the way it brings in a lot of functional stuff to an imperative language. It also seems really tied to the specific suitability retractions he proposes at the end of the talk - the language has strong support for unit testing and functional idioms. I think Gary Bernhardt didn't really like Rust when he messed with it, but I think its a good language for implementing the "faux-O" style he describes in [another talk](https://www.destroyallsoftware.com/talks/boundaries).
Wow, I'm learning Rust right now and just yesterday was looking at SDL, now this! Subscribing!
This seems like a bug. cargo should download the version of the dependency that matches the version specifier of all crates in the dependency graph, so even if yours is "*" it should download "0.2". Did you depend on glutin before gfx_window_glutin? Have you run `cargo clean` and then built with glutin specified to "*"? Maybe your Cargo.lock got pegged to "0.3.4" when you didn't have the other dependency, and that's the source of the bug?
&gt; pub enum List { &gt; Empty, &gt; Elem(i32, Box&lt;List&gt;), &gt; } &gt; &gt; cargo build &gt; Compiling lists v0.1.0 (file:///Users/ABeingessner/dev/lists) &gt; &gt; ...but this is actually a really stupid definition of a List. In particular, we're allocating the Empty at the end of every list on the heap. This is a strong sign that we're doing something silly. I don't actually follow. A list element is empty or contains a number and the next list element (which may be empty). What is wrong with that?
We have _affine_ types. A thing may be used once or not at all (implicit drop), unlike linear types, where a thing must be used exactly once. Implicit drop is dangerous in rust-sessions, because one end can hang up on the other one prematurely (doesn't cause memory unsafety, but the guarantees of rust-sessions get relaxed).
Also: *could* Cargo build more than one version of a dependency? (I guess not, because then the types that cross crates might not be compatible with the other crate's version)
Sure it can; they won’t be able to interoperate directly, but they can coexist just fine, as they are here.
/u/Gancro?
Yes, it should be possible to extend this to mutability. You'd have to deactivate the `owner()` getter basically, and change map to deal with `&amp;mut T -&gt; &amp;mut U` - not sure if it is doable with the same type, or if there would need to be a `OwningMut` type. Of course, you could also combine it with `RefCell&lt;T&gt;` right now. `&amp;mut T` and `&amp;T` could indeed implement `StableAddress`, but a `OwningRef` with them would be equivalnet to just a `(&amp;T, &amp;U)`, so I'm not sure how useful that would be. :) 
I have a [zip library](https://crates.io/crates/zip)! The features you describe are implemented, although that is about it.
I don't think that would be possible without dependent types. In other words, we cannot specify any constraints to the actual contents of the data transferred, only the types and the order in which they are transferred.
Perhaps the verb here should be "should". How could one convert between the types of two incompatible crates? One would need to create another crate that depend on both versions (which is currently impossible AFAIK).
I have an incomplete draft at [the github repo](https://github.com/llogiq/llogiq.github.io/blob/master/_drafts/traits.md). Perhaps you could read over it and spot any mistakes before I embarrass myself? ;-)
Did you mention this at the Rust 1.0 launch party in Copenhagen?
Yes, we talked a bit about it
Aha, right. It should even be possible to prevent the propagation. Too bad this doesn't work in a generic sense, because `impl&lt;T: X&gt; !DefaultFoo for T {}` [doesn't work](https://github.com/rust-lang/rust/issues/23072).
You could also call it one of the most tested libraries. We don't know if the other libraries are more secure if they are not as well tested. If you want to use a system library for TLS, OpenSSL is about the only choice you have, isn't it? BoringSSL or LibreSSL would have to be compiled manually. For projects where you can choose your own crypto, you should use sodium/nacl.
I'm getting an issue here: As far as I know, this is valid Rust code: Edit: Just ran it in the playground to confirm it works... fn main() { for _ in 0..50 { println!("Hello World!"); } } But I'm getting this output in stderr: hello_world.rs:2:15: 2:17 error: expected `{` but found `..` hello_world.rs:2 for _ in 0..50 { ^~ 
&gt; guess move semantics (move into function parameter) makes Rust more amenable to this than other languages. That's exactly what we found. Traditionally, session types have been implemented through compiler extension, as separate languages or using a complicated system of implicit channels (as in one of the Haskell implementations that uses Monads). Because Rust has affine types, our channels are just ordinary values with a specific set of values attached to them. In general, I think Rusts affine type system has many interesting applications and this is just one of them.
Like I said, your criticism is fair. Posted this in a bout of anger at our industry.
I ran into a similar issue myself when I bumped `winapi` to `"0.2.0"`, my -sys crates depended on `winapi = "*"`, but some people were depending on `winapi = "0.1"`, so Cargo would get `winapi:0.2` for the -sys crates and `winapi:0.1` for the main project, and things would break.
If you use `unsafe_no_drop_flag`, you opt into dealing with a lot of hairy details on your own -- Rust's regular Drop semantics don't work anymore. This feature is expected to go away or change significantly in the future. The garbage you are seeing is the effect of *filling drop* (another unstable feature: after drop, the memory is filled with a particular garbage byte value). [The reference says, section miscellaneous attributes:](http://doc.rust-lang.org/reference.html#miscellaneous-attributes) &gt; `unsafe_no_drop_flag` - on structs, remove the flag that prevents destructors from being run twice. Destructors might be run multiple times on the same object with this attribute. Like the name suggests, this flag is tantamount to introducing unsafe code -- it allows violating memory safety easily. Don't implement `Drop` on types you are passing to ffi. Implement Drop on wrapper types (non-repr(C), and not using unsafe no drop flag, simply use regular Rust structs).
Ah I do comment this: the code will always deadlock if both swappers are on the same thread! I should have made a bigger deal of this, sorry. Basically when you call on `b.swap()` the thread locks and waits for `a.swap()` to be called, except that never will happen because that happens after the thread unlocks (after you call `a.swap()`). [Here's a better version](http://is.gd/ClL7TL). Doesn't throw naming warnings and works fully.
No, they just haven't updated it.
See https://github.com/Gankro/too-many-lists/issues/7
OK, that makes sense, and I've looked up filling drop. Turns out that the value that's being output is in fact `0x1D1D1D1D1D1D1D1D`. I'm happy to do what you suggest, and use wrapper types instead of implementing Drop on the FFI types themselves. One question remains though - if a type using `#[unsafe_no_drop_flag]` cannot rely on any marker value inside it to determine whether drop has already been called, what actually is the use of `#[unsafe_no_drop_flag]`? I'm struggling to think of any.
They can use the filler values as marker with `#![feature(filling_drop)]` and the [markers in `std::mem`](http://doc.rust-lang.org/nightly/std/mem/index.html#constants), and some libstd types do this. You'll just have to be ready to cope with changes to this unstable feature in the future, and understand some rust internals. I'd definitely recommend avoiding all of this. Instead we look hopefully for future rust where all drop flags in structs will be gone, without any of these problems. Edit: The [next PR](https://github.com/rust-lang/rust/pull/26173) towards non-filling / non-zeroing drop is being integration tested as we speak. &lt;3 compiler hackers.
Previously it was zeroing drop, and types like Vec could see "oh my capacity is 0, sounds like I don't need to do anything!" Now it's 1d1d... and basically the same argument applies but by explicitly checking for that value. Yes, your Vec will leak if you manage to get that capacity. No, I don't really care (it's a ridiculous capacity and unsafe_no_drop_flag is slated for wholesale removal in favour of flags on the stack).
Only `Send` and `Sync` are OIBIT, `Sized` and `Copy` each have special semantics.
&gt; Don't implement Drop on types you are passing to ffi. Implement Drop on wrapper types (non-repr(C), and not using unsafe no drop flag, simply use regular Rust structs). That was very helpful to me. Thanks!
You are right. I did intend to use the buffer-less channel. If swaps are happening a lot, creating a buffer for each swap might be annoying, especially if it's ok for the threads to sync. Maybe what they swap is very heavy and you don't want to keep 5 instances of them within memory. Ultimately it's a contrived example to show a realistic use of this approach while still being simple enough to understand. The compiler doesn't enforce no-deadlocks. That is the compiler doesn't know of that. Just like the compiler doesn't know of a heap, but `Box` can handle that for you. Or how the compiler doesn't know of runtime-borrow constraints, but `RefCell` can express that. You can use the type system to create primitives that will never deadlock (such as the `ThreadSwapper`) and then use those primitives with the knowledge that your program won't deadlock. You can take it even further. You can use this system to guarantee that something has been done. For example we could have something like `Order&lt;BeenPayed: PayStatus&gt;`, were `Order` represents an order and `PayStatus` can be either two tag types `Paid` or `NotPaid`, a function could take care of this. Read the papers to read more on how session types can be used for interesting purposes.
I wonder if it's too late to have some *other* wild card added to mean exactly that, something like `"~"` or `"^"`
You could probably learn a few things by reading the code of other text editors written in Rust, like [SolidOak](https://github.com/oakes/SolidOak) (GTK + neovim), [iota](https://github.com/gchp/iota) or [rim](https://github.com/mathall/rim) (both CLI + vi-like).
Did you check out https://github.com/steveklabnik/dining_philosophers to see what Steve did?
You likely will need to be familiar with using Trait objects to use most gui libraries.
Thanks!
What happens when someone has taken the scope name I want? It's the same problem, just moved, not solved.
 |i, j| (i == j) as isize as f64 I think this here is a bit nicer: |i, j| if i == j { 1.0 } else { 0.0 } Or if you wanted to make the matrix generic over the scalar type: |i, j| if i == j { One::one() } else { Zero::zero() } The One and Zero traits are defined in the num crate.
That was the one that took half of #rust trying to explain to me. And that the size of a raw pointer isn't always the same size, very un-C-like.
Not exactly -- I cannot, for instance, generate an n-vector from an integer at runtime, and there is a lot of stuff to do manually (or through macros) at the moment. I have to provide all of the type arguments at compile-time. This is where the type system is limited, for the moment at least. Although there are some very smart people coming from Idris who are working on RFCs, so it may get better.
Author of KISS-UI here. Thanks for the mention! To OP, or anyone else, if you decide to try it out, feel free to let me know how it works for you or if you have any questions. The kind of feedback I'm looking for right now is on the ergonomics/Rustiness the API, and what features it's missing that people really want. IUP is only used as a nice abstraction over WinForms/GTK+. The user really only needs to be aware of it when trying KISS-UI for the first time, since the library has to be installed on the system or otherwise available to the linker. The big want for a future KISS-UI 2.0 is to ditch IUP and build an abstraction from scratch on top of the native APIs. It would eliminate a sometimes finicky dependency which has some core design decisions that I don't really care for.
Indeed. However, for the sake of demonstration, I do not see it as necessary; it feels a bit like premature optimization. ;)
I haven't looked much in associated constants, and using them in array-size position makes the compiler crash at the moment. Plus, it allowed me to rant about not having value parameters in generics. :P
There are many types of engineering. Why do programmers obsess with comparing programming to *civil* engineering in particular? I don't get it...
It would require serialization and deserialization of some form, but it’s entirely possible. You can’t create a crate that depends on both versions *directly*, but you can produce a simple two-crate form, where A depends on X1 and B depends on A and X2 and both A and B can do the appropriate serialization and deserialization. Messy, yes. But possible.
Yep, that's actually the main reason why I didn't add this code. I will write a new post when associated constants become usable. Between other things, we could have linked lists that know about their size at compile-time and can switch translate back and forth to arrays, and whatnot. Gankro might have yet-another-chapter to write! :P
I've been sitting on this project for a while but found some inspiration this afternoon to finish it up and post. The standard library is pretty great, but is notorious for its hacks and for pervasive use of unstable and rare features. There's a lot to be learned from it. 
I think that either this feature should be made explicitly supported (linking the "same" crate with two different versions, importing them with two different names via Cargo.toml) or it should result in a build failure. Right now it seems like a hack / corner case and more likely to confuse people.
Thank you for your response. I didn't even think of that, maybe this should be stated more explicitly in the docs. 
Based on your stated requirements, I would recommend the [`tendril`](https://github.com/servo/tendril) crate.
Could you explain to me why I should not do that? 
As someone who is looking into the same project: really consider the rust QML bindings crate. QML is a really powerful fast library with great text rendering and cross-platform native and theme able look and feel. Also look up using a piece table like the vis editor if you want fast editing of large files.
Cache locality. It's really inefficient to allocate many vecs spread out over the entire heap.
I'd love to hear more about what data structures you started with and then migrated to. And what issues motivated the changes.
This is awesome! One thing I've always wondered: what is the motivation for having `libcollections` separate from the rest of `std`? `core`, `alloc`, and `libc` are all fairly self-explanatory. Why is `libcollections` separated out? Is the difference that it isn't OS dependent?
Everyone can see a bridge...
Just define traits that inherit from the traits you want, then do a blanket `impl` for them. The first example that springs to mind is for some pathologically generic code I wrote for a random number generator. Starting from [`Engine`](https://github.com/DanielKeep/rust-pcg/blob/master/src/engine.rs#L11-L24), you will find the definition of those traits in [`lib.rs`](https://github.com/DanielKeep/rust-pcg/blob/master/src/lib.rs#L99-L146), along with some additional "Rust won't enforce non-`Self` where clauses on traits" hacks in [`bounds.rs`](https://github.com/DanielKeep/rust-pcg/blob/master/src/bounds.rs).
Its actually possible to take this one step further by using Peano numbers (sort of) as the dimensions. Then it becomes possible to have type safe folds as well and have all operations work on matrices of any dimension. ``` struct Cons&lt;T&gt;(usize, T); struct Z; type DIM0 = Z; type DIM2 = Cons&lt;Cons&lt;Z&gt;&gt;; ``` EDIT: It was easier than I thought to get my old repa port working so I uploaded it to github where I use the this technique https://github.com/Marwes/repa-rs
Okay, I tried: trait Traits: Ord + PartialOrd { } impl &lt;A:Ord+Hash+Copy, B:Ord+Sub&lt;Output=B&gt;+Copy&gt; Traits MyType&lt;A,B&gt; { fn cmp(&amp;self, other: &amp;Self) -&gt; Ordering { // stuff here } } but the compiler says: "error: method `cmp` is not a member of trait `Traits`". Does this only work if none of the methods actually need implementations?
No, you can't do that. I'm not sure what you believe that code to be doing. The code I showed you uses traits to collect disparate bounds together under a single name and constrain types based on them. So something like this: trait ATrait: Ord + Hash + Copy {} impl&lt;A&gt; ATrait for A where A: Ord + Hash + Copy {} trait BTrait: Ord + Sub&lt;Output=Self&gt; + Copy {} impl&lt;B&gt; BTrait for B where B: Ord + Sub&lt;Output=B&gt; + Copy {} struct MyType&lt;A: ATrait, B: BTrait&gt; { a: A, b: B } struct MyType2&lt;A: ATrait, B: BTrait&gt; { a: A, b: B, } impl&lt;A: ATrait, B: BTrait&gt; MyType&lt;A,B&gt; { } 
This is great! Looking forward for new additions.
I'd have thought that the size of a raw pointer is the same at least on one platform. The size of a *slice* however is obviously different.
As /u/logophobia says you want your whole matrix to be located in a contiguous memory region. The solution using two vectors has two problems: - too much pointer-chasing: two pointer chases for every element (one for each vector). One of them (the first one) points to an arbitrary location in memory. This is bad for cache locality, bad for cache hit/miss rates (when iterating the last element of a row the next one doesn't start right after), bad for the prefetcher (at the end of each row), and bad for the optimizer/vectorizer (if you happen to have one). This is particularly bad for small matrices that might completely fit in a single cache-line. - too much book-keeping: two vectors keep 3 words each. That's 6 words. If you have a 2x2 matrix the book-keeping is larger than the data itself. The optimal book-keeping for matrices of compile-time size is 1 word per matrix, that is, just a single pointer. The sizes are then encoded in the types. This means that you basically need to use `[T; N * M]` "somehow". These 6 words of book-keeping are also bad for large matrices due to register pressure. For resizable matrices a single `Vec&lt;f64&gt;` is not optimal, one wants `ptr, rows, cols, capacity`. Since `rows` and `cols` are needed very often, just adding e.g. `rows` to the `Vec` and computing cols from the number of rows and the vector size will be noticeable in some cases. For resizable matrices of compile-time bounds (that is, if the maximum size of the matrix is known at compile-time) 3 words is also optimal, but you still want to manually implement them using `[T; N * M]` plus two words (number of rows/cols) to be able to put them on the stack (since AFAIK you cannot put the data of `Vec&lt;f64&gt;` on the stack). For non-resizable matrices of run-time size a `Vec&lt;f64&gt;` is not optimal since you need only 3 words for them but using a `Vec` they would be 4. This turned up a bit long so I'm going to stop here. There are a lot of combinations of matrix memory layouts (e.g. compile-time bounds + 1 compile-time dimension + 1 run-time dimension...). However, `Vec&lt;Vec&lt;T&gt;&gt;` is always bad, while a single `Vec&lt;T&gt;` is ok but never optimal.
It's unnecessary to repeat all those trait bounds. Start with impl&lt;A, B&gt; Eq for MyType&lt;A,B&gt; { ... } and then add the bounds that are actually needed.
Note that some of those hacks aren't actually needed anymore – e.g. it would be possible to define `str` as an unsized `[u8]` in current Rust. This has not always been the case, which is why `str` is still a primitive type. I wonder if we could someday change the definition without breaking anything. For now, there is no compelling reason to do so, and there are many other efforts that deserve higher priority.
Yup. I've just got the URL to feed.xml from page source and it worked. Thanks.
No, like in this book: https://ucsd-progsys.github.io/liquidhaskell-tutorial/book.pdf Basically, you can define a variable as being e.g., NonZero and have it statically checked it'll never be zero. Haskell has it easier because of immutability though...
This means, there has to be some special lifetime handling for loops with variables of the outer scope, right? Because if it wouldn't be loop, then the mutable borrow of `push_str` and the immutable borrow of `extend` wouldn't collide. But even inside of a loop the immutable borrow of `extend` is only dangerous if `headers` holds on the borrow given to `extend`. Could the lifetime system help detecting this? 
Nice explanation. You can make the program compile by converting all the substrings into `String`. This way `header` will not contain references to `res`. fn main() { let mut res = String::new(); let mut headers = Vec::new(); for _ in 0..10 { res.push_str("test"); let substrings = res.split("t").skip(1) .flat_map(|s| s.split("e")) .map(|s| s.to_string()); headers.extend(substrings); } } EDIT (after Veedracs comment): Alternatively one could change the program logic so that res is immutable inside the loop. Then one doesn't have to copy the memory fn main() { let mut res = String::new(); res.push_str("testtestestestestestestest"); let mut headers = Vec::new(); for _ in 0..10 { let terms = res.split("t").skip(1) .flat_map(|s| s.split("e")); headers.extend(terms); } } 
&gt; This means, there has to be some special lifetime handling for loops with variables of the outer scope, right? I suppose you could just implement it as just duplicating the loop body. No idea how it's actually done. &gt; Could the lifetime system help detecting this? I'm not clear on what you're asking. What exactly are you trying to detect? Whether the borrow is popped from the vector?
https://doc.rust-lang.org/book/associated-constants.html looks unstable now, as you say-- but I'm curious what are the benefits of this? What's a scenario where this is useful? Thanks
I see what your saying now, that's awesome actually 
I disagree that it's a clear representation; it has other downsides: concat/split allocate/deallocate, it doesn't generalize to Persistent lists, and it doesn't generalize to Doubly Linked Lists.
https://github.com/rust-lang/rust/issues/19036 https://github.com/rust-lang/rust/pull/19612
&gt; Basically, you can define a variable as being e.g., NonZero and have it statically checked it'll never be zero. How could you statically guarantee that for e.g. subtraction?
Yes! I literally just bugged their admins (and told some other users to bug the admins) about this just yesterday! I'd like to think I had a large part in this :) Now I'm just waiting for CodeChef and CodeForces to add support. For both, emailing the site admins would help. For CodeForces, Mike needs to accept my pull request: https://github.com/MikeMirzayanov/binary-heap-benchmark/pull/13 I had asked him about the delay over email, but received no response. He probably needs more people to email him to see that there's an interest in this.
&gt; I think if you call /u/steveklabnik1 three times he appears. Fixed that for you ;)
Does anybody have a rust layer for spacemacs that brings racer?
I'm glad people liked this. As to maintaining it, I see it as more of an experiment in pull request art than documentation. It's not a big deal if it stays frozen in time. That said, if it continues to be useful I may update it periodically by opening a new PR and recreating the comments. Some of the commentary *may* be useful as real comments. If we see the standard library as a legit learning source then maybe it could even be put in there wholesale. Thanks to people who added additional color commentary.
Very thankful for your work (using it with vim). Looking forward to testing the long running process version! 
Yes the storage order should be encoded at the type level. Eigen3, among others, does this. &gt; Having the strides enables eg having views of a column for a row-major matrix (no copy of the data required). Yes, but note that in the opposite case one doesn't _need_ to copy the data either. Eigen views _can_ contain the matrix bounds at the type level which allows iterating over a column/row using a compile-time stride, that is, without performing a copy (for matrices and views of compile-time bounds). Depending on the matrix order, the stride is 0 at the type-level and the compiler eliminates it. I've remarked _can_, because Eigen might or might not do that depending on the whole expression one is trying to evaluate. It is however a very important optimization for reducing memory bandwidth. It enables iterating over the matrix once while copying few elements into a small temporary, as opposed to copying the whole column into a large temporary (load matrix from memory to CPU, store temporary from cache into main memory), and then having to load the temporary column from memory back again to do something with it. Some libraries like Eigen, NT2, and Blaze see the AST of the whole algorithm with all the data dependencies and have a cost model to make a decision based on it (how large is the matrix, what operations are you performing, is your algorithm serial or parallel...).
Fair enough, but I need a lot of the bounds for a lot of the `impl`s. Even with no bounds at all, it still seems redundant to have `&lt;A,B&gt;` on the structs themselves and then twice on each `impl`. I understand the reasons behind it, but it's just a lot of boilerplate.
Who wants to file their first Rust issue? Or make their first Rust contribution? This is a good one! I'll help you!!! &lt;3 PM me or email carol dot nichols at gmail if you have any questions!!!!
Ohhhhh okay.
&gt;If it can't be proven then it ~~can't be used~~ must be unsafe
Fantastic work, thank you
The openssl crypto components are tiny, extremely well tested and are generally still best in class. The byte wrangling stuff for things like ssl and what-not has been shown not to be great, but the crypto is still excellent.
Me too
Out of the 25 vulnerabilities this year in OpenSSL at least 11 of them have been in libcrypto.