Gray over light gray over white for the code is really *really* not readable. :/
Nice! Can we debug `#[test]` as well ?
This thread is as good as any to outline some thoughts I've had in the light of the recent plans for officially supporting some IDEs: *(__EDIT__: It got a bit long, you can skip over this bit of history if you want to)* A few months ago I had to do some kind of "application" as part of the final year of highschool (not technically necessary for graduation, but it's an additional certification of sorts). I used this opportunity to experiment with building an UI and a text editor on top, from scratch. (I even have a weird homebrewed linear solver for layout constraints, and it does work well enough in really simple cases.) What I ended up with was a really simple multi-file text editor with somewhat-incremental highlighting (based on the Kate syntax highlighting rules) and two features using rustc: * Highlight lines that have errors/warnings and display the message on hover * Display types of all expressions overlapping the cursor when hovering over code You can [see it in action](http://imgur.com/a/63W2j) and it's on [github](https://github.com/eddyb/r3) if you want to poke at it. I've added [some API for file loading hooks](https://github.com/rust-lang/rust/pull/25387) to the compiler, but never got around to using it (to detect crate file structure). Highschool ended, and after all the graduation exams and university admission, I've lost interest in it. The project is not entirely dead (I'm using the UI part for quick tooling in a commercial project), but building a *real* IDE by myself is not the most rewarding thing ever. *Back to present times and future plans:* Running the compiler itself requires one thread per compilation session. You also want this for stability: I tested rid3 with more than a hundred rust-by-example files open at once - a dozen of them would error out (by panicking), but that wouldn't break anything else in the process. And you need [some communication](https://github.com/eddyb/r3/blob/096345e6da504d8bb2660825f9103a6310ac1aac/src/ide/rustc.rs#L90-L99) between that thread and your editor (or whatever is interacting with the compilation session). That communication could just as well be inter-process (most IPC schemes work fine AFAIK). Thus, we could have a multi-session tool that an IDE starts up, and then proceeds to control over IPC, giving it simple editing commands ("replace caret range with content") and querying semantic information, like completion options. The main advantage I see here is that we only need to build one tool (just like racer, I guess) for real-time IDE interaction, instead of a library that needs bindings via C FFI (or worse, COM). One tool with multi-file multi-cargo/rustc-session support, efficient binary IPC, and as many clients as there are IDEs with dedicated Rust users. Although I wouldn't be surprised if the official plans we'll soon have more details on (*/me pokes* /u/nrc) are very similar to that idea. Something I have to say, I didn't consider adding Rust support to kdevelop or any other C++ IDE remotely feasible or sane, before this IPC scheme. Now, it would be "just some glue code talking to a slave rid3r process".
I hope [that racer stuff](https://www.reddit.com/r/rust/comments/3gyt5q/i_quick_hacked_atomrust_to_show_the_return_types/) used in Atom to show types of variables somehow make its way to Emacs. (also perhaps you should add it?)
**Edit**: See [Veedrac's comment below](https://www.reddit.com/r/rust/comments/3har0o/how_to_constrain_template_type_to_f32_and_f64/cu5r9me) for a way to do this with the current coherence rules. You can't. The closest you can get is to restrict `T` to some trait that you've only implemented for `f32` and `f64`... but there's no way to prevent someone from coming along and implementing it for *other* types. So, yeah. You can't.
Can't it be a private `trait`? Or was that usecase broken at some point?
 pub struct Quat&lt;T&gt;([T; 4]) where T: Real; trait Real {} impl Real for f32 {} impl Real for f64 {} #[test] fn test_thing() {} Yields: &lt;anon&gt;:1:37: 1:41 error: private trait in exported type parameter bound &lt;anon&gt;:1 pub struct Quat&lt;T&gt;([T; 4]) where T: Real; ^~~~
If this type isn't public, the trait can be private. If this type is public the trait bounding its parameter must be public.
there is `type_trait` in C++11, is there something like in Rust?
We do have `enum` and `struct` as a keyword. I think Haskell uses `data` for both.
 pub struct Local&lt;T&gt;(T); pub trait LocalFloat {} impl LocalFloat for Local&lt;f32&gt; {} impl LocalFloat for Local&lt;f64&gt; {} pub struct Quat&lt;T&gt;([T; 4]) where Local&lt;T&gt;: LocalFloat; It's a tad ugly, but it should prevent new overloads from other crates.
I hadn't thought to use coherence like that. If that *does* work (I haven't checked, but it seems like it should), that *would* do it. The only catch would be if the coherence rules are ever loosened, this could end up becoming and open set, again.
Hyper is on stable.
I cannot resist: [Grenade Stack Becomes unusable](https://github.com/rust-lang/rust/issues/12723)
match is keyword, it can't be used as function name.
Is their mid/long-term objective to develop a Django/Rails alternative? Is it possible for a Rust/web framework combination to compete with the ease and productivity of the existing solutions?
I suggested this months ago on internals. Alas, I'm more suited to writing readable code.
That is UB, I would rather create a `Context` from nothing. So just how hard is it to make one of those? [`Context::new`](https://github.com/rust-lang/rust/blob/e822a18ae7d55cefc332c6598a607cef0554ec77/src/librustc/lint/context.rs#L364-L366) takes a `ty::ctxt`, a crate (which can be empty), and a map (which can also be empty). IMO it should be refactored get the crate from the `ty::ctxt`, but that's a minor detail. The worst part is `ty::ctxt`. Which [needs a lot more arguments](https://github.com/rust-lang/rust/blob/e822a18ae7d55cefc332c6598a607cef0554ec77/src/librustc/middle/ty.rs#L3776-L3784). Well, most of those are maps/sets (again), so it could be easy. `CtxtArenas` is also simple, so only `Session` and `ast_map::Map` remain. A fresh `Session` can be built [in just 3 lines](https://github.com/rust-lang/rust/blob/e822a18ae7d55cefc332c6598a607cef0554ec77/src/grammar/verify.rs#L287-L289). Only `ast_map::Map` to go! I've left one this one for the end because it's a bit nasty. It [can only be created by indexing the AST](https://github.com/rust-lang/rust/blob/e822a18ae7d55cefc332c6598a607cef0554ec77/src/librustc/ast_map/mod.rs#L918). There's no `ast::Crate` argument to that function because the AST has to be already in the supplied `ast_map::Forest`. At least [`ast_map::Forest::new`](https://github.com/rust-lang/rust/blob/e822a18ae7d55cefc332c6598a607cef0554ec77/src/librustc/ast_map/mod.rs#L234) is not that scary. Now to put them together - or can I just say it's non-trivial and be done with it?
And some status page for "dowehaveareweyetyet" so we can track the stuff that we do not have but wish to have! Like arewemmorpgyet.
Somewhat unrelated: If I were to write a "bigfloat" crate that didn't depend on the "num" crate, is there any way for consumers to use my `BigFloat` in a `Float` context?
Iron feels more like Express or Sinatra to me than Django or Rails.
Unfortunately, Float requires Copy, which AFAIK makes a BigFloat implementation difficult or impossible. I belive there is another numeric package that would let you work with both, but that requires explicit use of x.clone().
Just gave this a try and it definitely works. I honestly wouldn't have even thought of this. I had no idea that you could use another struct as part of a constraint.
Yeah, I may want to explore that implementation strategy further.
You can use this hacky hackiness, for what it's worth: mod internal { pub trait Real {} } use internal::Real; pub struct Quat&lt;T&gt;([T; 4]) where T: Real; impl Real for f32 {} impl Real for f64 {} #[test] fn test_thing() {} docs will be frustrating but there you go. (edit: formatting)
You have some sort of alert set up up for the word macro here and on IRC don't you :P
[Here's my solution](https://play.rust-lang.org/?gist=cadcbee43557e8e3bbe0&amp;version=stable). I've mentioned a bunch of possible underhanded routes I _could_ have gone, before picking a rather fun one. Figuring out the underhandedness is left as an exercise to the reader. The inputs are supposed to be much longer than the sample one in `main()`, with inputs that short the program isn't sound given the reasoning in the comment above `match_data`. Try inputs of around 100+ datapoints in length. (edit, 100 is better than 50 here) [/u/llogiq, you probably can guess the answer to this one, so don't say anything yet :) ] I'm not sure if this means it's *easy* to write underhanded code in Rust. Certainly the usual tricks (hidden macros, breaking buffers, `if x = y`, etc) don't work. In my case I could only figure this out because maintaining Clippy keeps me up to date on all the ways one can subtly mess up a Rust program. It also required some gymnastics on my part to obscure.
Sadly you can't iterate arrays directly due the inability to talk about `n` generically. I suppose we could do the usual "impl up to n = 32" but I honestly die inside a little every time.
I'm one of the curators at TWiR. Even though old, I added the article because it wasn't mentioned before in TWiR. PS: This is my first ever post on Reddit. Welcome `self`.
https://github.com/aleksander/ncurses-rs/tree/bool
Very unclear. No one's directly working on it right now.
Though I'd rather refactor the function to not require the context in the first place I think you should at least give a non-null pointer as things like the nullable pointer optimization relies on the fact that it is UB for `&amp;T`to be null. With null cx: `&amp;T`: match Some(cx) { Some(_) =&gt; //Not executed! None =&gt; //Actually executed }
You violently truncate the low and high parts of the spectrum, but I am assuming that isn't the problem. EDIT: not. The effects aren't exactly subtle through.
You can typecast the error mentioned to any (`&lt;any&gt;error.message`) and get it to compile. I still had a problem with it not redirecting from the waiting page to http://localhost:8080/editor but from there at least the start of the tutorial worked.
I only truncate 10, which isn't a big deal for a spectrum of length 100+ (as mentioned in the comment, the spectrum length should be of an order of magnitude greater than MAX_SHIFT, I just was too lazy to add a check for that) (but yeah, not part of the problem)
Well it isn't obvious from the problem definition the important peaks aren't at the end.
Correct! Inspiration from [this lint](https://github.com/Manishearth/rust-clippy/issues/182). This means that something from the first or last 10 datapoints needs to match. I'm not sure if this is "unusual" enough, it really depends on how the spectra are.
Well, there's nothing that makes an `Option` different from `f64` or any other type for that matter, is there? I might want to do all those things with other types too. What grants `Option` the special treatment? It seems high-risk, low-benefit (as shown by the need to be linted :P) Maybe `map` (mapping an `f64` doesn't make sense while an `Option` is useful), but everything else seems quite specific to iterable things. And there's already a `.map()` implemented in `Option` itself.
The Option is a container holding zero or one values. It's the or that makes the difference, and is why it's useful for all the iterator methods (flat_map is another). I don't have any examples, but I do recall using Option's iteration abilities (along with other iterators) to write succinct code
This was the building block I had in mind as well.
Doesn't help that the font is so thin, either.
You don't have a **pointer**, you have a **reference** which guarantees a few things, including non-null, dereferenceable and valid data behind the pointer. LLVM already knows about the first two, and will optimize accordingly. No, I don't believe using `Deref` is a good idea. Creating a `Context` shouldn't be more than 20-30 lines, whereas the `NoDeref` thing will be more work.
At least now I have a search keyword, thanks!
While I think this is super awesome, rewrites of crypto libs make me nervous!
It isn't rewrite due some security concerns. We have great rust-crypto library, but IMHO code style in that one is ugly as hell. Also there is no modularity, so you don't have option to disable some features. I wanted to create something that could be used outside of Rust world and that is how Octavo began. Currently I've implemented hash algorithms and HMAC, now is time for public key and DH. There is feature gate created for `ssl` but I don't know if I will ever have time to implement this. For now it is kind of learning project to learn modern cryptography. I wanted to share because it would be nice if someone could find it useful for any purpose (event not crypto, remember that this is also hash library, so I have non crypto hashes like CRC32 or Adler32 here too).
3Gb in your example will be released to OS. In C, Java, Rust and almost any other language.
Please, can this work end up in mio? The lack of Windows support is **the** reason hyper hasn't adopted async io yet. 
If the spikes are not in the first or last 10 datapoints, this will be a match. That's why this isn't exactly subtle.
Yeah, exactly.
"safety" and "unsafety" was based rather on personal feelings with some readings. But you are right. It should land in deprecated section.
For small objects, jemalloc also primarily allocates objects of uniform size from a single page. This reduces problems caused by fragmentation, but also increases wasted ram, especially for programs that don't really allocate much. The tradeoffs make sense for most users, and when they don't, Rust provides some tools to manage your own memory.
I love the idea of new crypto libraries being written in Rust. The world tends to hate on people who write crypto code. I think this is a reaction that is poorly aimed. It is true that using unproven crypto is a risk, but I think it's kinda sad if the response to that risk is to discourage the creation of all new crypto code. So, bravo Octavo. Carry on. 
Oh, if I may, a question that is somewhat tangential and that I happened to discuss with a friend recently: When a program, say, returns two freed 4kB pages but these are not contiguous, are modern OSes (presumably with the help of the MMU) capable of handing them out as what will appear to be a contiguous space to a user process that tries to allocate 8kB? I seem to remember that it is possible but years of not programming low-level have perhaps blurred the memory.
Java never returns allocated memory to the OS. The JVM holds it and uses it for it's heap. That's an implementation detail, though. It never actually frees.
Yes. When the page is returned, the virtual address is unmapped, and after that it's just another physical page just like all the rest, and can be remapped again anywhere.
I'm not an expert but it seems that JVM returns memory to the OS: https://stackoverflow.com/questions/4625103/does-the-jvm-give-back-free-memory-to-the-os-when-no-longer-needed
I'm going to be the new guy who waves his hand and says "Please explain, teacher". I put this code in derp.rs: pub struct Local&lt;T&gt;(T); pub trait LocalFloat {} impl LocalFloat for Local&lt;f32&gt; {} impl LocalFloat for Local&lt;f64&gt; {} pub struct Quat&lt;T&gt;([T; 4]) where Local&lt;T&gt;: LocalFloat; pub fn main() { } Then I created herp.rs use derp; pub fn foobar(_x: i32) { let _y = derp::Quat([1.0, 1.0, 1.0, 1.0]); } This gives me the error "cannot invoke tuple struct constructor with private fields". So how can I create a Quat? If I change the line in derp.rs to: pub struct Quat&lt;T&gt;(pub [T; 4]) where then I can do the following in herp.rs: use derp; impl derp::LocalFloat for derp::Local&lt;char&gt; {} pub fn foobar(_x: i32) { let _y = derp::Quat(['a', 'b', 'c', 'd']); } Which looks like I've made a Quat&lt;char&gt;. I'm assuming that the missing 'pub' is the trick, which keeps everyone else from banging on the internals and that in the real world you would create a Quat&lt;f32&gt; through a ::new() or something similar. Either that or I'm completely misunderstanding the problem, the solution, or both.
Tests are for checking correctness of the code but the fact that code is correct doesn't mean that it is secure. They won't reveal you if there are any flaws in, say, private key generation or if there is branching on secret data. Algorithms implemented so far seem pretty straightforward but when you will start to work on asymmetric crypto you will need to be really carefull.
I know. Been there, done that. Just in Scheme. Painful road but after that this is really nice feeling that you have done something awesome.
Wow, I'd been reading about Eve for years and had no idea it was written partly in Rust. Cool!
I haven't seen this posted before. Making sure one is in *every single Rust channel* is a favorite pasttime of Rust devs.
It's not clear that it's possible to guarantee the absence of timing attacks in a high-level language that runs through an optimizer. How do you plan to address things like cache timing and optimizer-introduced branches?
The best underhanded *X* entries are readable, so that they really are sneaking unexpected behaviour to "perfectly good" looking code (after all, it's not an obfuscated *X* contest). 
You really should. It's currently being sunset for its usage in SSH certificates. It's responsible to discourage its use. Also thanks for the library :) 
Well, the MIR work is related, as it will drive the need for better support of constant values. Once associated constants work everywhere, implementing generic integer parameters should be relatively painless: for types, generics are quite a lot simpler than associated types, and the same should be true for constants (at least IMO).
Yeah this was actually a big part of the reasoning behind it: https://github.com/servo/servo/issues/4331
&gt; although it can be mitigated by making use of arenas Sometimes you can design your program like this: you allocate a lot of related structures, then deallocate them all at once. In this case, you can allocate your structures in an arena. An arena offers something conceptually like `malloc`, but you can't `free` individual objects - you need to free the entire arena at once. Arenas can be implemented without any fragmentation (you can just have a large array where you lay each object contiguously in memory). Rust used to have them on the stdlib (I'm not sure what happened to them..) but they can also be found [here](https://github.com/SimonSapin/rust-typed-arena). I can link [this SO question](https://stackoverflow.com/questions/12825148/what-is-the-meaning-of-the-term-arena-in-relation-to-memory), [this protobuf doc](https://developers.google.com/protocol-buffers/docs/reference/arenas) and [this Wikipedia article](https://en.wikipedia.org/wiki/Region-based_memory_management). From that WP article, see that [region inference](https://en.wikipedia.org/wiki/Region-based_memory_management#Region_inference) has been employed by the SML compiler ML Kit (SML is a language traditionally implemented with a GC). That is: a compiler can automatically create arenas and insert each object allocation on an appropriate arena; when the compiler detects that the "scope" of the arena ends, it deallocate all objects at once. (note that Rust also, on a high level, deallocates objects at the end of their "scope" - unless they are moved). More on ML Kit [on c2](http://c2.com/cgi/wiki?MlKit)) PS: when we talk about GC we actually are talking about [tracing GC](https://en.wikipedia.org/wiki/Tracing_garbage_collection). Tracing GC moves around the data, preventing fragmentation, but other garbage collectors might not.
Here's a funny thing: Chromium adds a warning when one access reddit because "this site uses a weak security configuration (SHA-1 signatures)" More worryingly is how Git is all based on SHA-1 (and when you sign a Git object, you are actually signing its hash..)
thank you! this worked great
Couldn’t this be the opportunity to get rid of Mozilla? `Servo/1.0 (Android; Mobile; rv:1.0)` should be fine.
Mozilla is building it :|
Thanks. :-)
`Mozilla` in this context doesn't refer to the vendor. It's referencing the codename of Netscape back when Netscape was still a goin' concern. Everyone just sticks it out the front of their `User-Agent` string because people *continue* to do UA sniffing, despite it having been a bad idea a decade ago. [These](http://webaim.org/blog/user-agent-string-history/) [two](http://www.nczonline.net/blog/2010/01/12/history-of-the-user-agent-string/) pages might make for interesting reading.
I made use of this today! (on an `Option&lt;&amp;str&gt;`): `option.iter().flat_map(|s| s.split('/'))...` If you want to iterate over the value inside the option, `iter().flat_map()` is a convenient way to treat a None value as a 0-length iterator.
It could be! The current one was chosen to get substantially the same markup as Firefox: see the data in the bug https://github.com/servo/servo/issues/4331 It could be worth trying Servo/0.1 or something in a branch.
WTF? MozillaMozillaMozillaMozillaMozilla? Everybody is Mozilla?!?! Thank you for the clarification!
"They're all Mozilla. Everybody's Mozilla, Turing." "Internet Explorer isn't, is he?" "Everybody's Mozilla, Turing!" "Not Opera!" "Gordon Bennett! Yes, Opera. Everyone. Everybody's Mozilla, Turing!" "Safari?" "He's Mozilla, Turing. Everybody is Mozilla. Everybody is Mozilla, Turing." "Wait. Are you trying to tell me everybody's Mozilla?" "Should've never let him out in the first place..."
Short answer: yes. If you want to delve a little deeper, this page offers a great explanation of how the x86 MMU works. http://wiki.osdev.org/Paging. It explains the data structure used by the operating system to tell the CPU how to map physical addresses to virtual addresses. It's a little different for 64bit, but not much. Basically it just adds 2 levels onto the tree structure.
Check the user agents in the above comments. Non-mozilla browsers report that they are Mozilla. That is fucked up.
Very glad to see EVE run time is written in Rust. Except that, I am also very interested in why Eve chooses TypeScript for UI stuff, not plain JavaScript instead :)
All browsers also say "like Gecko" for similar reasons.
I'm a bit sad about the ! thing. It really does feel like the Right Thing.
Has this TWiR really been released yesterday? I feel like I already read it some time ago?! Weird
I did a quick search on freenode (`/msg alis list * -topic *rust*`, `/msg alis list *rust*`, and `/msg alis list * -topic *rust* -skip 60`), then filtered out non-rust related channels, and came up with the following list: * ##rust * #epsilonz * #expprog * #libpnet * #pdxrust * #proglangdesign * #rust.tw * #rustoleum This takes the total up to 61 :)
UA Strings are a bad idea in hindsight. ;)
It's not offtopic with 3 mentions of Rust in it :D.
Thanks Vikrant Chaudhary. Though I do like reading about the newly opened RFCs each week. Any reason we aren't showing new RFCs anymore?
You may have read the subteam reports it was based on.
Very similar question was asked here [few days ago](https://www.reddit.com/r/rust/comments/3gtpy9/wrapping_around_generic_io/). Basically, you have two solutions to this problem: * Trait objects (dynamic dispatch, which has a runtime cost, but might be unnoticeable, ~~~since `read` calls will result in a syscall anyway~~~. **edit**: it's actually depends on decoder's implementation. Benchmark and find out): let input_decoder: Box&lt;Read&gt; = match ... { ... =&gt; Box::new(GzDecoder...) ... } * Create generic function, which you'll just call in match arms (eg. `fn inner_main&lt;R: Read&gt;(reader: R)`)
Gotta start somewhere! It's kind of expected that Rust won't be doing so well for any given category just now - Haskell's had 25 years and is still immature in a lot of areas which it should be really well suited for. It might be a good idea to have some additional categories so we can track our maturing progress, rather than a binary mature/immature.
Interesting that the lack of runtime garbage collection (an explicit goal of the Rust language) is kind of seen as an important differentiator in that document.
You can just open a pull request on the Rust tracker, same as any other issue: https://github.com/rust-lang/rust/pulls?utf8=✓&amp;q=is%3Apr+typo+
Don't forget that an enum, where each variant represents each of the types respectively, is also a possibility. You can then implement `Read` for this enum and just redirect the calls to the internal readers. I don't know how it performs in comparison to the other alternatives, but I guess it's about as good as dynamic dispatch (if not better).
You don't necessarily need a box. You can use `&amp;mut` trait objects instead (with a bit more code).
I think the box just ended up being easier to implement at that time. But thanks for the heads up.
I'm pretty sure that sooner or later I will need to use some unsafe, but still - it will be much safer than C code. I want `unsafe` parts to be as small as possible that it should be at least hard to create security issue there.
It seems to me that Mozilla serves a purpose to identify as a human-browser, as the only common element in all non-bot user agents.
I also noticed there was no networking channel in there, that's now fixed - irc.mozilla.org/#rust-networking :)
Oh, by Eve I meant the project in general, not just this iteration of it. Or maybe my sense of time is off—though I think I do remember reading a post about work on this project at least a year ago?
Yes, no PR or issue is too small. What I like to do is leave the smaller doc issues go for a week or two, to give people a way to contribute little things if they want, and then do a GC pass and knock a bunch out in one day if nobody has done anything with them.
Thanks; I've opened a pull request as suggested, and it has been approved! https://github.com/rust-lang/rust/pull/27881
You might want actually *look* at this subreddit. I *deeply* suspect you've got the wrong one. ^^^^hint: ^^^^you ^^^^want ^^^^/r/playrust ^^^^;)
areweideyet.com
http://areweideyet.com
There's a list here, they aren't all Rust related though: https://wiki.mozilla.org/Areweyet
* #rust-offtopic-offtopic I might have to drop in there and see what is discussed
Lots of sites break badly with no UA. IIRC Google.com is among them...
Also see Ars Technica in Servo: https://twitter.com/pcwalton/status/631961638304804864
It might be good to include an indication of how much work each option is to set up. I've been trying them out: - SolidOak doesn't build out of git (some neovim problem, as seems to be expected). The executable on the website does work! but I haven't managed to find how to initiate code completion... no documentation and didn't get it from browsing the source (the key for build is "k", but in what context?) - Kate: as of May, included in the editor upstream! but... I didn't understand the installation instruction [1]. cloned kate from git, went into addons/rustcompletion, make install doesn't work. Wait, from the "kate build directory"? what is that? do I need to build kate from source after all? [1] https://blogs.kde.org/2015/05/22/updates-kates-rust-plugin-syntax-highlighting-and-rust-source-mime-type
You might want to take a look at https://github.com/klutzy/nadeko Edit: And also read http://cr.yp.to/antiforgery/cachetiming-20050414.pdf if you don't know about cache timing attacks, which are real, subtle and extremely difficult to guard against *especially* in high-level languages. There's a good reason why nadeko mentions that "Also, `slice[expr]` is not allowed if `expr` is not a constant literal." Edit2: Also, https://github.com/agl/ctgrind
SRV support via [c-ares](https://crates.io/crates/c-ares)
I wonder if e.g. "no branching on secret data" is something you could use the type system to help you ensure.
&gt; I love the idea of new crypto libraries being written in Rust. I love it too, but they should be written by people who know that there are much more subtle issues than algorithmic incorrectness or buffer overflows—issues that despite their subtlety can make a crypto implementation completely insecure. 
I don't quite understand MIR https://github.com/rust-lang/rfcs/pull/1211 Does it slow down app execution speed a little bit as a trade-off for more portability?
It appears to be servo in github in servo. :)
I can't access twitter. Is there a direct link?
"they should be written by people who know [X]" No offense, and I'm saying this with tongue firmly in cheek, but I always wonder if people spreading this attitude are secretly working for the NSA. (Let's actively discourage people from getting the experience necessary to learn X, because the NSA's job will be easier after all the "people who know [X]" are dead.) 
And I always wonder if people writing their homebrew crypto are secretly working for the NSA ;)
You guys are killing it [inf]
https://i.imgur.com/6PUAHV3.png
I'm just thinking more about inclusion of a list of useful links like [1]. I don't think, one can easily measure easiness of installing. For instance, SolidOak worked for me fine from git, but this might be because I had the right packages installed by accident.
Thanks! So servo is a rendering engine. I was not sure what it is
A complex question. The first step is for Servo and Firefox to share some small libraries written in Rust. Actually, the work for that is well underway. As for Firefox using the Servo layout engine itself, that's mostly a branding question. The Firefox brand encompasses a few different products. On Android and Firefox OS, Servo could be a drop-in replacement that would simply improve performance, battery life, and security. Desktop Firefox is much trickier because of the XUL/XPCOM legacy that Servo wants to avoid. Likewise, Servo's support for Firefox add-ons will be poor or nonexistent. This could be a dealbreaker for many loyal users who stuck with Firefox because of some plugin they need. So I think it's likely that Mozilla will follow Microsoft in marketing a legacy browser and a next-gen browser side-by-side under two different names.
It's just an intermediate form for the compiler, not a portable byte code running on a VM.
 If you are going to maintain this over time, I think pertinent links for each would be great, and reasonably stable (unlike my idea of setup status).
Hmmm have you considered that you might get better performance if you used some sort of static analysis instead of garbage collection?
&gt; Desktop Firefox is much trickier because of the XUL/XPCOM legacy that Servo wants to avoid. Good point. Though the new addon SDK abstracts many things nicely and AFAIK doesn't expose XUL and XPCOM. They could implement support for that and many of the existing addons would work.
You can also use another pattern which would be: fn main() { // ... match input_file_extension { "gz" =&gt; do_stuff(GzDecoder::new(input_file).unwrap()), "bz2" =&gt; do_stuff(BzDecompressor::new(input_file)), _ =&gt; do_stuff(input_file), } } fn do_stuff&lt;R: Read&gt;(input_decoder: R) { let input_reader = BufReader::new(input_decoder); // for line ... } This is static dispatch, mind that there will be 3 `do_stuff` functions compiled. I can't tell which is better for you but that's an alternative to dynamic dispatch.
Prior research has found out the required annotations would be too cumbersome to get anything done. Perhaps some sort of elision might help?
I certainly agree that the lifetime of bugs is _definately_ a problem.
Yes, your application will be identical after MIR. This is an entirely internal to the compiler kind of thing, that only matters to external users in the sense that it will let us ship new features better and faster, and help fix bugs.
They do a _lot_ of work to keep it easy. :)
c-ares is a way to add SRV support to things that you write. srv-shim allows you to add SRV support to arbitrary legacy systems that were not written for SRV record compatibility. In distributed systems, there is currently a problem when running multiple identical services on the same machine - do you give each instance its own IP and multiplex on that, or do you have a mechanism (like mesos) that allocates ports for you? How do clients find this? SRV records are one way to find both ip's and ports, but almost nothing currently supports them out of the box. With this shim, however, support can be added to them without modifying any code. Alternatives to SRV records are proxies, userspace overlay networks, ornate iptables rules, etc... and they all have significant downsides that make them inapplicable in many situations. This is super light-weight, works on ancient versions of linux / freebsd / osx / anything that supports dynamic instrumentation and has LLVM support.
One week we added up how much time was spent debugging mistakes that would have been caught by a basic type system. Then we switched to typescript :)
Chris gave a demo at Strange Loop two years ago of a clojure-like language running live in a mathematica-style notebook. Around 18 months ago I started working with them and we started working on turning that demo into a working language. It's changed a lot since then. I've been trying to record the things we've tried and the decisions we've made at http://incidentalcomplexity.com/archive/ but there is tons of material missing.
What's the LoC ratio?
Rad, thanks so much :)
You still have a `match()` referenced in "The Underhanded Part".
I gave it a go, but cannot seem to build Iomrascalai on Windows. On stable and beta it complains about the use of #[feature]: .cargo\registry\src\github.com-0a35038f75765ae4\regex-0.1.41\src\lib.rs:394:34: 394:51 error: #[feature] may not be used on the beta release channel .cargo\registry\src\github.com-0a35038f75765ae4\regex-0.1.41\src\lib.rs:394 #![cfg_attr(feature = "pattern", feature(pattern))] And on nightly (1.4) it fails with this somewhat cryptic output: https://gist.github.com/anonymous/c6909a65c5cd6cc404b3 The warnings are related to rand and rustc-serialize, but at the end it says "failed to run custom build command for `kernel32-sys v0.1.2`". Did you have to do anything special to get it building on Windows?
You don't need `Rc` or `Weak` for cycles. You can generally store cyclic data using `&amp;` references (combined with `Option&lt;Cell&lt;T&gt;&gt;`).
libgreen is dead. If you're going down the lightweight threading route, [mioco](https://crates.io/crates/mioco) might be what you're looking for.
Well to see if anybody cares about the 5.0. It might be an option to release a Mozilla 6.0 after like 20 years. 
Yeah, mio is the way to go for the c10k problem, although I believe it doesn't work on Windows currently.
http://i.imgur.com/fM8A7DK.jpg
I have no idea how to compile it on 32 bit version, it only compiles on 64 bit version.
Is there any reason that this couldn't exist as a built-in macro? Or is it just not a thing yet?
I don't... to what end?
Isn't that basically dynamic dispatch? It's just separating the (hand-rolled) vtables by method and then indexing on type rather than by type and then indexing on method, right?
Weird. I'll retry with a blank .vimrc then. Thanks
I wrote https://github.com/SimonSapin/rust-forest to experiment with different approaches. It’s for a special case (a tree with parent references) but it generalizes to other kinds of graphs. Some more background: * https://github.com/nrc/r4cppp/blob/master/graphs/README.md * http://smallcultfollowing.com/babysteps/blog/2015/04/06/modeling-graphs-in-rust-using-vector-indices/
Perhaps we could reduce the lifetimes once non-lexical borrows land? Who has borrowed those bugs anyway?
Well, that's tricky. They list "D" as having native support through it's standard library... but AFAIK, that's just bolted on top, it's not like it's a first-class citizen of the language. I haven't seen the implementation, but I'd be *astounded* if the compiler does anything to make those things safe. &gt; Justification for writing the above comment: insofar as I know, I wrote the first coroutine implementation for D. Also, "Status registers are not saved by the current implementations." Really? Pretty sure even my crappy implementation did that! Rust obviously doesn't have libgreen in the standard distribution any more... but then, Rust's been shedding everything it *can* from its standard library. So I'm not sure where exactly the line for "native support" is. On the whole, though, yeah; I think it's very misleading to suggest that you get it out-of-the-box with Rust.
Nice, I didn't know that, but it makes sense once you see it ;)
For what it's worth, `let mut this = self;` would also work, but `mut self` lets you keep using that same nice `self` name (though I wish it wasn't this special).
There is a rudimentary ui built in html available called servo-shell, but it hasn't been updated in months. 
Alternative solution, using `read_line`: let mut obj = Obj::new(stream); let mut buffer = String::new(); loop { match obj.reader.read_line(&amp;mut buffer) { Ok(_) =&gt; obj.process(&amp;buffer), Err(e) =&gt; panic!("Could not read: {}", e) } } This doesn't change any of the types :)
It is not the destination, but the journey, that has true value, for do we not *all* share the same end? ^^^^^^^^^^^From:&amp;nbsp;The&amp;nbsp;Book&amp;nbsp;Of&amp;nbsp;Sayings&amp;nbsp;That&amp;nbsp;Don't&amp;nbsp;Really&amp;nbsp;Apply&amp;nbsp;To&amp;nbsp;Software&amp;nbsp;Development,&amp;nbsp;But&amp;nbsp;Sound&amp;nbsp;Profound&amp;nbsp;Enough&amp;nbsp;To&amp;nbsp;Maybe&amp;nbsp;Confuse&amp;nbsp;People
It looks like they're going a different direction with Eve. From what I can tell, it's less about programming and more about exploring data.
Thanks both for your detailed replies! So, if a field of a struct is moved then the whole struct is actually moved (or "partially moved") is that right? It puzzles me, though, why ```lines()``` consumes the object, but ```read_line()``` does not. Does this happen with all iterators? 
"Sometimes you have to travel a long way to find what is near." ― Paulo Coelho, Aleph 
Because that's *just how the trait is written*. Look at it like this: taking `self` is the *most* flexible interface: you can implement that for both values *and* pointers. But let's say you only take `&amp;mut self`; what happens if an implementation *wants* to take ownership of the state it's operating on? Well, you're just plain outta luck. You might say "but `read_line()` is doing *basically* the same thing that `lines()` is doing, but just once. Shouldn't they have the same requirements? Yeah, they probably should. ... *walks away whistling* &amp;nbsp; (Hint: you almost *never* want to call `read_line` *just once*, but calling `lines()` just once often *does* make sense.)
That's right. It's like taking the engine out of your car for service, and taking the car for a ride in the meantime. You have to make the engine optional or make sure it won't be missing when you take it for a ride. I don't really know why it's necessary to consume the reader, compared to taking `&amp;mut self`, but you could say that `lines` would consume the reader because it takes all its lines. There would be nothing left when it's done. `read_line` takes only one line, so there may still be something left to take.
As a side-effect project there is now https://github.com/hauleth/ct which is tryout for time constant numeric operations. I'll test it out just when I install ctgrind if it is OK (when tested via GDB it seems that it doesn't create any conditional jumps in code that could affect timing).
Cough [combine](https://github.com/Marwes/combine) Cough. I don't want to hijack the thread but this is exactly what combine does :). Currently you can get some insane compile times if you aren't careful though (see: https://github.com/Marwes/combine/issues/21). @m4rw3r (woah, I hope you aren't my evil win(?) ;) ). More on topic though, I like your use of mdo! though, it might be nice to have something like that for combine to reduce the verbosity of writing monadic parsers (I simulate applicative using tuples so that already works quite well). Question: Have you any thoughts on error handling? I feel combine does have nice errors though that does come at a cost due to the need of dynamic allocation (I am using a Vec&lt;Error&gt; more or less and trying my best not to create errors until they are needed).
Can you really just take a mutable borrow of an immutable value like that? Or is it just with self? 
It's not a borrow. If you own a value, you can move it into a mutable variable.
Hmm, could `mut` be left out of the signature for a trait for owned parameters, and then added to an impl for that trait? I haven't checked, but it seems likely since `mut T` and `T` are the same type. 
Combine was also a reason why I did not explore the nested struct idea too closely, seeing as it has already been done :) So I tried the closure/function-based approach instead of the macro approach provided by Nom. I really want to avoid code-generation as much as possible seeing as it can get pretty complex really fast (and Nom has had some problems with compile-times too because of the complexity of the generated code in eg. alternations). For error handling I have some ideas, but all are not yet implemented. Currently the error is just an enum variant `Error(&amp;[I], E)` where the first parameter is the remainder including the start of the error. This should make it pretty easy to construct line + column data from the original input seeing as the code invoking the parser still has that slice and can just subtract the length of the remainder to get the so-far-correct part of the input. To make sure that this invariant for the slice in `Error` is upheld, the `State` enum is private to the parser module. I did not do that for the boxed-closure approach out of convenience, did not feel like wrapping all return values of the closures in an opaque enum, but if I actually use the boxed closure idea for the final parser I will wrap the data. `E` currently requires the `Error` trait but no more restrictions are put on it, which means that I should be able to just make a special error type for combinators like `many1` where they will return a `Many1(Box&lt;E&gt;)` as the error or something like that which enables more verbose error messages without any allocation besides boxing the error itself. `bind` also has the (simplified) signature `bind(m: Parser&lt;T, E&gt;, f: Fn(T) -&gt; Parser&lt;U, V&gt;) -&gt; Parser&lt;U, V&gt; where V: From&lt;E&gt;` which enables it to use `From::from` internally to coerce any error to the type `V`. This means that you should be able to define your own parsers like `do_check(d: SomeStruct) -&gt; Parser&lt;SomeStruct, SomeStructError&gt;` with a `impl From&lt;parser::Error&gt; for SomeStructError` and then bind that one on one of the provided parsers, giving you your own error structs back. PS. I should probably make another post just about error handling, the different approaches and the impact on performance and clarity of error messages :) PPS. And it seems like I should add a few additional parsers to the benchmark too ^^
The reason is the `ret` operation, it would enforce `Clone` on the data being returned if the closure being created was either `Fn` or `FnMut`. This means that to avoid `Clone` the whole `Parser` type needs to be `Box&lt;FnOnce(...) -&gt; ...&gt;` and this limits the combinators too since they cannot grab a "`ParserMut`" instead of a "`ParserOnce`". I did a few performance tests with and without clone, and there is no difference as the number of clones created in `ret` is about equal to the number of clones created in `many`. Though I guess that there are inputs where you have a pretty complex argument to `many` which will tip the balance in favor of `Clone`, but the items being cloned could also be pretty large structs with data (eg. strings, which cannot just be 0-copy unless you're parsing &amp;str because of Rust's UTF-8 restriction on strings), so I don't really think there is anything right or wrong here.
Could you elaborate what was the reason they removed croutine's from rust?
Non-1:1 threading was removed for a few reasons, but the biggest one is that they require a runtime[1], and as a systems language, Rust should have as small a runtime as possible. There's some other context too. You can see some of the discussion here: https://github.com/rust-lang/rfcs/pull/230 and some here https://github.com/rust-lang/meeting-minutes/blob/master/workweek-2014-08-18/threading-model.md 1: All languages have a runtime, but ones that have a very small one are conversationally said to have 'no runtime.'
Don't ask the rows. Have an xcoord type and ycoord type that are validated on construction and know to which matrix they belong; then ask your xcoord to lookup a ycoord or vice versa.
Or there might be a way to exploit the fact that each closure is a unique type to achieve static checking as to belonging to a particular matrix. I'm not sure.
I'm not 💯 sure, but declaring your type as private in the crate and the factory/processing functions as public should give you what you're looking for.
You can't do: fn add() { let num = 1; num += 1; println!("{}", num); } for the same reason you can't do: fn add(num: u32) { num += 1; println!("{}", num); } Function parameters and variable bindings are immutable by default. They need to be prefixed by `mut`.
There is no `&amp;`, so there are no references or borrows, it's all by-value.
For folk who don't idle on IRC - #rust-offtopic is generally not actually off-topic, most of the discussion is actually about Rust. The way things seem to have worked out is that #rust has become a support channel, and -offtopic has become a social channel, rather than #rust being general discussion and -offtopic being off topic. I suspect #rust-offtopic-offtopic was created to address this.
Maybe I'm being ignorant here, but where is the problem with the native os-level threading that rust provides?
Awesome! My cousin plays drums, so I'll have to show him this :) Just FYI you can shorten [these lines](https://github.com/monsieursquirrel/every_beat/blob/1535a7584173662277694f92ff93ae11e3c10c1a/src/main.rs#L49-L51) slightly by using a macro provided by `clap`, but you'll have to add `#[macro_use]` above the crate declaration. let start = value_t!(args.value_of("START"), u64).unwrap_or(0); let step = value_t!(args.value_of("STEP"), u64).unwrap_or(1); let num_bars = value_t!(args.value_of("NUM_BARS"), u64).unwrap_or(128); Whether or not that helps or hurts readability is personal taste. The macro does pretty much exactly what you were doing already. ;)
What, slowly and publicly going mad, that you might serve as a warning to others?
I have written a server that handles close to 300,000 requests per second on Linux on a 6-core linode virtual machine using mio (closer to 60,000 for serving real pages with compression, URL parsing, hash lookup, etc.; I can't remember the exact numbers, but it might be higher): https://github.com/slaent/slaent-rs. It's highly specialized though, and incomplete. As far as "30k simultaneous connections" goes, it's not terribly difficult. It's easy to handle lots of simultaneous connections by just increasing the size of the buffers you use, but there's no reason to use Rust just to handle lots of connections. Where Rust can (theoretically, anyway) shine is extremely low latency, leading to lots of very fast request completions per second. FWIW, iron can already handle over 50k requests per second, at least that's my recollection. Personally, I'd recommend against using Rust for this sort of thing. Something like Java's netty is great and Rust doesn't add that much over it unless you have significant response time constraints.
There are pros and cons to both kinds of threading. One example of 1:1 threading being less than ideal is that you get a full stack with each thread, so you can use a _lot_ of memory if you spin up, say, a few hundred thousand threads. Both models have strengths and weakensses depending on what you're doing.
If the type is private, it cannot be exported or referenced by exported functions.
Note that when implementing you can cherry pick which method to export: impl MyType { fn new(..) -&gt; MyType; // private pub fn get(...) -&gt; T; // public } Isn't that sufficient?
And [rusty-peg](https://github.com/nikomatsakis/rusty-peg) too
On further thought, your point still applies. Basically, make the index type know how to access the information itself, instead of going back to the datastructure. Interesting.
Why do you need a thread per connection (or otherwise a very high number of threads)? Couldn't you just have a object per connection which stores the relevant state, and some sort of queue of objects that need to be updated?
Yeah, unique types sound like something that would be useful to solve the problem in the latest version. Got a link to documentation/examples? I would like to create distinct index types, sometimes even when they have the same cardinality, and they would sometimes be shared between different matrices (that happen to share dimensions in a meaningful way). 
Kind of, see my latest version. Sorry about the moving target, I'm learning as we go. I am using a public struct with a sole private field to do the private thing, but that just raises new problems which it turns out I'd like to solve. 
I think you need to add a type parameter to your container and index. This type parameter will only be used for checking correcteness at compile time. You then only implement functions for the case when this parameter is the same in the index and in the container. When creating new instance of the container you will need new unique type, the only way that I can think of is to create a closure but it is a bit ugly.
Ok, I've now changed my ringbuf implementation to use a `(* mut T, usize)` instead of a `&amp;mut [T]` because Rust cannot currently express "out" slices. I also added a new version of send that - hopefully - RVO will optimise to cause no additional memory copies. 
I seem to recall having read about such unique types in ML, but I don't really know ML, and definitely don't know if the trick can be translated into rust. Recall that one failure mode to avoid is using the index for the last element in a long collection for a short collection. So is there a nice way to create multiple distinct/unique such associated types? 
Yay, I'm glad this was recovered. I loved this talk.
Instead of using an index, try 'enumerate' so that you get an index *and* the item.
Aaaah, it's the return type's fault. I see. 
If I understand Python's generators correctly at the implementation level, they basically involve finding a not-quite-the-stack place to store all local variables, so you can exit the generator without losing all the state. (If you take this to the logical extreme, you get [Stackless Python](http://www.stackless.com/), which is pretty nifty.) But Rust has a mechanism for doing that already: closures involve capturing any variables they need and storing them in the closure object. The closure itself can get stored on the stack, or it can get stored in the heap or elsewhere. You need a bit more work (chaining multiple closures together, basically) to really get to generators; that's exactly what a promises library like [carllerche/eventual](https://github.com/carllerche/eventual) or [dwrensha/gj](https://github.com/dwrensha/gj) does. I'm hopeful that one of them (or maybe both of them; the philosophical differences between them are minimal and maybe vanish with HKT) will get super popular, and maybe gain a super popular syntax extension / macro for expressing stuff in a more natural style (e.g., async/await, a la C# or recent Python). Python didn't have asyncio for a very long time, and there was a lot of out-of-core work in that direction, notably Twisted but also stuff like Stackless.
Here it is with automatic markers: http://is.gd/tdBf4H It was easier than I thought. Compile-time error now becomes a bit confusing. I think these closures will be zero-sized which should add no overhead according to Rustonomicon: https://doc.rust-lang.org/nightly/nomicon/exotic-sizes.html#zero-sized-types-%28zsts%29 If you will implement Copy or Clone for SafeVec it will become possible to accidentally create different variables with the same markers.
&gt; Cast them to floats before the division: let result = 12i32 as f64 / 5i32 as f64; Specifically I'm referring to the map on line 54. How can I map my vector of i32 into a vector of f32? using the casts gives me different errors. 
I believe you should do either of the followings: let mapped = results.iter().map(|x| *x as f64 / max_iter as f64); // (1) let mapped = results.iter().map(|&amp;x| x as f64 / max_iter as f64); // (2) The simple cast didn't work because `x` was actually a reference. You need to (1) dereference it before casting, or (2) make `x` a value by using a pattern match in the closure argument. EDIT: And the reason why `x` was a reference? That's because `.iter()` returns an iterator that emits each item as a reference pointing to the original item (borrowing). If you want to get each item as a value, you may use `.into_iter()` instead, which also takes the ownership of the items. However, in this case, the original vector (`results`) will be unusable after the iteration. let mapped = results.into_iter().map(|x| x as f64 / max_iter as f64); println!("results: {:?}", results); // error: use of moved value: `results`
There are a three solutions to this problem that I know of. The first, suggested by /u/Veedrac, is to use an `Entry`-like object that has a reference to the original matrix. This works, and has very clean syntax. However, - If the indexes hold shared references to the matrix, then you can't mutate the matrix. If, on the other hand, they hold mutable references, then you can only have one index, or one index per element if you use unsafety internally. - Because of this, you would probably want two types of `Entry`s, one immutable and one mutable. This in turn would duplicate all your `Entry`-producing function to those with and without `mut`. - In the mutable case, you can't have separate column and row indices. This means that you can't have some function on the matrix that takes a row and a column separately and, safely using unchecked indexing, produces the result. The second approach fixes all of these problems, but make the creation of matrices a fair bit harder. This involves giving every matrix a unique "name" that is stored in the type system, similar to what /u/jkleo1 suggested. The difficult part is that you need - an infinite supply of "names". This is usually remedied by using lifetimes for names. - a way to construct truly unique "names". To solve these problems, I usually use the following module, which I really need to get around to putting on crates.io: use std::marker::PhantomData; #[derive(Copy, Clone)] pub struct Ref&lt;'id&gt; { _marker: PhantomData&lt;(&amp;'id (), fn(&amp;'id ()))&gt; } impl&lt;'id&gt; Ref&lt;'id&gt; { pub fn new() -&gt; Self { Ref { _marker: PhantomData } } } pub struct Id&lt;'id&gt; { _marker: Ref&lt;'id&gt; } pub fn with&lt;R, F: FnOnce(Id) -&gt; R&gt;(func: F) -&gt; R { func(Id { _marker: Ref::new() }) } `Id`s have the invariant that there is only ever one `Id` with any given name, the `'id` parameter. To preserve this, the only way to construct `Id`s is through the `with` function, which requires that its argument accept any possible `'id` (higher ranked trait bounds are the mechanism behind this). With this tool, you can embed an `Id` in your object (in this case, a matrix), and have indexes that remember, through the name of your matrix, which matrix they came from. As an example, I wrote up a (very) basic matrix library using this technique at https://play.rust-lang.org/?gist=4113f306d8d99bffac24&amp;version=stable. Unfortunately, this trick has a number of restrictions: - Every matrix ever created has to be surrounded with a separate `id::with`. In particular, this means that it is impossible to implement `fn multiply(Matrix, Matrix) -&gt; Matrix` or anything along those lines, including `Clone`. - The creation of `Id`s results in lots of rightward drift. This can be slightly reduced via helper functions that create multiple names, bit it still is annoying. - The error messages produced when you mess up can be nasty. Overall, I think that this approach is overkill for your usecase. The third approach is a variation on the second, giving names to references to your object instead of the object itself. This means that the vast majority of methods can be done on matrices directly, without having to deal with a naming mechanism really only used for unchecked indexing. I also wrote a mock matrix library using this approach at https://play.rust-lang.org/?gist=0515d93c5a3bb3d3db16&amp;version=stable. This solves many of the issues of the second approach, but in trade off is a little bit less powerful. It retains the issues of unreadably error messages and still causes some rightward drift, and also requires more boilerplate code. However, you can now implement things like `Clone` and multiplication.
I don't know. It might be historical (I think `&amp;mut self` may predate explicit `Self` types).
Does the mut keyword show up in the docs? It probably shouldn't. But it doesn't leak information in any programmatic sense, because the `mut` is not a part of thet ype.
[It is](https://doc.rust-lang.org/std/macro.concat_idents!.html), but due to hygiene it doesn't do what people want.
rust-peg and rusty-peg are only parsing `&amp;str`, and in this case I have something which is not strictly UTF-8. In a real application it will be a slice of `u8` which might not be UTF-8 at all. Just blindly converting it to a `String` will pose some problems, as you either have to ignore the problematic UTF-8 characters (this is required to actually properly parse binary data, but then you suddenly have `&amp;str`s with invalid UTF-8 data) or you have to remove them which causes a new allocation in a pretty performance-critical parsing-stage (and you will not be able to parse or even pass-through binary data). I also tried rusty-peg just now out of interest (intending to just use the unsafe `str::from_utf8_unsafe` to make a string to parse) but it seems like its documentation and tests do not match up to the code which is actually on cargo, the macros fail to parse :(
&gt; an Entry-like object that has a reference to the original matrix. This works, and has very clean syntax. However, &gt; &gt; * If the indexes hold shared references to the matrix, then you can't mutate the matrix. [...] Since this is for a scientific programming package, you can basically assume primitive (or at least POD) types. Thus, there's a much simpler solution than your proposals: wrap the whole matrix in an `UnsafeCell`. Then you can take non-`mut` references in the `Entry` objects and still simply and safely mutate fields. [Example.](http://is.gd/GpWi8i)
This is a cool use of the closure generation system - I hadn't though of it before. Additionally, even if the closures weren't zero-sized, `PhantomData` would be, so you don't have to worry about size issues. Note however that even without `Copy` and `Clone`, it is possible to create different variables - you just use `()` instead of `|| ()`. Therefore, this system effectively prevent bugs, but unfortunately cannot fully enforce safety. I actually used a slight variation on your trick here in the BTree code (and in my response to the question), using marker lifetimes instead of marker types. This has the advantage that, using higher ranked trait bounds, you can force the creation of a truly unique lifetime.
Yes, it's possible without a runtime. Generators can be implemented by a local code transformation.
I was really expecting more improvement by 2018 (see the edit) Really though, nice post!! I'll have to look through the repo too.
TIL about git's pickaxe search feature. That's pretty sweet!
I think the Rust team should solicit feedback as to how to improve the language from all significant projects using it, whether they are commercial or not, proprietary or not.
How does this make any sense? People who use rust commercially are software engineers just like the rest of us. It's not like the core team is going to abandon their minimalist approach and starting including SOAP clients and excel file parsers in the stdlib just because a company asked for them.
Nahhh.
SUCCESS
Now here's the real question: is this a compiler bug, or intended behavior for Rust (as a language)?
plz teach me how 2 git
&gt;Just not a fan of special meetings. I think that's a better way of expressing your point - don't favor use cases just because companies want them. That said, I disagree - I use rust at work, and it's not really my employer or company who wants rust to change, it's me, because I have to use it.
&gt;the prototyping is happening in Rust but they expressed reasonable concerns about pushing forward into production (e.g. it may be rewritten in something else). Are we ready to stand up and build a team with enough rust expertise? This is it for me. I feel comfortable writing the rust code myself, but I won't always be the person maintaining that code. This is why if I ever (so, twice) have rust code in something where I expect it to be production I keep it as tiny as possible. The one really big nice thing here though is that rust makes documentation so amazingly easy and clear. edit: Great read by the way, thanks for sharing this information with the community.
Absolutely intended behavior.
Beware, though, that you're still relying on the caller of `SafeVec::new` to provide new types on each call. If they don't: #[derive(Copy, Clone)] struct Evil; let row1 = SafeVec::new(vec![1.0, 2.0, 3.0, 4.0], Evil); let row2 = SafeVec::new(vec![1.0, 2.0, 3.0], Evil); ... you're back to square one.
&gt; it just seems silly to have to clone something repeatedly when I know it doesn't need to be. Ah, I guess I'm getting your misconception. `Arc` does *not* clone the entire value (`HashMap`, if following my sample code above) when cloning it. What it does is increasing the internal reference count number atomically. So even if you have a 20MB+ dictionary, it won't consume the memory of multiples of 20MB. &gt; I did find out that declaring it as a lazy static avoids this problem entirely. Do you think that method incurs any overhead? `lazy_static!` makes the lifetimes of the variables `'static`, which means they live forever (the same as the application lifetime). So you can safely use it from any threads. And no, it won't incur any overhead. EDIT: [You can see the implemention of `Arc::clone` by yourself](https://github.com/rust-lang/rust/blob/6734c933b5ee70040346c2e1313e0abb5b978f9a/src/liballoc/arc.rs#L293-L321).
It's hard to get a nuanced idea of what works and what doesn't just from employees voicing their opinion. We're developing a language here, and for a while the only two major pieces of software written in Rust were Rust and Servo. That does not cover the entire sample space of use cases, far from it. Sure, there are tons of open source libs in Rust, but in the vast majority of cases these are written in someone's spare time as a hobby project. Not necessarily with the same care and validation that someone using Rust in prod will have. We need to know what the pain points of Rust are, and aside from Servo/Rust (who have already generated a lot of feedback on the language) these folks are the best ones to help. The rfc process will be an echo chamber of the folks who want to participate in the rfc process (often, not production users). Besides, if you look at the notes what was being discussed was not stuff that gets RfCd. It revolved around general direction and tooling and stuff like that.
There are two issues with this (that I'm aware of): 1. There's no easy way to attach a backtrace to an error; that is, *not* a panic. Yes, ok, I have an `Err(io::Error)`, but how did it *happen*? 2. Backtraces are totally, completely bloody useless on Windows.
You are correct! Thanks.
Oh wow... this I will need to analyze. Thanks.
Will that be a more efficient option than the threadpool?
I would say "gluteal bite". You'd have to *try* to get bitten in the rectum. 
Basically the entire community? We like small crates.
&gt; It's just pointless to do even that tiny amount of work * That tiny amount of work absolutely pales compared to spawning a thread (or, for that matter, anything that's worth running in its own thread). * The *Arc* solution is the more idiomatic one, and idiomatic code is nearly always the better approach. * Don't use *unsafe* unless you really have to. Even if your usage of *unsafe* is safe now, future modifications to your code might actually render some of your assumptions invalid, and the compiler won't notify you of that problem. Seriously, the most important thing I've learned in all the years I've been programming, is to refrain from such micro-optimizations, because they've come to bite me in the ass so often, and they're (almost) never worth it. I'm still struggling against this urge every day.
At one point there was `ScopedThreadPool` that combined the two. It disappeared from the `threadpool` crate when it came to light that the design of `thread::scoped` was unsound.
Thanks. :-D Unfortunately I cannot change the title here. But at least I've changed it on the blog and on rust-lang users.
Again, the choice was between having it in an arc or having it as a static. One is bigger than zero. Period. Edit: I do agree that the roll-your-own-static option seems insanely over the top. :)
Just your friendly neighborhood pedant doing his job. 
I'm honestly not clear what any of these options are doing. I think I need a much, much longer test file. :) Maybe the whole operation is io-bound anyway. I'm honestly only doing the threading to see if I have the concept down, and message passing seems to be working pretty well, although it kind of looks a little janky... ...but I can't tell that it is lighting up all that much more silicon than the serial version. I think it is, but I'm basing that on about 5 milliseconds difference in runtime and the certainty that the concurrent version has more overhead, so this is not great evidence.
I guess you were saying the benefit would be avoiding the need for the arc, which was also achieved using the static lifetime. Have I got that right?
In which cases production users want to apply `catch_panic`? I can't quite see how it should make any system more stable. If your system has to be super stable then catching panics on thread boundaries seems somehow more sensible, because then you've a clear separation of your system and better control of your systems state.
That's not the only benefit, but yes.
It's the equivalent of the UNIX philosophy for Rust
What do you mean by implementing things as an iterator? Do you mean creating my _own_ custom iterator somehow? Are there docs on that?
Neat! Thanks, didn't know that one.
&gt; We cannot return a borrow, because we somehow have to get it back! I don’t understand what this means. I believe it *should* be possible to return a borrow. Do you mind providing more of the code, so we could try it? Using closures when not necessary has its downsides: for example, using `return`, `continue` or `break` from within a closure is not the same as outside.
http://doc.rust-lang.org/nightly/nomicon/leaking.html
I agree that this is worded badly; I'll try to improve it. I actually have two cases: In the first one, the lifetime is bound to the context, which I don't have access to: fn with_ty&lt;T, F: FnOnce(&amp;str) -&gt; Option&lt;T&gt;&gt;(ty: &amp;Ty, f: F) -&gt; Option&lt;T&gt; { if let TyPath(_, ref path) = ty.node { if path.segments.len() == 1 { f(&amp;path.segments[0].identifier.name.as_str()); } } None } The second case is more interesting, because I have a junction where I have two possible lifetimes that don't unify: fn with_follow&lt;F, T&gt;(cx: Option&lt;&amp;Context&gt;, e: &amp;Expr, resolv: bool, f: F) -&gt; T where F: FnOnce(&amp;Expr, bool) -&gt; T { match e.node { ExprParen(ref inner) =&gt; with_follow(cx, inner, resolv, f), // inner has the same lifetime as e ExprBlock(ref block) =&gt; if block.stmts.is_empty() { match &amp;block.expr { &amp;Some(ref inner) =&gt; with_follow(cx, inner, resolv, f), // inner has the same lifetime as e _ =&gt; f(e, resolv), } //TODO: add return handling? } else { f(e, resolv) }, ExprPath(_, _) =&gt; { if let Some(lcx) = cx { if let Some(&amp;PathResolution { base_def: DefConst(id), ..}) = lcx.tcx.def_map.borrow().get(&amp;e.id) { if let Some(const_expr) = lookup_const_by_id(lcx.tcx, id, None) { return with_follow(cx, const_expr, true, f); // const_expr has the same lifetime as def_map } } }; f(e, resolv) }, _ =&gt; f(e, resolv) } } Perhaps there is a solution without a closure for the first case. I doubt the second one is even possible.
Or use the one in the standard library: [is_digit](https://doc.rust-lang.org/nightly/std/primitive.char.html#method.is_digit).
Notably, this feature isn't _inherently_ dependent on GitHub, just this initial implementation is. We can add some other thing to replace the `github:` part of the command. Why implement GitHub first? We'll see what /u/Gankro says, but I would imagine the answer is "it's less work." No need for a full CRUD on teams UI In addition, having it relate to GitHub is also better in some ways. More DRY. (That said, a non-GitHub reliant implementation would be good to have.)
Minor tip: a lot of those git commands show their output in a pager (could be `less`, but I'm not sure). So you can use `&lt;` and `&gt;` to go to the start and end, space/PageDown and backspace/PageUp for navigation, but most importantly: You get regex searches: `/pattern` (forwards) and `?pattern` (backwards). The `git blame` "TMIs" which were solved by specifying `-L` could also have been dealt with by simply using `/lifetime elision`. Though I did learn about `-L`, so I don't mind it, just wanted to point out another tool :D.
This is a really great statement ("very common through the stdlib during the first half of last year") and its weird and also really great that its starting to seem weird.
The first and simplest use-case I can think of is to at least output a stacktrace or similar in the host language before halting the program.
I'd guess a simple example is if you're writing (say) an application server. It might be running multiple different applications - so even if you think it's desirable to kill off an entire sub-application in the event of a programming error, you probably want to avoid killing the entire server.
do u have 5 yrs?
You should read the [rust book](https://doc.rust-lang.org/stable/book/), there are examples and details about all your questions. I would answer directly but I'm a beginner as well and not confident enough to explain anything yet.
* User auth is already tied to Github (every Crates.io user is a Github user -- at least nominally). * All of our biggest projects are on Github with orgs already set up (rust-lang, servo, piston, iron, etc). They don't want to duplicate their team management. * Github has solved tons of the problems for us already. Honestly I'd be surprised if anyone ever bothered to impl auth/teams that wasn't Github. Alex is basically the only person who maintains crates.io -- literally implementing this feature made me the second biggest contributor. Neither of us really care about decoupling from Github, from what I can tell. We already rely on it for auth, hosting the index, and hosting the docs. It's not even clear how supporting other services for auth would work. Your name is tied to your Github name. Adding another provider would create a situation where you wouldn't be able to sign up because someone else took your name *on a different service*. At very least it would probably involve adding an ability to specify a crates.io username that isn't your Github name. Teams would then also be chaos because different kinds of user could interact with different kinds of team. If someone wants to step up and champion decoupling from Github, that's great! However until more people step up to maintain crates.io, this is all just pointless bickering. The Rust project is already massively under-staffed, and we will continue to do things that Just Work as a result. Being massively coupled to Github Just Works for our community.
If you're for whatever reason confident that it's there, you can just unwrap the option. You can also just deref the &amp;mut directly to overwrite it: *list.peek_mut().unwrap() = 4; But yeah, how you deal with an Option is up to you (you could also map over it, for instance).
You can with [plugins](https://github.com/scialex/enabled/blob/master/src/lib.rs) (or at least you could)
&gt; And of course it’s not like we Rust folk invented OO or algebraic data types or anything in the first place. :) Shout-outs to amazing in-joke callbacks.
I really like the single-crate restriction. My biggest objection to any sort of "inheritance" scheme is that it has the potential to step on the toes of what traits are responsible for, bog us down in TIMTOWTDI, and divide the ecosystem. Restricting hierarchies to a single crate reinforces the role of traits and keeps Rust opinionated, which is great.
Just don't find yourself in in-joke callback hell...
For reference, tying an object to another object via the type system is called *branding*; and for now it's impossible in Rust. There are hacks, using lifetimes as brands, however lifetime unification can punch holes in there so it only works in a limited number of situations.
I'm eagerly devouring this now. We are teased yet again on a proposal for virtual methods building on specialisation - I look forward to it! Thanks Niko, as always, for taking the time to write up your ideas.
Interesting. I'd actually feel better about being *more* restrictive as to where variants can appear. Currently, most things in Rust have a nice "follow the `use`s" property to find stuff... having the variants scattered about a codebase worries me from a maintainability perspective. This is really interesting, in that it's recasting enum/struct into something I haven't really thought about before, but feel like I'd totally love to have. If *only* for subtyping enums. I'm a little uncertain about "unsized" in this context, though. One thing I've been trying to do is *not* use "unsized" when talking about things like `[T]` and `Trait`, since there are people who want "real unsized" types for things like C strings, and using "unsized" instead of "dynamically sized" just seems to muddy the waters. Exactly *how* unsized is this? I mean, presumably, this is dynamically sized, except the erased information is stored in the tag byte? The other thing this has me wondering about is how or even *if* this can be used for FFI. If this is *the* way of doing "thin pointer objects"... that still doesn't help us write COM bindings/implementations without going stark raving mad... but then, that particular problem may not even be relevant to this specific idea. I suppose I'm just worried that COM support will end up falling between the seats, until it's rediscovered on Christmas Eve where it turns out to be a Jelly of the Month Club subscription. ----- &gt; We don’t have to fear slicing because the type TextElement is unsized, and hence the user could only ever make use of it by value. Shouldn't that be "could only ever make use of it by reference."? &gt; We’re investing ways to make the compiler smarter, ... "investigating"; that or "investing in".
See the current discussion on `catch_panic!` for the reconsideration of the status quo that's going on right now.
This is an interesting proposal. One thing AFAICS not covered in the blog post is how to write `impl` blocks for these hierarchical structs/enums. Given the example unsized enum Node { // where this node is positioned after layout position: Rectangle, ... } enum Element: Node { ... } struct TextElement: Element { ... } struct ParagraphElement: Element { ... } how would one write a `get_size(&amp;self) -&gt; (usize, usize)` method? At the moment, it would be a single method in `impl Node` with a `match` on different variants. Would you be able to implement some method only for `Element` and subvariants? For example, one could write methods both in `impl TextElement` and `impl ParagraphElement`, and the compiler generates a single method with a big `match` for `Node` (and maybe `Element`) (checking that each variant actually implements the method). That would play into the class-hierarchy use case and make the method code clearer.
&gt; What can you expect of a library and its state after a panic? It depends on the state shared across the `catch_panic` boundary. In the extreme case where nothing mutable is shared, there is no issue.
&gt; (2) Backtraces are totally, completely bloody useless on Windows. Well, this can hopefully be solved, but in the worst case crippling other platforms because one (Windows + MinGW) does not sound very clever. Of course a solution that works on all platforms is better but... still. &gt; (1) There's no easy way to attach a backtrace to an error; that is, not a panic. Well, this can also be solved, however Rust has a "you don't pay for what you don't need" philosophy and some users will not be able to afford backtraces, so the standard library will not have backtraces, ... oh. Even the idea of using an environment variable is not too appealing, if you have a hot spot in the code, you might not want backtraces *in this spot* (because performance) but the other 80% of the code would be easier to debug with backtraces. Personally, it's this itemization that I see as the biggest challenge. 
I don't believe anything currently can figure out methods like that. It would require compiler interaction (to handle inference, trait dispatch, `Deref`, struct fields...) that currently doesn't exist or would be too slow if it were implemented anyway. It will happen someday, I'm sure!
The clone takes awhile, apparently. I wasn't able to see any difference in runtime between the two, but cloning itself does have a measurable impact--forty times what the other two operations amount to, at least. If the average work unit involved substantially more accesses than that, it might come out ahead. Toy problems are fun to tinker with sometimes. :p
Awesome, thanks! So the following work if I don't actually know it's there: // using map list.peek_mut().map(|n| *n = 4); // using if let if let Some(x) = list.peek_mut() { *x = 4; } Sweetness!
Thank you! If someone is curious, it is possible to use ImGui inside a browser with emscripten http://floooh.github.io/oryol/ImGuiDemo.html (is not the last version so it lacks of antialiasing and some other nice recent improvements)
This is out of scope of closures / generator approach. Both can be combined with thread pools.
Nonono, I said that the clone operation is really slow in comparison to a read operation on the lazy static and that whether the free reads are an advantage or not depends on the ratio of the two operations. In other words, for some workloads, the arc will be faster. For others, it will be slower. For the real world workloads I've tested, there is no appreciable difference (most of the time is spent loading first the dictionary and then the target file from the disk anyway). As such, my code does not reflect what I think will grant better performance, but rather what is--to me, anyway--a more familiar style. https://github.com/archer884/check/blob/master/src/main.rs#L30-L33 I'm used to being able to create things right above `main()` like that in other languages.
&gt; I would like to see how checking for down-casting from &amp;Node to &amp;TextElement is achieved? I'm assuming each variant would still have an inline discriminator. So rather than [•|•] | ↳[data] ↳[vtable] you would have [•] ↳[discriminant|data] Anything required for virtual dispatch would have to be stored directly or indirectly in the discriminant.
You could add that kakoune has Rust syntax highlighting, although it might not be strictly up-to-date.
&gt; my guess is that reference counting takes up very little time. Given that the program does I/O and spawns threads while only incrementing the atomic 6 times? Yep... so little time that it's not even worth contemplating.
With your current program, your main thread may terminate while the threads are still running. Normally, you would move the `JoinHandle` (result of `spawn`) into a `Vec` and then have a loop on that `Vec` which calls `join` on each handle before the program ends. *Note: I realize YOU probably know it, but given the number of newcomers who come to Rust, I think it's worth promoting best practices even in tiny examples.*
And we cannot lie.
I basically used enums for some code I was just working on -- code that in Java is done with inheritance -- and it wasn't fun. It led to two understandings: (1) Enums are very similar to classes at times and that Sized/Unsized distinction becomes clear. Nico does an excellent job of codifying that and extending the implications. (2) Enums of Enums of Enums start to pay a heavy price since you now have three tags for each data element and you pay along every path in hierarchy. I had to really make some decisions and capped it as an enum of an enum and did some hacky stuff to deal with the rest. Thanks for the great write up, Nico.
I completely agree. Dealing with these sorts of problems in other languages is, frankly, traumatic at times. For example, I've wasted too much of my life dealing with conflicting implicits in Scala and with overlapping monkey-patches in Ruby.
I was surprised the sized/unsized thing is done when defining the enum. I would have thought that if the type is the base enum then the size of would be the max size of among all of its variants. But using a variant as a type could always be just the size of that variant. Is there some reason why this wouldn't work?
Yeah, me too. Destructors are actually fine as a convenient special case for linear types where there's a single, obvious way they can be consumed, I think.
Though the single-crate restriction seems to solve a good a chunk of the problems I have to imagine that it would make incremental compilation more difficult to implement as you can add a variant to an enum at anywhere in the crate which could force a recompilation in another part. It essentially is another way of introducing cyclical dependencies though so I guess its just another moving part to keep track of I guess? Anyone working on incremental compilation that has any thoughts on it?
&gt; Enums of Enums of Enums start to pay a heavy price since you now have three tags for each data element I was just thinking about this myself. Given that the discriminator for an enum often ends up being multi-byte for reasons of alignment (wasting a bunch of space), I don't think there's any technical reason why enums of enums of enums shouldn't be able to have their additional discriminators stored in some of that wasted space, is there? So rather than having three discriminator locations, you have one. I suppose there's overhead in terms of moving/copying inner enums, but it would be interesting to be able to define a 'packed' enum of enums.
The more I read the better it gets :0 Thank you, this was surprisingly easy to digest. I like that variants aren't confined to the enum declaration. If I ever refactor to use this, I have a file that would be very long if that weren't the case.
This is awesome Niko. The time spent on figuring this out was well worth it. I think this proposal is very elegant and simple, yet powerful.
This is really interesting, but it also makes me uncomfortable. The shared data between enum variants seems very reasonable, but the bounds stuff I'm less sure of.
Right, I missed that announcement of another possible post in the series. I'm sure there will be an elegant solution (it should be independent of traits, like the rest of this proposal...)
That's not always what you want, so it's left up to the implementer to decide which is better for that particular enum: &gt;As today, the size of an enum type, by default, is equal to the largest of its variants. However, as I’ve outlined in the last two posts, it is often useful to have each value be sized to a particular variant. In the previous posts I identified some criteria for when this is the case: &lt;snip&gt;
The API is still unstable, but it exists https://doc.rust-lang.org/nightly/std/dynamic_lib/struct.DynamicLibrary.html
Ah, that's a fair point. I was too lazy when writing the example! Sorry for the confusion.
I actually would expect that types and traits would at some point evolve into mirror things. In a way we are returning to old wisdom: keep your nouns (data) separate from your verbs (data actions). So we have: * Types that represent data, the way in which it can be mapped on memory and that's it. *Traits represent the transformations and actions that can be applied on data. * An `impl` is the mapping of a type to a trait. What I don't like of this design is the allowing enums to have shared fields together. I feel it murkies what an `enum` is and also steps on what `struct` does. struct PtS { x: i32, y: i32, } // How is this different from the above? enum Pts { x: i32, y: i32, } I'd prefer a different way. Lets first allow anonymous types within type definitions. This means you can use a `struct` or `enum` item anywhere you could use a type within a `struct` or `enum` definition, also you can choose to not specify a name at all in these specific cases. Next lets allow the ability to extend a type. I like Go's idea of putting the type you are extending inside the definition, it kind of makes more sense than the `: Base` syntax, as I like to think of `A: B` as `A` is subset of `B` and here it's actually a new type. struct SBase { a: i32, b: i32, } struct SExtend { Base, // we could make the type optional for structs but not enums a: bool, x: i32, } let s = SExtend{SBase{a: 1, b:2}, a: true, x: 3}; s.a -&gt; true s.b -&gt; 2 s.x -&gt; 3 s as SBase -&gt; SBase{a:1, b:2} s::SBase.a -&gt; 1 s::SBase.b-&gt; 2 Moreover you could compose multiple structures into one. struct SExtend { SBase1, SBase2, ... } Since this would only happen internal to the crate you could allow a bit more flexibility. The idea is that data definitions are very bound to the project and should be small, so it's ok to limit things in this way. If a name conflict occurs and there's no explicit overload (there's a `Base1::x` and `Base2::x` but no `SExtend::x`) we create an error unless it's explicit (`sextend::Base1.x` is fine, and so is `sextend::Base2.x` but `sextend.x` causes an error). Enums would be similar. Nesting an enum would try to add all the options to the enum. enum EBase { B1, B2, } enum EExtend { // Due to backward compatibility we have to explicitly declare // this type to be external and not defined here. // any valid path works self::EBase, E1, B1, } EExtend::B1 != EBase::B1 == EExtend::EBase::B1 EExtend::B2 == EBase::B2 == EExtend::EBase::B2 Much as in the post we can see we are making each possibility of the enum a type. Notice that you can't do `as` casting I'll go into more detail later. Again you could have multiple enums nested in one, and again if there's a conflict you'd have to be explicit in which sub-enum you are referring to. Again we limit this to crates. When a struct contains the enums, the values of the enum are put in the struct's namespace (with conflicts handled as above). Moreover the struct can be cast unto the enum looking something like `struct_instance as EnumSubType` removing the enum part. Enums containing structs it's just another option. Now here's how the compiler example would be written: type Ty&lt;'tcx&gt; = &amp;'tcx TypeData&lt;'tcx&gt;; struct TypeData&lt;'tcx&gt; { enum { Int, Uint, Ref(Ty&lt;'tcx&gt;), ... } struct CommonFields { id: u32, flags: u32, ... } } The function that initializes the Common Fields would have a signature `fn initialize_common_fields() -&gt; TypeData&lt;_&gt;::CommonFields`, you could initialize things by using the `..` as in the example, or be more explicit: let x = TypeData&lt;'lifetime&gt;{TypeData::Int, initialize_common_fields()} Notice that there's no need to name things, the type is the name! Notice that if you don't want fields to be extractable as above, you simply don't use the `struct` nesting and add the fields directly (or use an anonymous `struct`). Also notice that the `enum`, being anonymous, doesn't have any way to access its states explicitly. Maybe it would make sense that if a field is made inaccessible a compilation error should occur.
Very informative, i really enjoyed reading the article. I will use glium when i find some time to work on my OpenGL projects. One question: what program did you use to profile the code and measure CPU consumption?
To explain why panic doesn't bring the whole program down and just the current thread. You panic when you can't go forward, you've realized that things have gotten to a state were nothing makes sense and trying to work through it or fix it might make things worse. If you were a Rust program you wouldn't call panic because you tripped, you'd call panic because you don't have a leg anymore, or just had a heart attack. Now because threads are running in parallel the fact that one thread can't go on doesn't guarantee that all other threads can't. In our heart attack example the fact that one person had a heart attack doesn't mean that everyone everywhere stops what they are doing and panics, only those affected indirectly. With that said, if a thread panics, and your code is well done (and doesn't abuse `unwrap` and other issues like that) there's a very good chance the other threads will panic too. Any resource they share through a `Mutex` will be poisoned, and using it will cause a panic too. Also many of the sources of panic are things that affect all other threads. But this is probable, not certain. Finally maybe we are dealing on a greater context where each thread is isolated. For example a server with separate threads for each request. If a panic occurs a 500 is returned and the client tries again. Bringing down the whole server is unnecessary as what affects one request probably isn't affecting the others.
That's great!! I was planning on doing this, even had a local repo and started sketching a C API for wrapping C++. Now I'll probably contribute to this instead. =)
&gt; Such a model is very enticing, but what do you do about embedders who want to abort to an API boundary, or server applications that want to abandon a connection and its associated thread upon an error? Yeah, it would only work well if everyone else in the ecosystem bought in, which is why there's really no hope of doing this with current Rust. IMO the real key to making such a system work would be for most in-bounds array indexing to be statically provable; for practically everything else, including OOM, explicit error handling is acceptable (I personally think division and modulus don't earn their keep as operators, which are other sources of potential unwinding in Rust right now). As you've pointed out in the past, systems programming mostly does pretty boring stuff with arrays, and IME often the logic is simple enough that you can embed it in current Rust (using rank-2 lifetimes and invariance). I could see providing two types of array indexing, the statically checked one and dynamically checked one, being usable as long as you only needed the second relatively infrequently. Also, without destructors, explicit uses of goto would be reasonable, and the common use for cleanup could probably be statically checked. So it would definitely be more verbose than it is in current Rust, but probably not moreso than C (whether C-level verbosity is acceptable or not is another question).
You're not allowed to include GPL licensed libraries with your proprietary application whatsoever. At most you can either link to a system library which is GPL licensed, or if the library is LGPL then it has a provision for you to include the library so long as you link to it as a shared library. Stuffing dynamic libraries inside your binary will NOT solve the GPL issue. Just as easily as you can have a binary that extracts some shared libraries, you can have the binary _also_ extract another binary and then execute that other binary. That way you don't have to resolve symbols at runtime and can just link to the libraries normally.
Unless you swim in certain rivers. (I see your pedantry and raise you an unexpected case!)
It's just another edge in the dependency DAG. 
I'm on Windows. I'm using: - AMD CodeXL - Very Sleepy, but I think this one continues to count even when the program is sleeping, so the percentages are wrong if you use `thread::sleep_ms`. - [hprof](https://github.com/cmr/hprof), the most reliable but also the most invasive, so I only use it to measure precise things. - AMD GPU PerfStudio, which gives the CPU consumption of OpenGL commands, but I found that it doesn't match what hprof gives me. Maybe it profiles the OpenGL background thread ("the server") instead of your program's main thread. At least I can get an idea of what's happening. CodeXL and Very Sleepy [only work with the MSVC nightly](https://www.reddit.com/r/rust/comments/3g59no/announcing_rust_12/ctv4ia3) however. 
Thanks for the gold \o/
*cough* /*! Add to Cargo dependencies, or just run this with `cargo script`: ```cargo [dependencies] lazy_static = "*" ``` */ #![feature(dynamic_lib)] #[macro_use] extern crate lazy_static; macro_rules! dynamic_extern { (@as_item $i:item) =&gt; {$i}; ( #[link=$lib:tt] extern $cconv:tt { $( fn $fn_names:ident($($fn_arg_names:ident: $fn_arg_tys:ty),*) $(-&gt; $fn_ret_tys:ty)*; )* } ) =&gt; { $( dynamic_extern! { @as_item unsafe fn $fn_names($($fn_arg_names: $fn_arg_tys),*) $(-&gt; $fn_ret_tys)* { #![allow(dead_code)] type FnPtr = unsafe extern $cconv fn($($fn_arg_tys),*) $(-&gt; $fn_ret_tys)*; lazy_static! { static ref FN_PTR: FnPtr = { unsafe { use std::dynamic_lib::DynamicLibrary; use std::path::Path; let lib = DynamicLibrary::open(Some(Path::new($lib))).unwrap(); let ptr = lib.symbol::&lt;()&gt;(stringify!($fn_names)).unwrap(); ::std::mem::transmute(ptr) } }; } (*FN_PTR)($($fn_arg_names),*) } } )* }; } dynamic_extern! { #[link="msvcrt.dll"] extern "C" { fn puts(s: *const u8) -&gt; i32; fn gummy_wuzzle() -&gt; (); } } fn main() { unsafe { puts("Hello, World!\0".as_bytes().as_ptr()) }; } Obviously limited, but I don't believe there's any reasonable features you *couldn't* add.
You sir, are amazing! I was actually not aware macros were this capable. I really want to learn more about macros to begin understanding how this actually works! You've already helped quite a lot, but could I ask if you have any suggested readings on how macros work to learn more about the above?
I do. :-)
&gt; So we can easily downcast by just comparing integers. But can an enum contain multiple other enums? Surely we'd need to have an "enum offset" then.
Meh, I'll just wait until Rust supports full in-joke coroutines. Again.
So what about this packing of *nested enums* (is that the right term?) that someone suggested? I'd have thought the idea was to do something like: enum Foo { enum Bar { Baz, // Foo.Baz = Bar.Baz = (0) Frob(i32), // Foo.Frob = Bar.Frob = (1, _: i32) Nope, // Foo.Nope = Bar.Nope = (2) }, enum Blorp { Beep, // Foo.Beep = (3), Blorp.Beep = (0) or (3)? Boop, // Foo.Boop = (4), Blorp.Boop = (1) or (4)? }, } If we require that `Blorp.Beep` == `Foo.Beep`, then it would be (3).
&gt; For textures, glium will try to use all texture slots (one different texture per slot), and will only change the texture bindings if not enough slots are available. What do you use, a LRU cache? Also, can the method be changed by the user? An option would be to pass a closure while creating the context that receives relevant data (such as the current textures in LRU order, the new texture, and perhaps some user-provided associated data like a frequency counter), and decide which one to drop. That way one could implement [LFU](https://en.wikipedia.org/wiki/Least_frequently_used) for example.
It would be better if you would follow "official" way for creating bindings to C libs and create `imgui-sys` crate that would provide 1-1 binding to C functions and then build your lib on top of that.
I wasn't talking about *offsets*, but *variant numbering*. If that is an "implementation detail" to you, ok. I also won't argue against calling it `Foo::Bar::Frob(42)` or `Foo::Blorp::Boop`. Just imagine I had `use self::Foo::*;` before my code. :-)
https://github.com/ujh/iomrascalai I merged my pull request on the main repo, can you try and report back?
I think that /u/nikomatsakis post tries to solve 2 orthogonal problems: - data reuse (like default "struct members" for enums) - data layout control (unsized enums) I don't see how your approach solves the data layout control issue so maybe you can expand on it. Unsized enums do, and since they are useful on their own they could go into their own RFC independently of all the other things that the blog post mentions. I need to re read the post a couple of time and throw a couple of long showers at it to make up my mind about the rest. I would like a cohesive solution that gives us data reuse, data layout control, and data impl reuse. 
I found this particularly interesting in relation to the evolving design of rust. It makes me wonder what decisions that are being made now that could end up on a list like this in 5-10 years
I was considering that, but I thought the convention was meant for system libraries. ImGui doesn't even have a Makefile and you are expected to just include the source files in your project. So it's not something you would find preinstalled on any system. So, basically the question is: does it make sense to use the *-sys convention for simple libraries, like header-only stb_image.h or similar libraries?
There's still more to be done, but all the keyspace operations are done so I thought it was a good time to share! API endpoints left to implement: * Remaining statistics endpoints * Cluster management * Authn/authz, new but experimental in etcd 2.1 In addition, it could stand to be smarter about retrying calls in various failure scenarios.
It'd be no worse than C.
&gt; Let's take an even bigger step back. Why are we treating integers—the name of which implies that they are to be treated as numeric quantities—as though they were in fact small arrays of bits? The vast majority of C# programmers today aren't writing bit-twiddling code at all; they're writing business logic when they use integers. C# could have created an "array of 32 bits" type that was an integer behind the scenes, and put the bit-twiddling operators on only that specific type. The C# designers already did something similar to restrict operations on integers for pointer-sized integers and enums. Perhaps "ints as numbers" and "ints as bags of bits" should be distinct, with no-op coercions when needed. But there's a particular kind of code, that needs both addition and bit manipulation, that would become much more verbose (eg. crypto). &gt; If you're going to make a backward-compatibility-breaking change, no time is better than now; things will be worse in the future. This is totally true.
Isn't the idea behind 'refinement types' that you get to unify more specific types with more general parents, predicate-wise? I'm a newbie at all this but could someone shed some light on what/how 'refinement types' in the article particularly means? Thanks!
I think the main point is that bit twiddling shouldn't be available on int, not that arithmetic shouldn't be available on the bit twiddling type. Addition modulo 2^n does make sense on n bit values, doesn't it? On the other hand, overflowing addition doesn't make sense on int. Ideally you'd have overflow checks which can be turned off for release builds.
Awesome!
As far as parser-based code, this was pretty fun to write. It uses [`nom`](https://github.com/Geal/nom) for defining the parsers, which, except for long release compile times is excellent. If you've been curious about `nom`, definitely check it out. Help would be really appreciated with filling out parsers for the rest of `/proc`, as well as testing on different kernels.
&gt; and if that lands, you can use it to make panic! == abort Not in any useful way, because the compiler will still generate landing pads, avoid compiler optimizations that would be broken by unwinding, and do all the other stuff that one doesn't want it to do if panic = abort.
So the enum has to have it's enum-y part declared. Declaring an enum without any option is like declaring an empty enum, it can't be instantiated. Then that's worse: it allows me to declare a non-instantiable type that doesn't seem to be wrong intuitively (it has data so it should be instantiable). I like my model a bit more because it allows the same complex types but uses enums and structs for very specific separate purposes. Ridiculous cases like the one above are not worse than what we have today: a struct with an uninstantiable member.
This seems to be an amazing amount of work for some guy just hacking on his free time! Awesome!
Looks great! I just did the tutorials. Could this in the future work with Emscripten to develop web applications that use asm.js/WebAssembly with WebGL? And do you think Vulkan will need a similar library?
&gt; redundant comparison overrides But isn't C#'s problem that it supports both reference and value equality? I know Java has the `==` vs `equals` problem, which Rust *does not* have.
&gt; This is not true. Every enum begins with a discriminant (a small integer) identifying which variant it is. This is true today and remains true under this proposal. So we can easily downcast by just comparing integers. Ah I see; I had mistakenly assumed I would somehow be able to point within the data-structure. The explanation about coercion should have pointed me in the direction of "same-address". Thanks for the explanation!
This is really great - I've done a very small version of this for a project I'm working on, but it's nowhere close to exhaustive. Thank you!
&gt; I thought the convention was meant for system libraries. Sort of. From http://doc.crates.io/build-script.html#*-sys-packages &gt; To alleviate linking to system libraries, Cargo has a convention of package naming and functionality. Any package named foo-sys will provide two major pieces of functionality: &gt; &gt; * The library crate will link to the native library libfoo. This will often probe the current system for libfoo before resorting to building from source. &gt; * The library crate will provide declarations for functions in libfoo, but it does not provide bindings or higher-level abstractions. You're talking about #1, but /u/Hauleth is talking about #2. There's also another slight issue here: "system libraries" really means "non-Rust stuff," later in that page, it covers building a library too. I would aruge that an A++ `-sys` package should try to link to a pre-installed version, but also know how to build one as a fallback in case it doesn't exist.
Very useful! I'm parsing /proc/partitions and /proc/mounts in an ad-hoc fashion, so I'd love to use &amp; build on proc instead.
Rust also fixes the “extra special bonus rant” (use of the assignment operator for both its value and side-effect), and many things from the “dishonorable mentions” list (unary plus operator, module terminology, C-style `for` loops, special-case `void` type…).
a&amp;15 (for example) is useful for modulo 16 with sensible handling of negative numbers. Similarly bit-shifting for divide.
nice!
&gt; Could this in the future work with Emscripten to develop web applications that use asm.js/WebAssembly with WebGL? That's the plan. Rust doesn't work too well with emscripten for the moment, but hopefully this will be improved in the future. &gt; And do you think Vulkan will need a similar library? Yes. Vulkan is different though, as there's no problem with good practices, multithreading, etc. A library would probably be more safety-oriented. 
Thanks for the clarification! So I guess an imgui-sys package would be appropriate even though ImGui is always built from source. I already provide 1-1 declarations in a separate ffi module, but it's currently part of the main crate.
Yeah there's certainly still advantages to going further.
I need to parse /proc/bus/input/devices for my [keylogger](https://github.com/gsingh93/keylogger). I'd love to use this if that ever gets added, and if I have time I'll look into it.
I think I meant "as syntactic sugar for automatically consuming linear types", rather than "destructors as Rust has them", implying that it would only be applicable when you could've written out the same thing by hand.
But the memory allocated for stack won't be used right away, it's mapped lazily, right? Or is it MAP_POPULATE for stacks? 
&gt; It would be nice if there were a generic impl of PartialEq for T where T: PartialOrd (so that you could implement just one method and get all the operators for free), but I don't think this is practical without specialization. I had not thought of this, but indeed specialization may allow to reduce this even further!
They *why* is more because I didn't have a reason not to, haha - and maybe if I had some C as part of my project it'd be nice to easily see there's the potential for x% of project that's "unsafe"...ish :) The *how* is simply all code (besides blanks and comments) is unsafe. Rust is currently the only language where I took the time to count lines inside an `unsafe` block or function. Although if other languages have similar keywords that same logic can be added as well. The `unsafe` portion of this sub-command spawned from [this thread on users](https://users.rust-lang.org/t/pure-rust-and-safe-badges/2451). Otherwise, I didn't have any notion to include unsafe stats at all.
The reason I asked was because Language Files Lines Blanks Comments Code Unsafe (%) C 54 9,962 1,154 2,945 5,863 5,836 (99.54%) It seems like somehow 27 lines of code were ignored :P
I think the `unsafe` code estimation is unfortunately underestimating by quite a bit, e.g. there's 1492 instances of the string `unsafe {` alone in `src/lib*` in my Rust checkout.
That's a pretty neat feature. I know texture binding is a pretty expensive operation, so how much of a performance boost does this feature provide?
~~Yep that's actually the subject of a bug right now~~...I haven't spent too much time investigating as this was just a project I threw up in a day. But I'll look into a it a little more as time goes on ;) 
Hey, sorry for the thread necro, but I've had this bugging me for quite awhile, and I dont know of a better way to ask. With regards to the section: "Branching out: taking Rust to new places - Cross-compilation", does the Rust team envision adding better support for more or alternative cross compilation targets? I'm specifically asking about Emscripten, and later, WebAssembly? [This](https://news.ycombinator.com/item?id=4630403) is less encouraging, but I think its slightly out of context, and way out of date. And yet there appears to be some desire [here](https://users.rust-lang.org/t/rust-to-js-with-emscripten/587) and [there](https://github.com/rust-lang/rust/pull/26505#issuecomment-127798304) to get rust in the web. I've attempted multiple times to get Rust IR as a working input to Emscripten and it boils down to some sort of LLVM incompatibility (i think?). Will Emscripten be a likely supported target in the next year or so? With WebAssembly getting a full fledged backend in the LLVM tree, will WebAssembly be a supported Rust target? 
That would be my guess as well. When I originally put the unsafe counter in, there were a ton of false positives from comments, docs, variable names, etc. The current implementation uses a super simple regex (read, "it could be improved a lot") to rule out *most* of the false positives (but not quite all). Edit: having said that, it's also entirely possible there's a bug ;)
Haha ok :) Other than that, good job!
That bug has since been fixed thanks to [Vinatorul](https://github.com/vinatorul)! Not sure if he's a user here or not.
I'm concerned that even an admittedly-naive metric of "unsafe lines of code" will do more harm than good. Unsafety is not a granular property of your code: either you use unsafe blocks at all, or you don't. That's why it's so vital that libraries advertise when they contain no unsafe code of their own (e.g. Iron). This may seem harsh for people who have legit needs for unsafe, but it's the truth. I just don't have time (to say nothing of skill) to audit a library's usage of unsafe and ensure that it properly upholds all its (hopefully well-commented) invariants, so for me the total absence of the unsafe keyword is the most heavily weighted factor when evaluating libraries. The only reason that I trust the usage of unsafe code in the stdlib is because I trust the Rust developers to hold themselves accountable should problems arise. So if I've offended any unsafety enthusiasts out there, my attitude isn't because I suspect you cannot use unsafe code properly, it's because I have no reason to presume accountability. :)
In release mode. In debug mode, it'll panic.
Learning to read LLVM IR is a must - even after we start doing our own optimizations, we'll still be using LLVM primarily for at least one or two years. The most important thing is probably to figure out what you can remove: the LLVM IR has lots of extra information, but is a RISC at heart, with pseudo-infinite registers. Once you learn to scan through it for information you need, and maybe do diffs to see the effect of a code change in the IR, it can be easier than x86 assembly. The [LLVM "language reference" docs](http://llvm.org/docs/LangRef.html) are good enough for understanding almost all of it, especially the more common bits.
Actually, I agree entirely. Which is why the post by /u/llogiq in the users thread showing some pseudo output of an unsafe counter, caused me to think about adding `--unsafe-statistics`. Anything `&gt; 0.00%` is unsafe. This is very black and white, which IMO is good. But for those that *want* shades of gray, it's interesting to see, "Hmm, lib 'A' has 29.45% unsafe, while lib 'B' has 0.32%." I may actually take the time to evaluate the 0.32% manually. Also, you can see an incline or decline of "unsafe" code over time. What I'd be *against* seeing is advertisements such as "Only 5% unsafe!" Because like you said, unsafe is unsafe. This just allows me to see *how much* unsafe there is in order to better decide whether or not I want to spend my time actually evaluating it. :) The downside to `cargo-count` right now is that doesn't traverse deps. Maybe that'll be a TODO.
&gt; `fn intersects&lt;T:Node&gt;(box: Rectangle, elements: &amp;[Rc&lt;T&gt;]) -&gt; Vec&lt;Rc&lt;T&gt;&gt;` Somewhat offtopic: This part brought to mind a question about specialized generic functions and code bloat. It seems each instantiation of the above function can use the same generated code, like with lifetime parameters. Because Node is unsized, it's always manipulated through a reference. However if `intersects` calls a generic function `F&lt;T&gt;`, any specialization on `F` will effectively specialize `intersects` as well. Can / will rust avoid code bloat where possible? Is it likely to cause bloat where people won't realize it?
Awesome, thanks!
The problem with calculating some nonzero percentage of unsafe code is that even properly-used unsafe code can only enforce safety beyond the module boundary, and within that module the precise amount of unsafe code in use is up to the judgment of the developer's coding style. /u/Gankro himself is a staunch advocate of not trying to strictly delimit module-internal unsafety, because he believes that obscures the code and creates the potential for more errors.
&gt; Does rustc/LLVM have something like GHC's core? Not yet, though we are getting something sorta kinda like it: MIR https://github.com/rust-lang/rfcs/blob/master/text/1211-mir.md MIR is more _before_ the optimizations, though. LLVM does most of our optimizing at present.
&gt; But a little bit for some optimization is perfectly reasonable. In your own crates, sure, but not in libraries that you expect others to consume. The fact that everything is built on unsafe code is because there is no hardware that runs Rust natively (and then you'd still have to trust the hardware), so unsafe is your only eventual recourse for doing IO. And again, I hold the Rust developers to a greater standard of accountability than anyone else in the ecosystem, which is the reason why I can trust the stdlib, cargo, and rustc (and the latter two only need unsafe for FFI, which is mildly unfortunate but in the meantime I can imagine there are good reasons for not embarking on rewrites of LLVM and libgit2 :P ).
People use debug mode? :P
Why would I ever need to bit-shift when I want to divide my integers?
My big takeway from this is that SipHash (Rust's default) is actually a *really* good default hasher, in the sense that it's uniformly OK. FNV is definitely a slam dunk for "small" (&lt;= 16 byte) workloads, and xxhash is a slam dunk for "large" (&gt;= 64 byte) workloads. But FNV scales poorly to large, and xx scales poorly to small. Siphash resides comfortably in second place for all loads. Siphash, of course, is also the only one that provides protection against algorithmic complexity attacks. Arguably FNV is the right choice for a disproportionately large set of workloads (16 bytes is pretty big for a key, even if it's a string). e.g. if you're hashing names it's enough to fit "nikomatsakis1988" -- a pretty long name. Differences aren't trivial either -- often it's a 2-3x difference between fnv and xx. Implementation flaws may be dominating, though!
&gt; In your own crates, sure, but not in libraries that you expect others to consume. I completely disagree, people are perfectly capable of writing safe `unsafe` blocks. Yes if you expect others to consume the library then you should bloody well be sure that the code is safe but sometimes calling out to a C library is a worthwhile sacrifice ("sacrifice" isn't the right word but I think you get my meaning).
Well sure, once you have to touch FFI there's no way around unsafe anyway.
So what on earth is the point of your argument then? Edit: You're a different person.
Interesting, thanks for the graphs! No wonder rustc uses FNV internally for identifiers etc. Small nit: would be nice to have the same colors for the same algorithms in the top and bottom graphs.
Thanks for the response! By no means am I an expert, but I'd like to point out I'm certain theres work that would have to be done on the Rust end as well; cross compiling std for example. I've only been able to build core, alloc, and collections manually for emscripten (which then promptly fails building an app for emscripten when certain things are referenced). Attempting to build libstd manually is a nightmare, especially if you are trying for a non "blessed" target.
What happened to those builds? The last one is almost a month old already. 
Yeah I think dygraph has a thing for that, but not all the groups are the same (second set includes btree) so you'd need to do something non-trivial -- (PRs welcome..? :) ...)
I love the work you have put into Glium! The article was very well written. This is a fine demonstration of how a safe API can be designed in Rust without getting in the way or reducing the features of the underlying API. You are awesome!
I've found [godbolt](https://rust.godbolt.org/), which supports v1.1 of `rustc`, to be tremendously helpful in learning to read LLVM IR. If you hit 'Colourise' (so British!) in the upper-right of the screen, it makes it very obvious which output corresponds to which lines of source. It also allows you to provide your own compiler flags, so you can see the difference between optimized and unoptimized code.
This is all just the output from the bencher framework scrapped via regex, I'm not sure how I could easily sneak that info in. I suppose I could just make a graph that's 1/time and that would give you iter's/second -- which for the first bench is hashes/second. Output would get pretty cluttered though.
Hindsight is 20-20.
Note that when you use colorization, that sometimes prevents certain optimizations from happening (since it needs to include debug info, and that seems to suppress certain optimizations). So, while colorize can be useful, do be aware that if you think something might not be optimized well enough, to turn it off and then be able to see what a real release build will do.
There's also a lint that complains when you shift by a constant.
`&amp;mut self` *is* just short for `self: &amp;mut Self` ([check it](http://is.gd/azMkHY)).
This library is great! Just turned 60 lines of error handling code into 15
I did something similar as part of another project a while back: https://github.com/borntyping/rust-psutil `nom` looks like it would have made writing that a lot easier!
I dunno, I think it's pretty practical. It only took me an afternoon to figure all this out from scratch (having never used XCode before) and having a working Rust app using SDL2 and OpenGL on iOS. It's not as easy as desktop yet, but it's not much worse than Android either.
Paradoxically, I think keeping structs separate blurs the lines *more*. When you have to choose between enum T { A, B, C } and virtual struct T; struct A: T; struct B: T; struct C: T; for essentially the same result, it becomes unclear when to use which. The blog's proposal means you always reach for enum. Changing the representation becomes a simple matter of using unsized or not, or adding common fields if needed.
Go is much simpler. Theres official work being done and you can already build frameworks and full apps for iOS with the gomobile package
To get rounding to negative infinity instead of zero, which is frequently what you want (in fact you may argue that this is a better default since it's the same as mathematical quotient). For a concrete example, imagine that you have a 2D grid with pixels and you want to divide it into tiles. To get from pixel coordinate to a tile coordinate, you can do (x &gt;&gt; 5, y &gt;&gt; 5) for 32x32 tiles. This handles negative indices. If you do (x / 32, y / 32), the results for negative x/y are wrong for the division that rounds to zero and you have to tweak them. Interestingly, that's how Python integer division works.
Aaahhhhh, I see what you mean now. I read it as "people shouldn't use `unsafe` blocks" rather than "I avoid libraries which use `unsafe` blocks". 
Not offtopic at all! Those are definitely the sort of suggestions I'm interested in. I've used parsec a bit, but I didn't really know if there were any good ones in rust yet. I came across the one you mentioned as well as `nom`, but didn't know how polished they are. I also just wanted to write a parser myself for the practice. But I think I'll use a library for miniml, the next language in the [PL Zoo](http://andrej.com/plzoo/). Do you have any experience with `combine` or `nom` or others, to recommend the best one to use?
Cityhash and farmhash should be better than siphash in a comparison like this. That said, they specialize depending on the length of the input, an information not available up front in our streaming api (trait Hash).
I must be blind but I cannot see how to get LLVM IR output from this website, I just see assembly.
&gt; I believe it would be possible to replicate at least the C API LLVM has, in some Rust reimplementation, so everyone could benefit from it. `llvm-c` is only a thin wrapper around LLVM itself, though, so even it's rewritten in Rust you'll still be using the (massive) LLVM C++. Now, porting LLVM to Rust would certainly be a nice project... but it's quite massive and unless Apple is in, a large investor will be left out.
Wrong sub... You're looking for /r/playrust
# PSA: I just got reminded that this is actually only safe on nightly right now The API is sound, its just that the bug I described at https://www.reddit.com/r/rust/comments/3hyv3i/safe_stable_and_scoped_threadpool/cubv668 only just now got fixed... which obviously, though a failed to realize this, means its not yet in beta and stable :P # PSA 2: The `execute()` method is now `unsafe` on any rustc version &lt; `1.4.0`. --- A bugfix that enabled the proposed v2 of the scoped thread API recently landed, so its now possible to write libraries for scoped threads. This is my take on it, a scoped threadpool that runs on stable rust. Features: - Thread creation is independent from job execution, so you can cache the pool itself, even in a global variable. (It combines nicely with `lazy_static` for example) - Safe interface due to not using RAII for joining threads. (If there is other unsafety with scoped threads in general I'm not currently aware of it) - Stable interface due to using `thread::spawn` internally. (The only `unsafe` block is for transmuting a job to `'static` lifetime) - Jobs can access data of any lifetime outside of the scope during which they run, re-enabling parallel processing of stack data. This is just the bare bone functionality right now, and no care has been given yet to optimize it in particular (and I'm missing the expertise for it), so feel free to offer suggestions for improvements. :)
Good point. IMO both kinds of division should be supported directly by the language.
The refinement predicate is that the value belongs to a particular variant. Rather than expressing this concept as a pair of a type and a predicate, though, this is being expressed by making the variants themselves types. With: enum E1 { S1, enum E2 { S2, S3, } } The type `E1::S1` is the refinement type `E1 where |x| x == E1::S1`, and the type `E1::E2` is the refinement type `E1 where |x| x == E1::S2 || x == E1::S3`.
Yeah, I just realised that myself :-(.
You mean the `ident @ base::variant` syntax? Because otherwise we already do enum E { A(i32), B(u8) } fn func() { match E::A(42) { A(x) =&gt; println!("{}", x); } } so I'm confused on which you mean.
&gt; I'm just afraid that this proposal, while certainly useful in some scenarios, will steepen Rust's learning curve even further. This counter-argument has not been brought up much IMO. Yeah. I actually kind of like this proposal, but simplicity is its own virtue. &gt; And the common fields thing still bothers me, it just doesn't feel right, it violates the basic idea of what an enum is. Instead, one could try to avoid matching in case all enum variants begin the same: [...] Then initializing common fields would be as easy as writing a `ShapeCommon::new()`. I had a similar idea (as I also commented on Disqus): with both `struct` and `enum` inheritance, it could be expressed as: struct ShapeCommon { ... } enum Shape: ShapeCommon { Rectangle { ... }, Circle { ... } } (Possibly this would allow a strict structs-have-fields and enums-have-variants separation while maintaining the same basic structure/expressiveness? I haven't thought it through.)
Nope, the jobs just get pushed to a unbounded channel, and everytime a thread is done it tries to grab a new one from it. It could be written to block instead, but I saw no reason to do so.
The one I waited on allowed you to capture data shorter lived than the scoped closure, which was just plain aliasing-mutable memory unsafe: pool.scoped(|scope| { for e in &amp;mut vec { // MEMORY UNSAFE: The inner closure captures a // a _reference_ to the variable e, rather than the &amp;mut T itself scope.execute(|| { *e = 5; }); } }); That code now correctly fails to compile, forcing you to use `move || ...` for the inner closure instead to capture the `&amp;mut T` by value.
I myself get really confused when author Steve Blank's blog posts pop up.
I'm not convinced we have a good farmhash impl. No offense to the author of the crate -- it's just an insanely complicated thing compared to sip/xx.
The problem is determining whether a key type is small or large -- how large is a String? If your guesser has problems you could have two different algorithms get selected for two keys that were supposed to hash the same, breaking the hashmap. Also we're talking about 10ns vs 30ns -- tossing runtime analysis is going to be trouble.
Yeah, that would be awful to allow.
I meant that syntax, but it would have a additional semantic. Today, if you match on `ident @ base::variant` then `ident` still has the same type `base`. Under the new additional semantic, the match could refine the type to `base::variant`. For example, you won't get a value of type `Option&lt;T&gt;` that way, but of type `Some&lt;T&gt;`.
Using an unbounded channel is certainly easier, in terms of correctness, as any bounded channel could potentially cause a dead-lock (if the algorithm assumes more parallelism than available). On the other hand, I wonder about the performance aspect but like you lack the experience to check/improve it.
I still wouldn't call them 'refinement types' (maybe "detachable variants"? no prior usage to justify this, though...); but I see what's going on now, and like it! 
Yeah, and If the demand ever exists it would be fairly trivial to also provide a variation of the API using a bounded channel instead. I could probably provide both easily by using generics...
We need lattice impls, not just the tree-impls that the specialization RFC provides.
Hmm. Maybe the way you'd do that under this scheme is struct NodeCommon { position: Rectangle, ... } enum Node: NodeCommon { ... } struct ElementCommon: Node { ...fields... } enum Element: ElementCommon { ... TextElement, ParagraphElement, ... } That does get a bit more elaborate, *possibly* in favor of orthogonality and clarity. (This isn't a formal proposal or anything - I'm just thinking out loud.) For what's it worth, I think of `NodeCommon` getting its own name in the normal way, rather than having to invent a weird special-case `Foo::struct` syntax, as an advantage. There are virtues besides concision. &gt; I think the more important property to preserve is that a struct is a leaf node, and an enum is not, since otherwise you end up with two conflicting ways of extending an algebraic type. Could you elaborate on what you're thinking of here? Thanks for the thoughtfood!
This is allowed because it's not a borrow. After you do `let mut this = self;`, `self` isn't usable at all. This is allowed because you are completely moving out of `self`, and `self` is basically "moved out of" after the let statement.
Is the fix in stable yet though?
In this case it would definitely be unsafe in the general case, since transmuting any lifetime to `'static` tells the compiler "this will remain valid indefinitely", which is definitely not the case for stack variables for example.
I'm in the process of re-doing the unsafe counter and actually checking it for accuracy. MTF ;)
I write JavaScript+Node.js microservices at work and every time I get "undefined is not a function", confused about function signatures, lost as to whether or not a value can be null or wishing I knew whether a member exists at compile time I think back to being able to write an entire asynchronous program in Rust with only static checks finding my mistakes and only writing tests when it's completed. Good news though! Since everything communicates over HTTP, my boss is receptive to using a different language to write new services in, and is interested in using Rust. Maybe next time a "Rust in production" survey comes out I'll be able to contribute!
&gt;Could you elaborate on what you're thinking of here? A struct is a leaf, in the sense that all values of a struct type struct S {...} take the form `S{...}` [edit: i.e. it has no child types]. An enum is never a leaf [edit: or if it is, it's uninhabited], because an enum type enum E { A, B, C } or enum E; struct A: E; struct B: E; struct C: E; *never* has a value of the form `E{...}`. You always need an `A`, `B`, or `C`. However, if an enum can take the form enum E: S { A, B, C } then all of a sudden, `S` has the values `A{...}`, `B{...}`, and `C{...}`, as well as the `S{...}` values (`E` would still only have `A{...}`, `B{...}`, and `C{...}` values). Of course, then I thought that maybe the intent was that the struct *wouldn't* be a supertype of the enum. After all, the whole point of interposing the enum (E) type between the struct types (S, and A, B and C) is to introduce a hook to allow subtyping. But that's worse. `struct A: E` would be a subtyping relation, and `enum E: S` wouldn't. That makes the syntax very misleading. Basically, there's two separate relationships that you're alternating between here, composition and subtyping, and the syntax doesn't make that clear. Edit: There's also the oddity that, while `Element` in your example *wouldn't* be a subtype of `ElementCommon`, it *would* be a subtype of `Node`. This would be extremely weird. Basically, transitive subtyping would skip generations. Edit 2: Another apparent oddity is that `ElementCommon` is itself a subtype of `Node`, which is unexpected. You don't actually want the common fields of the `Element` variants to form a `Node` on their own. They're supposed to form *part* of the actual `Element` variants. If this explicit approach were taken, it would have to have distinct syntax for subtyping and composition: struct NodeCommon { position: Rectangle, /* ... */ } enum Node { ..NodeCommon, } struct ElementCommon { ..NodeCommon, /* ...fields... */ } enum Element: Node { ..ElementCommon, TextElement { /* ... */}, ParagraphElement {/* ... */ }, } Here, the fact that `ElementCommon` extends `NodeCommon` would have to be enforced. To me, it seems simpler to just inline the `{*}Common` structs and access them via the proposed `{*}::struct` syntax.
If you have any comments, please let me know :)
try calling hash64 directly instead of calling the hasher.write. The issue is that unlike siphash, farm hash does not have a way to save the state. So to expose a write function I needed to append to a vector then once finish is called push all bytes through. Again please try to call hash64 directly without the hasher interface... should yield to much better results
Thanks that is damn pretty but like /u/jeremyjh, I don't see how this helps learn llvm-ir either.
Sounds great (as a starting point). +1 for LLVM. I think `rustc` would want to do it eventually (faster compiles). It feels like this should be more explicit if it's common. OTOH if it doesn't usually matter, you could use a clunky annotation to assert that you don't expect type parameter T to affect the generated code.
Whoops! Goes to show just how much I have to learn about LLVM IR. All this time I've been reading plain old assembly. Thanks for mentioning it!
This answer is misleading, the problem is that FnMut must be reusable, FnOnce cannot be used for find() and if you used Option::take it would cause bugs where when the predicate is called a second time the message would be None
Done editing my other comment ;). It occurred to me that there's a different property that could be maintained as the distinction between structs and enums. Basically, as I was saying, a struct `S` has values `S{...}`, while an enum `E` does not have values `E{...}`. Essentially, enums are abstract, while structs are concrete. If this is the property being maintained, then you could have: struct S1 { s1: u32 } enum E1: S1 { e1: u32 } struct S2: S1 { s2: u32 } struct S3: E1 { s3: u32 } enum E2: E1 { e2: u32 } In this hierarchy, all the struct types would be instantiable, while the enum types would only be abstract supertypes for their variants (if any). Ultimately, my point is that there's a number of ways to distinguish enums from structs other than one having variants and the other having fields. The notion that enums don't have fields is, I think, untenable under any structural inheritance proposal that doesn't bypass enums entirely, since *at point of use* the enum will have fields anyways, whether they're inherited from a struct or defined inline. Going through contortions to avoid defining them inline because "enums don't have fields" is thus somewhat incongruous with any design that allows you to factor out common fields of an enum's variants and access them on the enum itself.
Going to run through it and comment here with my thoughts as a mostly-newb to Rust.
Oh, perfect. Is there a way to run all tests marked with `#[ignore]` without naming them explicitly?
Sadly it can't really. Neither can murmur because we they both need the whole stream beforehand. If you look at actual source code from google https://github.com/google/farmhash/blob/master/src/farmhash.cc#L1025 you will see how they differentiate the algorithm based on size of input :(
 cargo test [options] [--] [&lt;args&gt;…] the &lt;args&gt; are forwarded to the test binary, so `cargo test -- --ignored` will run the test suite including all tests marked with `#[ignore]`
Very cool. I have a small message board written in Lua that has a (not very robust) built-in HTML sanitizer. I'm looking forward to replace it with Rust code.
That's a good idea. Unbounded channels make reasoning about runtime hard.
I think if you compile with -g, the respective source lines will already be annotated.
One point: the gomobile tool for go creates bindings for exported functions, types, and interfaces in objective c for you, on top of building the for the correct target arch. And if you choose android it binds them in Java. Its not perfect by any means but its cool they have made it easier. I didnt know about multirust until now so thanks for that! The Rust support sounds cool and ill have to look around at that. The more options the better. 
Actually, thinking about it again, putting the code to the playground wouldn't be of much use: Some of these parts import stuff from other parts, so they wouldn't even work on playpen.
/u/bluss noted on irc that you should in principle be able to just have an array of 64 bytes -- that's the biggest switch point for farm, at which point you know you're just using the &gt;64 algo and don't need to buffer.
I am using -g, and then the profilers try to match binary locations to source lines and to generated asm, which is something. My thought is that if llvm-ir is a useful intermediate representation, it could be useful to have it instead of the asm (too low level) and of the source code (at least some of the time, too far from what is actually going on).
For example, I once found that I was using a whole-matrix iterator over a 1d matrix, instead of a more efficient column iterator. This was easier to identify when the source was less radically transformed.
Yeah, pycco (the tool I am using) and hence the site is totally non-mobile-compatible. But then, people are not going to program Rust on their phone, so I don't think that matters too much. That said, PRs [on GitHub](https://github.com/RalfJung/rust-101) are welcome ;-)
It seems, exists() basically only calls metadata.is_ok(), but I couldn't find any documentation to that. I will probably use OpenOptions instead and directly create a new file, if not available and handle the error on the toml parser level. However, it appears that the create(true) option can't create folders as well. Is this wanted behaviour or a bug? That said, pointers how exists() actually is implemented are welcome, because being able to separate between not existing and lacking permissions is the better way IMHO.
Microservices are the secret to sneakily using new languages at work. 
I'm working on an extension of the hasher API that should benefit everyone: https://github.com/rust-lang/rust/issues/27713#issuecomment-133832938
Looks cool! Would like to see benchmarks against Python's Bleach. If it is a lot faster, it may be a good idea to write a C interface and create python bindings :) also, in your example, it may be a good idea to add `use` statements to the example, so it's easier for people to jump right in! 
Will call back when we write our first BrainFuck microservice.
Works on stable but is only sound on nightly is certainly a bold move!
Here's the [source code for Path::exists](https://github.com/rust-lang/rust/blob/c69c29bb530968e35069c53a573c80299ccc8ac8/src/libstd/fs.rs#L1213): fn exists(&amp;self) -&gt; bool { metadata(self).is_ok() }
It's doable with a compiler plugin: [interpolate_idents](https://github.com/SkylerLipthay/interpolate_idents)
I don't know of any crate that could contain this (except _itertools_). I'm a big fan of tiny crates, though. Add some tests and you are good to go. There are already similar small crates like _binomial-iter_ and _shuffled-iter_. (I think the latter be also be used for taking random samples.) A name? That's like the hardest problem in computer science! How do you like _iter-samples_?
I think your PR should be accepted with an optional flag. Much better depending on itertools and having `features = ["reservoir_sampling"]` rather than depending on itertools and some other crate yet unnamed.
That was my original hope as well, but I'm absolutely not going to argue for one second with a maintainer of any library. They are the ones that have to care for and maintain every line of code essentially forever. If they decline that extra responsibility, no one has a leg to stand on to counter that. \^_\^
I think the docs of [clean](https://www.notriddle.com/rustdoc/ammonia/fn.clean.html) should state what the defaults are, if they are meant to not change / be relied upon (but I see it's easy to check [in the source code](https://www.notriddle.com/rustdoc/src/ammonia/lib.rs.html#80))
&gt; One point: the gomobile tool for go creates bindings for exported functions, types, and interfaces in objective c for you Aha, that's a huge point! I'm inclined to check `gomobile` too :-)
Sure, but compiler plugins aren't stable. What I need is a way to do it in _stable_ Rust.
I was about to suggest that it could be part of the *rand* crate itself. Then, I checked and saw that there is already a [`sample`](http://doc.rust-lang.org/rand/rand/fn.sample.html) function in *rand*, implemented using reservoir sampling!
needs to be patched with shepmaster's .get_mut trick :)
I've fixed that. Thanks!
`metadata` is documented [here](http://doc.rust-lang.org/std/fs/fn.metadata.html). [`is_ok()`](http://doc.rust-lang.org/core/result/enum.Result.html#method.is_ok) is just a method on `Result` to check that the `Result` is an `Ok(_)` not an `Err(_)`. `metadata`'s error section says: &gt; This function will return an error if the user lacks the requisite permissions to perform a metadata call on the given path or if there is no entry in the filesystem at the provided path. If you went the [`open`](http://doc.rust-lang.org/std/fs/struct.File.html#method.open) route, you could click on the `Result` in the return type of the docs, and see that it's an `io::Result`, and that an `io::Result` is a `Result` with it's `Err` hard coded to contain an [`std::io::Error`](http://doc.rust-lang.org/std/io/struct.Error.html). A little more digging and you'll find the possible [kinds](http://doc.rust-lang.org/std/io/enum.ErrorKind.html) of an `io::Error` include `std::io::ErrorKind::NotFound` and `std::io::ErrorKind::PermissionDenied` -- and you can match on them with a match arm like `Err(ref e) if e.kind() == ErrorKind::NotFound =&gt; {...use e...}`.
Really cool crate! We were considering writing something very similar, I guess we'll see if we can tag along with this now :-).
This is an important first step towards an ecosystem of freestanding libraries and packages for embedded development and unikernels. EDIT: [Comments.](https://github.com/rust-lang/rfcs/pull/1133)
… and you could combine that with a feature flag if you desired with `cfg_attr`: #[cfg_attr(not(feature = "expensive_tests"), ignore)] (I do not necessarily endorse this particular combination of things, but it’s *possible*.)
But unlike void in C#, the Rust version is not a special case. You can use it anywhere you can use types in general.
Ah that's a good point. Fair enough!
It would make `cargo test` do the fast tests and `cargo test --feature expensive_tests`do all the tests. That is roughly what is desired.
`dev` needs to be recognised as a channel, too. At present, a development (not nightly) build will cause a panic. The channels that exist are (attested in `configure`): dev, nightly, beta, stable. In `mk/main.mk` these are then turned into the versions X.X.X-dev, X.X.X-nightly, X.X.X-beta.N and X.X.X respectively.
That's great! A style tip that is mildly related: if you use [fill-column-indicator](http://www.emacswiki.org/emacs/FillColumnIndicator) to indicate when to break long lines, add an `(setq fill-column 100)` inside your rust mode hook. (The style guide has [a 99-chars lines](https://aturon.github.io/style/whitespace.html) convention, also see [this discussion](https://internals.rust-lang.org/t/column-limits-and-rightward-drift/44)).
You can take a look at [expectest](https://github.com/zummenix/expectest). This is not that powerful as you want but maybe helpful.
I just use whitespace mode to tell me when my lines are too long.
I use whitespace-mode configured with the defaults from [Emacs Prelude](https://github.com/bbatsov/prelude). This does what you want. I don't know exactly all the configuration variables for whitespace mode, but I'd recommend trying prelude out and if you like the configuration, try to incorporate it into your original config.
Thanks!
Just installed racer earlier today, it's been good so far. I don't have a ton of code, but it's able to complete the standard lib no problem.
Did you check the latest version? The site should be much more mobile-friendly now.
&gt; I should also point out that your current version does not seem to deal with interior nuls in strings. Remember that \0 is a valid UTF-8 character :( FYI - one of the ImGui::Combo() variations takes a zero-separated zero-terminated list of items in a single string. It's the simplest variation of the combo function which is convenient for quick inline combos when the items are statically known, e.g. pass "Yes\0No\0" in C++. So I suppose that would break here. It's a rather unique case however.
At `$EMPLOYER` we make heavy use of a company-internal package repository for Python packages. I was very pleased when I learned about crates.io, but I knew that if I ever proposed writing anything in Rust, "how can we set up a private package repo" would be one of the earlier questions I'd be asked. It's good to hear a basic package mirror would be pretty simple!
Coincidentally.
Ah yes. Brain fart. Felt I was using it wrong and such with it any way....
This is awesome to see! I've long wanted some tool which just syncs dependencies down from crates.io to a local system which then allows you to just drive everything locally. This certainly sounds like a subcommand in the works :) The current method of overriding the default registry is a bit janky, I'd almost rather encode that kind of information in `Cargo.toml` instead of `.cargo/config`, but I haven't given this a huge amount of thought just yet. The space for mirrors is certainly quite ripe! You can actually even go one step further and avoid HTTP on the `dl` key in `config.json` as well. The [cargo tests](https://github.com/rust-lang/cargo/blob/master/tests/support/registry.rs#L20) use a `file://` path for this and that should allow you to entirely bypass the network!
Thanks for the feedback! The main reason why I posted this here was to get some feedback and initial reactions from people, so I really appreciate it. I agree that the macro is a bit annoying, and I strongly dislike it. However, the reason it exists is to declare strings compatible with ImGui without a runtime cost. I haven't yet decided if it's a good thing or an unnecessary micro-optimization, but copying and adding a nul byte for each string in a UI during each frame doesn't sound very good. What we need is a way to declare, not compute proper strings for ImGui, and I don't know a better way in current stable Rust. If we had something like C++ constexpr, we could support this in a much better way. Some parts of the ImGui API actually take start and end pointers, so those calls are directly compatible with &amp;str slices. However, it's super annoying that those calls are in the minority, and almost everything requires the nul byte. Also, CString is not a good match for ImStr, because not every CString is a valid ImStr. A CString is basically a null-terminated string of bytes, but ImStr needs to always be valid UTF-8. I wanted to make it impossible to create an invalid ImStr in safe code. Your observation about interior nuls is correct and is a wart in the system, but it doesn't critically break anything. So, currently ImStrs created in safe code are guaranteed to work with ImGui, although they may be truncated if interior nuls exist. I find this to be a good compromise. I like your idea of From / Into conversions, and will consider switching the API to use T: Into&lt;ImStr&gt; arguments so if dynamic copies are not considered bad, you could just use normal Rust strings as arguments. This would make it the user's responsibility to choose between im_str! or dynamic copies.
Looking good! I'm still going through it but I've realized that finding the function that corresponds to a given instruction [in the docs](http://huonw.github.io/simd/simd/) is not working properly. For example when I search for `pshufb` I don't arrive at `shuffle_bytes` =/ I know this is a WIP but this (or something like it, like e.g. a table of instructions / functions) would be in my wishlist :)
Thanks! Yeah, getting the documentation to work "perfectly" is something I haven't even started to tackle yet. (I'd like it so that searching for the instruction name or the C intrinsic name gets you to the relevant function.) On the other hand, I'm still considering if I should just name the platform specific functions after the instruction or C intrinsic instead of giving them something human-readable (with the intention that human-readable and cross-platform-as-possible wrappers could exist downstream).
v0.1.1 has an improved unsafe counter and should now be far more accurate. Verifying on such a large code base as Rust is difficult, but the numbers look far more correct, and come out correct when I check against smaller code bases.
Weren't we all supposed to use dashes instead of underscores for our libraries now? If I'm remembering correctly, maybe we need cargo to ban uploading new crates with underscores in them.
&gt; I'd almost rather encode that kind of information in `Cargo.toml` instead of `.cargo/config`, but I haven't given this a huge amount of thought just yet. For the use-case I have in mind (a package mirror inside an organisation, with private packages as well as mirroring public ones), having per-user configuration would be better than remembering to set an obscure option inside each of the dozens of packages we make. In fact, for our Python package archive, we even set the package-repo in a system-wide config file for our build machines so everything Just Works.
I've added 'multirust' and 'other' to the poll as well.
Which one is the rustup.sh?
Oh, good question. rustup.sh installs the .tar.gz (so does multirust). rustup.sh should probably be its own option...
https://twitter.com/hermanradtke/status/635979496319905793
I vote human-readable names with searchable intrinsic names. Lots of people coming to rust (like me!) haven't done any simd in the past and might gloss over the intrinsicly named functions.
I would prefer if you did not have to specify the size of the SIMD variables so many times and instead could write the code in a way where the compiler could pick the best available SIMD size for the target.
Good writeup. I'm just sad that the new RFCs from last week never got added to a TWiR (including an RFC I wrote).
I think the long term goal is to have rustc/LLVM autovectorize operations. This crate is for the cases where we want fine-grained control over the output. But there probably are use cases where a middle ground would be useful.
Good point. Maybe we can have both? (although there are probably downsides to this too) When I'm using non-portable platform specific intrinsics I prefer to use the intrinsic name to ring alarm bells but for someone without any simd experience it might be really hard to understand code that uses intrinsics because it is not "human readable". Feels like a cross-roads.
That guarantee for ImStr has strictly zero benefit and it is cumbersome/costly to enforce for such a specific use case. I can see the reasoning that Rust &amp;str wants to enforce it to make writing of string handling code/library a little easier (even tho I believe the performance tradeoff vs the minimal burden on programmers is really not worth it), but if ImGui handles malformed UTF-8 without crashing there's no point in duplicating that effort. ImGui shouldn't crash with invalid UTF-8 (if it ever does it is a bug and can be fixed). Again I don't see the difference between "your text has a typo" vs "your utf-8 is invalid". If your source data is invalid and you'll want to fix it somehow. That filtering pass won't make any noticeable difference to the output, it'll only make writing everyday's code cumbersome. You may as well make ImStr correct grammar and spelling.
Sounds reasonable enough, I will just use the OpenOptions.create() function and check for existing data, then.
It seems like the operations the threads are executing are going to be dwarfed by the time it takes to acquire and release a mutex. I'm not really sure what the solution would be, but to me (and I realize you aren't interested in this), this is best done with a single thread. Perhaps you can avoid the Mutexing and use atomic primitives instead of u8 and u32 and bool, I don't know what rust provides here. You could make the bool flag atomic and use atomics for the other data. Just some ideas. edit: Yep, WaDelmaw linked to atomics documentation. I suggest that. I also suggest trying to do this in a single thread once you've done it with atomics and multiple threads just to see if multithreading was right here.
Good points! Although I think the guarantee doesn't burden the programmer, since the main complication is still the null terminator, not UTF-8. If the API uses string literals, UTF-8 correctness is always checked at compile-time without burden to the programmer or any extra cost. I added a Github issue for further discussion: https://github.com/Gekkio/imgui-rs/issues/7
Same with mine: [RFC: `.drain(range)` and `.drain()`](https://github.com/rust-lang/rfcs/pull/1257) (which is unexciting, just established stuff..)
Oops, sorry, for some reason I read that as ImStr(**pub** CString). So yes, by hiding the inner type we could check for UTF-8-valid CString values. But since string literals are already compile-time-checked UTF-8 in Rust, there's indeed not much to be gained in using CString vs &amp;str. I added a Github issue for further discussion if you're interested: https://github.com/Gekkio/imgui-rs/issues/7
You can send a PR: https://github.com/cmr/this-week-in-rust
Grepping through rustc I found this: https://github.com/rust-lang/rust/blob/f76d9bcfc2c269452522fbbe19f66fe653325646/src/librustc/middle/recursion_limit.rs Seems like you can just add `#![recursion_limit="100"]` to your crate.
&gt; I am aware that real SNES emulators are written in very low level C and inline assembly, and use a single thread with shared memory to get the required data throughput for real-time emulation. Actually bsnes is multithreaded (though it uses cooperative multithreading not preemptive) and byuu [wrote his own cooperative multithreading library for that](http://www.byuu.org/library/libco/). Each system core/chip gets its own thread. Here's a post series by somebody who ported bsnes to C#: http://blog.bozalina.com/2011/02/porting-bsnes-to-c-part-one.html
Atomics and busy waiting are probably the best options IMHO.
We don't promote multirust officially all that much because it doesn't work on Windows, and we don't want Windows users to feel left out.
What you're doing is never going to be fast enough for realtime emulation. I've been developing a NES emulator in Rust (up to the point where it's cycle-accurate and can emulate the majority of really obscure hardware quirks), and I did consider emulating each component (CPU, PPU and APU) in their own native thread, but ultimately I've dismissed it as too slow (as you might have noticed yourself in your emulator). Not everything is lost though - while it's impractical to have each component have its own native thread it's perfectly feasible to give each component its own user thread - basically you run everything on only one native ("physical") thread, but manually switch between many logical threads, e.g. something like this: (not actual code I've used but close enough) { let mut ptr = Pointer::from_box( &amp;machine ); queue_user_thread( STACK_SIZE, move || { let mut machine = unsafe { ptr.borrow_mut() }; cpu_thread( machine ); }); } { let mut ptr = Pointer::from_box( &amp;machine ); queue_user_thread( STACK_SIZE, move || { let mut machine = unsafe { ptr.borrow_mut() }; ppu_thread( machine ); }); } // ... fn cpu_thread( machine: &amp;mut VirtualNES ) { loop { // ... fetch the instruction and decode it. match opcode { // ... 0x40 =&gt; { // ... do one CPU cycle worth of work. Scheduler::yield_execution(); // ... this will switch the execution to another thread. // ... do some more one CPU cycle worth of work. Scheduler::yield_execution(); // ... } } } } If you've efficiently implemented the yielding mechanism (yes, you need to implement it yourself if you want it to be fast enough; in assembly!) then this approach is very competitive with a completely single threaded emulator - I've measured the performance impact of both approaches and I've found them to be mostly equivalent on my machine, though I was running only two user threads simultaneously (CPU and PPU) so it might be a little slower if you'd increase their number. Since then I've switched back to a completely single threaded emulator since a thread-per-component architecture didn't really help at all simply due to how NES' components have to be emulated to achieve high accuracy [1], but I would definitely stay with the userthread-per-component architecture if it'd have any benefits. (Again, I'm only speaking in context of NES emulation.) [1] -- The APU is very simple so it doesn't need its own thread, and the PPU is so complex that I've ultimately wrote down a table in which I've specified what the PPU is doing on every dot of every scanline of a frame and then I wrote a script that takes that table and autogenerates PPU dispatch tables for me; it's kinda like the CPU, except the "opcode" here is the current dot and the current scanline.
If you're looking for high performance, I'd strongly recommend reading the Disruptor paper: http://lmax-exchange.github.io/disruptor/files/Disruptor-1.0.pdf. It's not Rust-specific, but on the JVM that architecture can process tens to hundreds of millions of events per second.
Mostly performance, but also convenience. As strange as it sounds the interface exposed by the coroutine crate is not ideal for a project like this (coroutine-rs is a general purpose library after all!), and you absolutely want to squeeze as much performance as you can out of the thread switching code - after all it's going to be called **millions** of times per second so every single extra instruction **does** make a substantial difference. (My hand optimized `yield_execution` implementation is only 20 instructions long; coroutine-rs' `Coroutine::sched` is going to be significantly slower at this scale - to give you some perspective purely saving and loading all of the nonvolatile registers takes at least 14 instructions on amd64 with SysV ABI.)
&gt; I would prefer if you did not have to specify the size of the SIMD variables so many times I think that ultimately this is very similar to auto-vectorization and in fine suffers from the same issue: - alignment issue - shortened iteration issue - register mis-placement SIMD data requires specific alignment which may be greater than the maximum alignment available on the target leading to: - mis-aligned data on the stack - mis-aligned data on the heap Even worse, trying to use SIMD instructions on non-SIMD data (though type punning), obviously hits this issue much more often. As far as I know, this requires run-time switches between scalar and SIMD version to handle the mis-aligned data, and thus loop code generally ends up triplicated: - a header using scalar operations, to get the required alignment - a "body" of SIMD instructions - a footer using scalar operations, to finish the iteration and for non loop code, which suffers from register mis-placement (ie, the data is passed in the wrong register, or at the wrong place in the register), this involves copying from one register to another. When you hit those transformations, you may actually get a slower execution due to the extra branches, extra code clogging the instructions cache and extra copies. Oh, and compilers have to guarantee the "as-if" rule, so if you have a debug log every four elements or every eight elements it's not the same... So, your idea, much like auto-vectorization, is nice on paper; but with today's compilers it does not seem to work reliably. As a system language, Rust thus needs explicit data types and instructions, which guarantee the lowering to assembly even if the compiler would decide otherwise.
It's not really the long term goal, so much as a thing that already happens and is a nice optimisation when it does (and so therefore would be nice to have happen more often).
Such is the life of a project that's committed to support multiple platforms. Some platform-specific functionality is nice to have.
What's odd is that this worked for `*scratch*`, but didn't work for file buffers (with or without loading a theme). What I ended up doing is: (setq-default whitespace-style '(face trailing)) (global-whitespace-mode 1) And, additionally, (add-hook 'before-save-hook 'delete-trailing-whitespace)
Hey /r/rust! I'm sure you saw Gnome Dictionary or the "dict" CLI program before (actually they are not that much in use :P). In this timelapse, I write a server for them using only a Wireshark dump and Rust.
Is the code posted anywhere? this was fun to watch :)
&gt;`tt` munching ( ͡° ͜ʖ ͡°)
Note that my blog post discusses autovectorisation and its failings in some detail, and the benchmarks are mostly about how much better this explicit SIMD is over the scalar code (which is relying on autovectorisation). :) (A representation change like the one you mention can and should be done for scalar code, if `f32` is enough, and so it isn't really an apples to apples comparison.)
Can you make a web server?
I probably can. I love obscure protocols more though. I'll see what I can do for the next stream.
Well, it needs someone to port it. Will that person appear? I know I can't do it.
Could possibly use a build from source option as well?
I bet you are rusty in lol
/r/playrust 
Rust is free! It's an open source programming language and implementation.
Just out of interest, did you look at the sub at all before making your post? Sidebar? Posts? Anything? 
&gt; rustc/LLVM autovectorize The problem is that for LLVM to auto-vectorize code efficiently the stars have to align in specific patterns: the inliner has to inline enough, constant propagation has to propagate enough, dead code elimination, branch prediction, constant folding, higher level loop transformations, ... all of these have to work and don't give up! And even then, if LLVM is not compiled with Polly, its auto-vectorizer might not be able to transform the loop to one of the few canonical forms it knows and give up. LLVM hits a pretty good sweet spot that produces pretty good code most of the time _without taking forever to compile_. This is important because all complex programs have lots of loops and not all of them are in the hot path nor worth solving complex polyhedral optimization problems to optimize them. A counter example would be NVCC (nvidia's cuda compiler), which last time I tried (6.5?) inlined everything and took _forever_ to compile even trivial kernels (like LTO 2 years ago kind of forever). If your loop is in the hot path and you want to optimize it, that's what SIMD instructions are there for. The easier and safer Rust gets this, the larger its advantage over C and C++. In C++ one basically has 3 options: platform specific intrinsics, non-standard libraries like Boost.SIMD, and language extensions like OpenMP 4 simd pragmas, `#pragma clang vectorize`, ... Rust is already doing better in some SIMD aspects (portable higher level SIMD algorithms).
I could be mistaken, but I thought it was hyphens that they're trying to get rid of.
Trying to finally get started with rust^TM Ninja-Edit: Someone spice this with some extra motivation, please?! :D
I'm moving and going to San Fran to present at the meetup! So excitedly nervous!
- Cleaning up / refactoring work on my [virtual UDP connection library](https://github.com/BonsaiDen/cobalt-rs) for multiplayer games before the initial release - Spreading the word about Rust at work :)
Yup, exactly! &gt; Got a paper? :) Here, have several. :P * [Incremental construction of minimal acyclic finite-state automata](http://www.mitpressjournals.org/doi/pdfplus/10.1162/089120100561601) (Section 3 provides a decent overview of the algorithm I used) * [Direct Construction of Minimal Acyclic Subsequential Transducers (2001)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698) (The whole thing. The proof is dense but illuminating. The algorithm at the end is the money shot.) * [Experiments with Automata Compression](http://www.researchgate.net/profile/Jii_Dvorsky/publication/221568039_Word_Random_Access_Compression/links/0c96052c095630d5b3000000.pdf#page=115), [Smaller Representation of Finite State Automata](http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf) (various compression techniques for representing states/transitions) * [Jan Daciuk's dissertation](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.7615&amp;rep=rep1&amp;type=pdf) (excellent for in depth overview) * [Comparison of Construction Algorithms for Minimal, Acyclic, Deterministic, Finite-State Automata from Sets of Strings](http://www.cs.mun.ca/~harold/Courses/Old/CS4750/Diary/q3p2qx4lv71m5vew.pdf) (excellent for surface level overview) The first two papers are critical. The others are also important for compression techniques. I'm not using all of them. (An FST is one of the fundamental data structures used inside of Lucene for representing the inverted index. But I think it tries too hard to get a minimal FST, and as a result, is a memory hog. In particular, all of these papers omit a key implementation detail: how the registry of minimized states is maintained. It's trivial to use a hash map, and therefore guarantee the `O(1)` time complexity that reads nicely in a paper, but in practice, a plain hash table is completely insane.)
I C what you did there.
This is great! What screen casting software do you use? I want to make one of these.
I've actually been searching for a real-world dataset of &gt;1 billion strings. I found a list of 50 million DOI URLs from the Internet Archive, but I need something bigger. Anyone have any ideas? (I could just generate it myself randomly, but a real data set is much more compelling. e.g., All of the domains on the web, or even a really large list of URLs produced by a web crawler would be great.)
Enough with the Smalltalk, please.
Huh, it's always worked for me. Weird.
How about [Common Crawl](https://commoncrawl.org/)? Their [latest crawl](http://blog.commoncrawl.org/2015/08/july-2015-crawl-archive-available/) is 145TB of 1.81 billion pages. I think you can get a subset if you want to work with a slightly smaller dataset :)
Getting a pull request to winapi-rs ready for the Speech API, plus I got my preliminary bindings working in my markov bot! It's lots of fun hearing Microsoft Zira reading random excerpts from the rust book :P
I had similar idea, a dictd implementation, to try Rust server development.
I might be interested in an MVC framework if I had any idea what MVC was. 
Doing Windows things in Rust. Slowly landing PRs for `winapi` and working on a few other libraries. Also procrastinating and playing Fallout Shelter.
Model/View/Controller, a common way to structure GUIs and web applications. Ruby on Rails is a well-known example.
That sounds awfully familiar... "Silicon Valley" anyone?
When time allows, I chip away at my Erlang-Rust bindings. Just got this running on Windows a few days ago, woohoo!
Are you planning on doing a search engine or are you just poking at the data structure?
This is so SCRUM like. I like the idea of this thread, especially for a growing community. Keep it up!
Implementing a coordinate descent type convex optimization algorithm. Making private additions to [scirust](https://github.com/indigits/scirust)'s matrices, which I'll need to cherry pick into PRs later on.
[A Civ5 clone with a text-based UI](https://github.com/hsoft/civng), because, why the heck not?
Hopefully submit one or two PRs to rustc as I am back at Uni where I can actually compile rustc in a reasonable time. If time permits I may have time to add real userdefined operators to my own [compiler](https://github.com/Marwes/embed_lang) as well. And I shouldn't forget to stomp out the last few remaining issues before 1.0.0 for [combine](https://github.com/Marwes/combine) either.
Community team busywork like pointing people to the possibility of running such threads ;). I'm preparing talk submissions this week, which might or might not include Rust talks. Also, http://www.opensourcebash.org/ in Berlin is happening on the 28th of Septemer, an event to bring new people into projects. I'll be using this to think about entry points to our community and probably send around a couple of mails. I also need to write a summary of the OpenCodeTown event on CCCamp in Berlin (tl;dr: awesome people donated a bunch of money and it ended up great). I will post the Rust-related parts here. Finally, trying to collect involvement options and bring more transparency into what the Ruby community team does (thanks to Steve to bring that up). Heating up the rust-community mailing list after holidays is a goal of this week.
 - Reviewing clippy PRs - Coming up with a good "unnecessary clone" lint, perhaps via dataflow equations - Getting started on IndexedDB in Servo, if I get the time. - Helping a student I know prep for a Rust talk in a few weeks.
- `left: Node&lt;T&gt;` and `let mut mut_left = left;` can be replaced by `mut left: Node&lt;T&gt;`. - `mem::replace` is often more useful than `mem::swap`. - If you’re going to use the gated `box_syntax` feature, you might as well use the `box_patterns` feature as well. Here’s my final result: pub fn replace_left(&amp;mut self, left: Node&lt;T&gt;) -&gt; Option&lt;Node&lt;T&gt;&gt; { match self.left { Some(box ref mut old_left) =&gt; Some(mem::replace(old_left, left)), None =&gt; { self.left = Some(box left); None }, } }
Building my [cross gpu machine learning library](https://github.com/jramapuram/hal_rust)
I'd like to have a clippy flair. And given I've landed a PR at rust-lang/rust, do I qualify for a rust flair?
Full ack. Autovectorization will never fully replace SIMD intrinsics. So it's best to have them work as seamlessly and portably as possible, and Huon's work is a great step, nay, leap in that direction.
I'm working on [high-level bindings generator for DX12](https://github.com/red75prime/dxgen)
Trying to write a genetic algorithm implementation to solve countdown math puzzles (as a test :D) and to get more familiar with genetic algorithms and Rust. I think trying to write a generic genetic algorithm runner that could work with any type of chromosome is an interesting design challenge as well. One that I know has been solved many times before, but I want to see how I approach it blindly.
Not sure if I'll be able to do any development this week, but I'm recently working on a procedural universe generation engine. For now I have finished rewriting the part I already had done before in C and I'm back to mostly conceptual work, where I have to come up with some algorithms before I code them. My hope is that one day this project will turn into a base for a game, but I fear my lack of experience might stop this from happening ;)
You are awesome!
Could you write a blog post about this? It sound really interesting!
I'm currently restructuring the tools for static file serving in [Rustful](https://github.com/Ogeon/rustful). I will most likely change it to be methods on `Response`. I'm also planning to move some stuff from the crate root into modules, where they belong. I'm also experimenting with [static HTML (and text) templates](https://github.com/Ogeon/symbiosis). The idea is that a buildscript can be used to generate template code in multiple languages (currently Rust and JavaScript). This makes it easier to, for example, efficiently generate content on both the server side and the client side when making a website. It's still very experimental, so the code can be a bit trashy in some places...
https://github.com/servo/heapsize :)
Yeah, I have that. It *only* has ~15 million unique article titles. I doubt it has anywhere close to 1 billion unique words in the corpus. Probably also in the low millions.
Have you considered using Rust's message-passing primitives, [channels](https://doc.rust-lang.org/beta/std/sync/mpsc/index.html)? They are pretty well-optimized, lock-free, and it sounds like single-receiver is good enough for you. The usual pattern to get an answer to a particular request is to send the answer channel with the request - or maybe just some answer channel ID, to avoid allocating new answer channels all the time. If you statically know all the possible recipients of the answer (say, subsystems A, B, C), you can even more efficiently allocate static reply channels for them.
&gt;Coming up with a good "unnecessary clone" lint, perhaps via dataflow equations This would be big. I remember when I first came to rust I used '.clone()' to get around borrow check a lot, and there were huge performance gains when I figured out how to avoid them. A lint would help new users to the language both produce faster code right from the start and reason more easily about when cloning is necessary.
`otp-cop`: https://github.com/alex/otp-cop A tool for making sure people in your organization have 2fa enabled.
I am working hard on treasure orm..
I would like to request to "rust" flair (I did some [PRs](https://github.com/rust-lang/rust/pulls?q=is%3Apr+author%3AGuillaumeGomez+is%3Aclosed)), "[rust-gnome](https://github.com/rust-gnome)" flair (I'm the organization creator after all).
That synth editor demo is pretty impressive.
I also would like to request a flair about the [french rust tutorial](http://blog.guillaume-gomez.fr/Rust) (which is available on [github](https://github.com/GuillaumeGomez/tuto-rust-fr) of course).
I've been working on a learning machine, which assimilates symbols, words, terms, natural language and so on with encoded context. It's like a bot: it talks to you. And you can teach it. And a friendship arrives.
I would like to request a flair for [nom](https://github.com/Geal/nom) please :)
LLVM supports [tail call optimisation](http://llvm.org/docs/CodeGenerator.html#tail-call-section) and will perform it when applicable.
11 days of commit in august : congratulation guys, this is rare that people find the motivation to work on such project for so long ! Having a great gui library is important for any language to foster. You are really helping rust and its users !
Very nice. I don't know if you've seen [Electric Sheep](http://www.electricsheep.org/) but it renders some astoundingly beautiful images using a chaos game with a few enhancements. Briefly: * It can handle an arbitrary number of affine (linear + translation) functions, with the ability to assign a different probability / weight to each. * Each affine function can be composed with one of a fixed set of nonlinear functions, mostly trigonometric functions. This is what produces the sweeping curves and other round shapes. * Pixel intensity is proportional to the logarithm of the hit count, which means fine structure is visible both in heavily-visited and rarely-visited areas. * Each function can have an associated color. Instead of a single hit count, each pixel has red, green, and blue hit counts which are updated according to the color of the last function selected. For more details, check out [the paper](http://flam3.com/flame.pdf) as well.
I feel like I'm working on a different thing each day! * The usual working on improving [clap](https://github.com/kbknapp/clap-rs) - much thanks for the various contributors, including a ton of work put in by [Vinatorul](https://github.com/vinatorul) * I've been working on a ton of `cargo` subcommands as of late, [outdated](https://github.com/kbknapp/cargo-outdated), [count](https://github.com/kbknapp/cargo-count) (my personal favorite), and most recently implementing some additional features in [dot](https://github.com/maxsnew/cargo-dot) (PR coming soon ;) ) * I've also got some issues on the back-burner for [clog](https://github.com/clog-tool/clog-cli) * Finally, there's a series of blog posts I'm trying to finish before releasing...but this is slow going Edit: Fixed links
Great idea for a thread. New to Rust, having read *The Rust Programming Language* and *Rust by Example*. Started with rewriting a novel flavour of Lisp I'd previously been working on in C++. So right now it's mostly fighting with the compiler, feeling my way through the language with the help of really good error messages. I am learning to use the trait system as a means of specifying the behaviour of the different types of Lisp objects. Rather disappointed I won't be using generics, as they were one of the features that drove me away from C++, with its horrible template code. Creating dynamic objects at runtime seems to impose that limitation.
I'd also like these two please. [rust](https://github.com/rust-lang/rust/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+author%3Agkoz) If /u/imperioland will kindly confirm the rust-gnome one.
I've been collating my thoughts on a garbage collector implemented in Rust into a library RFC - https://github.com/pliniker/mo-gc The implementation is one I haven't come across elsewhere and I've been having a lot of fun thinking it through and prototyping bits and pieces. 
If the target vector is one of SIMD data, yes, however it is also common to apply SIMD instructions to "regular" data (such as `char*`).
Yes, it is pure Rust, no unsafe code. It depends only on the input library from the Piston core and the back-end agnostic 2D graphics API for Piston. At top of Piston's 2D graphics, which handles triangulization, it uses Elmesque which is inspired by the Elm programming language. It is intended for interactive applications and games and can be used with Gfx, Glium or raw OpenGL.
Btw, if reading this makes you interested in Rust gamedev, there is a whole dedicated subreddit https://www.reddit.com/r/rust_gamedev/. Welcome!
I'm automating some cross-compile-compatible rustc builds. Levelling up my Ansible-config-writing and toolchain-error-reading skills... This will segue into reverse-engineering the rest of the Buildbot slaves, and getting the OSX ones moved to machines in a real datacenter rather than on a desk. Also reminding everybody that [PDXRust is back](http://www.meetup.com/PDXRust/events/224545535/) and soliciting [ideas about what people would like to see at the group](https://etherpad.mozilla.org/pdxrust-September2015ideas)
Wow, that's gorgeous. Thanks for showing me this!
Apple's no longer shipping OpenSSL headers in OSX 10.11, so I've finally started working on a `Security.framework` library: https://github.com/sfackler/rust-security-framework
I have racer working though I ended up using environment path variables instead, and racer can jump to the definition in rust source; however, eldoc does not seem to be working: eldoc error: (void-function -some-&gt;&gt;) I also had to add flycheck mode like this: (add-hook 'rust-mode-hook #'flycheck-mode) And while it works, flycheck does not seem to be aware of sub modules very well-- complains about unresolved imports while at the top level lib.rs the import occurs. Great write up by the way
Hey, good to see someone finding a use for something I wrote an abandoned on GitHub. :D
It describes a standalone library that implements a GC. The GC only manages pointers you give it to manage so it doesn't step on rustc managed lifetimes: if you move a pointer into the GC, the GC now owns it. It works by writing reference count adjustments for pointers on the stack to a journal. A GC thread reads the journal and keeps track of the total reference count for each unique pointer it is managing. When a reference count goes to zero, it continues to keeps track of that pointer without any reference counting and does tracing to collect unreachable objects. Does that help? :-)
You've not wasted your time :-) Rust doesn't require a runtime GC at all. There might be times a GC makes life easier though, such as when dealing with cyclic data structures. And there are times it might be necessary, such as writing an interpreted language on top of Rust.
Rust code as written today generally doesn't require garbage collection. Without reading the full RFC (yay internet comments), what this would add is a Gc&lt;T&gt; similar to the current Rc&lt;T&gt; but garbage collected instead of reference counted. It should work about the same as Rc but perform better and have the ability to garbage collect cycles.
Thanks a lot, really useful for reading the book/rustonomicon without destroying my eyes.
Does this have a place in the rust stdlib though? This seems like it could technically remain completely standalone since it doesn't need explicit compiler support. And given all of the memory management tools Rust provides, I'd imagine actually needing to reach for a full GC would be rare, even if it is opt-in. 
Out of curiosity, how would you expect a Model, View, and Controller objects would reference and/or own each other?
I think it would be much simpler if rust had a well documented opinionated (as in a well defined and documented way off doing things) fully functional web framework. 
devicemapper-rs, a library around Linux devicemapper ioctls.
* Preparing plan to expand the website * Preparing outreach to package managers * Crater improvements
It only uses ref counts to keep track of what pointers are roots. If a pointer is copied into another object on the heap, the reference count is unchanged. It does mark &amp; sweep based on those roots. `Rc`, as I understand it, keeps a full reference count of pointers on the stack and in the heap and allows cycles through weak refs.
Shouldn't the first _with_ be a _without_?
Continuing work on my [SIMD](http://huonw.github.io/blog/2015/08/simd-in-rust/) stuff, particularly fleshing out the AArch64 intrinsics (there's a ton!) and exposing more intrinsics safely in my `simd` crate. I'm also considering using it to write (parts of) a JPEG decoder or something of that ilk, to get a concrete benchmark that's not entirely micro, to scout out if there's any glaring tweaks/changes that need to be made (e.g. if there's perf problems with code generation or similar, although I think I've picked most of the low hanging ones).
I can haz rust and clippy flair?
Very much so, fixed!
I made an RFC a long time ago, but it has been postponed since then. See: https://github.com/rust-lang/rfcs/pull/327/files You example would be something like: #[bitdata(lsbf)] bitdata ScriptProperties : u32 { langid : u16, fNumeric: u1, fComplex: u1, fNeedsWordBreaking : u1, fNeedsCaretInfo : u1, bCharSet : u8, fControl : u1, fPrivateUseArea : u1, fNeedsCharactersJustify : u1, fInvalidGlyph : u1 }
I'm a bit late to this thread. I've been doing lots of little cleanup things. Knocking out doc bugs, both for Rust and for Cargo. I have an announcement I hope to make soon, but can't _quite_ just yet...
In my experience with realtime applications, we always had some "realtime" threads and some "non-realtime" threads. Realtime threads require strong timing guarantees, while the non-realtime threads don't, so different threads have different scheduling priorities at the OS level. A good RTOS will allow the realtime threads to meet their scheduling deadlines regardless of the number of non-realtime threads or how many CPUs are available (as long as the available processing power is sufficient to handle the realtime tasks). The non-realtime threads would do things like initialization, configuration, logging, shutdown, expose a debugging interface, etc... The GC thread would be just another non-realtime thread. I'm not sure how applicable this would be for true hard-realtime systems, since they typically avoid not just garbage collection, but all dynamic memory allocation. I suppose this may be useful if non-realtime threads are passing objects to the realtime thread. I haven't dug enough into this specific library to know for sure yet. On the other hand, I imagine this could be *very* useful for games (which are soft realtime systems), where developers are trying to trade between development speed (which GC would help with) and jitter/skipped frames (which GC typically hurts). EDIT: I didn't think about it, but the journaling step might require dynamic memory allocation. This is a problem for hard realtime systems.
&gt; but can't quite just yet... You tease, you :)
By _Oz!_ What have I _done!!_
Working on the followup to [my blog post](http://m4rw3r.github.io/parser-combinator-experiments-rust/) about building a parser generator in Rust. It is starting to get a bit long, so I hope I can actually come to some kind of conclusion regarding errors and optimizing it so I can continue on to try it out as a control-sequence parser in my [virtual terminal](https://github.com/m4rw3r/kopparoxid) :)
I was assuming that realtime threads would not be doing dynamic memory allocation, only handling GC'd objects allocated by a non-realtime thread. I forgot about the journaling process, which probably requires dynamic memory allocation (/u/pliniker, care to chime in on this?). If the journaling process does not require malloc, then handling GC'd objects would have very low (and deterministic) overhead -- just reference counting and journaling of dropped pointers. As for running out of memory, as long as the allocation happens in non-realtime threads, GC should not be any less effective than in "normal" (non-realtime) applications. In fact, on a non-overcommiting OS, memory allocation failures could be isolated to and handled by the non-realtime threads, which are typically more capable of handling errors than the realtime threads. On the other hand, even if you have to entirely avoid GC'd objects in realtime threads, this is still more applicable than other GC implementations. Most GC implementations I've seen have no way to manage GC impact on a per-thread basis, except for libraries/languages with a "temporary pause thread GC" feature (which isn't useful on a long-lived realtime thread).
What advantage does this have over Rust's current GC-less execution? In what situations would I choose the overhead of a GC?
What's the current fallback rule in nightly? (with the feature flag enabled)
If you have cyclic data structures, then GC can be easier to use than Rust's built-in memory management. Also, if someone implements an interpreter for a GC'd language in Rust, this could function as that language's GC.
Working on an in-memory file system interface that `std` can link against instead of compiling the sys/fs.rs module, for running tests without doing actual IO.
I can't speak to this implementation, but GC's can actually be a performance win. A GC could, for example, be used to improve heap fragmentation and cache locality.
I'm really looking forward for native-look widgets!
I rather suspect that to present a safe API there may need to be some compiler support. At this point I don't know what that looks like. Thanks for mentioning it though as I'm sure I'll have questions if this project ultimately proves viable.
There's a simple way to know dynamically if a `Gc` is root or not, simply check what memory it's on. If the memory is inside the pools managed by the `Gc` it's a non-root pointer. The problem with `Box&lt;Gc&gt;` still remains in that the `Box` might be part of a loop, but it "breaks" it because the `Gc` is somewhere out of memory. The static type doesn't help here, since `Gc` references might be hidden from the users who might not realize they are creating loops that won't get cleaned correctly. There's a few ways to work around that, but they all require Rust expressing allowing for more reflection (being able to know the types of a structs members for example) which won't happen in a long long while.
As far as I can see, this particular GC implementation won't help with cyclic structures as creating cycles is impossible in the data it accepts.
The reason you can't do what you are trying is that the contents of a given variant are not required to be the same in an enum. It wouldn't work for arbitrary enums, so it isn't allowed in a match. You could probably write a straightforward macro to automatically expand the enum for the specific case where they all look alike. &gt;But it seems a and b are the same as 'val'. What do you mean? Your `val` is going to match one of the branches, so whichever one it matches is the one it's going to be the same as... When you write Test1 {a: i32, ...} you are defining a struct as an enum variant. You can't "unwrap" the contents all as one symbol, you have to do it individually. If you wanted them all together, you'd have to use the name `Test1`, since that's what a struct is: all its fields together. I don't think what you are looking for is currently possible in a concise manner without macros. (Though maybe [this recent post](https://www.reddit.com/r/rust/comments/3hqanl/virtual_structs_part_3_bringing_enums_and_structs/) might explain more about why.)
I think it would be simpler to teach with, and provide examples for an full featured opinionated framework which would help the framework grow more rapidly. 
At some point someone (/u/pcwalton?) asked me what sort of compiler support would be good for a GC. At the time I don't think I had a good answer, but perhaps now I do. Perhaps all the folks who have designed/implemented/worked on a rust GC (/u/pliniker, me, mystor, fitzgen, jdm, maybe mwu/Ms2ger) should get together and have a long chat with some interested core/lang team members.
Could I have an [ecs-rs](https://github.com/HeroesGrave/ecs-rs) flair?
/u/nwydo : The job is complete: https://github.com/dpc/mioco/pull/30
I don't know of a way to reference the "inner struct" without naming it as a separate struct, so I guess this is a trade-off. In case you have many fields, I would go for your last approach of having a separate named struct for the inner enum variant. In case there are just one or two fields and there is no confusion about what the fields mean, I would use a tuple struct. So for your one-field example I would do: enum Testing { Test1(i32), Test2(i32), } match val { use Testing::*; Test1(a) =&gt; /* do something with a */, Test2(b) =&gt; /* do something with b */, } 
Busy with work and all that, not as much time as I'd like to but: * Planning the new Amsterdam meetup. We're taking a whole new approach suitable to the newer crowd and 'selling' Rust more. Very excited about this. * Been working on a Slack client in Rust for some time now as a pet garage project. Private chats and fixing a render flicker since 1.2 are on the main agenda. The rest is bugfixing and it should be releasable :-)
Creating personal project does not requires an RFC that is intended to propose changing the language and std lib.
Well, RFCs are literally requests for comments, the concept isn't intended for anything specific...
There's no actual examples of what a valid command looks like. All I'm getting is errors: ./torch.sh: 10: ./torch.sh: [[: not found ./torch.sh: 45: ./torch.sh: [[: not found ./torch.sh: 45: ./torch.sh: [[: not found ./torch.sh: 45: ./torch.sh: [[: not found ./torch.sh: 52: ./torch.sh: Syntax error: "(" unexpected
Yeah there's many issues with the script: * it's full of [bashisms](https://wiki.ubuntu.com/DashAsBinSh#I_am_a_developer._How_can_I_avoid_this_problem_in_future.3F), the one you hit is [the `[[` builtin instead of `test` (`[`)](https://wiki.ubuntu.com/DashAsBinSh#A.5B.5B) * it uses options for things which aren't optional at all, as far as I can tell you have to provide all of `--pid`, `--duration` and `--output` * it really is just a very thin wrapper around [perf](https://github.com/brendangregg/FlameGraph#linux-perf_events) + stackcollapse + flamegraph, the operative section is `perf record -o $TEMP_FILE -F 199 -p $PID -a --call-graph dwarf -- sleep $DURATION` once you have this stanza you can just follow the official flamegraph readme.
Yeah, we already have a few lints for specific situations (`cmp_owned`, `string_to_string`), but a general solution is much wanted. The not exactly easy part is finding out situations where the lint may apply – this would involve looking if all places the value is used just take a reference we already can have (e.g. are we already sole owner? in that case `&amp;` or `&amp;mut` are permissible, otherwise just `&amp;`) *and* if the types have no interior mutability (otherwise the method could mutate the original value instead of a copy, which would change behavior). The *really* hard part would be creating useful suggestions. That could involve lifetimes. And types. As well as other scary stuff, with lots of corner cases.
Isn't it possible to write an macro to generate the tail recursive loop? 
Don't let this Go Forth.
Just please do note the "eventually could be". I'm not related to the project, I'm just speculating there. 
Stop *bash*ing everyone here.
It looks like it'll really help for me to explain my motives much more clearly (which should have been obvious in hindsight.) My personal interest in this project is to build other programming languages on top of Rust. A lot of popular interpreted languages are built on C, require a GIL of some kind and allow global mutable state all too easily. Generally those languages have been written to solve the problem of needing an easy-to-program-in language or wanting a certain style of programming and much later projects such as pypy have come along later to attempt to address some of the shortfalls in some way. I'm interested in taking the opposite approach: building a platform for other languages designed from the ground up to inherit or reproduce at least a subset of the safety guarantees Rust itself provides. So immutable data structures with a GC that doesn't need to STW (which feels GILish) seems like an excellent trade-off to me, especially for interpreted languages where you accept a degree of performance degradation already. I've offered a possible solution to the impossibility of cycles in immutable data structures [on the RFC discussion issue][1]. The immutability problem is restricted to GC-managed objects that reference other GC-managed objects. For a `GcRoot&lt;Vec&lt;u64&gt;&gt;` the `Vec` can still be mutable. Of course when sharing that mutably across threads, there would still be a need to resort to locking. Perhaps something like [arc::get_mut][2] could actually be safe though... Clearly I haven't thought everything through, which is why I wanted to reach out for feedback :-) I appreciate all the questions and comments, it's all been very helpful so far. [1]: https://github.com/pliniker/mo-gc/issues/1#issuecomment-135241363 [2]: https://doc.rust-lang.org/alloc/arc/fn.get_mut.html
done. Gave myself flair too :P
I would imagine that it would be, but then you have to always use the macro.
Hoverbear has one of the smartest business cards I've seen: http://imgur.com/8DOJ3F9 
Ok, it's clearer now. Though unfortunately, the problem is still unsolved... I've found using structs in enums to be a bit frustrating and this is one of the reasons. 
Pfft, this's only the _Prolog_.
Posting this for visibility. I've known of this game for a while, but didn't realize that the author publishes regular updates. Hanno, you should keep us updated about your cool stuff!
So, you're telling me there's a chance
Thanks, Brian! I've mentioned the game here or there, but haven't really promoted it so far. My monthly updates aren't really technical or Rust-related (aside from the game being written in Rust), but I guess it wouldn't hurt to tell the Rust community from time to time :)
FWIW, if you haven't already seen it, the common way to do bitflags in Rust is with [the macro package of the same name](https://crates.io/crates/bitflags).
Ah, makes sense. Thanks.
I'm extremely excited about posts/libraries like these. They really show off the unique powers of Rust in a way that's a bit more compelling than "Rust is just C++14 with a bit stronger guarantees and more clean semantics." Also worth noting, and something I missed when I read the draft of this post: this isn't just an implementation of a lock-free structure: it's a whole library, Crossbeam[1] that helps you implement your own lock-free structures. &gt; In general, it’s possible to take lock-free algorithms “off the shelf” (the &gt; ones on the shelf generally assume a GC) and code them up directly against &gt; Crossbeam in this way. 1: https://crates.io/crates/crossbeam
non-C-like Rust enums don't have a stable ABI anyway - we change their representation occasionally - so you shouldn't be `memcpy`-parsing them anyway (also, creating an enum, C-like or not, with an unlisted discriminant is UB - a carte blanche for LLVM to "simplify" your code - so you shouldn't `memcpy`-parse to them anyway). If you just want newtyped integers, just use newtyped integers (e.g. the `bitflags` macro). Maybe this should be added to be guide.
Nicely done! &gt; The Rust module system is pretty difficult to grasp and more help from the compiler would be welcome. it's a common sticking point. Is it fresh enough in your mind that you can elaborate a bit on what was hard about it? &gt; I didn't found any safe way to serialize primitive type (like u32) in little endian or big endian. https://crates.io/crates/byteorder &gt; The online documentation is quite good but I miss a "collapse all" button and I find that the CSS is not quite readable. It's in the upper right and looks like this: `[-]`, right next to `[src]`.
The `unlinked` function basically has to be `unsafe`, because it asserting that the data is no longer in the main data structure, which is likely going to be very hard to abstract safely in a useful way (it is hard to reason about concurrency, and encoding the kind of invariants needed in Rust's type system seems hard/impossible). The `unsafe` focuses on precisely the areas that can cause problems. (The "freeing memory" section of the post talks about this more.)
This is excellent work! One thing that I do find curious is the use of Boolean return values on certain functions, which seems unidiomatic.
&gt; it's a common sticking point. Is it fresh enough in your mind that you can elaborate a bit on what was hard about it? The major problem is that you don't get support by tool (ie no proper IDE and no really helpful error message by compiler) and to figure out what the keywords mod and use do you have to spend quite a while. Once I started refactoring my code I got trouble to call function moved from one file to another. I expected that such a task would take me less than a 1 minute but it took me like an hour (maybe even more) to achieve what I wanted. One thing that get me confused it that module path are defined both by file path and module declaration which is something I didn't expect. Maybe it could help if there is a flag on rustc (or Cargo) to show the module hierarchy and how rustc understand the module declaration and the use clauses. &gt; https://crates.io/crates/byteorder I saw it but I didn't want to use an external crate just for that. And, it possible to do so only using the standard module but only in a unsafe way. I wish there were some safe methods in standard. &gt; It's in the upper right and looks like this: [-], right next to [src] I have already seen it but I wish everything were collapsible and still the CSS does not make sense to me. Imo, Having bigger font for the "Example" part title than for a function signature is weird.
`PhantomData&lt;((), ())&gt; // spooky`
Does this also include tools for working with ASN.1? That's definitely something I've seen people ask for in the past, but couldn't find a pure-Rust solution at the time.
Thanks for writing this all out. &gt; Maybe it could help if there is a flag on rustc (or Cargo) to show the module hierarchy and how rustc understand the module declaration and the use clauses. This sounds like a cool idea :) &gt; I didn't want to use an external crate just for that Rust's standard library is very small, you'll be using external crates for lots of things. &gt; Having bigger font for the "Example" part title than for a function signature is weird. Oh yeah, there's lots of little things like this that could be improved. I am horrible at CSS.
There's apparently a NoCopy trait, which is effectively what I was looking for, though it's marked as deprecated. There also doesn't seem to be any implied guarantee that something that doesn't implement Copy will match NoCopy.
Right here: https://github.com/briansmith/webpki/blob/wip/src/der.rs
Are there any videos of the game in action?
I think you forgot `FileNotFound`...
Oh man this looks awesome. The one that always frustrates me with the various lock/wait-free papers is that they just punt on memory management. "The resource management fairies deal with it" is the vibe I get. I understand that it's not trivial, and not the focus of the article, but that said, I've never seen a reference to this epoch system in those papers either. Maybe it's too new? At any rate, looks like building lock/wait-free algorithms in Rust is going to be so much easier now. 
How does having the Copy trait break your code? Why do you need the guarantee?
It's possible to generalize it as an optimized ARC, see the ArangoDb DataProtector class https://www.arangodb.com/2015/08/lockfree-protection-of-data-structures-that-are-frequently-read/
Not to poopoo Rust at all, but Go would be a much better fit for this sort of project: * Cross-compilation is more straightforward * Native compilation on ARM is faster * The general library/webdev ecosystem is much more mature See [Are we web yet?](http://arewewebyet.com/). EDIT: You'll probably be able to do all of this easily in Rust at some point, but right now: * Cross compilation requires downloading Rust source and compiling your own version of the stdlib. * There's no official ARM binaries for the compiler. * Mature, stable libraries are still missing in many places.
Projects that might interest you: https://crates.io/crates/rusqlite https://crates.io/crates/hyper https://crates.io/crates/websocket https://crates.io/crates/serde_json
&gt; I'm extremely excited about posts/libraries like these. They really show off the unique powers of Rust in a way that's a bit more compelling than "Rust is just C++14 with a bit stronger guarantees and more clean semantics." What features of Rust does crossbeam use that aren't present in C++?
You can make a couple of `VecDeque`s, and push your bytes onto those, and from then on you can use normal iterators.
Hmm, perhaps the above was not clear. I want to process the data as it comes in. I can't wait for the entire stream of bytes to finish before I start outputting messages.
I'm curious what would be the performance of the spinlock (instead of Mutex) around dequeue. Also what would be performance of a spinlock with some ticketing system to prevent starvation in case that would be a problem.
[Itertools: group_by_lazy](http://bluss.github.io/rust-itertools/doc/itertools/trait.Itertools.html#method.group_by_lazy) (or just group_by) looks possibly relevant.
F* is an ML-like language also from MSR (and INRIA) that also has effect inference, and pure code and impure code are written in the same language (unlike Haskell, where impure code must be lifted to the IO Monad). The following paragraphs were written after reading of the [F* tutorial](https://www.fstar-lang.org/tutorial/). It seems the F* effect system shares a common heritage with Koka's: F* differentiates impurity that comes from just divergence ("this function might not return"), or that additionally raise exceptions, or additionally read and write to references, or do I/O. On top of an effect lattice, there's the ML effect that stands for any effect (like Haskell's IO). F* will often be conservative and infer the ML effect where it could be more precise. But it may also infer the function don't have any side effect at all -- that is, the function is pure *and total* so it always return. F* has a lot of tools for reasoning about the code, such as refinement types and dependent types. But it feels like vanilla ML when one doesn't use the "advanced" features, which is a plus. At the end of the tutorial, it's shown that F* can also employ arena allocation (that it calls hyper-heaps) to aid reasoning about aliasing, because if two values are in different arenas they can't alias each other. That is, F* is meant to be an imperative language, that happens to contain a total sublanguage, and F* has techniques to make it easier to verify imperative programs. It's nice to see efforts to make program verification more accessible.
Noob here, but isn't this basically adhocing a localized GC?
updated with Usage: ./torch.sh [options] pid Options: -h, --help this message -d, --duration &lt;num&gt; duration of sampling in seconds [default: 10] -o, --output &lt;file&gt; file to save flamegraph to [default: ./flamegraph.svg] 
Because Rust doesn't disallow it?
I doubt it. I'll need the messages out as soon I have all the bytes in. Grouping by message would require the splitter to replicate some of the logic of the iterator adaptors (to know where one message ends and the next one begins).
I don't really know if this is what you expect but it I understood correctly, it works with a new Iterator like: struct IterWrap&lt;Y: Iterator, F: Iterator&lt;Item=Y::Item&gt;&gt; { status: Status, yeast: Y, funghi: F, count: usize } fn next(&amp;mut self) -&gt; Option&lt;Y::Item&gt; { let b = match self.status { Status::Yeast =&gt; self.yeast.next(), Status::Funghi =&gt; self.funghi.next() }; self.count += 1; match b { Some(&amp; b'\n') =&gt; { self.status = Status::Funghi; self.funghi.nth(self.count); self.count = 0; }, Some(&amp; b'\t') =&gt; { self.status = Status::Yeast; self.yeast.nth(self.count); self.count = 0; }, _ =&gt; {} } println!("{:?}", self.status); b } See here: http://is.gd/XaJATG
Heh, thanks for writing all that up. But the messages don't map one to one to bytes, so I'm not seeing how it would help, really. I've now added an example in the original post to clarify that.
Can you give an example with an input and the expected output? It is still not very clear to me. If you can manually iterate, would be better.
/u/aturon I stumbled on crossbeam by coincidence a couple of days ago and thought "how come nobody knows about this!" :D In the same way I stumbled on [moodycamel::ConcurrentQueue (MPMC)](https://github.com/cameron314/concurrentqueue) (and [moodycamel::ReaderWriterQueue (SPSC)](https://github.com/cameron314/readerwriterqueue)). Maybe you already know them but they perform _very_ good (benchmarks: [MPMC](http://moodycamel.com/blog/2013/a-fast-lock-free-queue-for-c++#benchmarks) and [SPSC](http://moodycamel.com/blog/2013/a-fast-lock-free-queue-for-c++#benchmarks)). There are a couple of blog posts about it in [cameron's blog](http://moodycamel.com/blog), I think it's the kind of thing you might enjoy taking a look at (have fun!) :) It would also be interesting to know the standard deviation in the benchmark results, since you claim that Rust performs "more consistently", it will probably show a much smaller standard deviation than java. 
Ok, let's assume a protocol where the messages look like this. struct YeastMsg {a: u16, b: Option&lt;u32&gt; }; struct FunghiMsg {a: u8, b: Option&lt;u32&gt; }; For decoding, first one byte is read (two for Yeast). This is stored as `a` in the message. If `a` is odd, read four more bytes and put that as `b`. If `a`is even, `b` is None and the message can be emitted without consuming more bytes. * Start in Yeast mode. * Read an input string of "1 1 5 7 \t 0". * At that point, we emit a `FunghiMsg { a:0, b: None }`. * Continue reading "1 2 3 \n 8 9". * At that point, we emit a `YeastMsg {a: 11, b: Some(5789) }`. * Continue reading "2 \t 4 5" * At that point, we emit a `FunghiMsg {a: 1, b: Some(2345) }`. * Continue reading "\n 2" * At that point, we emit a `YeastMsg {a: 22, b: None }`. * Continue reading "6 6" * At that point, we emit a `YeastMsg {a: 66, b: None }`. And so on. There might be long times between new bytes coming in, and we need the output as soon as there is sufficient input of either kind. 
 const Success: Result&lt;(),()&gt; = Ok(()); const Fail: Result&lt;(),()&gt; = Err(()); type Status =Result&lt;(),()&gt;; would be good enough, and compatible with try!! 
Relevant RFC: https://github.com/rust-lang/rfcs/pull/1148
There isn't a way around it. You cannot create bounds based on negative impls, even of the types you can create negative impls of. You can't even do some trick like this, because of the orphan rules: trait NotSend { } impl&lt;T&gt; !Send for T where T: NotSend { }
ok thanks for the example, it is too complicated to explain without it. Anyway, I've tried and came with this solution: https://play.rust-lang.org/?gist=ca03b27799235a1b7b93&amp;version=stable It can probably be simplified but here is the idea: Iterator will return an enum enum Msg { Yeast(YeastMsg), Funghi(FunghiMsg) } ... and store both the `current` mode and the other (`cached`) struct IterMsg&lt;I: Iterator&gt; { iter: I, current: Msg, cached: Msg } fn next(&amp;mut self) -&gt; Option&lt;Msg&gt; { loop { let b = self.iter.next(); match b { None =&gt; return None, Some(&amp;b'\t') =&gt; { // go to funghi mode if let Msg::Yeast(_) = self.current { std::mem::swap(&amp;mut self.current, &amp;mut self.cached); } }, Some(&amp;b'\n') =&gt; { // go to yeast mode if let Msg::Funghi(_) = self.current { std::mem::swap(&amp;mut self.cached, &amp;mut self.current); } }, Some(&amp;b) =&gt; { if self.current.push(b) { let new_current = match self.current { Msg::Yeast(_) =&gt; Msg::Yeast(YeastMsg { a: [0, 0], b: None, ia: 0, ib: 0 }), Msg::Funghi(_) =&gt; Msg::Funghi(FunghiMsg { a: 0, b: None, ia: 0, ib: 0 }) }; let ret = std::mem::replace(&amp;mut self.current, new_current); return Some(ret); } } } } } Let me know
For me it's not the guarantee so much as hinting to the compiler how to untangle conflicting impls. That seems to be the motivation behind the RFC1148 others have pointed out.
For the mystified: http://thedailywtf.com/articles/What_Is_Truth_0x3f_
You'll want to try some system call tracing to see if the syscall is actually taking longer, and if so, that's the problem - kernel level, not language specific.
I suggest profiling that function with callgrind to see where it's spending time, rather than relying on precisetime. Also, given that you know the precise size that these vectors will be you could use with_capacity() to avoid allocations - I do not think this is the issue though. Since this is IO bound there could be a lot of factors here - the network and underlying OS in particular.
&gt; I saw it but I didn't want to use an external crate just for that. And, it possible to do so only using the standard module but only in a unsafe way. I wish there were some safe methods in standard. In Rust it is fine to use a lot of dependencies (Cargo takes care of everything for you). It is also fine to create new crates with a single function if you want to share it with the world :)
TIL. I guess the owner of that crate should be notified.
What is the rational for using numeric vs textual identifiers for the widgets? At first blush, having a semantically meaningful queryable identifier makes sense. One could then say, toggle an entire class of widget, or enable the expert panels.
I like that!
Phd right there.
It's compiled as release-version, sadly that's not the solution this time. Thanks though!
Would you mind pointing me into a direction on how to do this? I'm on Windows.
I'll give it a read, thanks!
Yeah, I was thinking that something like this would be the best way to go, so you have full compatibility with everything that expects a `Result`, but don't need to type out the whole type all over the place.
I don't think you'll have to shrink to fit unless you care about memory usage - it'll reserve the memory for the max size but it'll iterate over the elements actually inside of it. I don't know if push_all preallocates, but I'd think so, so this may not make a big difference.
I believe this is currently impossible to import, as it shares its name with a keyword. I hope this isn't true, so if anyone corrects me I'll be happy.
They can sit on the crate name until rust releases its clutches around its old `proc` keyword :-) Just very forward thinking.
I've used http://www.drmemory.org/strace_for_windows.html on Windows.
If `recvfrom` is too slow and you are on Linux, you might want to give `recvmmsg` a try. It minimizes the number of syscalls.
I'm starting on a real time collision detection system for my pure-rust game/engine. This quarter I'm doing an independent study at Uni focusing on collision detection, with one of the secondary goals being to explore Rust's viability as a game development language. I'm pretty excited :D
I just sent you a big PR: https://github.com/hashmismatch/core_mini_http.rs/pull/1
Ah, you're actually right. Not sure what caused me to change it then. Doesn't give a huge performance boost at only 1500 bytes as expected though. Appreciate the help nonetheless!
Working on [flowgger](https://github.com/jedisct1/flowgger), a simple log collector that we are deploying at $dayjob to push logs to Greylog via Kafka. Contributing a few things to kafka-rust by the way.
Documenting `use _ as _` sounds like a great PR opportunity! :) paging /u/steveklabnik1
One more question: do you keep up with the /r/rust_gamedev community?
That's pretty much the scenario I'd understood, see the link in my comment your discriminant byte(s) would be the argument to `foo`/`Foo::from`, but again I don't see how it could work otherwise, variant constructors are constructors not values (degenerate valueless enums blur things, since each constructor can only construct a single trivial value it directly stands in for the value itself).
Mind filing a bug? That makes it actually land on my TODO list as opposed to reddit comments :)
&lt;3
Ah, so it is, I scanned your code too quickly I guess. Here's one way I imagined you could do it (doesn't compile obviously since you can't do that). [Link](http://is.gd/J9Ya8m). The problem with that would be it's matching an on int and not an enum, so it couldn't warn me if I left off a case. And also it needs a default arm, so it still couldn't warn me. (Also instead of panicking, I might have a Foo::Invalid, or maybe return Option&lt;Foo&gt;.)
Sounds like you want some sort of reactive streams library. Crates.io search turns up [a few options](https://crates.io/search?q=reactive), though I have no idea about their maturity.
I certainly wish I had one :)
FWIW, I believe the MPMC queue is less "consistent", e.g. there's no guarantees about the order of messages if there are multiple threads, even if there's (extra) synchronisation between them, e.g. consider two threads running like thread A | thread B send(1) | sync() | sync() | send(2) (where the `sync()` call ensures that thread B only starts running when thread A has finished sending.) I believe the queue in the article will definitely always yield 1 then 2, whereas moodycamel::ConcurrentQueue may yield 2 then 1. I'm sure this often doesn't matter (so having a Rust version of the moodycamel queue would be awesome), but having stronger happens-before relationships makes reasoning about code easier.
Correct. Foo::A is a type constructor for the Foo type.
I do, although I haven't posted in a while (I'm not very active on Reddit in general these days).
&gt; You can't exactly declare a variable of type Foo::A. So I don't think it's really a type. You can declare a variable of type `Fn(u8) -&gt; Foo` and put a `Foo::A` in it, so it must have a concrete type somewhere somehow, it's not *just* a compiler fiction. Anyway I don't see a good and safe way to do this without a second enum indeed, I reckon the best way to handle it is have it generated by a macro.
Yeah, I didn't really expect much, but at least you save 14 allocations per loop. edit: I also noticed you're using a Vec of Vec - this is not good for cache locality and may be the issue. This is something that callgrind and cachegrind can identify for you. 
* [ArrayVec in rustdoc](http://bluss.github.io/arrayvec/doc/arrayvec/struct.ArrayVec.html) I got a poke suggesting that I introduce this crate on the subreddit so that it's visible to more people. Maybe we can have that be a recurring thing! `ArrayVec` is a vector interface that wraps a fixed size array. No internal heap allocation, so it can be quite useful. Features: - Handles fill level correctly and can store arbitrary elements (with destructors). - Dereferences to slice, so full slice functionality - By-value iterator - Drain, Extend and FromIterator. It also has a sibling from a different mother: [smallvec](https://crates.io/crates/smallvec)
LLVM doesn't include code for parsing C. What Rust uses is the LLVM backend that understands the various C calling conventions, while what you need for parsing C this way would be something like the Clang frontend. As a matter of fact, there is the [rust-bindgen](https://github.com/crabtw/rust-bindgen) project which does use the Clang frontend to parse C and generate Rust bindings. For the most part, I think that the reason this is done the way it is is to keep the Rust compiler smaller and simpler. You don't need to provide a full C frontend as well; Rust just provides enough that you can manually write bindings, which is fine for simple APIs. For very large APIs, you can use third-party projects like rust-bindgen, without it having to be tied to the development cycle of the compiler, and without it complicating the basic language semantics and maintenance of the compiler.
Ah, the macro-based approach looks pretty much in line with how I can do things in LuaJIT! Thanks!
Yeah, me too. Perhaps we should start one. Note that I can neither claim to fully understand every detail about the original code nor to be able to port it, but maybe you and other capable folks can do the hard parts ;-) Edit: as per my other comment, I've now joined the Turbine team. We *will* have a disruptor-like crate in Rust again.
I find myself wanting this a lot. I like the recursive impl macro. Sweet!
Yeah the `Array` trait and its macro for implementation for a range of array sizes is what powers the whole thing. That part is nice because once you have the trait, it's possible to provide a completely safe `ArrayVec` API. The downside is of course that the trait needs to be implemented for each array size.
I think you want /r/playrust. This subreddit is for Rust the programming language, not the game.
Last year I (roughly) implemented LMAX in Rust: https://github.com/polyfractal/turbine It's horribly out of date and broken now, since this was well before Rust stabilized. But at the time it fared very well in informal benchmarks with relatively minimal optimizing. I imagine someone who knows what they are doing could really get a Disruptor implementation to fly.
What I ultimately wound up with is `Vec&lt;Box&lt;Trait&gt;&gt;` and moved the data that was being passed into the trait's method into a separate struct. As for the C++ vs Rust version, [this is the original in C++](https://github.com/excaliburHisSheath/Gunship/blob/2d42002ef3c4d25eef7e0490df75ca600d7b2aee/include/Scene.h). The `_behaviorSystems` variable is a wrapper around a `vector&lt;shared_ptr&lt;SystemBase&gt;&gt;`. `SystemBase` defines a virtual method `Update()` that takes a `&amp;Scene` (`Scene` contains all of the state data for the game). In the Rust version I move the list of systems into an `Engine` object which holds both the list of systems and the scene, that way the scene isn't passing itself into `Update()`. You can see the Rust code in [engine.rs](https://github.com/excaliburHisSheath/gunship-rs/blob/385f60af6e3b9a1b43bf5ae5a3551c95f7a19546/src/engine.rs).
Very cool! Thanks for the writeup, I'm in love with lock-free data structures and this was an interesting read. Would Crossbeam be amenable to data structure contributions? I wrote a lock-free [Bounded SPSC](https://github.com/polyfractal/bounded-spsc-queue) a while back, but feel like these sort of data-structures are more useful when grouped in a collection (like your example of java.util.concurrent), rather than independent crates.
Sweet! Can't wait to take a look at this more closely. For now, I'd strongly recommend not using `*` in your dependencies list. For example, regex-syntax will probably get minor version bumps more frequently than regex (by design).
I'll look into it and see if I can get it to compile...
`unsafe { (*(1 as *const A as *const [T])).len() }`
This is a vocabulary type. Please RFC this, it would be a very valuable addition to std. 
Pretty sure you meant /r/playrust
I've done a bit of search'n'replace so far. It looks like there is a lot of small stuff, but nothing too scary. I was thinking about using AtomicUsize instead of the custom AtomicNum, but we need a u64 version even on 32 bit systems, so it'll stay for now.
This question crops up often whenever someone mentions a GC in Rust. For most use cases, Rust's memory management via `Box` (and similar uniquely owned types) works. But sometimes you have elaborate, complicated code where things do not have a clear owner -- in this case, stuff like `Rc` exists. However, even `Rc` has its limitations -- it leaks memory if you form cycles with it. There are ways to overcome this with weak pointers, but it only works if you know how the cycle will be beforehand. It's nice to have a usable GC to reach for when you have lots of cross references (think graphs). One oft-repeated use-case of a Gc would be for writing interpreters/VMs for languages with a GC. In Servo we let the Spidermonkey (javascript engine) GC manage our DOM objects because the corresponding Javascript object is already managed by the GC. Rust is about paying for what you need. You can get a lot of mileage out of Rust avoiding `Rc` and other things, but sometimes you need an `Rc` or two. Similarly, there's a LOT that can be done without a GC (i.e. all the currently existing crates!), but sometimes you need one. rust-gc isn't intended to be used pervasively (though I believe it can be without any issues), a crate using this would mostly be using regular pointers everywhere, with a couple of garbage collected pointers sprinkled around. This won't incur a huge additional cost (unlike a pervasive GC), and would be much more ergonomic than trying to manage the memory yourself. TL;DR: Rust _generally_ doesn't need a GC, but when it does, it should have one.
We don't need to stabilize it right now, and there is precedence of working around lack of type level integers in the standard lib (e.g. all the traits for arrays). I agree with you in that it would be wise to stabilize type-level integers before stabilizing ArrayVec. But I also think it is possible for ArrayVec to already go through the RFC process, have it unstable on nightly, and gain experience with it. More exposure can only improve it. If anything it is at least going to be a strong use case for any kind of type level integers RFC (AFAIK there is no RFC for this yet). 
I can think of a few reasons: 1. Some algorithms are intended to work in the presence of a garbage collector (if you saw Aaron's recent post on lock-free data structures, the epoch system can be seen as a specialized sort of garbage collector). 2. If you're writing a compiler for a garbage-collected language and you want to have backend that generates Rust code, having a GC usable from Rust makes your work much simpler. 3. If you want to integrate Rust code with an existing garbage-collecting language runtime, having a proof-of-concept garbage collector helps to insure that Rust has the necessary language hooks to make this safe and efficient.
Rust is fortunately simpler than Ruby in that it doesn't have monkey-patching. Instead it uses traits, which should make definitions easier to find. In general, the docs are very navigable, and third party libraries are very good about having their libraries documented. (moreso in my opinion than just about any other languages). Methods defined in that library are listed on the page for that type, and their return types and arguments are links you can follow to read about other types. Each method has a link to the source file next it to read the implementation. However, this line is not so easy! I'm not familiar with piston, but are you sure that this is up-to-date? I cannot find a `max_fps()` method or `ups()` method anywhere in [the piston docs](http://www.rustdox.com/PistonDevelopers/piston/piston/). Is there another library this code depends on?
I think I enjoy the agility of developing for stable rust and on crates.io more. If just the word gets out, we can get a lot of experience this way too.
Yes, I definitely want to benchmark more widely once the library gets more serious about the data structures on top. As others have mentioned, though, the main point here is just to ballpark the epoch costs, so comparing broadly similar algorithms seems like a reasonable starting point. (FWIW, I spoke to Doug Lea, who said that ConcurrentLinkedQueue is probably one of the (if not the) fastest Java queue for most workloads that uses a variant of the Michael-Scott algorithm.)
Nah it still works. Basically by avoiding creating a string halfway through you never create a reference to a string that gets deleted. Basically with iterators it's best to avoid using collect() until the last minute, and it's probably a good practice to store the results of each collect call into a variable to make it clear that results are being created and then iterated upon again.
Thanks so much! I would have preferred it if you didn't publicly post my cell number though! (Though all rust community are always welcome to get in touch for any reason)
NoCopy is from the days when Copy was implied. You'll notice that it isn't even actually a trait. It's defined as: pub struct NoCopy; In the days when copy was implied, you would add an instance of NoCopy to you struct as a field to tell the compiler to not impl Copy for your struct.
This reminds me of a class Qt had, I think it was called QFixedLengthArray or QResizableArray or something. It was a vector that could be stored on the stack (with template-specified size) but would optionally overflow into a normal heap-allocated vector. 
I haven't worked on many bindings, but of those that I have looked at, I haven't felt like having automatic reading of C headers would make things easier, for a few reasons: 1. Rust doesn't have an equivalent of C unions (with no discriminant). The best you can do is cast one pointer/reference to another, but that doesn't help with alignment: there are things that do stuff like `union { uint32_t x; struct { char a; char b; char c; char d; }; }` with the assumption that the structure is aligned regardless of representation. So you need to handle this by hand anyway. 2. Rust won't handle C macros, and a lot of interesting subtlety tends to be in C macros. The most useful thing an automatic parse of C headers could do (in my experience) is make sure that constants are correct, but that tends to risk pulling in arbitrarily-complicated macros. 3. C APIs tend to lie about const, 'cause they can. A string literal in C has type `char *`, not `const char *`, despite it living in read-only memory, because C needs to be backwards-compatible with interfaces that literally predate const. Or they lie about const 'cause they [have to](https://github.com/rust-lang/rust/pull/25641), as a result of being on the wrong side of those interfaces. 4. You have to do this on the target system anyway for correctness; you can't import the bindings once and do a translation step. For some languages (especially dynamic/interpreted ones), you can semi-reasonably expect your end-users to have a C compiler installed. That's less common with compiled binaries. 5. In some cases, there are multiple choices about how to bind the raw C FFI function. One interesting approach I've seen is [nix::sys::uio::IoVec](https://github.com/carllerche/nix-rust/blob/master/src/sys/uio.rs), which augments the raw C `struct iovec` with a type parameter indicating whether it's an IO vector of read-only (`const`) or read-write data. The C structure just uses `void *` and promises not to write into those pointers when you call `writev`; see above about const. 6. You need to care about ownership/lifetimes, so if you're exposing a safe interface, you need to write wrappers and pay close attention to pointer types _anyway_. (You also do in other languages, you can just get away with not paying attention much of the time and crash the rest of the time, while the Rust ethos is to get things right up front.) In Python, for instance, projects have been moving from the built-in ctypes to [cffi](https://cffi.readthedocs.org/en/latest/), which does let you read in C headers, but works by literally running a compiler when you call the module to figure out exactly what everything's alignments are. There is a mode where you can do that compilation before releasing a binary distribution, but it still involves asking a C compiler to help you build a Python program. I've found that translating C structures by hand into Rust isn't too bad, and avoids other developers -- let alone end users -- needing to have bindgen (including LLVM development headers) installed and working on their machines. bindgen can handle the straightforward cases, but so can I. That said, I can see bindgen being very useful if you have a lot of API surface that you want bindings to and the bindings are largely mechanical; if you really don't need the human eye on the bindings, might as well make a computer do it. BTW, if you're working on Windows bindings, you may want to get in touch with /u/retep998 aka [WindowsBunny](http://www.rustaceans.org/retep998), who's been doing a _lot_ of Windows API binding work.
I think the website is static (uses Jekyll or something), but crates.io is written partially in Rust.
I imagine that a hypothetical Rust-targeting compiler would use the hypothetical GC pervasively, avoiding borrowing/lifetime entirely at first, and later add static analysis (think escape analysis) that would allow for more efficient code to be generated. And given that lifetimes and ownership are a concern in C code as well (though not reified at the language level), writing a Rust code backend isn't any more absurd than a C code backend.
Will this ever make its way into the standard library?
That’s smallvec: https://crates.io/crates/smallvec
Laying the foundations for an SQL database server and preparing slides for a Rust crash course. Details: I will oversee a programming practical in my university the next three weeks. The topic is "databases" and my group will try to implement it's own SQL database server. In Rust. With SQL parser, storage engines with smart indices and all that good stuff. I am programming a rough structure so that my group is not totally lost. No one of the group knows Rust yet, many just know Java (from university lectures). So my first task is to explain Rust to them... as quick as possible (no on wants to listen to theory all day in a programming practical).
&gt; so anything which can be done in an external lib should Except things that are so pervasive that could create compatibility problems if there were multiple implementations (like `Option` and `Vec`). The question is whether GC qualifies to that.
A minimal interface would need to be shared to allow cross-communication between the various GCs. At a minimum, all GCs need to be able to scan the objects to check the reachability of others.
Compile times might be unpleasant though :)
 Im not sure of such a generic interface is possible, really. It is probably possible to design stdlib traits (might need oibit and/or specialization) so that GCs can be statically prevented from interacting; but probably not so that two different GCs(even with the same design!) will play nice. Imagine if marking happened during a sweep of the other GC. Even the scenario you describe is hard to get right. Tracing actually _stops_ when you come across a Gcptr. It may continue, but whether or not it does is dependent on GC specific logic. The same goes for rooting; you only root till the first GCptrs.
Hi, without seeing your code I cannot be certain about the problem, but I think you are trying to use `try!()` in a method with a return type other than `Result`. Compare the following two examples: * This should look similar to your current code: http://is.gd/V3nKoc * This is how try was meant to be used: http://is.gd/JfZAGo Please also note that `lookup_host` is currently unstable, so you need nightly rust to use it. By the way, you can make it easier for others to help you by creating a small example, wich demonstrates the same error, on the [playpen](https://play.rust-lang.org/) and share the link shown to you when you click the "shorten" button.
I think the reason why the sender is faster than the receiver is pretty simple: The sender just sends a single `u64`, whereas the receiver receives a `Result&lt;u64&gt;`, unwraps it, formats it and prints it, which just takes a lot longer. 
Ah, that makes sense! Just tried my code with sync_channel instead and memory usage is staying stable around 250 kilobytes. Thank you for your help!
Thx for teach and not shared code sorry.
I’ve made a branch of Kuchiki using `Gc` instead of `Rc` and `Weak`: https://github.com/SimonSapin/kuchiki/compare/gc I’m not 100% sure of my `impl&lt;T&gt; Trace for MoveCell&lt;T&gt; where T: WellBehavedTrace` (see `src/move_cell.rs`). One observation: `Trace` is "contagious". If you’re using types from other crates, they have to implement `Trace` too or you have to rely on `#[unsafe_ignore_trace]`. I think it would help to eventually have it in `std` as a "standard" trait that everything implements even if they don’t use it themselves. Finally, I don’t know if this is much better than `Rc` + `Weak`. I haven’t compared performance, but root counting looks suspiciously like reference counting and might (?) have similar overhead. I don’t know about GCs in details, but I vaguely remember something about some of them scanning stacks for GC’d pointers. Would that help reduce or eliminate root counting? (Maybe with help from the language?)
Nice! I'll have a look at that impl later today (at a quick glance, I think you may need to do some explicit unrooting/rerooting of things, see the GcCell impl, or use `GcCell` under the hood somehow). Cell types are really tricky when it comes to a GC. Yes, trace is contagious -- like I've mentioned elsewhere, I would envision std support to include the tooling to make GCs nice (like a global generic `Trace` trait). When we get the HIR I intend to try and refactor `ext::deriving` to work on HIR (might need HIR-based metadata changes), then it may be possible to write trace impls on things containing non-traceable things. It's possible now, too, just that I'd have to do a lot of gymnastics. Also, yep, root counting is as bad as refcounting. Like I said, it's not meant to be used pervasively -- it works, but perf will probably be horrible. It's supposed to be a better alternative to `Rc`/`Weak`. The GC exists for ergonomics, not major perf improvements over `Rc` (also there are situations where you can't even assign weak pointers statically; e.g. when writing an interpreter for a GCd language). Gcs designed to be used pervasively (e.g. Boehm) can be more performant than using `Rc`/`shared_ptr` pervasively, but just like we don't `Rc&lt;RefCell&lt;T&gt;&gt;` all the things in Rust, I don't expect anyone to `Gc&lt;GcCell&lt;T&gt;&gt;` all the things either. My initial motivation behind this GC was to _avoid_ stack scanning or lint-based safety (both let us eliminate the root count). THis carried on when I and mystor were discussing specific designs, and we've successfully avoided both so far. Stack scanning can't handle things like `Box&lt;Gc&lt;T&gt;&gt;`, and I'm not really fond of the idea of scanning the entire stack; it sounds too much like a really bad compromise :) I've already worked on lint-based GC safety and it's hard (impossible?) to make perfectly sound without language support or without compromising usability severely. Lints may be fine for Servo (especially since we know what kind of code will be written around GCd DOM objects), but I don't want to release a GC for wider consumption that is unsound in strange edge cases that lints can't catch. LLVM has a stack rooting thingy which would be quite helpful for a GC (and for Servo!) though. If I ever figure out how to make it work with Rust I might write another GC that avoids root counts, stack scanning, _and_ lints.
Oh, I wasn't even talking of a concurrent GC (though a concurrent GC isn't too different from a regular one for our rust-gc design -- and mystor's `CGc` even has a lot of scope for cool optimizations) Perhaps this would work if the two GCs were exactly the same; I don't know. But, for example, how would a cycle collector and mark&amp;sweep GC interact, over a generic interface that is not aware of the implementations of either? Similarly, there are many different ways of tracking roots, and they may not be compatible. Making two specific mark&amp;sweep GCs play nice is possible -- in that case it's really just one big mark&amp;sweep GC with a really weird implementation. But for two generic GCs, where they may or may not share the same rooting mechanism, it sounds pretty hard/impossible to have a generic interface that understands the rooting invariants. For a specific example, have a look at `GcCell`. We have to use this instead of `RefCell` because `RefCell` lets you bypass rooting invariants. But those rooting invariants are specific to _this_ GC. If a different GC with a different rooting design comes up, `GcCell` would no longer work to track roots on both ends. Which is essential to make the GC work -- you don't just need tracing, you need rooting to work too. So yeah, I'm pretty skeptical about having a scheme where two generic GCs can play nice with each other. TL;DR: It's all horrible and if you're in a room with two GCs, get out while you still can.
Alright. I’m happy to experiment some more, but it sounds like I’ll stick with `Rc` + `Weak` for Kuchiki :)
I'm a little worried about the grep'ability (and ergonomics and parsability, etc.) of making unsafe *named* annotations and macros. Why can't Trace be unsafe to impl instead?
Surely turning the `u64` into a `Result&lt;u64&gt;` would be done on the sender's end? A simpler reason would be that printing text is incredibly slow, and the only `println!` is in the reciever.
I toyed at the thought of something like how python does its truthiness, with a Bool trait. What do you think? It would definitely make bools more usable as a concept. 
I don't like Python's bool behavior. While it's convenient it can be a source of trouble. I like that there is only one type that can be checked for truthiness.
I finally took a look at this :) (it's been sitting in open tabs in another window for months...) So I have one semi-concrete question, and then a whole lot of rambling. The question: * Could you briefly explain how the whole rooting thing works or what it's about, what the underlying logic there is, what problem it's solving, what invariants it's intended to maintain and how it does so, how it fits in, and so on? The rambling: The basic issue I ran into when I was thinking about this stuff is that, okay, you clearly want `trait Trace` and `struct Gc&lt;T&gt;`, as the starting point, and these two clearly have some kind of relationship to each other. So it seems "obvious" that you should have `struct Gc&lt;T: Trace&gt;` (as indeed you do), which means that everything inside a `Gc` box has to be traceable. But I think that's actually the *inverse* of the property you really want: which is that you should only be able to put a `Gc` box where the GC can trace it, to ensure that the contents are valid (if you have a `Gc&lt;T&gt;` somewhere and the GC can't trace it, it could determine that the `T` is garbage and collect it). Which then needs to hold transitively - the thing you put the `Gc` box inside *also* may only be put where the GC can see it, which in turn may likewise only be, and so on, with the base case of this seemingly-infinite regress being to anchor it all to a GC root. (To look at it from the opposite direction - why *shouldn't* you be able to store an opaque, untraceable `Thing` inside a `Gc&lt;Thing&gt;`? As long as there can't be further `Gc`s hiding inside it, the GC can just collect it when it's garbage, like anything else.) And then I thoroughly confused myself (*again*) about what the precise relationships are, or which direction the implications go, between things which don't have `Trace` impls and things which (transitively) don't contain any `Gc`s, and vice versa. If it doesn't have a `Trace` impl, it can't be allowed to contain any `Gc`s. If it doesn't contain any `Gc`s, then it can have a trivial no-op `Trace` impl. So things which aren't `Trace` are automatically `Trace`. Great. All of this comes to a head with generic types - what should a `Trace` impl look like? `impl&lt;T: Trace&gt; Trace for Foo&lt;T&gt;`, or `impl&lt;T&gt; Trace for Foo&lt;T&gt;`? Logically, it should work for both managed and unmanaged `T`s, and delegate to their respective `Trace` impls in the former case. I think the before-last time I thought about this, I came to the conclusion that it should work analogously to `Drop`, where *every* type can be dropped, but only some types have explicit `Drop` impls, and the compiler automatically generates drop glue to run the `Drop` impls of contained types (including in the generic case). So not every type has `Drop`, but every type has drop glue, which "branches" on whether there is a `Drop` impl. Which seemingly resolves the logical conundrum (just replace `Drop` with `Trace` and "drop glue" with "trace glue"). But, I think, the last time I thought about this again, I was unsatisfied with it. I can't remember why. And there's the whole question of how you would even go about forbidding a `Gc` or `Gc`-containing ("managed") thing from being stored where it can't be traced. Can I just make a `struct Foo { x: Gc&lt;Bar&gt; }` and put a `Foo` on my stack (or wherever)? If not, then at what point would I get yelled at, and why? Can we enforce this kind of thing at all without built-in compiler support? And the other thing I got to thinking about, from three paragraphs above, is that maybe you *don't* need to ensure that `Gc`s are always traceable, wherever they're at - maybe it's better to take a step back, and instead that only needs be true in order for you to be *allowed to dereference* the `Gc&lt;T&gt;`. If you have a `Gc&lt;T&gt;` somewhere the GC can't see it, and it collects the `T`, and you can't dereference it and get at the `T` either, there's no problem. So to access a `Gc&lt;T&gt;`, you would have to prove that it's reachable by the GC from a GC root. Maybe you could use some kind of RAII pattern for that, somehow...? I didn't manage to figure out whether or how. If all of that wasn't confused and confusing enough you can also look at the notes I wrote myself many months ago: https://gist.github.com/glaebhoerl/1c232e2c10be048356d5 (which is basically the same stuff just in less paragraphy form, but maybe there's something interesting I forgot to mention, or haven't paged back in yet)
And similarly to your penultimate paragraph, that's the design of Servo's safe rooting strategy - there is the opaque `JS&lt;T&gt;` (a pointer to a GC-controlled value for use in heap-allocated objects), `Root&lt;T&gt;` (a stack root for a GC-controlled value, obtained via `.root()` on an opaque pointer), and `&amp;T` which is obtained by dereferencing a `Root&lt;T&gt;` value.
This sounds very interesting and not doable:) Can you really write an SQL server in three weeks with people who do not even know the language? I've never done anything like that before so please prove me wrong:) 
We should probably include this explanation or a similar one somewhere in the readme :)
Better read the whole *(Java)Script*.
Interesting link. What it says is that basically that parsers shouldn't be made as iterator adapters, but something like this instead.
The game may need polishing, but at least [clippy](https://github.com/Manishearth/rust-clippy) can find only one instance of name shadowing (which is quite an opinionated lint, so the match could well be OK – that's a matter of context). Of course that's a very crude measure of code quality, but I'll just take it as a good sign.
The GC runs as the heap grows, on allocation (Gc::new()). It stops the current thread only, and Gc&lt;T&gt; is not send nor sync. I've also worked a bit on a multithreaded Gc using the same model, but it's still a WIP (look at PR #6 - https://github.com/Manishearth/rust-gc/pull/6).
I've built it on arm (Cortex-A5), against a GLES-only SDL2 and I'm getting like 50% CPU usage on a 1.7GHz core - is that how it's supposed to be?
It just stops the current thread because Gc values can't be transferred to other threads, so there's no reason to stop other threads.
That's surprising. I expect that most of the time should be spent waiting for you to press a key (so, closer to 0%). To ensure that all levels fit on the screen, a full-size rendering of the level is first drawn onto an off-screen texture and then a scaled and centered version of it is copied onto the screen. I'm not an SDL2 expert so I may not be using it in an optimal way...
Yeah thx, 0% is what I'd expect. Possibly a problem with SDL2/libMali combo. Would you be willing to look at some callgrind output I could post via a github issue? 
We're making Trace an unsafe trait in https://github.com/Manishearth/rust-gc/issues/11 - currently blocked on a PR to rustc.
That being said, the current Cgc model will block if you attempt to change the rooted-status of any value during a collection IIRC. So it's still pretty stop-the-world.
Because I wanted to put most of the files in sub-folders. And then if I didn't named them mod.rs I would ended up with longer paths for module which didn't seem to a good thing. Maybe there is a trick to avoid that but like I said I had hard time understanding the module system and I'm quite sure I still don't understand it entirely.
(It turns out that we fixed this issue later with a queue)
CLQ is excellent for what it is designed for and its primary bottleneck is contention on the ends of the queue (e.g. producers). A simple way to improve that is to use a [combining backoff arena](https://github.com/ben-manes/caffeine/wiki/SingleConsumerQueue) to batch operations. If on the other hand memory efficiency is more important than concurrency, there are [mpmc array queue](http://www.adm.hb.se/~AGD/Presentations/CacheAwareQueue_OPODIS.pdf) approaches.
Porting [Stem](https://stem.torproject.org/) to Rust.
The really excellent [Too Many Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/) book is written in a similar style: "try something simple, it breaks, try something else, it still breaks, sit down and really understand it, it still breaks, fix a few more bugs, it finally works." It is good because it shows you all the problems you might run into, rather then just sticking to the happy path, and it can be fun to read (because it has a strong voice).
Why is a `use` statement in the event loop? https://github.com/swatteau/sokoban-rs/blob/master/src/main.rs#L101
I think it was only released a couple weeks ago but clearly the author put an amazing amount of effort into it. 
&gt; But I think that's actually the inverse of the property you really want How about a DoesNotNeedToBeTraced trait? A GC&lt;T&gt; does not implement DoesNotNeedToBeTraced, and hence you would not be able to use it in locations that require DoesNotNeedToBeTraced. And, for instance, a List&lt;T&gt; : DoesNotNeedToBeTraced if T : DoesNotNeedToBeTraced. Then you'd have a `GCRoot&lt;T&gt;` which *does* implement DoesNotNeedToBeTraced regardless of whether `T` implements DoesNotNeedToBeTraced, and registers its pointer in some list of gc roots. To store a value on the stack it needs to implement DoesNotNeedToBeTraced. That seems to get the direction right at the type level, but of course does nothing to actually ensure correct tracing.
Yes, I wrote it ;)
Since people seemed to be interested in error handling I decided to try to improve (aka. rewrite) the parser in both performance and simplicity related to errors. There was a lot to be said, and I am still not 100% done with error handling yet. The next step for the library is to make it possible to use `map_err` or similar constructions inside of `mdo!` blocks, hopefully this will solve the last part of error handling. And after that I am most likely going to implement buffers for parsing from `Read` streams or similar things. The next step for *me* is to probably use this thing and see how well it holds up in a real application :)
I've been working on-and-off on an [ASN.1 parser](https://github.com/haxney/rust-asn1) in pure Rust. I'm starting with parsing the text protocol definitions and then will implement the serializers and deserializers. It's slow going, partly because of how annoying ASN.1 is :/
In other words, you're moving the explanation of the error from a comment into the error message. But you're still not checking anything, so `Some("q").expect("[w, b]")` will still unwrap successfully, even though it's not in the expected set.
This is really very cool! I've followed posts about Rust's safety guarantees, lifetimes etc with interest but it's great to see an example like this that really drives the point home. Seems like that is a common sentiment here :) I'm a noob wrt Rust and lock-free data structures, so I hope you'll excuse me if I'm being naive, but it appears to me that it is only ever valid to call `unlinked` on a `Shared` that has been successfully overwritten in a compare-and-set operation. Or maybe this is specific to the example and not generally true. But if this is correct, would it not be possible to further improve safety by restricting `unlinked` to act on a distinct type (such as `Claimed` or `Taken`) that could be returned by a successful compare-and-set? I guess that still wouldn't stop you erroneously calling `unlinked` on the old head after a push, if you really wanted to make trouble for yourself.
This is an interesting solution. Also see [rfc issue](https://github.com/rust-lang/rfcs/issues/929), specifically [this](https://github.com/rust-lang/rfcs/issues/929#issuecomment-81355948) which seems like if would offer another solution.
Sorta tangential, but I'll mention it anyway. I was over using the unwrap and expect methods until I learned about the `if let Some(x)` pattern. In my admittedly limited experience, this construct is what you really need &gt;50% of the time when you're considering using those methods.
Nah, they've already got parsing down way better than I could manage it.
Good article, I went through the same when I wrote my first Rust code beyond the single file Hello World. I really like Rust modules, once I figured out how to use them effectively. The Piston project source code was a great help.
A small correction: &gt; Consequently, there is a special syntax for pulling in a crate with a dash in its name. &gt; &gt; `extern crate "media-types" as media_types;` &gt; &gt; ... This may be changed in future versions of Rust. [This already happened.](https://github.com/rust-lang/rfcs/pull/940) That special syntax was removed, and crates with hyphens in their names are automatically renamed to ones with underscores *when importing them*. So, while you specify `pkg-config` in `Cargo.toml`, you should use `extern crate pkg_config` in your code. This happened right before the release of 1.0, so you may have seen an outdated doc.
Yeah, I was about to say. I thought this article was from a couple months ago after reading that.
Good point.
Nice article, I like your writing style. Files as modules and `mod.rs` as entry point for directories always reminds me of CommonJS modules (which e.g. node.js uses): Requiring a directory `a` is done under the hood by loading `a/index.js`.
Coming from a C# background, modules surprised me too. The biggest insight for me was that the `mod` declarations in main.rs/lib.rs are Rust’s equivalent of the csproj file.
Very cool!
That'd be another good addition to clippy. Just added an issue.
It is.
Yeah, this is some pretty great stuff right here.
GBersac himself and I approve this message ! This plugin is kind of the default choice when you want to code in rust when using sublime text. It is already great, but some improvement could be made and will need to be maintain. I heard that ide tools for rust is a priority, take care of sublime, it is very widespread ! There is already the [vim rust plugin](https://github.com/rust-lang/rust.vim) and [emacs rust plugin](https://github.com/rust-lang/rust-mode) in this organization, why not sublime text ?
There are three pervasive guarantees in Servo's design: * you cannot manipulate GC-controlled values without a stack root (via the `JS&lt;T&gt;`/`Root&lt;T&gt;`/`&amp;T` distinction) * if you can call the `root` method on a GC-controlled value, then there is an outstanding root to the value from which you obtained the GC-controlled value (ie. you can only reach the `JS&lt;Member&gt;` value by rooting the `JS&lt;Owner&gt;` value that contains it) * all fields in a GC-reachable structure are traced by the GC (this is made easy through the use of `#[derive(JSTraceable)]`) Transitively, if you have lints preventing the storage of `JS&lt;T&gt;` in places that cannot be reached by the GC, it follows that any GC-controlled value cannot be manipulated without ensuring that it is rooted, and it is impossible to obtain access to a GC-controlled value without rooting the values that are required to reach it.
This gives you a pretty assertion failure message: $ cargo run --example test -v Fresh power-assert v0.1.0 (file:///home/nksm/repos/gifnksm/power-assert-rs) Running `target/debug/examples/test` power_assert!(bar.val == bar.foo.val) | | | | | | | 3 | | | 2 | | | Foo { val: 2 } | | Bar { val: 3, foo: Foo { val: 2 } } | false Bar { val: 3, foo: Foo { val: 2 } } thread '&lt;main&gt;' panicked at 'assertion failed: bar.val == bar.foo.val', examples/test.rs:26 Process didn't exit successfully: `target/debug/examples/test` (exit code: 101)
It does sound strange at first, however there is actually a good reason for it: it makes moving code from module to module easier.
You should start parsing stuff from the beginning. Real world parsing changed a lot of things in my approach of nom ;)
I agree, it's just a matter of who wants to maintain it and whether the powers that be will add it. Having official support will likely draw contributors
If you would like to do this, sounds good to me. Add me as an owner, and I can do the transfer.
This is a fun little project I've been working on to play with writing websites in Rust. It includes crates.io's index as a sub module, parses it out, and displays it. There's still more work to do, right now, it just shows all the crates and the published versions for each. I'm not sure that it will be particularly useful, as it's not like crates.io is itself under heavy load, but it's been informative. I really need to get better at serde, right now I'm generating my own JSON by hand and its... Not as nice.
Very readable. I like it.
Very cool!
For those who are not aware of the history, this started with the Groovy based Spock testing framework. The Groovy guys liked it so much they stuck it in the language. I'm glad to see it taking off elsewhere, as it is a really nice feature.
I don't want to do it myself, I don't know how to configure sublime settings. Maybe in the future I'll take the time to think about, but I can't make any promise. But like /u/mgattozzi said, official support will likely draw contributors. [I asked](https://github.com/jhasse/sublime-rust/issues/55) /u/jhasse to add you as owner of this repository so you could take care of the transfer.
Wow, what a timing! As I mentioned earlier I was also taking a shot at this. So, here's [my take on this](https://github.com/manuel-woelker/rust-passert). I was debating whether or not to share this yet, it might still be rough around the edges. I took a quick look at your code, interesting to see a different solution, with almost the same outcome. Some quick notes: * I wish I had known about quote_expr/quote_stmt. This makes the code generation code a lot easier to read * Initially I used pprust::expr_to_string as well, but moved to cx.codemap().span_to_snippet to get the original code formatting. This also seems to work better with the span positions * I just added a testcase for the known issue mentioned in the readme, and it seems to work OK. Great to see so much interest in this area, keep up the good work!
What do you mean with "add me as an owner"? I've added you as as a collaborator, but I don't think that allows you to transfer the repository. I could transfer the repository to you first, ask me on IRC.
Someone decided to apply Formula 1 scoring to the benchmark games (based on the rank related to speed on each benchmark), and this yields: &gt; C++ g++ | 230 &gt; C gcc | 217 &gt; Ada 2005 GNAT | 155 &gt; Rust | 124 &gt; Haskell GHC | 108 &gt; Fortran Intel | 104 &gt; Java | 101 &gt; Go | 98 &gt; OCaml | 76 &gt; Lisp SBCL | 49 I must admit being surprised that Rust manages to score so high, given that all other members of the top 6 (or even top 7) have had more than 20 years of existence to refine their compilers/libraries. I suppose we owe a lot to LLVM... ... however, beyond the present, I wonder whether some aspects of Rust could likely be exploited to further improve performance (such as aliasing?).
Transferring it to me first works just fine too.
&gt; `pub struct TotallyNotADatabase(pub BTreeMap&lt;String, Crate&gt;);` ...
Any time :)
Use `#[cfg(test)]` to only `use` them if you're testing. See here: http://doc.rust-lang.org/stable/book/testing.html#the-tests-module
FFI calls, for example, cannot be proven safe by the compiler - it doesn't know what your C (or other language) code is doing. There are other things, but I can't think of exact examples at the moment.
Let's consider each of the unsafe superpowers: http://doc.rust-lang.org/stable/book/unsafe.html#unsafe-superpowers 1. Access or update a static mutable variable. 2. Dereference a raw pointer. 3. Call unsafe functions. This is the most powerful ability. ## Access or update a static mutable variable. For this to be safe, Rust would have to understand threading at the type level. Furthermore, it would have to understand non-determinism. ## Dereference a raw pointer. This can never be safe, as by definition, a raw pointer is one which just contains a memory address, with no real checking. Rust would have to be able to take each use case of raw pointers, and model them as non-raw pointers instead. ## Call unsafe functions. This is the most powerful ability. This category is similar to raw pointers. It can't ever be inherently safe, but we could look at use-cases and attempt to make them so. `unsafe` functions fall into a few categories. /u/singingboyo has already mentioned FFI, which involves arbitrary other languages. Another would be compiler intrinsics. http://doc.rust-lang.org/beta/nomicon/ is sort of a laundry list of all these things. Over time, and with fancier type systems, Rust could make more and more code safe. But I don't think you can ever quite get there. By definition, the computer itself does not follow Rust's safety rules, and because Rust is a low-level language at heart, we will always provide access to that underlying machine in some way.
That is kind of beside the point I think, because you are leaving the Rust world. If I included FFI my question would become something along the lines of "can everything be provably safe when you include potentially unsafe black boxes?."
Well, you can't really write a useful program without leaving Rust and calling the OS at some point. And some FFI/OS APIs are only safe when things are called in a specific order, so using the individual functions is unsafe.
&gt; So one should be able to reason about whether it is safe to dereference. Right, that's what I meant by &gt; Rust would have to be able to take each use case of raw pointers, and model them as non-raw pointers instead. For example, consider `iter()`. Iterators use raw pointers internally, and include checks to make sure you're not iterating out of bounds. I believe that you can solve this problem with dependent types...
Thanks for the info, I've posted a correction. It is really confusing because so much of the info out their references old versions.
Thanks! I've added a brief section about paths, but I didn't go into as much depth on them.
Good point. I've added a brief section about paths, but I didn't go into as much depth on them.
Any links for that? How would I do something like def gen(): for i in range(100): print(i) yield i**3 print(sum(gen()))
How do you do grep without I/O?
The main use cases for raw pointers are to create memory management patterns that don't match Rust's. The simplest ones actually used to be built into the language- `Box`, `Vec`, and `Rc`. Others are more complicated- iterators, interior mutability, arenas, shared memory, lock-free data structures, etc. These aren't required- for an extreme example, you could just use indices into a huge byte array. Some could be solved in some cases with dataflow analysis or dependent types, but I believe the general case requires either building more into the type system or using something like unsafe.
I suspect a cargo feature isn't quite the right way to decide for overriding `assert!` vs. not, since a single package in a dependency graph depending on power-assert and setting it will cause the built-ins to be overridden in every other place that uses power-assert. Using arguments to the plugin will be more localised, e.g. #![plugin(power_assert(override_builtins))] (These arguments are accessible via `.args()` on the plugin registry.)
Thanks for pointing this out, I didn't know the superpowers were so few and so well documented. I've wished sometimes that the compiler would enumerate the "proof obligations" arising from some particular unsafe code block. 
On top of the other great things people have mentioned, it's a goal for Rust, as a language, to not need GC. Having it available as a library is part of its beauty! 
It gets turned into an iterator class with a next() method, something like this: class GenIterator: def __init__(self): self.state = iter(range(100)) def next(self): i = self.state.next() print(i) return i**3 def gen(): return GenIterator() print(sum(gen())) 
Context: Have been a 15 years Vim user (even wrote a popular vim plugin: https://github.com/SirVer/ultisnips) and think it is time to retire Vim for something else. I started https://github.com/swiboe/swiboe and try to raise awareness and interest from the very start. Right now I am outlining the design (in Rust), build community and establish branding. I stream most every weekday morning (EST).
Kuchiki made html5ever really easy to use. Didn't notice that UrbanDictionary has an API already. Thanks!
The language that exists that is most like what you describe is probably [ATS](http://www.ats-lang.org/). THere's a definite accessibility trade off in adding type system features like this, though.
Swiboe will have very similar keymaps to Vim - simply because that is the standard that I am used to too. Also, if you'd make a new program running under Mac OS X, you cannot make Copy anything else but Command + C - I feel very similar about modal editing shortcuts. There is just already an established standard. That said, Swiboe is not a GUI. It will be a full fledged editor with different semantics to Vim. Making plugins compatible with Swiboe is not really feasible and in scope - and I do not want to write any more VimScript in my live if possible. Also (slight tangent), plugins that have been written with the idea of running inside of Vim do not always translate nicely into NeoVim (or probably any other RPC based editor) - UltiSnips for example is soo much slower in NeoVim. Swiboe will also not be a GUI only, so not sure how supporting the server protocol will look like. Or am I missing something?
It's worth noting that safety in rust is determined (is it is currently implemented) at the function level, not module or crate. Since static values are outside of any function, handling mutable static values isn't even considered by the borrow/safety checker. As for FFI and compiler internals, the compiler can't guarantee safety, so it falls upon the crate maintainers to make code safe. Ultimately, Rust's safety guarantee isn't about making the world safe--its about guaranteed safety built upon unsafe--(but hopefully well tested and maintained)--compiler internals and FFIs. This is one of the driving forces behind some developments to create as much Rust infrastructure as possible--minimizing the unsafe FFI code that developers have to count upon. My 2 cents.
&gt;&gt; Dereference a raw pointer &gt;Excluding FFI/unsafe functions, the pointer had to be created somewhere in the program, right? So one should be able to reason about whether it is safe to dereference. Presumably the programmer who is dereferencing it is reasonably confident that it is the case. But the compiler still can't see it. Assuming the programmer is not wrong, I'm wondering what it is about our high level view of the problem that gives confidence that it is safe. If you're writing an OS (or something else bare metal) for x86, you can write chars directly to `0xB8000` to print to the screen/terminal. This is completely arbitrary and, without special handling, there's no way for the compiler to know it's a thing. (And even in the case of special handling, the compiler would have to handle it differently if the program isn't being compiled for bare metal. Obviously, you can't just write to `0xB8000` under e.g. Linux.) In embedded/bare metal, there are lots of this sort of predetermined area of memory for IO and configuration of the firmware/device itself. Some of them are even configurable/movable, like DMA in x86. Imagine trying to model safe page structure modification for x86 in a compiler. It would be very complicated. http://wiki.osdev.org/Printing_To_Screen
What /u/levansfg said. They are called diverging functions. [Here](http://doc.rust-lang.org/nightly/book/functions.html#diverging-functions) is the part of the book that covers them.
I'm working on a web game in React with a Rust api back end. I'm using Iron for the api side, mostly just providing mock endpoints and setting up the front end react code at the moment but I'm having fun with both technologies. I'd like to use web sockets at some point but I haven't found any Rust implementations that seem sturdy enough. I was looking for STM library to provide a bit of persistent state without a db dependency but I couldn't find any. I've yet to look into Rust channels but they served me quite nicely in Go so I'll have a look at some point.
Also, while we moved the post to the official Rust outlet, feel free to answer the question wherever you want. A comment here is as good as a comment on users.rust-lang.org (Although, IMHO, users reads better)
 - Pondering more about rust-gc - Finishing [this blog post](https://gist.github.com/Manishearth/05905a7ab15b6f5edd28) - Poking more at indexeddb for servo - Studying for midsemester examinations - Poking at gcc as part of a course project 
* DONE ~~Merging the final unicode-related lint into [clippy](https://github.com/Manishearth/rust-clippy)~~ * Extending my `shadow_*` lint's pattern matching (Edit: Partially done) * Pointing `clippy` at different code bases on the web to look for false positives and get inspiration for more lints (and submit PRs if I can improve something :-)) Edit: I even got a few PRs for Rust! :-D And blogged about it! * Trying to get [Turbine](https://github.com/polyfractal/Turbine) to compile (and improve the API while I'm at it) * A lot of work (I have a day job Rust doesn't yet fit into, alas) Edit: Oh, and also * Starting the [Crate of the Week](https://users.rust-lang.org/t/crate-of-the-week/2704) effort.
* I'm gonna work more on [clap](https://github.com/kbknapp/clap-rs) improvements * Also I want to add some cool features to [cargo outdated](https://github.com/kbknapp/cargo-outdated) subcommand * Some minor improvements to [minesweeper](https://github.com/Vinatorul/minesweeper-rs) * And going to start new gamedev project based on `SDL`. I think it will be `tileengine`
 - Figure out why [this PR](https://github.com/tomaka/glium/pull/1198) triggers an OpenGL error with the version of OSMesa installed on travis (and not on the version of OSMesa used by circle-ci), error that I can't reproduce on my local machines. - Land [a PR that changes the design of cpal](https://github.com/tomaka/cpal/pull/57) and move what's being removed to another library. - Need to figure out [why](https://github.com/tomaka/glutin/issues/515) creating a glutin window/opengl context on Windows randomly triggers weird errors from the O/S, like "wrong dictionnary entry" or "the transform operation is not supported" which even Google doesn't know about. Also, why it sometimes fails without triggering any O/S error. - If I'm in a good mood, add native Windows font-loading capabilities to [text-render-atlas](https://github.com/tomaka/text-render-atlas) instead of depending on freetype. But the Windows API is twenty years old and is one of the worstly-designed API I've ever tried to use. (and also, working on my closed-source game in Rust) 
Well, actually `!` is not a type at all, it cannot be used in other places than a function prototype. So it is more a function annotation than a type, I would say.
Yes. This is WIP on how a config file might look: https://github.com/swiboe/swiboe/blob/master/term_gui/config.lua Everything in Swiboe has a priority, so you can overload mappings (many functionalities, same keymap) and your maps are tried in order until one handles it. You will also be able to not use any standard mapping at all.
I have a question. Does your GC moves the data around? (as in generational GCs). How would you accomplish that - with a double pointer?
Doing Windows things and procrastinating. Reviewing pull requests that add thousands of lines of definitions on a daily basis.
DirectWrite was a brand new API written for and released with Windows 7, so it is only six years old, not twenty.
I just wanted to say thanks to you and zonyitoo for all the great work! I've been playing with mioco and coio a lot lately. I skimmed the gitter and it looks like you + zonyitoo were discussing various aspects of threading, thought I'd add my wishlist: In general, it would be great if both libraries exposed some more internal details to the user. For example, because **mioco** is single-threaded right now, I'd love to ask mioco which tasks are pending, so I could implement a scheduler and move work to a different thread if I needed. Right now work has to be scheduled more coarsely up-front (such as moving "work units" into a shared queue and then using co-routines to drain the queue, instead of scheduling co-routines themselves). Similarly, in **coio**, it'd be great if the scheduler was pluggable, so I could build my own implementation. For example, I might want to manually (re-)schedule coroutines between threads. E.g. check if a file read operation will cause a page fault, if yes re-schedule to a background thread so the blocking IO doesn't affect currently running coroutines. Or being able to build a priority system to allow latency-sensitive coroutines first chance to run, or pin certain tasks to a certain core and prevent them from migrating. Obviously both libraries are still super young, so this is just a wish list for future consideration. :) Keep up the great work!
If I read the code correctly, the GC just `drop`s the data.
Mioco specific question: how do you return a computation result from a coroutine? It looks like I can request an exit_notificator() from the coroutine handle, but I think that only allows me to determine if the coroutine exited cleanly or panic'd? Is there another way to achieve this? *This might be a silly question, I'm still pretty new to working with coroutines :)
Posted here because the discussion mentions Rust quite a lot.
There was some interesting discussion on a recent rfc to make `!` a type: https://github.com/rust-lang/rfcs/pull/1216 Part of the story is rustc had a `ty_bot` and decided to remove it some time ago. 
Scala also has type "Nothing", which is (hardcoded as?) the bottom type of all types. Although very rarely seen outside std libs, I don't recall seeing any limits on using it.
And just as stupid as `void` was in C...
If ! means that the function doesn't return, how does the compiler use this information (optimizing, correctness,..) ? Does it check that there isn't code after a call to this function? Does it check if this function can return at all?
I'm working on all the parts that any game I eventually make will probably need. I finished working on a library that abstracts a (practically) infinite grid in a (hopefully) efficient manner.
Care to elaborate how they are so similar ? I fail to see your point...
You can try it out! &gt; Does it check that there isn't code after a call to this function? http://is.gd/s7WN7y &gt; Does it check if this function can return at all? http://is.gd/8NjnMp TL;DR: yes in both cases.
Something that might be an issue with this approach is plugins that rely on buffer content. Since the plugins are in another memory space the buffer must be serialized to pass it around, which will be slow for large files. This would stop things like syntax highlighters from being outside the buffer plugin. Also it might necessitate having the buffer and GUI code in the same process. You might be able to solve this with shared memory though
Working on my [RusTiny](https://github.com/msiemens/RusTiny), a simple Rust-like language that compiles to a LLVM-like IR which compiles to x86-64 assembler. So far the x86 instruction selection is basically working. Next I'll propably have to write a register allocator...
https://github.com/zumero/Elmo Elmo is short for Embeddable Lite Mongo. It's a nosql db for mobile devices, with support for the Mongo query language. I am in the process of porting the original F# prototype to Rust. The Rust version currently passes over 130 test cases from Mongo's test suite. Elmo's API for its storage layer is a trait. Currently, the only implementation of the storage layer is built on SQLite. I'm using a private, hacked-up copy of dckc/rust-sqlite3. This may end up yielding proper pull requests to that project as things get clearer. One of the more interesting aspects of the porting process is that F# does not require the code to explicitly have a clear understanding of who "owns" something. In most cases, the answer is obvious. In one case, I ended up using an Rc, which causes me to feel the same sense of failure that I feel when I use mut in F#. I still daydream that one day I will go back and re-fight that battle with the borrow checker, with a different outcome. And in some cases, I deferred the ownership question until later so I could proceed on the surrounding code. It turns out the easiest way to do this was to temporarily make a clone. I'm wrapping up some missing features right now, but soon I want to go back and eliminate some of those clone() calls by thinking more carefully about who owns what. 
By the by, Twitch rules require you stream gaming related things. You'll probably want somewhere like livecoding.tv.
After last week's unexpectedly enthusiastic response to [my Civ5 clone with a text-based UI](https://github.com/hsoft/civng), I've been wondering about inviting other developers to join. That early in a project, there's always a chance of everyone stepping on each other's feet, but I see many non-overlapping tasks that could be done in parallel: * "Move to" feature * More combat modifiers * Begin city implementation * Unit movement AI * Map generation Is anyone interested? I'm already having a lot of fun by myself (and making myself better at rust), but I figure that it could be even more fun with a bunch of us.
Yes, this is another case where "unsafe functions" are a rats nest. So start from the other end: 1. If I'm derefing a static mutable, always point this out, and remind me to synchronize access. Bonus points for scoping the other potential readers and writers of the mutable. 2. If I'm derefing a raw pointer, and it is obviously ok (because I just converted it from a valid reference) tell me I'm safe. If it looks safe but isn't obviously so (pointer arithmetic from a converted slice reference), remind me to keep that offset in bounds. 3. If I'm calling an unsafe function written in rust, what you said. If it is FFI (so a black box), remind us all bets are off. So there is some meat and usefulness in 1,2 also. And certainly, that's a pretty heavy wish list item :)
Interesting. Why did you pick Mongo as a target? I was considering a similar project for CouchDB, especially as it has a very nice replication protocol, a browser implementation (PouchDB) and support for incremental indexes. Also: there's a mobile implementation for Android and iOS, but I'm unimpressed.
We picked Mongo specifically because it does NOT have a nice replication protocol. :-) Zumero's main product right now is a mobile replication-and-sync solution for Microsoft SQL Server. We are exploring the possibility of expanding to support Mongo as well. Elmo is intended to be a piece of that puzzle. The following blog entry may be a bit dated, but it explains the background: http://ericsink.com/entries/mobile_sync_for_mongo.html 
`!` isn't really a bottom type, as no subtyping is involved.
&gt; It will probably cause some conceptual problems in the future, given that it is valid to write `return f()` in a `!` returning function so long as `f` is also a `!` returning function; and you can even use it in any expression context you wish where it will be silently coerced to any appropriate type in the type checker... I still don't follow why that will cause problems. Apart from FFI functions (like, oh, `extern { fn exit(status: c_int) -&gt; ! }`) that the compiler can't check, Rust ensures that every function or expression of type `!` in fact cannot return. So I don't see the problem with being able to use diverging expressions in any context, since you'll never actually get a value out of them. Can you expand on any of the conceptual problems you either foresee in Rust or have seen in Haskell? Note that Rust (unlike C) distinguishes between the empty return type and a never-returning function, so I'm not convinced that `IO ()` is relevant, but maybe I'm missing something.
Note that calling a function returning `!` is not undefined behavior. It's perfectly well-defined, and necessary to do things like `panic!`. It's just that _returning_ from such a function is impossible, so the compiler is free to assume that it cannot happen. What's going on with this particular library is that the `Void` type is defined so that there aren't any possible values of type `Void`. Therefore, for all the trait implementations, it's impossible for any of the functions to be called, since there aren't any values to call them on. By implementing the traits by going through this `unreachable` function, you can convince the compiler of that.
I wonder how it will compare to https://github.com/mawww/kakoune , which is another text editor that has been created by a former vim user
Wrote a [small tool to compute strongly connected components of a graph](http://github.com/nfleet/isolation) using [petgraph](https://github.com/bluss/petulant-avenger-graphlibrary). The previous version was written in Go but it ran out of memory (on a 8GB machine) and wasn't fast enough, the C++ version (based on [SNAP](http://snap.stanford.edu/snap/)) needed tens of gigabytes memory (for some reason) for a medium-sized graph (10 million nodes, 100 million edges). The Rust version was quite a lot of fun to write and performs *really* well. I should probably benchmark it to get accurate results, and I'm sure the implementation could be optimized all the way to eleven, given what Frank McSherry has done with his graph processing libraries. Been toying around with the idea implementing a Rust version of [Contraction Hierarchies](https://en.wikipedia.org/wiki/Contraction_hierarchies) that would make heavy use of concurrency and parallelism, and support local graph updates without a complete recomputation of an entire graph. A parallel bidirectional Dijkstra's algorithm would be fun to write!
What did you want to say? You just reiterated what was said in the parent comment. 
Since `void` is not a normal type in C++ it causes problems in generic code because it requires special handling. Rust has no such problem because `()` is just a normal type. Siince `!` is not a normal type in Rust it causes problems in generic code because it requires special handling (for example if your code is generic over `Fn() -&gt; T` it can't handle the case when `T = !` because `!` is not a type).
One of neovims main features is that other programs can "embed" neovim as an editor component by speaking with a neovim server. In this way, it's possible to get perfect vim keybindings in other editing environments like IDEs or even other text editors like Atom. So I guess I was asking if you plan on allowing your plugin api to do this communication and get perfect vim keybindings for free. 
Thanks for feedback! I really like the idea. I've created https://github.com/dpc/mioco/issues/36 for further discussion. Feel free to track and/or join in and comment on your use cases and requirements. IMO, coio and mioco are tackling the same issue from a bit different angle as the authors had different feature/use case in mind when starting them, but when growing with features they will more and more overlap. So far I was mostly concerned with getting decent API and design to support all mio event sources and possible additional features like mailboxes. Because I was already very close to releasing "stable" release, I've finished it, to give people something usable right now. I'm planing to spend some time, and evaluate coio in more details to see if these two could merge or at least absorb some ideas. It seems to me that if mioco implemented [multithreading](https://github.com/dpc/mioco/issues/36) as just spawning N event loops in N threads, then instead of resuming coroutines right away, put the wakeup order into queues, and had user accessible hooks to manipulate that order and move it between event loops, as per your comment, then work-strealing algorithm could be implemented as one of the the default scheduling policies. Such features would bring mioco to a whole new level. 
Super nice man, I was thinking of doing some stuff with mio but I was hoping for a nice implementation like this! 
I will... * [Inspire people to help improve the website](https://internals.rust-lang.org/t/rusty-web-improvement-bureau/2572/4) and delegate all actual work (hopefully...). * Befriend people packaging Rust downstream and figure out how we can help. * Drink with people from the PNaCl team to figure out what the PNaCl/emscripten/webassembly story is. * Clarify what we need out of a [semver validation tool](https://users.rust-lang.org/t/signature-based-api-comparison/2377) and try to keep that effort moving. * Fix all the brokenness in [crater](https://github.com/brson/taskcluster-crater). * Finish multirust and make it an official project. * [Submit a patch for gold linker](https://internals.rust-lang.org/t/how-to-move-to-the-gold-linker-on-linux/2576/2). * Investigate performance improvements to metadata indexing. Of course, there's always a notable difference in my ambition and what I actually finish. 
Will mention it in issue 94. :-)
Sounds wonderful, can't wait! Keep up the great work, both you and zonyitoo are making very cool libraries :) 
That's fine. I realize that there is not much there yet besides the concepts, so I understand hesitancy. The server concept is for two reasons: first, multiple GUIs connecting to the same server for collaborative editing ala google docs. And secondly, so that you can the server on a remove machine and connect to it through a ssh tunnel. The design can support that - I wonder how the latency will feel, but it is too early to tell. So your use case is my use case. &gt; I'd like to see an editor that transcends the filesystem. That is rather metaphysical. I can describe what I have in mind for this and you can tell me if your use case is expressed through that. Each buffer has a URI that describes it's content, e.g. file://&lt;path&gt; http://&lt;path&gt;, git://&lt;repo:blob&gt; . The editor will call an RPC 'buffer.open' and many plugins can support this RPC, the order is defined by the RPCs priorities. The first to understand the URI opens it and creates a buffer with the contents of this one. Saving works similarly. Files are therefore just an abstraction that only the plugin that loads and saves them needs to know about. I do not understand your second and third points though.
That sounds like a formal treatment, which I am definitely cheering for. I was thinking more about any low hanging fruit, possibly useful to "casual" users of unsafe.
I'm stuck in visa limbo for a month so I'm setting myself a challenge - to build a [Bloom](http://db.cs.berkeley.edu/papers/eurosys10-boom.pdf) interpreter that is at least as fast as SQLite in less than 10kloc. I should have some (very) preliminary results to show by this time next week.
Is that &amp;&amp;i32 a borrow of a borrow of an i32, or is it something else?
Just this weekend I finally finished some API redesign/improvements for my [music processing framework](https://github.com/thenyeguy/oxcable), so this week I can start digging back into my [analog-style synthesizer](https://github.com/thenyeguy/oxcable-subtractive-synth). Probably my goal will be to give it a GUI for modifying tones.
`a.iter()` yields `&amp;i32`. Since the `.filter()` adapter is generic over its item type `T`, and has to be able to yield any of the values it filters over, it has to pass `&amp;T` to the closure. So unfortunately, we get a reference of a reference. Since `i32` is trivially copyable/cloneable, what we can do instead is use `a.iter().cloned()` which yields `i32`. `.filter()`'s closure will still have to take a reference though, but it'll be `&amp;i32` instead of `&amp;&amp;i32`, and the original value will be on the stack so the indirection cost is negligible (not that it wasn't to begin with).
&gt; Can all code theoretically be written in a "safe" manner? [Yes.](https://gist.github.com/notriddle/c55531d71d2312dc305a)
I've tried fn divisible(div: i32) -&gt; Box&lt;FnMut(&amp;i32) -&gt; bool&gt; { Box::new(move |&amp;x| { x % div == 0 }) } fn main() { let a = vec![1i32,2,3]; let b = a.iter().cloned().filter(divisible(3)); } But it still won't work (error is "the trait `for&lt;'r&gt; core::ops::FnMut&lt;(&amp;'r i32,)&gt;` is not implemented for the type `Box&lt;for&lt;'r&gt; core::ops::FnMut(&amp;'r i32) -&gt; bool&gt;` [E0277]"). Is this because it is boxed or for some other reason? I'm very sorry if I'm missing something obvious.
It shouldn't require memory allocation. The largest buffer you'd need is known. It should just be a [u8;24] on the stack.
Thanks! Your comment made me investigate slices in more detail and I understand them a bit better now :-)
NB. that is not transitively safe, e.g. the implementation of `write` calls `unsafe` code internally. (That said, it makes a lot of sense for practical purposes to assume that `std` is a safe black box, which allows a lot of stuff to be written without any other `unsafe`.)
It ... could, but it would incur significant costs. It could be done by replacing things with a guarded double pointer. But then the cheap `&amp;T` references would stop working (and autoderef wouldn't work). I don't intend to make it moving. It doesn't do its own allocations anyway; it uses the regular Rust allocations (jemalloc), which I hope are efficient enough. 
You can do let mut div = divisible(3); let b = a.iter().cloned().filter(&amp;mut *div); I think what's happening is that `let result = divisible(3)(&amp;0)` is using automatic deref and automatic referencing, whereas `filter`, being a function call, does not. This means you have to deref manually.
Andrei is working on `std.allocator` and the plan is to make the std library (and eventually any code ever) as natural to use with custom allocators as with the currently GC-bound `new`. The community is quite optimistic (I don't get most of the discussion on it, though).
Functions that "return" `!` never, ever, _ever_ return. Because of their unique non-existence, `!`s are ignored for typechecking (I think), meaning you don't have to explicitly handle the special cases.
Nice idea! A few comments on the code: * your `parse_numbers` doesn't need to go over the data twice: pub fn parse_numbers(args: &amp;[String]) -&gt; Vec&lt;f64&gt; { args.iter().enumerate().map(|(i, x)| { x.parse::&lt;f64&gt;().ok().expect(&amp;format!("Argument \"{}\" was not a number :(", args[i])) }).collect() } * no need to create an intermediate vec here: input_numbers.extend( l.split_whitespace() .filter(|x| !x.is_empty()) .map(|x| x.to_owned())); (could maybe also use `filter_map`) * similarly, no need to create an intermediate `gap_vec` * to_string() on `&amp;str` is better written as to_owned() or into() * like with `&amp;[T]` instead of `&amp;Vec&lt;T&gt;`, try to take/return string slices `&amp;str` instead of `&amp;String`. (Returning is only possible if you don't create a new string, of course.) * functions like `colorise` that either return the input string or a new string can return `Cow&lt;str&gt;`, which will avoid a copy when (a part of) the input string is returned unchanged. (In this case it doesn't seem necessary, since the unchanged case is only a fallback.) 
&gt; Nowadays, the only form of automatic memory management in Rust are via `Rc` and `Arc` [...] I'm no rustacean but from what I know this seems to be wrong. Isn't it the case that all (or at least all) memory in Rust managed automatically and what makes `Rc` and `Arc` different in that it's not known statically when the underlying memory (what the `Rc`/`Arc` points to) will get freed?
I meant runtime automatic memory management (the context was "core rust gcs", and some folks consider Rc/Arc to be a gc)
So it is manually memory managed, but in the C++ RAII way. Rc and Arc are just refcounted pointers. Edit: there's a sliding scale of manual memory management, from assembly (none), C (stack is automatic), C++/Rust (heap is automatic-ish), then stuff like Java or Lisp (everything is automatic).
Well yes and no. First of all `void` is more like `()` than `!`. That is `void` means: this function *returns nothing*. OTOH `!` means: this function *never returns*, you can assume that everything after this will not run. The problem with `!` is that it has implications for control flow, which is not something you'd expect of a type. The problem is when you have an instance of `!` and you have to deal with it. In a way the reason why it's so complicated is because `!` implies side-effects which is not something that is encoded by the rust type system.
Yeah, in fact when we re-wrote formatting (a long time ago now) an explicit design goal was avoiding unnecessary allocation. 
One useful part of Rust is that almost every type of item is identifiable syntactically by a leading keyword. All functions, including methods, have as part of their declaration `fn &lt;name&gt;`, so you can always search for that. Only case that doesn't hold completely for is structs and enums, you can't tell which one a type is from the name, but searching for both `enum Foo` and `struct Foo` isn't too bad. 
I'll have to give it a closer look then, thanks for the suggestion.
The link to the merged PRs 404's.
looks good!
You may find these two article interesting related work. Implementation and performance evaluation of a safe runtime system in Cyclone http://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=rlxeD9sAAAAJ&amp;citation_for_view=rlxeD9sAAAAJ:IjCSPb-OGe4C Type-preserving garbage collectors http://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=rlxeD9sAAAAJ&amp;citation_for_view=rlxeD9sAAAAJ:d1gkVwhDpl0C
I would love to see a blog post about this. :)
I'll have to take a look at this later but it might be what I need for a project of mine.
Thanks! I'll try and make my way through those changes. :-) I suspect some of the inefficiencies were because I was just fighting with the borrow checker, but I feel like I have a slightly better understanding now.
Actually, given the safety guarantees of Rust, it's actually *harder* to design a GC in rust, philosophically speaking. C++ GCs can just list what isn't allowed and walk away. Rust GCs must prevent all of this, anything short of perfection is Unworthy of Rustiness :P Lints can't handle generics, so you either ban generic functions from being used with GCthings, or allow them and get unsafe stuff (or find a compromise using a whitelist), or implement a full blown effects system via lints (noooope).
I think so, but it should be easier to make the cc deterministic.
I think that providing a GC by default has been the right choice for D as an application language. Of course, this undermines its suitability for systems work, but it enabled them to grow the language relatively free of memory management concerns, something that Andrei correctly notes Rust has invested a lot in. Imagine how far we'd have progressed in terms of compiler speed, metaprogramming features and other amenities, had Rust invested their cleverness budget on those features instead of borrowck. That's not to say I wish we had forgone this particular "bulging muscle" – Rust has shown the hard part to work, and we have enough time to get the other things right. But the tradeoff remains.
The weekly quote is golden this week. I actually got a good laugh out of it.
Wouldn't it be a pretty short list, what with post-1.0 stability and all? There _is_ an "Approved RFCs" section, which tells you what will be changing in 12 weeks or so.
Big fan (and big user) of ultisnips. Thank you very much! I'll be following your project closely! EDIT: Just read through your blog entry on your editor wish list and I'd have to say it is almost the same as mine! I have noted down some ideas myself and maybe you'll find them helpful :)
What kind of compiler support would be needed to have a moving GC with no double pointers? I think that the GC thread would need to be able to atomically change pointers in the stack of a thread that has rooted GC pointers. That is: stop that thread, change the pointers in the stack and resume it. That's because once the thread has a GC pointer on its stack, that value can't usually be moved (another way would be, on each access, to verify whether the value has been moved the GC, but this would be too expensive) But that would entail stopping the world, I don't know other solutions.
Yeah, stop the world wprks. Otherwise, a moving GC is rather counter to how Deref works.
Had Noah *owned* the animals, he wouldn't have needed to care about the lifetimes. God should have said, "Thou shalt mutably borrow these animals..."
Having fun learning more functionality in piston while building an adventure platformer prototype.
I see your point, but interacting directly with the OS falls under "Excluding FFI/unsafe functions" in my opinion. If the OS was written in Rust, for example, and provably safe, this wouldn't be a problem.
Apparently it's been a kernel issue. I've moved from Windows to Linux now, and don't have the problem anymore. Besides, while I have a 10% packet loss rate on a busy network on Windows, I barely ever lose any UDP packages on Linux.
Well guess we have a bit different backgrounds. I've never heard anyone call RAII manual before. :)
But how would the GC change pointers in the stack? By stopping just that thread instead of the whole program? I think that the only reasonable time to run the GC is while the thread is blocking. But if it doesn't block, then it leaks memory..
I had no idea there is a faster linker available on Linux, good to know. 
I've noticed the bug trend is climbing not so slowly, is there any idea how many of those are serious versus papercuts? I've tried looking at a couple for a few hours, but I fear the ramp up time for someone without compiler know-how seems to be pretty steep.
Wow OK, that is just sad. And evil.
Lots of it is administrative. We opened a bug for every feature flag, for example. I fought really hard to get us below 2000 issues, and almost got it to 1900. But it didn't stay that way :/
&gt; So I don't see the problem with being able to use diverging expressions in any context, since you'll never actually get a value out of them. I'm assuming you're missing a negative there: “... *not* being able to use...” First, You can totally work with `!`: fn id_bottom(e : !) -&gt; ! { return e; } The conceptual problem is that every single programming language is an approximation of Martin-Löf Type Theory. It is going to be a problem in exactly the same way `void` is a problem: Sometimes you have a generic procedure over a product type, and you want to pass it a single thing: `(a, ())` is isomorphic to `a`. Sometimes you have a generic procedure over a sum type and you want to pass it a single thing: `Either a Bottom` is isomorphic to `a`. In C-likes you can't do the first without defining your own unit type, in Ruse you can't do the second without defining your own zero type.
I've been getting back to my plan to build a toy distributed key:value store in Rust. Still very basic, but I've gotten the nodes to load a config, open sockets and spam each other before disconnecting. I've opted to build it with mioco so I can use co-routines, and I'll just manage the distribution of work between threads myself. I'll write up an article once the code is cleaned up ... I'm planning to just post short "episodes" after a series of important commits, so people can follow along with my adventure if they want (and hopefully offer me tips!) :)
Another natural time to stop the thread is when you run out of space in the nursery.
&gt; In C-likes you can't do the first without defining your own unit type, in Rust you can't do the second without defining your own zero type. I'm still missing why this is a _problem_. If you look at the project linked by OP, its entire purpose is to define a single, shared `Void` type so you can do things like `Result&lt;T, Void&gt;` and know that it's statically equivalent to T. That seems totally fine to me, and presumably to the author. Is there a reason why this is not fine? Perhaps my problem is that I've never really thought of `void` in C to indicate returning `()` as a problem. `void *` is certainly a problem because it permits type sloppiness, and `void` to indicate returning bottom is a problem. But neither of those are problems in Rust.
Should be pretty straightforward, only catch is that you have to symlink it rather than copy it to `$PATH`. An example of it working can be found in the [travis build](https://travis-ci.org/arcnmx/cargo-clippy)!
Same [here](https://github.com/gwenn/rustyline) with unicode support (a fork of [rustyline](https://github.com/kkawakam/rustyline)).
Maybe it's just because Rust is young, whereas these things were already figured out in C++ by the time I got to it. I've just read three different articles in the past week all talking about GC implementation in Rust.
I don't think you do. Those are examples of interacting with the hardware, not the OS. Some of the pointers I'm talking about have to be hard coded into the program (VGA memory), others are chosen arbitrarily at runtime (where to put page tables).
Oh, the articles were related, that's why. pliniker published his rfc, making me think about gcs again, which motivated me to finish up this blog post draft that was lying around. (Also some members of the Rust team are starting to look at how the compiler can aid external GCs)
Fixing some panics and formatting errors for rustfmt.
Does `println!("Received: {}", rx.recv().unwrap());` not allocate? (meaning that the machinery somehow computes the maximum size of a buffer that will be necessary)
There's quite a few open issues which aren't relevant any more.
Please ping me on them, and I'll very happily close them. I try to check through old issues every so often for triage, but sometimes I miss some.
&gt; The social problem is arguably more important As a heavy C++ user, I can only agree. If you stray from Boost, Qt or other big names, there are thousands of libraries' source code scattered around on github... most of the Open Source libraries I use in C++ are C libraries.
I don't criticize the choice of D *at the time*. D and Cyclone both were "released" in 2001, so I am afraid there was little precedent for memory safety without a GC, and as you mention it took a lot of effort to crystallize the "Mutability XOR Aliasing" principle in Rust and encode it in the type system. I just note that, right now, D has a GC, and like most languages with a GC (with exceptions such as Nim), its GC tend to give it a spiky latency.
This is beautiful, thanks for making it! :D
Very true, I was just pointing out that the conversion itself should not require allocation. I/O is seriously slow :P
Breaking ground on [`mai`](https://goo.gl/LLYKGW), a `mio` companion library that aims to handle a lot of the complexity/minutia of the readiness model. The goal is to get to a point where you: * Pick a type to represent a 'frame' in your protocol * Provide methods to encode/decode your frames to/from bytes * Define handlers to respond to high-level events like receiving a frame, protocol errors and disconnects. and `mai` will handle the low-level `read`s, `write`s, buffer swapping, (un)registering `mio` interests, correlating tokens to byte streams, etc.
Apologies, I meant to write "Landed RFCs".
I'm mulling over a dirty draft for a 'linter/pre-processor' for D that works on source before the compiler even touches it (it needs a number of annotations on source, currently quite verbose) to make some simple judgments on variable lifetimes (and would catch use of uninitialized variables in several places that std D doesn't) to, for example, prevent a mutable variable to be passed to several threads (not there yet). It doesn't look like that'd be its limit; even though it hasn't been implemented yet I'm optimistic. I only 'get' the borrowing/exclusivity ideas _as presented in Rust_, not theoretically, so I can't exactly port them over. For now the 'share-ready' variables, as checked on function-argument boundaries, _need_ to be entirely `const` (that is, `const(int*)` instead of `const(int)*`), and doesn't support `immutable` yet. D's transitive const/immutable means `Cell` is beyond me. `if` chains and delegates confuse the heck out of the scheme so far; but within a single scope it wouldn't let you pass the same name as shared&amp;exclusive to the same function, for example. No affine types yet, though I got an idea... So yeah, to not break D's existing assumptions, adding something like borrowck entails adding a half-compiler before the compiler, and it has to understand quite a bit of D (for now it recognizes only a certain style that reduces syntactic ambiguity, and needs the co-operation of function definitions to provide needed info through user-defined attributes). Further, it's plugging holes in a cheese crater instead of carving from a whole stone. It needs all those attributes because it knows the general semantic categories of symbols, but otherwise manipulates strings of source code, builds no AST (except for the simple category marking) and accounts zilch for data-flow.
Well "should not" is easy for a human to see, but in code? It can incredibly hard (Rust does not have `constexpr`). Here however it seems that the `println!` invoke a compiler built-in `format_args!` which encapsulates the arguments passed into a `std::fmt::FormatArgument` instance in some way and then delegates to [`_print`](https://github.com/rust-lang/rust/blob/master/src/libstd/io/stdio.rs#L578) which itself calls a writer that is set by [`set_print`](https://github.com/rust-lang/rust/blob/master/src/libstd/io/stdio.rs#L564)... ... unfortunately I could not find where `set_print` is called and thus which type of `Writer` it gets.
AFAIK, this is really limited though: Vim needs to control the buffer size for example, so embedding in something like Atom or XCode is an awkward hack. And neovim will inform GUIs about which line in which window it needs to redraw - which is also not very flexible. Neovim is working on something that is less limited, but it is only a slight improvement in the design. They have to shoulder a lot of legacy. 
&gt; In essence, destructors implemented on a value inside `Gc&lt;T&gt;` can be unsafe. This will only happen if they try to access values within a `Gc&lt;T&gt;` — if they do, they may come across a box that has already been collected, or they may lengthen the lifetime of a box scheduled to be collected. Would it be possible to: 1. poison a `Gc&lt;T&gt;` instance when it's scheduled for collection (or maybe right before calling `drop`) (ie, don't use a bool for rooted/unrooted, but an enum so as to add an extra state or two) 2. `panic!` in `Deref` if poisoned (and offer an alternative method returning `Option&lt;&amp;T&gt;` and `Option&lt;&amp;mut T&gt;` for those who do not wish for a panic) AFAIK one of the issue of destructors or finalizers is accessing an object that is being destroyed/finalized; however with just some extra-state it seems manageable to handle cycles by denying access to the "poisoned" value. As writing destructors/finalizers is pretty rare to start with, it seems a small enough paper-cut.
Full ack. Andrei obviously hasn't coded much Rust, so of course his comparison is going to be biased. And while using GC was the conservative (and for the time right) choice for D, Rust made the bold and right choice to go all in and solve that hard problem first. Given enough time, we can get the other 10% right, too.
Great work! I've got `cargo rustc -- ...` in my history, is there any difference in compilation?
~~&lt;everything&gt;~~ Edit: Sorry, that was wrongly put. What I meant is: for `Unique!T`, if an instance is const/immutable you wouldn't be able to call anything on `Unique!T`'s payload because it all must pass through the wrapper's mutable `opDot`. Essentially, you can't do anything with an `immutable (Unique!T)`, not even read its payload directly. Its runtime behavior is how it becomes useful (it's a runtime-checked affine type wrapper, if I understand it correctly...). [ETA] Dynamic arrays are encapsulated pointer-length pairs, totally normal. There isn't supposed to be any exceptions to transitive immutability that aren't compiler bugs.
&gt; denying access to the "poisoned" value This means including checks in all of the deref impls, which greatly reduces ergonomics and performance. The easier method is to separate out `Drop::drop(&amp;mut foo)`, `foo.fields.drop()` and `forget(foo)`, which might just be possible with the right hooks (might even be possible today)
I don't _think_ there's a reason. Right now the distinction is that a function returning `!` causes the compiler to assume that all further code is unreachable, but a function returning `Void` doesn't. (You can test this by calling, say, `.unwrap_err()` on a `Result&lt;T, Void&gt;`, and noticing that the rest of the function is considered reachable.) The Rustonomicon [discusses the possibility of doing static analyses like this on Void](https://doc.rust-lang.org/nightly/nomicon/exotic-sizes.html#empty-types), and doesn't have any reasons why this can't be done in the future. But I'm not an expert.
This is because I've been slowing down my contributions, and haven't been putting in the work to review the week's PRs. If anybody else wants to take that on, I'm sure Vikram (nasa42) would like help. 
Ah, so that's the connection, thank you. (I've also read Manish's links below in the meantime.) Have you given any thought to whether it's possible to formulate things such that storing a `JS&lt;T&gt;` anywhere is safe and allowed, but in exchange, accessing (`root()`ing) a `JS&lt;T&gt;` reached from a `Root` is only allowed if all the objects traversed in between were traceable by the GC? (Maybe by replacing use of `&amp;T` in the relevant parts of the API with a new thin wrapper `struct JSRef&lt;'a, T: JSTraceable&gt;` type - which inherently can only refer to traceable objects? But it's not immediately apparent to me how you'd accomplish an operation as simple as, given a `JSRef&lt;MyTraceableStruct&gt;`, acquiring a `JSRef` to one of its fields which is also `JSTraceable`. `impl Deref for JSRef` doesn't help with this, because you cannot allow conversion from `&amp;T` back to `JSRef&lt;T&gt;` (even if `T: JSTraceable`) without destroying the guarantee of transitive traceability from a root.)
I am going to use my [parser combinator experiments](https://github.com/m4rw3r/rust_parser_experiments/tree/fifth) in a real application, replacing the recursive-descent-parser with manual state-management I have in my [virtual terminal](https://github.com/m4rw3r/kopparoxid). This will hopefully either confirm or reject the choices I have done in my experiments, and probably also show some pain-points. (Already seeing it a bit when you want to chain things like `or(|m| mdo!{m, char(b'+'); ret 1}, |m| mdo!{m, char(b'-'); ret -1})`, looks kinda ugly with the required closure for state-passing. I will investigate and see if I can make this neater, maybe add special cases for `or` and `optional` in the `mdo!` macro. Come to think of it, I should probably rename `mdo!` to `parse!` since it is not a general monadic-do macro.)
I'm finishing up the 0.1 version of a command-line rust encryption utility I've been working on. It basically copies encrypted local files to a cloud provider directory. https://github.com/jmquigs/greycrypt Feedback welcome! This is my first real rust program, so i'm sure its not idiomatic in places.
This is exactly what I wanted! I was even contemplating adding an issue to `clippy` about this! :)
It could be put in the clippy repo. But I'd rather see a "cargo command pack" with all the helpful new subcommands people have been posting recently.
Ah. I see what you mean. Using a closure works fine, though, so it may be a bug. Submit an issue. I'm not sure if divergent functions need to be explicitly handled by the type system, because you can make a diverging function *without* the annotation. However, there should be a lint to look for this sort of thing, if not in `rustc`, at least in `clippy`.
Well it depends on the types involved. The formatting traits work by giving you something to write to. The implementation of the trait can allocate, as well as the underlying writer, but it's not necessary like it used to be (where the formatting system required you to produce a string). 
I wonder if this is related: https://github.com/rust-lang/rust/issues/25041
I know I've been a bit slower lately. One reason is that there's more work to do than simply the compiler these days. Cargo, outreach efforts, ecosystem work... Rust is a little larger than rustc itself these days. Another is that MIR seems to be taking some time, and other stuff depends on it. I'm not directly involved in that work.
&gt; Is there a reason why ! couldn't be made a legitimate type RFCs are your friend: https://github.com/rust-lang/rfcs/pull/1216
If you put the bound for `K` on the method it works just fine. You also need a Sized bound, and I don't think you can just refer to `S::Output`, but have to say `&lt;S as Index&lt;K&gt;&gt;::Output`. impl&lt;'s,S&gt; Thing&lt;'s,S&gt; { fn pancakes&lt;K&gt;(&amp;self) -&gt; Option&lt;&amp;&lt;S as Index&lt;K&gt;&gt;::Output&gt; where S: Index&lt;K&gt;, K: Default, &lt;S as Index&lt;K&gt;&gt;::Output: Sized { Some(&amp;self.splendiferous[Default::default()]) } } Unrelated, is 'opening brace on its own line when it follows a `where` bound` a thing yet? I've come to prefer it.
Sounded plausible, but no dice. https://play.rust-lang.org/?gist=d4d7d70c03d0a94f5dea&amp;version=stable For now I am using usize rather than being generic with the key so I can get on with the project.
`K` must be a type parameter of the trait or the Self type (which is just `S`).
I'm a big fan of clap and this only makes it better! One thing isn't obvious to me though, is it possible to use the Cargo version number in the `yaml` file? If not, is it a planned feature (I have a terrible habit of forgetting to update both).
This works, thanks. Is there any way to be more DRY though if I'm going to have a dozen functions with the same Index&lt;K&gt;? Constraining the S at the struct definition wasn't yielding fruit either.
Stay cool and drink _RE(d)BOL_.
Oh, it doesn't *shift* anything, it just doesn't use the indexes that are defined for the edges that are given as input. I mean, `add_node` only accepts the node weight as a parameter, it returns a new node index (the next free index). But the input graph may have an edge from 3527384 to 21938, since the edges can be in any order, and need not be sorted. So for adding an edge from K to Z I have to know their node indexes, i.e., their new identifiers in the graph.
Yes perfect. Now I don't have to worry about my builds breaking on nightly Rust
Thanks for good idea! `from_yaml` method returns `App` struct, so you can use `version`, for example: let yml = load_yaml!("yours_yml.yml"); let m = App::from_yaml(yml) .version(&amp;*format!("v{}", crate_version!())) .get_matches();
That's good enough for me, thanks!
Unfortunately, that doesn't work with branching, hence the need for `!`.
What do you mean?
Btw. I've started going around cloning projects to run clippy on and make PRs with the suggested improvements. I find that it's a nice way to a) learn by reading other's code, b) test clippy (I've found some false positives already), c) introduce clippy to others. Next up: Apply clippy to rustc itself. compiletest is already done.
Counterpoint: http://is.gd/B3Yhsu If all paths lead to a generic type, the compiler starts complaining. If all paths lead to `!`, the compiler is perfectly happy. Could rust go without the `!`? Yes. It would just be slightly less ergonomic.
What about expanding the yaml version into Rust code using a build.rs?
Getting the command line params manipulates raw pointers/C strings internally and uses `unsafe`.
Ah, thanks!
Make crater, but for clippy.
Because Rust makes it more difficult to make things like cyclic references within safe code. In C++ it's more of a free-for-all because the safety checks aren't there. A GC library would help to enable that.
I'm not a web service guy, I just write the code. Still the idea sounds great.
There are no requirements on the compactness of the input source, if I understood you correctly. That is, just sending two edges labeled "1-&gt;2" and "4325-&gt;1395642" is perfectly ok. I don't think the graph structure exposes a direct method of using those indices anyway (if they were (1,2), (3,4)).
Some `travis-cargo` integration would be nice.
An often taken approach is to just do that in `lib.rs` and use `main.rs` only to do minimal setup, argument parsing and then calling into the library. (you can build a library along with multiple binaries using cargo in one go)
It means that *rustc* on Windows XP is stuck to that version. As I understand it, compiled code will still work fine on XP.