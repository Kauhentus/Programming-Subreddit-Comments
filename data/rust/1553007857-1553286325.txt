I would put it behind a Mutex in lazy_static and make the tests lock themselves instead of needing a additional cli config.
Indeed, there's [Binet's Formula](http://mathworld.wolfram.com/BinetsFibonacciNumberFormula.html).
you also need to have sufficiently fast SSD and enough RAM.
lock { ... } is just mu.lock(); ... mu.unlock();
How long have you been using it between restarts? It's Java, so it needs to get some usage to properly JIT everything. In my experience it gets much snappier in 10-15 minutes or so. I barely ever restart the process, so personally it's a non-issue.
You're welcome to use parts/all of this: [https://gitlab.com/jrop/rust-calc](https://gitlab.com/jrop/rust-calc) It's quite fun to get into parsing/evaluating expressions.
Try to increase memory heap to 2-4 GB (via \`JetBrains Toolbox -&gt; IDEA -&gt; Settings -&gt; Maximum heap size\` or \`vmoptions\` [https://www.jetbrains.com/help/idea/increasing-memory-heap.html](https://www.jetbrains.com/help/idea/increasing-memory-heap.html)). BTW IDEA and CLion works good on medium projects (\~50 KLOC) my MacBook Pro 2011 with SSD and 10 GB RAM, so hardware requirement is not so high.
Tokens not text, that explains it! Thanks!
Gotcha, thanks!
Yes and yes. The preceding :: is auto stripped out.
So, that struct showed above: &gt;trait Test&lt;'a&gt; { fn get(self) -&gt; &amp;'a i32; } struct TestStruct&lt;'a&gt; { t: Test&lt;'a&gt;, } is actually unusable?
My cargo.toml contains no edition, but I’m using use to bring in other external crates, so it’s definitely working.
&gt;There's an exact formula to derive Fibonacci numbers but it includes the golden ratio as irrational number if I recall correctly. Yes, it's `$(\varphi^{n}-\psi^{n})/\sqrt{5}$`, where `$\varphi=(1+\sqrt{5})/2$` and `$\psi=(1-\sqrt{5})/2$`. Reddit doesn't support LaTeX, so \[picture\]([http://www.sciweavers.org/tex2img.php?eq=%24%28%5Cvarphi%5E%7Bn%7D-%5Cpsi%5E%7Bn%7D%29%2F%5Csqrt%7B5%7D%5Ctext%7B%2C%20where%20%7D%24%5Cvarphi%3D%281%2B%5Csqrt%7B5%7D%29%2F2%5Ctext%7B%20and%20%7D%5Cpsi%3D%281-%5Csqrt%7B5%7D%29%2F2%24&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0](http://www.sciweavers.org/tex2img.php?eq=%24%28%5Cvarphi%5E%7Bn%7D-%5Cpsi%5E%7Bn%7D%29%2F%5Csqrt%7B5%7D%5Ctext%7B%2C%20where%20%7D%24%5Cvarphi%3D%281%2B%5Csqrt%7B5%7D%29%2F2%5Ctext%7B%20and%20%7D%5Cpsi%3D%281-%5Csqrt%7B5%7D%29%2F2%24&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0)).
I love this project - I'd been on the Game Boy dev scene since the late 90s, and it seems like all us emulator people have found eachother again as rustaceans. Any chance you'll be building this for wasm, maybe even with glsl?
Yes, the channel is public
It's possible that this works, but in that case it's a bug, because it's not supposed to work (as far as I know), are you absolutely certain that for the other crates there is no `extern crate` declaration, and that there are no local modules with names that match the "imported" crates?
Probably learning along the way?
Yup I have a couple crates that pull in SDL2 that are one file and have no extern crate in it. I thought if you had NO edition, you use the latest from the compiler version you have. In this case I would be using 1.33. Which has the new import system. I’m using other new features too, I’m not sure why that would be a bug.
Thanks this is really helpful.
According to the Rust book, [a missing edition is equivalent to 2015](https://doc.rust-lang.org/cargo/reference/manifest.html#the-edition-field-optional), so it does seem odd that you can \`use\` things without \`extern crate\`.
My config is: ``` -Xms4096m -Xmx4096m -Xmn2g -Xss128k -XX:MaxPermSize=1536m -XX:ParallelGCThreads=20 -XX:ReservedCodeCacheSize=240m -XX:+UseConcMarkSweepGC -XX:SoftRefLRUPolicyMSPerMB=50 ... ```
I'm not restarting it at all.
&gt;it might be the problem It's not a problem for virtually any other application I use.
as previously stated, JetBrains IDEs have a high minimum requirement. They're not efficient at all. But they can work in a pleasant manner with the right hardware.
I asked about this before: [https://www.reddit.com/r/rust/comments/aslb1w/implicit\_borrows\_im\_guessing\_its\_been\_discussed/](https://www.reddit.com/r/rust/comments/aslb1w/implicit_borrows_im_guessing_its_been_discussed/) If your interested you can read the comments there. I eventually found some very limited discussions about the issue in the Rust forums, which seem to be the best place for a deep discussion about something like this. (Which isn't to say we shouldn't discuss it here on Reddit, nothing wrong with that.)
 use super::super::to::the::root::gl;
Would you know if there is a RIIR of `offlineimap` in the works?
Not as far as I'm aware, no. But that'd be awesome! Or better yet, a copy of `getmail`.
Understandable. Copy is a subset of Clone (cheap and implicit), so it's easy to miss that "clone" is usually the word you want when explicitly calling a method. (`copy_from_slice` is the one exception I can think of at the moment)
Here are some more approaches. `map` is pretty general, and `match` is the most general of all: let with_ref: Option&lt;&amp;char&gt; = Some(&amp;'a'); let by_val1: Option&lt;char&gt; = with_ref.cloned(); let by_val2: Option&lt;char&gt; = with_ref.map(|c| *c); let by_val3: Option&lt;char&gt; = match with_ref { Some(c) =&gt; Some(*c), None =&gt; None, }; 
Definitely. My experience learning Rust iterators, and many other things in the language, is constantly discovering that there's already a method that does what I want. I remember writing manual loops like this one before finding out about \`enumerate\` too.
&gt; I’m using other new features too, I’m not sure why that would be a bug. Any features that required being in the 2018 edition are available in the 2015 edition. The number of features that are 2018 exclusive is pretty small if I recall correctly. Because you do not have `edition = 2018` in your `Cargo.toml` file, you are not using the 2018 edition which means you're using 2015 paths and import logic with all of its quirks. 
When learning Rust my guess was that "automatic borrows" (or "discarding ownership" as some of the language maintainers have called it) would have been part of the language. I had to play around on the Rust Playground for a few minutes to convince myself it wasn't. I've since thought a lot about this, and I think there's a few different perspectives to consider this potential feature from: **From the perspective of a function user** \- From the perspective of a function user the ship has already sailed on this feature; **it is already possible to create a function that accepts both `T` and `&amp;T`**. I've seen people argue that they wouldn't want to pass `T` to a function when they actually meant to pass an `&amp;T`, but this is already possible and wont be going away, so I don't find this argument worth much consideration. Implicitly converting ownership into a borrow would not have a performance penalty, as far as I understand. Adding automatic borrows to the language would encourage the creation of functions that accept both `T` and `&amp;T` and this would affect the users of functions, because there would be more functions which accept both. I think this would be a good thing, but this is a matter of opinion, I can understand that some might think more polymorphic functions would be a bad thing. **From the perspective of a function author** \- This feature would mainly affect function authors. We can already write functions that accept both `T` and `&amp;T` using traits, but it could be made much easier with implicit borrows. I originally wanted automatic borrows when implementing `Add` for a type I created. I had to do what the standard library does, I had to implement `Add` for `T` and `T`, for `T` and `&amp;T`, for `&amp;T` and `T`, and for `&amp;T` and `&amp;T`. It was just the same code copy and pasted 4 different places; awkward to say the least! With automatic borrows I could have just implemented `Add` for `&amp;T` and `&amp;T` and been done. Automatic borrows would take away an authors ability to create a function which *only* accepts references. A lot of care goes into designing the best APIs, and there may be times where I want to accept only `&amp;T`, but not `T`. With automatic borrows I could no longer do this. There is little technical difference between ownership and a borrow (in the context of automatic borrows), but there may be a big semantic difference that the author wants to use the current explicit borrow syntax to convey. **From the perspective of an interested user, a middle ground** \- Users of functions often read the source code of the function their using. They are not the author, but the implementation matters to them, because they read the implementation. A new Rust user, who reads the implementation of a function, might get confused about the automatic borrows. "Why does it accept ownership when it says it takes a reference?" they might ask. Lastly, traits are the primary method of achieving polymorphism in Rust. Automatic borrows would, in some sense, add another way of achieving polymorphism. It's kind of nice to look at a function and see that it's not using traits, and thus it's simple and straight forward: "It doesn't use traits, and it says it accepts a reference, thus it only accepts a reference, simple." Or you might see a function accepts a trait, and then you know it is polymorphic and can accept a number of different types.
Ye most likely. Right now it's mostly just a wrapper around the *image* library with a fancy UI. But i'm thinking of making hardware accelerated actions for it since I'm trying to learn glsl, so expect more to come. and about .raw formats. The problem is that there's not one raw format, each camera maker have multiple implementations of their own. So it's more logical to leave the encoding to some other program. 
This stuff is way over my head. What's a good book that would help me understand what he's talking about?
You're not 5, but you didn't provide MVE, link to your source, ... so we're trying to guess with crystal ball.
That seems very confusing. It looks like that’s the case though. “Only some new features are hidden behind a feature flag” is not... great. Now to find out why use is working on other places 🤔
Check out the itertools crate for a lot more iterator goodness.
Hi, this 24-line program compiles to 360k executable on Windows via `cargo build --release`. Is there a simple way to reduce the size? Maybe somehow use `toml` without `serde`? https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ab84679847b7afb5a1cd459f538105d1
&gt;why would I ask a question and give the error for a different statement? Am I 5? When you have a problem like this one, engineers you ask for help will wonder how it could possibly have happened. How does the `use` statement break? So they will ask the only things they can think of, and those things will often sound stupid or even condescending. You should take these questions seriously anyway because one of them is most likely going to be exactly right, even if it sounds like a mistake you are 100% certain you haven't made.
Re “By default, Rust will throw an exception if any arithmetic operation overflows.”, it is true only for debug mode, not for release mode. 
&gt; all us emulator people have found eachother again as rustaceans What makes Rust particularly attractive for emulator development?
I don't find this argument compelling, because in deciding whether or not to implement automatic borrows we are *not* deciding whether explicit borrow syntax is required at the call site, but whether the author of a function must use traits for the implicit borrow. *It is already possible to have a function `f` such that you can call `f(&amp;x)` with a reference, or you can call `f(x)` and `f` will implicitly borrow it using traits.* In other words, it seems to me you're arguing that we shouldn't have implicit borrows, but we already have them. You're adding arguments to a debate that has already been settled. The debate is now about whether "automatic borrows" should be achieved with a little compiler magic, or with traits like we currently do.
yes but you pass in the object to mu.lock &amp; mu.unlock
Can you give an example of this? I can't conceptually understand why it would make a difference. If I pass \*ownership\* to \`println!\`, it's as though I'm saying "I'm giving you X, use it and then throw it away." If I pass a \*reference\* to \`println!\`, it's as though I'm saying "I'm giving you X, use it, and then give it back to me so I can immediately throw it away." Is there really much of a difference between the two cases?
`data` function here is fine due to `&amp;mut self` bound, requiring exclusive access.
Yeah, it is possible that it isn't worth it. I think it is worth investigating and at least getting an idea of what the numbers are like, though!
I'd say 0 cost abstraction and a type system good enough to encode only valid states
Please edit the post with the *location* of the job, or if remote, the countries from which one can operate.
Closer to the metal, but also thread and type safety. C and C++ are both fine languages for emus, but they're often tricky at the interfaces between the hardware's components (CPU to MMU, GPU, keyboard, etc.) Very easy to make little errors with them referencing one another, and hard to abstract. Higher-level languages, on the other hand, are very easy to write things like the emu's menu, controller interface, screenshots, etc. in, but all that extra abstraction penalty becomes crippling at the low level. If I'm writing in js something as low as a CPU instruction set, even something very simple becomes much more expensive, e.g. "SUB C" says "subtract C from A, set the subtract flag, increment the program counter and run this in 4 cycles", which are all things that implicit casting, GC, even the lookup for the method itself, *might* make more expensive. JS devs like to imagine that the engine will optimize everything amazingly well, and that's a fine philosophy for a form on a web page, but for CPU instructions it's very poor.
👍 One thing I can't recommend enough for people just learning rust is rustfmt and clippy. Rustfmt will take your project and reformat it to match the rust standard. It can be a great help for others reading your code, including your future self. Clippy identifies common discouraged patterns and suggests. It's not perfect, but it can definitely help learners discover common idioms they might not have known about. It would have even caught [zipping with the length](https://rust-lang.github.io/rust-clippy/master/#range_zip_with_len).
Well it's better than "we completed this feature but for marketing reasons you can't use it for another 2 years". That's *terrible* 
Comments or Topic? I see only a single comment with a negative score; and the comment is not particularly useful. The topic itself may have garnered quite a few downvotes for two reasons: 1. Globals are a touchy subject. 2. The interface used to be unsafe *to use*, which is a big no-no for a library being promoted. Number (2) has been corrected, so hopefully no new downvote will occur because of it, however number (1) may still cause downvotes.
&gt; The reason we generate so many individual tests, is that it turns out that quite a lot of crate tests in the rust ecosystem don't actually work. I find this comment rather concerning. I would have expected all crates on crates.io to pass their own test-suites, at least in the default configuration.
&gt;My emulator actually uses quite a few Box objects. Mostly this isn’t to allocate memory, but to assign data a fixed memory location so I can safely take mutable pointers to it. You might consider using the newly stable [Pin](https://doc.rust-lang.org/std/pin/struct.Pin.html) API!
&gt; Compilers try to avoid emitting division instructions, since they are slow. However, division by a non-constant defeats many of the optimizations it uses. Interestingly, your example appears to trigger a bug in an optimizer outsmarting itself trying to make code branchless, not realizing it made the code slower even assuming that branch predictor always mispredicts this branch. Division is very slow. That said, replacing modulo with subtraction makes sense, after all, that's all you need to do.
I largely agree, but I will say that the implicit borrow using traits is slightly different, since `f(x)` _still moves `x`_, whereas autoborrowing would have it borrow `x`. This is a choice made by `f` to allow for people to waste owned objects this way. The debate is "settled" by allowing functions this choice, which is not the same thing as having all functions do it. Your argument applies better to `x.f()` since it's not always clear what the move is there.
&gt; I have no idea what they do with all those resources. JVM
I've got two `&amp;[u8]` arrays. What's the best way to see which one is "bigger" when interpreted big-endian?
Holy shit you need a fucking account to change settings. I hate modern software.
Not strictly unusable, but it's pretty unwieldy. See also this [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=683555f61bb79bd72ea5500251b3c52b) (I changed the lifetimes around a little bit).
"Best" depends on your goals. The simplest way is to compare byte by byte. If the lengths may be different, you also have to define whether the shorter will be padded on the least or most significant. In keeping with the "simplest" theme, I would special-case different lengths. But if you want better performance, I'd certainly consider processing multiple bytes at once, maybe even switching to C or inline assembly for SIMD.
I am working on a cli program that dynamically generates and compiles audio synthesis and processing modules. The user can write these modules in rust, then the program compiles them on the fly using cargo. The user can then route modules into eachother and then to an output, which can either be an audio file or just the system's audio output device. The eventual goal is to make it able to do everything a convential DAW can do, albeit in a more low-level way.
Well, memory isn't free when you move data but it's also not using any extra. But yes smaller scopes are safer usually. If data only lives as long as it has to and no longer it saves memory.
&gt; Is there a simple way to reduce the size? Not currently. I wish the static linker was smart enough to remove dead code but it's not.
I'm not sure how an implicit borrow can be implemented with traits, would you be willing to write a minimum example? As far as I know the only time something borrows implicitly is when something takes &amp;self and while I do agree that this counts as an implicit borrow, all the other params have explicit information about them and it's usually obvious by the name if a function mutates (assuming it's well designed). Eg, iter vs iter_mut, push vs as_ref. Sometimes I forget what types the hashmap API takes though. Entry vs just regular .get
a lot of my time don't with Rust has been reading the Iterator docs
Session types! Neat. I wanted to do this, but I hated working with IMAP lol so glad someone built a session type imap client.
&gt; You can't replace POSIX tools with non-POSIX-compliant tools and expect things not to explode. Let's be clear these also must be *GNU* compatible, especially for linux but realistically all unixen, which is a both a much larger ask and a shifting target.
&gt; You can't replace POSIX tools with non-POSIX-compliant tools and expect things not to explode. Let's be clear these also must be *GNU* compatible, especially for linux but realistically all unixen, which is a both a much larger ask and a shifting target.
I meant if you move something then its lifespan is shorter, do it will be freed sooner than if you borrow it and it's still available after the borrow, right?
Hypothetically yes but you could have a multithreaded example that just sits on its values for 24 hours before doing anything. Of course this isn't practical but typically yes passing ownership tightens the scope on data.
I guess it would depend on the implementation. I picture implicit borrows (which is the term I prefer to use) as actually giving up ownership in the callers scope, but then implicitly converting it into a borrow in the called function. Some on the language team have called this "discarding ownership" which describes this behavior well. From the callers perspective, it would be giving up ownership, which is only a compile time construct anyway. You cannot accidentally give up ownership, because if you do end up using the value again you'll get a compile error and then have to explicitly make it a reference, which is just one character, no big deal. To me, it's not really about the \`&amp;\`, that's just one character and it doesn't matter much either way. The main advantage of implicit borrows would be in trait implementations. The \`Add\` trait for \`u8\` (and other number types) has to be implemented 4 times, for each combination of \`T\` and \`&amp;T\`, which is a lot of awkward code duplication. Implicit borrows would facilitate having a single implementation. I do strongly agree with your other point, with implicit borrows we would no longer have the choice of accepting *only* references. I feel like this is a strong and objective observation. We would lose the choice, but not really gain anything new, other than some terse syntax. Although I haven't made up my mind about the feature as a whole, I do agree this is a con.
I'm not quite sure what you are looking for and what `rustc_private` or `cc` have to do with it, but for the enum, maybe you want something like this: #[repr(i32)] pub enum Foo { A = 0, B, C }
For that matter, the Shunting Yard algorithm is fairly straightforward to implement and gets you postfix notation from common algebraic notation. In turn, postfix notation is pretty easy to turn into an AST.
Maybe fn big_endian_cmp(a1: &amp;[u8], a2: &amp;[u8]) -&gt; Ordering { // Lexicographic comparison in base256 let num_digits = a1.len().max(a2.len()); let digits1 = repeat(0).take(num_digits - a1.len()).chain(a1.iter().cloned()); let digits2 = repeat(0).take(num_digits - a2.len()).chain(a2.iter().cloned()); digits1.cmp(digits2) } 
[https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1f9b58b21137a8d05a97f4a8e42f6627](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1f9b58b21137a8d05a97f4a8e42f6627) In this Rust Playground example, `Borrow` is implemented on `Bar` by a blanket implementation in `core`. Notice that `f` can accept both `&amp;b` and `b`. Note that I mean "implicit" from the callers perspective. In the first case: the caller has ownership, makes a reference, and then the function operates on that reference. In the second case: the caller has ownership, passes ownership, and then the function makes a reference and operates on that reference. In both cases the caller has ownership and the function ends up operating on a reference, even though what happens in between might be a little different. Remember that implicit borrows would not force you to pass ownership, you could either pass ownership or a borrow and the function would end up operating on the borrow only. From a higher level perspective, traits let us create polymorphic functions, we can make functions that operate on `T` and `&amp;T`, a function that accepts more than one type is polymorphic. Implicit borrows would merely be a terser syntax for making a polymorphic function in this one case, but with or without implicit borrows we will always have polymorphic functions.
The problem isn't that the rectangle instance isn't in scope, but rather that the `Rectangle` type isn't in scope. If you declare it inside the `main` function, it can't be used outside of it.
&gt; Any chance you'll be building this for wasm, maybe even with glsl? Shameless plug, I wrote an NES emulator in Rust too a few years back, and [I have a WebAssembly version of it here](https://koute.github.io/pinky-web/). (:
Could I do this instead? ``` fn big_endian_cmp(a1: &amp;[u8], a2: &amp;[u8]) -&gt; Ordering { if a1.len() &gt; a2.len() { return Ordering::Greater; } else if a1.len() &lt; a2.len() { return Ordering::Less; } else { return Ord::cmp(a2, a1); } } ```
If by "compile SPIR-V" you mean "manually emit SPIR-V" then I'd probably be a good idea to just use the [`spirv_headers`](https://crates.io/crates/spirv_headers) crate which already has all the relevant definitions.
Well no, my point is that if *some* are behind a feature flag, all should be. That’s one thing C++ versioning does correctly.
https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=8fe4b4c7d2102803da9f6f2d08f21cfa Sorry but I just don't agree that this example is showing what you're intending it to. Yes, the function accepts both borrowed or owned values but you're still passing ownership at the callsite which is exactly what I'd expect from rust. In the playground link above you'll see that you can't re-use `b` after passing ownership to the function even though the function borrows the data immediately. &gt; In other words, it seems to me you're arguing that we shouldn't have implicit borrows, but we already have them. This isn't automatic borrowing. It's using dynamic dispatch to allow a function to take something that can be borrowed but doesn't have to be. You couldn't use any properties of that trait now because the only function rust knows is `.borrow()`. Ex: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d5bd563212304dc2ccaf8f958f16cff1 ---- What op is suggesting is something like this: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9574942bf374835ff28bdaab7837a959 This would allow no extra syntax to declare that a value is being passed by reference. I prefer that `&amp;` or `&amp;mut` are required at the _callsite_ because it lets me read and understand my code much faster. I can look at the body of a function and understand where, if anywhere, my data is being mutated, used somewhere else, or completely removed from my scope.
But that's not how Rust works. There's a new compiler version every 6 weeks. Why would we implement features only to have them be unavailable for months or years until the new edition is finished? Perhaps my initial wording was wrong. Editions aren't really about features, they're about *breaking* features. The only features specific to an edition are the breaking ones. The other stuff highlighted in the release announcement shipped many releases ago and is really just marketing fluff.
This works if you drop the \`.unwrap()\` call.
This is something I'm curious about as well. I have a machine translated version of nurses that I'm slowly converting to be more rustic. I would love to see a Linux system with some system libraries or utilities swapped out just to see if it explodes. exa and similar tools aren't POSIX compliant to support this kind of total replacement, but things like relibc and other Redox projects meant to support existing software might be good to keep an eye on.
"Not found in this scope" is not a lifetime issue or a borrowcheck issue, it's a code structure issue. type declarations generally belong at module level (unless they are associated with a trait).
mu is the object
C != LLVM
Updated job posting. 
You're right, I missed that. Was piecing together my playground code and his example :)
Take a look at this project: [https://github.com/mesalock-linux](https://github.com/mesalock-linux) and specifically this: [https://github.com/mesalock-linux/mesabox](https://github.com/mesalock-linux/mesabox)
&gt; you need a fucking account to change settings. [Um, no](https://www.jetbrains.com/help/idea/tuning-the-ide.html#configure-jvm-options)? You change a config file and then restart the application
You can try it out now: [https://vlang.io/play](https://vlang.io/play)
&gt; Not strictly unusable I must be pretty annoying, but does it mean that there is an example of such structure that can be actually used? &gt; playground Yeah, that's something I've come into with my own experiments - reason why am I asking about actual usage :)
More specifically, it's that the `fn area (rectangle: &amp;Rectangle) -&gt; u32 {` line can't see `struct Rectangle {` if you move it inside `main`.
Well, you're not using curve25519 in your Rust code at all. Are you sure you're comparing the right things?
I'd look into [this](https://hoverbear.org/2016/10/12/rust-state-machine-pattern/).
Love that!
that's interesting. thank you.
I guess there's different ideas about exactly what is being proposed. My idea of implicit borrows is that `f(b)` would give away ownership of `b`, but rust would implicitly give `&amp;b` to `f`, and then `b` would drop once `f` returns. From the callers perspective nothing has changed. If you give away ownership, you know the value you gave away ownership of will be dropped when the function you called returns. This is how it is now, this is how it would be if we had implicit borrows. If you gave away ownership, presumably you actually meant to give away ownership (if it's a mistake the compiler will tell you). As the caller, you don't care if the function makes an explicit reference to the value you gave it ownership of, or if the compiler makes an implicit reference; you now longer care about the value, you have given away ownership, you meant to give away ownership, and now you're done with the value. From the called functions perspective, it would receive a reference, and that reference would be valid for the duration of the function. Maybe the caller explicitly gave a reference, or maybe the caller gave ownership, but since the called function doesn't want ownership Rust *could* implicitly discards the ownership. Either way, the function receives and operates on a reference. (Some of the language team have called this proposed feature "discarding ownership", a good description.) With or without implicit borrows the following can never be made to work: ``` f(&amp;b); f(b); f(&amp;b); // error ``` With our without implicit borrows, the following can be made to work work: ``` f(&amp;b); f(b); ``` From a callers perspective, nothing is changed by implicit borrows. The primary difference is that the author of the polymorphic function would have a more terse syntax to use to make the polymorphic function. I'm not sure exactly what others have in mind when they suggest this feature, but this is how I picture it, and all the arguments I have made for or against this feature are based on this understanding. By my interpretation, this is what the OP had in mind as well, but that interpretation may be wrong.
&gt; https://vlang.io/play Thanks for the effort Definitely will try it
Thank you all . I understand that my question was indeed trivial. 
See for example https://www.reddit.com/r/rust/comments/at6793/why_is_the_iter_version_performance_so_much/, https://github.com/rust-lang/rust/issues/58622 and https://medium.com/@robertgrosse/how-copying-an-int-made-my-code-11-times-faster-f76c66312e0f.
I've been told that lifetime annotations don't affect scope. However, here's a gist I'm confused about: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b9b66b19b34ccec5dc8a1ba313333ee0](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b9b66b19b34ccec5dc8a1ba313333ee0) &amp;#x200B; Since the \`a\` is created within the function, shouldn't it be dropped by the end of the function block? I would think that if I can put a \`&amp;'a str\` as a return type and that it works, that lifetime annotations do actually extend scope. &amp;#x200B; What's going on here?
See - we all write emus! Nice use of the shaders, too. So many web emulators see the display as an afterthought and simply chuck it all up on a canvas. I want to write a recompiler to build the roms themselves, and detect code that can be optimized further on a modern machine (like big data reads/writes, or loops that repeatedly ADD because the CPU had no multiple instruction). Then I can achieve my dream of running 600 parallel games of tetris on my phone!
Not sure if this still works, but I wrote a tool to synchronize my GMail account into a PostgreSQL table: &amp;#x200B; [https://github.com/djc/mailsync](https://github.com/djc/mailsync)
Java code hade one redundant line, fixed.
As /u/K900_ mentioned, you didn't include any curve25519 calls in your Rust example, so it's impossible for us to reproduce your final output and see what you got. To make it easier to help you, please put print statements after every line in both programs, and figure out where exactly they start to disagree. If you've found the disagreement, but you're not sure why it's happening, then you can give us complete runnable code in both languages, and we can try to help you understand what it's doing. Here are a couple ways to convert unsigned bytes to signed bytes, in case that's tripping you up: fn unsigned_to_signed1(unsigned: &amp;[u8]) -&gt; Vec&lt;i8&gt; { let mut signed = Vec::new(); for u in unsigned { signed.push(*u as i8); } signed } fn unsigned_to_signed2(unsigned: &amp;[u8]) -&gt; Vec&lt;i8&gt; { unsigned.iter().map(|i| *i as i8).collect() } 
I've been programming for a long time but I have no idea how people create emulators. Does anyone have a good reference for how it's done?
I can only handle so much convenience. I think I maxed out when I found out about the upcoming \`Vec::drain\_filter\` iterator.
&gt; I must be pretty annoying, but does it mean that there is an example of such structure that can be actually used? I don't know enough to answer that precisely, but given that you are forbidden to even initialize an unsized struct, there are only very few possibilities. The only one I can think of is to transmute the struct from e.g. a bytearray, but whether that can ever be done soundly (safely?) I do not know. 
You're using a linear search for memory mappings - have you considered something like a segment tree? It should be just as fast as your current implementation in the best case, and much faster (O(logn) instead of O(n)) in the worst. 
SPIR-V uses integers instead of variable names and an enum is used to define names and measure the BOUNDS which is also nice.
This may work, thanks.
Some of this may be a bit technically incorrect, but I'll explain it as best I can. The lifetime parameter is very much like a generic type parameter. Basically, your function is saying that "I'm going to return a `str` with *some* lifetime 'a, even if I don't know the lifetime until compile-time." In this case, the lifetime is only tied to the output, which has a `'static` lifetime. Therefore, the compiler will always treat 'a as 'static for this function. Case in point: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=62413fe23df4bd96871f02769b250063 Notice how in this example, you can take the output of test(), and pass it to print(), which expects a `&amp;'static str`, however you cannot take the output of test2() and pass it to print(), because now the lifetime is tied to the input of test2 (and the input of test2 does not have a 'static lifetime).
This is so cool, I'll probably take it for a spin one of these days (:
Ok I have figured it out with a help from a friend. The thing to be done is this 'resvg = {version = "0.6.0", features = \["cairo-backend"\]}'
RefCell is what they should be using.
start here [https://yizhang82.dev/nes-emu-overview](https://yizhang82.dev/nes-emu-overview) then read this [http://nesdev.com/NES%20emulator%20development%20guide.txt](http://nesdev.com/NES%20emulator%20development%20guide.txt)
I am not sure about that `Ints.toByteArray(0)` does, so I'll replace it with `[0u8; 4]`. Using [RustCrypto](https://github.com/RustCrypto) crates you can write this code like this: use blake2::{Blake2b, Digest}; use sha3::Keccak256; const SEED_PHRASE: &amp;str = "seed"; let hash1 = Blake2b::new() .chain(&amp;[0; 4]) .chain(SEED_PHRASE.as_bytes()) .result(); let acc_seed = Keccak256::digest(&amp;hash1); println!("result bytes: {:?}", acc_seed); BTW are you sure you want `Keccak256` (the variant before padding changes) and not SHA3-256? Also I don't get why the hell you get negative numbers in the result.
You can parallelize the recursive calls, like this [rayon demo](https://github.com/rayon-rs/rayon/blob/master/rayon-demo/src/fibonacci/mod.rs)... but that's horrible. I only added that to hammer on rayon internals.
Fun fact: I'm responsible for the terrible pun in the title only to later figure out that the author was to young to know what it punned on...
Signed bytes... java doesn't have unsigned number types (well, there's char - effectively a UTF16 character)
&gt; I think that learning Rust is making me write better C code Heh, this isn't strange at all in my experience!
&gt; I think that learning Rust is making me write better C code Heh, this isn't strange at all in my experience!
[This looks like a very detailed overview into what an emulator is and does.](http://emulator101.com/) [This one is way less detailed, but still conveys the important parts, and gives a working example.](http://www.multigesture.net/articles/how-to-write-an-emulator-chip-8-interpreter/)
Thx man, you're the real mvp
Sure thing. VMs like these are really interesting to me.
For small range numbers with i32 type, if you want to implement `if x &gt;= a { x - a } else { x }` you can do `x - (a &amp; (x.wrapping_add(std::i32::MIN.wrapping_sub(a)) &gt;&gt; 31))`. Here's the [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=aedcab3a08fa2aab36f521418de944a6) link. It compiles to efficient code (5 instructions).
Oh nice. You went the no enscrpiten route like I did with my gameboy emulator.
&gt; So many web emulators see the display as an afterthought and simply chuck it all up on a canvas I feel attacked. Just kidding, but I don't understand the alternative using shaders. How would that work?
2d canvas vs rendering with webgl shaders. You can look at their source to see an example. 2d is easy, because it works similarly to the hardware itself. Shaders are a bit trickier because it takes more reasoning about how you want things to look to the end user. You can go even further and write complex shader logic that executes in the gpu by writing it in glsl My goal is to write a shader that renders the tiles directly inside the glsl, and slides them around positionally. You wouldn't get the same scanline accuracy as the original hardware (eg you couldn't replicate the sprite-limit on one horizontal line without a ton of work), but things would render and slide around incredibly smoothly at a negligible cpu cost, since you'd be doing it in the gpu. 
&gt;The references are hard for the compiler because of aliasing, and (even in C) taking the address of a variable often means that the value must live in memory as opposed to a CPU register Can the rust compiler optimize that away (since safe rust has no aliasing)?
Awesome! Thanks!
When I think "test case" I think of the function that the parameters are applied to, so I was surprised when I looked at your README and saw more than one `test_case` attribute being applied to a single function. How about `test_param`, since you're declaring a set of parameters to call the function with? Also, there are some typos in your README: &gt; "when both operands are possitive" &gt; (sicne 0.2.0) I think you meant "since" Overall, I think you could use a proofreader. Here are the corrections I noticed being necessary: &gt; This crate provides **a** `#[test_case]` procedural macro &gt; Under the hood, all test cases that share same body are grouped into **a** `mod` &gt; Additionally you have to enable **the** `proc_macro` feature and include **the** crate. &gt; Don't forget that procedural macros are imported with **the** `use` statement: &gt; you can pass the expectation as **a** macro attribute &gt; followed by string literal at the end of **the (or your)** macro attributes. &gt; Note: **The** word `inconclusive` is only reserved in test name given after `::`. ...and, for this one, I have trouble suggestion a way to improve the "unless they contain `=&gt;` part because I'm having trouble grasping what's going on in the example snippet. &gt; Attributes and expectation may be any **expresion** unless they contain `=&gt;` As for this: &gt; If test case name (passed using :: syntax described above) contains word "inconclusive", generated test will be marked with #[ignore]. I don't like it because not all ignored test cases are inconclusive, so requiring that they be marked that way may mislead new developers. (Often, a test is perfectly conclusive but should be ignored for other reasons, such as a project that's using Test-Driven Development but knows they won't get around to implementing a block of things for several months and don't want the intended "not yet written" failures to interfere with some kind of status check.) Ignored means ignored and you can't infer any further motivation from it, so it should be "ignore". 
Eh. The macOS coreutils are derived from FreeBSD and are definitely not GNU compatible.
Technically not even true then, since rust doesn't have exceptions.
A regularly curated [list of Rust books](https://reactdom.com/rust)
Lovely post! Here's some more resources for [learning Rust in 2019](https://reactdom.com/rust)
&gt; is it worth converting Fibonacci series in concurrent programming Rust ? For learning purposes? Absolutely! Fibonacci is a cool problem that has a couple different approaches, which gives the implementation a lot of room for experimentation. For practical purposes? Absolutely not. Fibonacci is trivial to solve on a single CPU. The naive version of Fibonacci is amenable to fork-join parallelism, but the time complexity is still O(2^n ) despite parallelism. The dynamic programming version is not amenable to fork-join because both branches need to share the memo. Most of your time will be spent fighting over the lock since the actual math is only like 2 instructions. This has a time complexity of O(n), so it'll beat even the parallel naive version for large n. It has a space completely of O(n) though. The iterative version is O(n) time and O(1) space, plus it has essentially no overhead, so it's even better than the DP version. That version is not amenable to fork-join because there's no way to split the problem up. The constant time version is obviously best for large n, and there's no room for parallelism there. /// Iterative Fibonacci fn fib(mut n: usize) -&gt; usize { let mut a = 0; let mut b = 1; while n != 0 { b = b + a; a = b - a; n -= 1; } a } &gt; concurrent programming Do note that [concurrency is not parallelism](https://youtu.be/cN_DpYBzKso).
Their own checking isn't 100% accurate yet, turn on "analyse with cargo check" in the plugin options. 
This has much better autocomplete than any racer based experience. 
If you worry so much about the cost of copying the Machine type, you may consider seperate the machine and its state, so instead of copying Machine around you will have a MachineState type spawned by the machine with all its moving parts. The MachineState type may bring a (immutable) reference to the Machine for convinence. That'a basically what I did in my last toy project (a naive regex engine) however no idea if it's possible in your project though. &amp;#x200B;
I like the distinguished badge color for 1.0 crates on crates.io . It pushed me to release 1.0.
The fact that gen\_range() is constrained to produce values in \[1, 3\] is not known to the compiler; as far as it can prove ai can have any value from 0 to 2\^32-1, and you don't have match clauses for any of those values. You need a default clause, and since you know it's impossible it can/should just panic.
I made a small maze generation + pathfinding app using rust and webassembly last week. I spent some time optimizing it today, and I'm going to try and improve the UI and perhaps add animated pathfinding. It can played around with here if anyone is interested: [https://www.brick.codes/mazes/](https://www.brick.codes/mazes/) left and right click on the grid to set source &amp; destination for pathfinding (set a pathfinding algorithm with first dropdown) try setting the source &amp; destination to the same cell to get a pretty visualization. --- Oh, one performance tip: ```rust let mut new_path = cur_node.path.to_vec(); new_path.push(cur_node.i); ``` is much worse than: ```rust let mut new_path = Vec::with_capacity(cur_node.path.len() + 1); new_path.extend_from_slice(&amp;cur_node.path); new_path.push(cur_node.i); ``` in the second case, we potentially avoid a costly re-allocation. My pathfinding speed went up 2x making this change. --- thanks for reading, let me know if anyone has any questions about any of the above :) 
You can use [`unreachable()!`](https://doc.rust-lang.org/std/macro.unreachable.html) to panic in the impossible match arm.
try match ai{ 1=&gt;println!("Computer threw rock!"), 2=&gt;println!("Computer threw paper!"), 3=&gt;println!("Computer threw scissors!"), _ =&gt; unreachable!(), //explicitly declare that the program should not be able to reach this state. If it does then it will panic. }
You can use `${fileBasenameNoExtension}` to refer to the current filename.
First, a bug the compiler won't catch, you want gen_range(1,**4**) not gen_range(1,3). The range doesn't include the high endpoint. See the documentation [here](https://rust-random.github.io/rand/rand/trait.Rng.html#method.gen_range). The compiler probably knows that `gen_range(1,4)` returns a result in `{1, 2, 3}` as a result of looking at the implementation of `gen_range`. The thing is the language is specifically designed so that it *doesn't* surface that information to the programmer in any way except using it to make your code run faster. The compiler is purposefully as dumb as possible about things like this to - Make the messages it gives you easier to reason about - Avoid implementation details changing resulting in the public interface changing All the compiler is "willing to admit to knowing", is that `ai` is a `u32`, and `u32`s can take any value between 0 and 4294967295 (2^32 - 1) inclusive. In your match statement you've told it what to do with values `1`, `2`, and `3`, but not with the rest of them, so it's complaining. There are two ways you could fix this, one is to tell it to give you an error otherwise let ai: u32 = rand::thread_rng().gen_range(1,3); match ai { 1 =&gt; println!("Computer threw rock!"), 2 =&gt; println!("Computer threw paper!"), 3 =&gt; println!("Computer threw scissors!"), _ =&gt; panic!("Gen range returned an unexpected value!?"), } The other is to just assume that if you don't get a `1` or `2` you got a `3`: let ai: u32 = rand::thread_rng().gen_range(1,3); match ai { 1 =&gt; println!("Computer threw rock!"), 2 =&gt; println!("Computer threw paper!"), _ =&gt; println!("Computer threw scissors!"), } PS. Don't be afraid to put some spaces in your code. Code is meant to be read, and space makes things easier to read.
Is there such a thing as constraint primitive type?
Soon sam sid said... depending on the details of what you mean the implementation of either that or a precursor to that is ongoing and nearly ready for testing: https://github.com/rust-lang/rust/issues/44580
Is this similar to pi/dependent types?
[Ligature support for Alacritty.](https://github.com/jwilm/alacritty/pull/2181) However, I think that I'll start over by making a generic shaping API that is rasterizer-independent, and operates directly on font files.
Can you interface Rust libraries (in Rust) dynamically? Hypothetical example: If I have a Rust function called a: use a_traits::A; #[no_mangle] pub extern "C" fn a() -&gt; A { A { /*...*/ } } And (in another crate) a function called b: use a_traits::A; #[no_mangle] pub extern "C" fn b(a: A) { /* do stuff with a */ } If I compiled crates a and b as a cdylib, and I had a crate c, which did: use a_traits::A; let crate_a = libloading::Library::new("crate_a.so")?; let crate_b = libloading::Library::new("crate_b.so")?; unsafe { let fn_a: Symbol&lt;unsafe extern "C" fn() -&gt; A&gt; = crate_a.get(b"a\0")?; let fn_b: Symbol&lt;unsafe extern "C" fn(A)&gt; = crate_b.get(b"b\0")?; fn_b(fn_a()); } Crate C (the one above) would successfully compile and work, right? (Assuming all crates were compiled with the exact same build of rustc/cargo.)
In the impl Savable for u32, you could implement them with `u32::to_le_bytes` and `u32::from_le_bytes`.
Panics are equivalent to exceptions, the main difference in Rust is that they're generally considered to be fatal errors that you can only recover from at a very coarse granularity, and return values are used for "expected errors." But functionally, unless you compile with panics as aborts, panics are fundamentally equivalent to exceptions.
You heard right that lifetime annotations don't affect scope. Scope is purely lexical, with each variable getting dropped at the end of the block that owns it. Lifetime parameters are just *constraints* on how long things need to stay alive, and the borrow checker's job is to make sure your program is satisfying those constraints. I wouldn't think of lifetime parameters as being created "within" the function. It's usually the opposite, really. You mostly use lifetime parameters when you're dealing with things that outlive your function. The literal string "testing 123" satisfies your constraint by default because it's compiled as a static. It's a string that will always be around, baked into your binary, so a reference to it can satisfy any lifetime constraint. (In fact, since you're not taking any arguments with a lifetime of `'a`, a static is the *only* thing that can satisfy your constraint.)
Consider using an `enum` type like: enum RockPaperScissors { Rock, Paper, Scissors, } then find a way to generate a random one of these and match on that. I might use something like: let choices = [RockPaperScissors::Rock, RockPaperScissors::Paper, RockPaperScissors::Scissors]; let ai_choice = choices[rand::thread_rng().gen_range(0, choices.len())]; match ai_choice { RockPaperScicssors::Rock =&gt; println!(“Computer threw rock!”), RockPaperScicssors::Paper =&gt; println!(“Computer threw paper!”), RockPaperScicssors::Scissors =&gt; println!(“Computer threw scissors!”), } It’s better not to use “magic” `u32`s to represent choice variants. Use `enum` to give each possible variant a meaningful name. That’s what they’re there for! It’s easier than trying to remember what each number means, and it will let the compiler see that you have in fact covered all the possible values. If you are insistent on using `u32`s, then the best thing (mentioned in another comment) is to add the default pattern `_ =&gt; unreachable!()` to your match statement.
Similar, yes. "An implementation of", probably not. I'm not really the right person to ask, but I'm pretty sure if you search around you'll find some comparisons.
You say "don't do this" but that's actually the syntax used by unsafe code to erase a lifetime before passing it through `std::thread::spawn` or something similar. It's a Rust equivalent of saying "hold my beer" but sometimes you have to and it's correct as long as the actual lifetime (freeing or aliasing the borrowed location) is obeyed. 
&gt; The thing is the language is specifically designed so that it doesn't surface that information to the programmer in any way except using it to make your code run faster. The compiler is purposefully as dumb as possible about things like this to This is super-duper important and easy for introductory material to miss. A tutorial usually focuses on how to do things and much less on the ways *not* to do something and why the compiler is as picky as it is. 
Why does Box&lt;dyn Trait&gt; work but Vec&lt;dyn Trait&gt; doesn't?
Because the items of a Vec are stored continuously in memory and must therefore have the same size, which dyn types don't guarantee.
Hold on. Oh. OH. 
Hmm, sounds reasonable, thanks.
From their [twitter feed](https://twitter.com/Gankro/status/1108232734076518400)
Nice thank you. These are not hard to understand. &amp;#x200B;
How do I actually get rust to run tests that I write in a module? I have my main.rs I have myModule/mod.rs I have myModule/somefile.rs I have my Somestruct declared in myModule/mod.rs, and then defined in myModules/somefile.rs. But when I put my mod tests into myModules/somefile.rs then rust just won't bother running those tests. 
Did you add `mod somefile;` to your `mod.rs`? Tests are only run if the module they're in is included. If you haven't read it yet, there's an entire chapter on testing and test organization [in the book](https://doc.rust-lang.org/stable/book/ch11-00-testing.html).
Note: if you indent your code block 4 spaces instead of using single backticks, it will preserve formatting.
I kind of suspected the answer would be SIMD. In systems languages, usually a the two most drastic optimization operations are pretty much always "cache line friendliness" and/or "SIMD" or "branch prediction". And those kinds of things don't play well with complex conditionals.
An enum is absolutely the right answer. If you're dealing with a discrete number of elements, it's best to let the compiler *know* there will only ever be 3 possibilities.
Nothing wrong with that; trivial things can ruin an otherwise good day if nobody explains them!
Adoption of SIMD seems to be really slow everywhere. And the _entire point_ of Hashbrown is to exploit SIMD.
Currently it really can report false-positives. You can disable this inspection via `Preferences | Editor | Inspections` (search for `Unresolved reference` inside **Rust** section). We're working on it, and if you faced the problem you can create an issue on our [tracker](https://github.com/intellij-rust/intellij-rust/issues) to help us fix it!
Or even [unreachable\_unchecked](https://doc.rust-lang.org/std/hint/fn.unreachable_unchecked.html), to avoid the check and the panic altogether. A bug in `rand` leading it to return a value out of [1,3] would cause UB though.
Title says free eBook, but store page says that eBooks is €21.00. Who do I believe?
[removed]
 pub struct CpuPpuInterconnect { ppu: *mut Ppu, cpu: *mut C6502, } Doesn't this end up causing UB very easily? Example call chain: 1. [C6502::store_write_target](https://github.com/MichaelBurge/nes-emulator/blob/5407d526b438f0afa707994155fc1a189f45d4e6/src/c6502.rs#L630) 2. [C6502::poke](https://github.com/MichaelBurge/nes-emulator/blob/5407d526b438f0afa707994155fc1a189f45d4e6/src/c6502.rs#L1228) 3. Let's assume we're poking some address that is mapped to CppPpuInterconnect 4. [CpuPpuInterconnect::poke](https://github.com/MichaelBurge/nes-emulator/blob/ce768d7b090688a68f4ef732f9d1f18f1d29542a/src/ppu.rs#L507) 5. let cpu = unsafe { &amp;mut *self.cpu }; 6. And now we have two live mutable references to the CPU: the original `&amp;mut self` in `C6502::store_write_target`, and the local variable `cpu` in `CpuPpuInterconnect::poke`.
There's no reason to venture into unsafe code here...
Repost: https://www.reddit.com/r/rust/comments/b2xd6c/implementing_a_nes_emulator_in_rust/
Now it is called "Run external linter to analyze code on the fly" since we can use clippy instead of cargo check.
I find this library useful on two counts: &amp;#x200B; 1) Just as a theoretical exercise in how a person would go about using globals in Rust. 2) In some circumstances, it could be helpful in debugging code. &amp;#x200B; One thing that would greatly improve the code, as has already been mentioned, would be to add some form of synchronisation. &amp;#x200B; I think most people realise that this type of library would not be suitable/advised for a Rust program. 
Nice, didn't know. So the `.vscode/tasks.json` file can look like: ```json { "version": "2.0.0", "tasks": [ { "type": "shell", "label": "Run current bin", "command": "cargo", "args": [ "run", "--bin", "${fileBasenameNoExtension}" ], "problemMatcher": [ "$rustc" ] } ] } ``` I just tried it and it really works.
I'm apparently too young as well but love puns. Could you kindly point me to the referent so I can enjoy the terriblenes? 
No EPUB download is provided this time, that's a shame.
I'd like to add this variant, which I came up with, but another kind soul noted down :). [https://dev.to/mindflavor/lets-build-zork-using-rust-1opm](https://dev.to/mindflavor/lets-build-zork-using-rust-1opm)
I don't know the specifics of what the OP is referring to, but Debian runs tests on a lot of architectures other than arm and x86 (like mips and s390x), and this can expose bugs in the test suite. The Debian folks, for example, found a byte-order related bug in `bytecount` that wasn't caught because nobody (including me) had CI enabled for a big-endian architecture.
Long live erotic jokes in software.
I agree, I just wanted to remind it exists. IMO there's no reason to add a panic either where you can just use an enum and everything verified by the compiler.
&gt;Or is this forward/backward compatibility? This. And it also means that third-party plugins (like [this one](https://plugins.jetbrains.com/plugin/12046-pest)) can now safely use Rust plugin API.
Where can one get started with this kind of programming?
Hash brown is a rust implementation of swiss tables. Could you explain, why some design decision were made, which differs from the c++ implementation of Google? For example, a power of two buckets are allocated and not a power of two minus one? (the latter is used as `bucket_mask` in hashbrown however`) Do you wrote down your review anywhere and could provide a link? 
Thanks for the explanation. I don't know if I'll try that route. I've never really wrapped my head around graphics programming in any meaningful way. But I have to admit it would be nice to match the silky smooth performance that /u/kouteiheika got.
You are just passing the panoc branch to the indexing, at that point why not unreachable!()?
Well, you could always copy&amp;paste each page into a Libreoffice document, with only images needing to be adjusted manually. I was surprised how straight foward that ways, with very consisten styles, and it's easy to e.g. add a grey background or a left border to the code snippets (formatted text).
So what does this mean for the Entry API? Is it now redundant?
No, some archs may not have SIMD. Also, I find the Entry syntax more explicit in intent - when I see it I expect an update-or-create operation.
It is still useful for `BTreeMap`, where the lookups are tree traversals. Other containers, even other hashmaps (if they have different implementation tradeoffs), maybe also benefit from it.
There are still ergonomic advantages, imo.
Probably https://github.com/rust-lang/rust/pull/56241#pullrequestreview-185374853 (that whole PR has a lot more info).
Nine Inch Nails first album is called [Pretty Hate Machine](https://en.wikipedia.org/wiki/Pretty_Hate_Machine). Also, I didn't come up with it, there's [two](https://prettystatemachine.blog/) [blogs](https://prettystatemachine.blogspot.com/) of the same name.
From the repo * Around 2x faster than FxHashMap and **8x** faster than the standard HashMap. * Lower memory usage: only 1 byte of overhead per entry instead of 8. &lt;vader&gt;Impressive, most impressive&lt;/vader&gt;
Data structures can often be intimidating to comprehend and read about, but this is a fantastic easy-to-follow post!
The google folks had to do quite a lot of iteration to really reap the benefits of SIMD here, and it came with very real tradeoffs that require measurement to justify. Also SIMD isn't quite so obviously good here because we don't ever actually "rip through" a big chunk of the array. Latency of a single operation is a real concern here, and it's why ARM's native SIMD (NEON) isn't ever used, despite SIMD being the raison-d'etre of the implementation.
Nice use of loop as an expression, by the way. 
If you do `get() { } else { insert() }` yourself you will still do: ``` hash(); get(); hash(); get(); really_insert(); ``` in the worst case, so the Entry API is still saving a good amount of work.
The only documentation of SwissTable is [this hour long talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&amp;t=3s) which necessarily skips over tons of details (but is still interesting and informative). I had no interest in reading over *two* implementations, so I just reviewed hashbrown standalone, with little interest in deviation from the original. So basically, ask Amanieu *shrug*. That said I would assume you're just misreading Google's implementation. You store `2^k -1` because it's a perfect bitmask for computing `index % 2^k`, so you should be able to store `2^k` elements. Unless google's doing something cute with trailing sentinels for weird C++ reasons?
Try adding this to your Cargo.toml: [profile.release] opt-level = 'z' lto = true The first option says to optimize for small size not speed. The second option turns on link-time optimization, which can perform additional optimizations beyond `--release`. You can also shave off a little bit more with: panic = 'abort' but this won't give you a stack trace or call any `Drop` implementations if your program panics.
&gt; PS. Don't be afraid to put some spaces in your code. Code is meant to be read, and space makes things easier to read. Pretty sure html ate those spaces, since it wasn't in a code block.
I remember finding a blog documenting the decissions taken on the SwissTable by one of the Google Engineers. Are you sure that is the only available documentation?
look at some decompiled C# https://sharplab.io/#v2:EYLgHgbALANALiAlgGwD4AEAMACdBGAbgFgAodAZlwCZsBhUgb1Oxd0vSmwFkAKAe2AArAKYBjONgGCAlM1ZMSrJdmR9RAa2z8hsxcpYL9LAL5yTpY0A ``` IL_0004: ldloc.0 IL_0005: ldloca.s 1 IL_0007: call void [mscorlib]System.Threading.Monitor::Enter(object, bool&amp;) ``` what this does is push the object onto the stack, push true onto the stack, and then the function is called, passing in the object and boolean. in the finally, the monitor.exit is called and the object is passed into it
FYI for anyone finding this thread: https://github.com/espressif/llvm-xtensa exists now, not sure about the quality though or how difficult it is to make these changes work with the llvm used by rustc
NOTE: these numbers are a bit deceptive because FxHashMap *is* the standard HashMap with the default hash function changed to the same one SwissTable defaults to. This hash function is very good but isn't adequate for defending against HashDoS attacks, so you need to be careful when using it. That said SwissTable has been an excellent time-usage *and* memory-usage reduction in our more comprehensive benchmarking.
If you know it, I would love to see it! (never found such a thing)
Markdown eats spaces at the start of the line, and multiple spaces in a row, but not spaces between things like =&gt; and println or , and 4.
One way to accomplish this is something like: use std::ops::{Deref, DerefMut}; pub struct Primus { pub foo: bool, } pub struct Secundus { pub bar: bool, primus: Primus, } impl Deref for Secundus { type Target = Primus; fn deref(&amp;self) -&gt; &amp;Primus { &amp;self.primus } } impl DerefMut for Secundus { fn deref_mut(&amp;mut self) -&gt; &amp;mut Primus { &amp;mut self.primus } } Now if you have an `x: Secundus`, you can access `x.primus.foo` by just typing `x.foo`. 
Wait, I figured this was purely a backend thing - would they change the API?
What do you mean by "implicit memory management", exactly? Most of Rust's memory management is already implicit.
Rust does perform a lot of implicit memory management inferred from the source and based on the types used (RAII). While you can explicitly `drop(value)`, Rust automatically does this when a value is out of scope / use.
Do panics have the same kind of overhead as exceptions in other languages?
What about [References and Borrowing](https://doc.rust-lang.org/1.26.1/book/second-edition/ch04-02-references-and-borrowing.html#references-and-borrowing)? Can borrowing be inferred as well?
The reference in `CpuPpuInterconnect::poke` is never actually used to modify the original value. I'll change it to a non-mutable reference. Is it undefined behavior to take multiple mutable references to a value but not use any of them? What could go wrong in that case?
This is exactly what Rust does.
More or less, but for various reasons they aren't intended to be caught most of the time and so it doesn't matter. Once your program has decided that it needs to exit, how quickly it does so usually doesn't matter as long as it's Fast Enough. ;-)
You can leave a varaible uninitialized, and initialize it in the loop: let foo; loop { match something() { Ok(x) =&gt; { foo = x; break; } Err(_) =&gt; println!("Retrying!") } } println!("Foo: {}", foo); The compiler is smart enough to figure out that `foo` is always initialized after the loop. &amp;#x200B; Another option, which I like to use in simple cases like this, is to use `loop {}` as an expression, using `break` with a value: let foo = loop { match something() { Ok(x) =&gt; break x, Err(_) =&gt; println!("Retrying!"), } }; println!("Foo: {}", foo); &amp;#x200B;
There are some replies to the reddit announcement for [hashbrown](https://www.reddit.com/r/rust/comments/9sn4ze/github_amanieuhashbrown_a_faster_hashmap_for_rust/). &amp;#x200B; One of the [replies](https://www.reddit.com/r/rust/comments/9sn4ze/github_amanieuhashbrown_a_faster_hashmap_for_rust/e8u97hb/) even covers why SwissTable does double (or even triple) lookup: 
There's some serious bugs in this crate, even ignoring all the undefined behaviour: let handle100 = globals::add(100i32); let handle200 = globals::add(200i32); let handle300 = globals::add(300i32); let handle400 = globals::add(400i32); globals::remove::&lt;i32&gt;(handle200); assert_eq!(*globals::get::&lt;i32&gt;(handle300), 300); Results in: thread 'main' panicked at 'assertion failed: `(left == right)` left: `400`, right: `300`' Looks like the `remove` function shifts all the globals after it, silently changing the meaning of all those handles. (Also, the handles are just `usize`s. They should probabl be wrapper structs instead.)
Disclaimer: I wrote the linked crate. I recently started exploring procedural macros, so much potential to explore! 
Wow. I feel ashamed. Thank you for this very detailed feedback. I will definitely fix README. `#[test_param]` is definitely an interesting option. 
I assume you wanted to post this to /r/playrust
BusyBox aims to be mostly GNU compatible. With mac I'm constantly have to port scripts and snippets that assume GNU flags, or install the gnu tools. Also remember that OP is asking to replace the coreutils *on linux* where scripts most definitely assume GNU.
Thanks for the reply. Especially the blog post is interesting to read. For example states explicitly, that `kSentinal` is not needed. Regarding the allocation: Googles code uses `2^k - 1`, which is calculated in `NormalizeCapacity(size_t)`: return n ? ~size_t{} &gt;&gt; LeadingZeros(n) : 1; However, _hashbrown_ uses `2^k`, as calculated in `capacity_to_buckets(usize)` (which itself is directly passed to `alloc` in `calculate_layout(buckets: usize)`): adjusted_cap.next_power_of_two() Internally, _hashbrown_ uses `buckets - 1` (stored as `self.bucket_mask`) for masking, which results in googles implementation, but with one extra bucket allocated (which, as far as I can see, isn't used)
No, and that's correct.
thanks man, I need to add some sort of allocator like system
You need a wide_null version that puts the 0 on the end.
Check subreddit description before posting. And do it on any subreddit.
The compiler can not know that you don't want to have the file locked until now\_do\_lots\_of\_work() is done.
I figured you'd just "\0" at the end. I find it a cleaner, simpler API. It could probably use a comment in the doc string.
No
If all the implicit borrowing does is adding a &amp; before the argument when the programmer forgets, it would not change anything. Is there some other way implicit borrowing could work?
If your app is performance sensitive and needs to handle tens of thousands of connections, I would recommend `actix-web`. Rocket will probably mostly catch up in it's next release (which is planned to use the newer async version of hyper), but for now actix is much faster.
That's bad for ergonomics, but it's your lib.
Why would I want to? Is there any counter argument to just assuming this? 
Unfortunately I don't think that fixes the problem, because the core rule is *mutability XOR aliasing*. Once a mutable reference is taken to something, no other references may exist. I don't think the precise semantics have been formally defined, but you are basically trying to do this: let mut cpu = 42; let ref1 = &amp;mut cpu; let ref2 = &amp;cpu; That code fails to compile with the error `cannot borrow `cpu` as immutable because it is also borrowed as mutable`. What you've done is force it to compile by using unsafe: let mut cpu = 42; let cpu_ptr = &amp;cpu as *const _; let ref1 = &amp;mut cpu; let ref2 = unsafe { &amp;*cpu_ptr }; But forcing it to compile doesn't mean it's ok, because unsafe just disables checks and makes you responsible of maintaining the correct semantics
Rust is much smaller than C++ for sure, and the development team is generally really careful with what gets added.
You have to claim it to your account, or something like that.
Rust is a gigantic language. That said it is also very approachable since a lot you won't need/see, for example because you don't need to do FFI stuff, and also because the compiler is really helpful with great error messages.
I’m also just learning Rust but couldn’t you also define an implementation block for the type Rectangle which takes a reference to self or am I not understanding your explanation properly?
Unwinding panics are generally implemented using the same mechanism that drives C++ exceptions. They technically don't *have* to be, as the language regards unwinding into or from different languages to be undefined behavior, but that is how the current implementations work. But it's also worth remembering that Rust panics don't *have* to unwind either. You can compile with `panic = "abort"` and panics will instantly terminate your program instead.
/u/burnsushi (author of ripgrep) once said that he would be unable to maintain ripgrep if it was written in C, but he could maintain it because it is written in Rust.
A feature of Rust is that the compiler barres you from writing certain kind of things (e.g. the (in)famous borrow-checker). As a result, Rust code is harder to write* then to read, because all Rust code works as expected, but not all code you would expect to work works in Rust. *except I don't really believe that: once you grasp the underlying concepts, all makes a lot of sense and you start writing correct code without the borrow-checker's help.
One interesting thing about the size of rust is that it’s got a lot of bits that fit together. So it can feel bug, but that’s often because you’re combining two or three simpler things. Different people have different opinions on how “big” that feels, of course.
From the perspective of primarily a Go developer for the last ~5 years: Is rust approachable/readable/writable? Yes and no, it depends. In most scenarios I think Rust is totally as approachable, readable, and writable as Go. Which, if you know Go, is a definite complement. You have to get past the syntax soup *(for things like turbo fish `::&lt;&gt;`, but I found that to happen fairly quickly. Once you get past that the language is very expressive and gives you many tools to handle the complex features. Where it can fail to me is when you get a bit more complex with generics, associated types and etc. Though, honestly this isn't likely a fault of Rust - just of complex type systems in general. Nevertheless, some design patterns just involve a lot of complex generic features which can lead to non approachable/etc code - in my opinion of course. But, I'm sure C++ is similar - I can't comment too much there. With that said, I think it heavily depends on how you use the language. In my writing I largely feel my Rust code is similar, if not more clean, than my Go code. Basic things like iterators and pattern matching make common types of code *much* cleaner to express in Rust than Go. In short, I definitely think the language is approachable / readable / writable in most cases; but you can write spaghetti in any language :)
&gt; if the client initiates a large number of requests at the same time (perhaps thousands to tens of thousands of requests per second), what is recommended for the stability of the connection DDOS protection services. What's the usecase? If you really want to handle that (though I can't image doing any useful things with such amount of connections, other than rerouting them somewhere else), you'll need to get very familiar with OS, and its settings. Web framework is most likely not to be your first bottleneck.
To me who tried C++, Rust is way more approachable than C++, although (only having a Java background), and really struggled to understand Rust, ownership system and lifetimes, I went on learning and now can say that I am happy I did. If you have used C++, I guess you will find it much easier than I did to switch, since you already know some concepts of rust (deref, drop etc). I found c++ always very difficult to read, java very easy. rust is somewhere in between. What I learned thought it depends on the developer. The rust community on reddit is very helpful, and I guess you master the switch very quickly. Only takes 3 minutes to setup rust and get going. 
Can someone comment on what this means for the Rust community?
[https://users.rust-lang.org/t/why-can-lifetimes-not-be-inferred/25645/9](https://users.rust-lang.org/t/why-can-lifetimes-not-be-inferred/25645/9)
The article does a good job explaining tombstones and the SIMD approach that [SwissTable](https://github.com/abseil/abseil-cpp/blob/master/absl/container/internal/raw_hash_set.h)/hashbrown uses. It lacks some depth on why `insert()` works like it is. In fact [SwissTable](https://github.com/abseil/abseil-cpp/blob/master/absl/container/internal/raw_hash_set.h) does up to a *triple* lookup in `insert()`. At high level it works like this: 1. Search for the element; if found return it (or in case of hashbrown overwrite and return). Otherwise fallthrough. 2. Find the first empty or tombstone slot. If the slot is a tombstone, insert at this location. If the slot is empty and there is capacity available, insert at this location. Otherwise fallthrough. 3. Grow the container, find the first empty slot and insert at this location. Let's break them down one by one. 1. This is a loop that will terminate after finding the element or finding an empty slot. This is well explained in the article. 2. This is another loop similar to (1). This loop either terminates before the previous one did (on a tombstone) or at exactly the same slot (the first empty one). Computationally it can be much cheaper as it does not require checking if the element is in the table since this was done in (1); there is no equality check on the key. Memory wise it is also cheaper as it will access at most the same cachelines that we accessed in (1). In practice it virtually never incurs cache misses. 3. This is a very expensive operation where we allocate a new container, rehash all the keys, move the elements in it, do the *third* lookup into the table and deallocate the original. In relative cost terms cost(2) &lt; cost(1) &lt;&lt; cost(3). (3) is supremely expensive - 100s if not 1000s times more expensive than (1) or (2). Armed with this knowledge (and also lots of traces of systems running in production) we can now understand why the algorithm is optimal (numbers are for exposition, albeit not far from reality): 1. 99 out of 100 times we find the element and return early. 2. 99 out of 100 times we insert without resizing (either on tombstone or capacity is available) and return early. 3. The rest of the time we resize/rehash, which is extremely expensive compared to the previous steps. Question: Why not merge (1) and (2) together and avoid the lookup in (2)? Answer: Because merging (1) and (2) together makes (1) more complicated and thus more expensive. (1) is executed 100 times more often than (2) and (2) is cheaper to boot. A 1% slowdown in (1) is the whole cost of (2). Also we implemented and measured it, and it indeed results in slower code if (1) and (2) are merged. For the curious, (1) and (2) can be merged in a single loop, the trick is to stash the first tombstone slot as we search through the container. Question: Why do a *third* lookup when more capacity is necessary? Isn't this madness? Answer: After a resize and rehash, one extra lookup does not matter. In order to resize and rehash, we've allocated, rehashed N elements, moved memory over and deallocated. One extra lookup in this extremely rare case does not affect overall performance. Note: in step (2) of the algorithm we only need to grow if capacity is not available *and* we are inserting on an empty slot. If we are inserting on top of a tombstone, capacity is irrelevant. [hashbrown](https://github.com/Amanieu/hashbrown/blob/master/src/raw/mod.rs#L739) unconditionally requires extra capacity irrespective of the insert location. This is a performance regression.
I'm really excited about `gloo`. Just landed my first contribution, hopefully the first of many. :)
Rust is using a fork of LLVM, so likely nothing as of right now. 
Every x64 CPU has SSE2, which means virtually every desktop. This instruction set is 19 years old.
Do you know if LLVM 8 contains any upstreaming of improvements in the rust fork?
Rust is a big language, but its features are very _cohesive_. The more I learned, the more I was able to intuitively figure out how something works or that some feature exists. And I know I'm not alone in this experience - I see tweets all the time along the lines of "rust has x, so I wondered if it also supports y, and it turns out it does!".
Does this mean that the current stdlib HashMap ought to be retained on simpler targets such as ARM?
I didn't mean hardware adoption. I meant there isn't as much code using it as one would expect.
Hashbrown is almost a 1-to-1 translation of the C++ implementations. I'd also love to see the rationale for the few deviations it has. For a layperson an explanation why two lookups are better than one is probably useful. For an expert, however, it would be so much more useful to know why hashbrown has fused the 2nd and the 3rd lookups from the C++ implementation and ended up with just two.
That I do not know. I'm sure you could check the commit logs in the LLVM version control for anything from the Rust maintainers or anything mentioning Rust.
I think it's getting closer? Of note this release is that support for WASM is on by default (e.g. the default compiled version of LLVM required you to recompile it to get WASM support)
Then I don't understand your original comment. Almost all hardware has SSE2. Hashbrown takes advantage of SSE2. Therefore hashbrown will be fast almost everywhere. What am I missing?
Not sure if this is an "easy question" but I've been trying to implement a string interner which doesn't duplicate data -- the storage is kept separate and the hashmap keys are &amp;str instead of String (so it is an intrusive data structure). Here's a [playground link](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=88e3658efacfe6be56c23d24de1699aa). I understand why the code doesn't compile (the mutable borrow is alive for too long which means I can't reborrow it immutably) - but I'm not sure how to fix it. Do I need more lifetimes? Some simple code rearrangement?
The point is that not that many things take advantage of SIMD, and Hashbrown stands out as one that does. 
I think Rust is on the "small language" end of the spectrum. Compared to other functional languages, it is definitely larger than OCaml, but has less tricky concepts than Haskell. Compared to C++, I'd say the different concepts fit better together, so the standard library needs less hacks. Also, since Rust is expression-based (almost everything you write is an expression with a type, and expressions can be combined in arbitrary ways) and has (probably?) sound semantics, it feels less hackish and therefore smaller than C++. One reason not to use Rust instead of C++ include poor interoperability with existing C++ code. Even though Rust can interoperate very well with C, in both directions, it doesn't really work with C++ objects (unless you write C wrappers to your C++ code, but then you lose many benefits of C++).
No, the generic implementation is also good (and at worst, more memory-efficient)
Rust is a complex language, but it deliberately takes on complexity so you don't need it in your code.
Oh, of course. It was silly of me to misread the common. &amp;#x200B; I believe the reason why so few data structures and algorithms take advantage of SIMD is that using SIMD is really hard. Hashbrown happened only because Google has open-sourced SwissTable, enabling a translation to Rust for a tiny fraction of effort it would take to write the code from scratch.
It's only reliable if the entire dynamic interface is compatible with FFI, because Rust doesn't have a stable ABI, meaning different invocations of the same compiler are allowed to implement the same type's layout differently. In practice, exact same version of the compiler usually *do* work, but there are no sanity guarantees. And if this goes badly, it's glitch city and fair game for all sorts of interesting security bugs so it's not recommended.
The Rust fork is just a place to cherry-pick patches as needed. Apart from adding some allocator metadata, it does not contain any Rust-specific improvements (at least not to LLVM, there is some work on LLDB Rust integration). Any fixes or optimizations always land upstream first and might be backported to the Rust fork.
Consistency is also important. Rust has a lot of moving bits, but at least they interact nicely with each other. C++ has a lot of strange edgecases and weird interactions between things which make the language as a whole difficult to keep in your head. For example, all the different types of constructors in C++ and when and when not the compiler generates which ones for you is difficult to grasp.
I am a noob so you can safely ignore my opinion, but still here it is: Rust should focus on less complication/complexity. Not more. I know everybody wants their favorite feature, but focusing on a few hard basics is much better for the long-term future of Rust. 
Ah! Indeed, platform specific issues make sense.
Thanks :)
I haven't looked at your code too much, but it seems that you are trying to create a self-referential struct (one that has references to stuff that it owns), which are not possible in safe Rust (well, they are possible, but they cannot be used in any meaningful way). So my short answer is - extra lifetimes or code rearrangement won't help, you will need unsafe here (or a crate that encapsulates this for you - I'm not very familiar with those). Also, short note about your `get_str_debug`: let x: &amp;str = something; std::str::from_utf8(x.as_bytes()) is entirely equivalent to `Ok(something)` - because if `something` was not valid utf8, then you have already caused UB.
Also, unlike C++, there's not really different "dialects". In C++ different people/organizations often seem to use different sets of features in different ways. In Rust I haven't seen this happen much; everyone writes more or less the same style of Rust, with a large set of more obscure, specific features that are rarely used but there if you need them. Part of it is that tools like clippy try to keep us on the same page as the language evolves (?, impl trait, dyn trait being the main things I recall), and part of it is probably just that Rust isn't 30 years old yet.
I agree on the enum.
I'm working on a personal library tracking system, and I have a struct to hold data about one book like so: ``` #[derive(Debug, Clone, Eq, PartialEq)] pub enum BookIdentifier { ISBN(String), LoC(String), None, } impl Default for BookIdentifier { fn default() -&gt; Self { BookIdentifier::None } } #[derive(Debug, Default, Clone, Eq, PartialEq)] pub struct Book { pub title: Option&lt;String&gt;, pub author: Option&lt;String&gt;, pub id: BookIdentifier, pub secondary_authors: Option&lt;Vec&lt;String&gt;&gt;, pub publication_year: Option&lt;u16&gt;, pub publisher: Option&lt;String&gt;, pub cover: Option&lt;Vec&lt;u8&gt;&gt;, pub copies: Option&lt;usize&gt;, } ``` It is the case that any given book may be missing some or all of these attributes, so I've made them all options, but this feels wrong. Is there a better way to do so? (I'd also like to enforce that there is *at least one* of these things present, but I don't think there's any way to do so in the type signature.) 
&gt;`std` aware cargo YES! FINALLY! Xargo being deprecated really hurt just about everyone targeting uncommon platforms. Cargo-xbuild tried to fill the void, but it wasn't universally transitioned to. 
I'm in the same boat. I've been using C++ for ~10 years, and JS for 5, but with the growing complexity of C++ I've started looking for something else, and Rust has been a good change of pace. There's a pretty steep learning curve, you pretty much have to read the Book, but in that regard there are TONS of resources out there, and the community is super helpful. In terms of readability, I was browsing `firecracker`'s source code and it was pretty easy to grok. It's actually what convinced me to start learning rust. Writing rust code is pretty straightforward as well. The compiler is very nice to work with unless you do something it's REALLY unhappy about, but it still offers very helpful suggestions and references into the standard that explain why you can't do something.
You should be putting nginx in front of your app for SSL purposes, serving static assets, etc, and it probably has a way to throttle individual clients also. &gt; p.s. During the use of iron, I have encountered the problem of "too many open files", but the ulimit of ubuntu has been opened to the maximum... I've noticed this with rocket (I think rocket and iron are both using hyper) but not with actix-web. There's a bug somewhere. After some digging I found those "too many open files" were not network connections but calls of /dev/random. I did also notice a different about benching with http://127.0.0.1 and http://localhost. I don't understand any of it, as you probably can tell.
I am not sure about how your post is relevant to the proposal. Have you read it? There are feature which will get stabilized, it's inevitable, and discussing features worth is outside of the proposal's scope. A serious problem in my opinion is that Nightly works worse and worse by the day, so less and less people play and experiment with unstable features, instead the target audience waits for feature stabilization to start using it. This translates to pressure on Rust teams to stabilize those feature. And as a result I believe this makes a possibility of stabilizing a flawed feature noticeably higher. The proposal aims to fix this dangerous trend.
I fail to see a situation in which `copyless` would help in any way. I couldn't find any example anywhere and a reverse lookup on crates.io didn't turn up anything. Can somebody please share a use case for `copyless`? I don't get it. :)
&gt; self-referential struct (one that has references to stuff that it owns), Any tutorial suggestions on how to do this? Seems like quite the rabbit hole... &gt; because if something was not valid utf8, then you have already caused UB. Before using it elsewhere? The docs say &gt; If this constraint is violated, undefined behavior results, as the rest of Rust assumes that &amp;strs are valid UTF-8. So I agree that you're technically correct, but I can't think of any set of circumstances in which the code could be optimized to give the wrong result 🤷‍. That said, I'll fix that code, it's not a big deal.
&gt; Rust should focus on less complication/complexity. I think we all agree on the principle, but disagree on the application. So, for reference, what do *you* mean by that?
The fork was already updated to LLVM 8 and the submodule update is only waiting on bors: https://github.com/rust-lang/rust/pull/59285
One notable change is that WASM target is no longer experimental.
No, we can \*read\* a free copy. On your site. This is still awesome, but it's not the same thing as claiming (which implies ownership). This is more like borrowing and being allowed to read it over your shoulder.
Note that hashbrown is not *quite* a translation as I understand it; Amanieu used the ideas of SwissTable, but with their own spin on things.
I was looking into self-referential structs and I don't think the Interner qualifies as one. I don't have references to other fields -- which would be problematic when you move the struct, as those references get invalidated -- but I think moving an Interner should be perfectly ok without any such problem. However, I do have shared references into the heap. This means that if the storage just (say) clears out the vector, then the keys in the hash table have dangling references.
I would note that it can be hard to *benefit* from SIMD. Just recently, I've been working on FastFIX decoding. The protocol encodes nearly everything as a variable-length sequence of bytes with the last byte of the sequence having the high-bit set. Spotting the STOP byte in the stream seems like a job for SIMD, no? And indeed, it only takes a couple of SSE2 instructions to crunch through the data 16 bytes at a time. Except... that FastFIX is a delta protocol, where most values are encoded in a couple bytes (often 1 or 2), and thus actually just checking the individual bytes manually is often faster than setting up the mask, masking and finding the index of the first non-zero. And so my SIMD implementation ended up performing more slowly on realistic workloads :/
I can't find it now. I remember watching the talk before it was popular in Reddit and in the Rust community and the blog result was already really buried in the Google results. Now I just can't find it in the noise.
Context. Based on the code, it's likely this is a game for rock paper scissors. If the underlying random number generator library fails, it's not a mission critical issue and we shouldn't pretend that people will die if it doesn't work. We would need to either scream at the rng library author, or modify our code to work. Either way, it's not a death situation, so playing \*that\* carefully and \*insuring\* that every last edge case is 100% always and completely covered isn't needed. What is more important? &amp;#x200B; Readability. &amp;#x200B; This is very readable and says exactly what it is supposed to do. It doesn't pretend that rng is likely to fail when it isn't likely to. It doesn't wave hands and do a lot of magic ceremony when it's just picking a value. etc. Is it perfect? &amp;#x200B; Of course not. Under a different context this might not be acceptable at all. Your complaint assumes a higher level of worry over panics, but under a different context the \*memory\* allocation for the array to pick from might be a giant blinking red flag. why not worry about that instead of the panic? Because of the context here. We are likely in a large free memory environment where a small allocation is no concern. I'm sorry if this comes off as sounding preachy or harping, this is just one of my pet peeves. I see blanket advice from programmers without mentioning a) the context for why that makes sense or b) ignoring the context the programmer is obviously using. Your concern is well founded under some conditions and irrelevant under others.
On mobile atm but real quick: i think its good to only have stable and nightly. If you really need nightly you can use it. But overall I think a strong desire/drive to get stuff into stable is a good thing. 
&gt; Note: in step (2) of the algorithm we only need to grow if capacity is not available and we are inserting on an empty slot. If we are inserting on top of a tombstone, capacity is irrelevant. hashbrown unconditionally requires extra capacity irrespective of the insert location. **This is a performance regression.** Paging /u/Amanieu: does hashbrown actually always reserve extra capacity and if yes, why?
That's probably a big part of it!
Glad to hear improvements being made. One thing I noticed when looking through the codebase is that there was a note about how there are 2 separate locks sure to a lack of upgradeable rwlocks. After a little digging, I found `parking_lot` has exactly that. Another is the frequency reading and writing `u64`s by casting byte pointers, which my rustc didn't appreciate too much. Is there any particular reason to not use `byteorder` In Pikelet meanwhile, I saw that you seem to be reimplementing `arrayvec`'s `ArrayStr`, though it does put the length after the array rather than before, so a straight `memcpy` would require a little more effort. P.S. I'm sorry I can get a tad snobby when seeing unsafe where it's not 100% necessary and when seeing dependencies not used to their full potential. I do understand that some people prefer to keep dependency counts low.
Do you just mean it's not a linux system package manager (like yum or pacman or apt)? crates.io is the first result on google for "cargo rust", and the description that comes up is: &gt; cargo is the package manager and crate host for rust.
Yes, it's the added "spin" that demands I hedge my statements about hashbrown's performance. To get matters straight, hashbrown is almost a literal translation of SwissTable. Go read the code. It's the same classes, same functions. The code is extremely subtle but without knowing how it was produced it's impossible to know why certain things just have to be done this way. The deviations in hashbrown are few (this is a great thing!), and their effect is almost certainly negative. It's almost impossible to replicate the effort it took to measure the effect of hundreds of different variations on production loads. Some people mistake this for running benchmarks, and they do it to their own detriment.
It's possible to use vectorization without vector instructions: using `u64`. The idea is to memcpy 8 bytes into `u64` (or reinterpret the memory, depending on whether alignment matters), then perform bit-tricks on the u64 to handle 8 bytes at once.
From the [webrender newsletter](https://mozillagfx.wordpress.com/2019/03/20/webrender-newsletter-42/) where I read about it: "Kvark also extracted a useful bit of WebRender into the copyless crate. This crate makes it possible to push large structures into standard vectors and hash maps in a way that llvm is better able to optimize than when using, says, Vec::push. This lets the large values get initialized directly in the container’s allocated memory without emitting extra memcpys."
I mean you can get crafty with generics but if you have two functions that do two different things, don't give them the same name. 
These claims seem to be based on the results of [bench.rc](https://github.com/Amanieu/hashbrown/blob/master/benches/bench.rs). I don't doubt that hashbrown is faster than some other hashtable (given that it's a translation of SwissTable -- the fastest hashtable implemented in C++), but these benchmarks cannot be relied upon to make judgement about relative efficiency of hashbrown vs something else. I remember reading elsewhere that the rust compiler has seen significant speedup and memory footprint reduction when it switched to hashbrown. This is a more reliable benchmark. Something you can trust. &amp;#x200B; The benchmarks that SwissTable devs had used are [open source](https://github.com/google/hashtable-benchmarks/blob/master/hashtable_benchmarks.cc). You can check the level of sophistication there. However, even these benchmarks were never used to judge which implementation is faster. You simply cannot do this with synthetic benchmarks. You have to run something real, like websearch at Google, or a rust compiler, to see what's more efficient. Benchmarks are only a means to understanding what's going on. Kind of like a debugger. They can be very useful (especially with the kind of differentiation between hot/cold workloads, small/large objects and low/high densities that SwissTable benchmarks have).
Parking_lot sounds cool. However, one goal of Sanakirja is portability, and relying on external dependencies is not always the best way to achieve this, especially for OS-dependent features like the ones we use here. &gt; Is there any particular reason to not use byteorder I could certainly have used byteorder, but before the question even arose, I had to think a lot about how to manually align memory properly, and about differences between architectures (endianness, memory page size, etc), so the natural thing to do was just to do it half-manually. By the way, Sanakirja is one of the first Rust crates I ever wrote (back in 2015, IIRC). What version of rustc gives problems? What kind of problems? &gt; P.S. I'm sorry I can get a tad snobby when seeing unsafe where it's not 100% necessary No need to be sorry, I appreciated the constructiveness of your comments. Sanakirja will necessarily contain lots of unsafe stuff, for many reasons: - Allocations happen in a file, and are transactional (they are robust to a `kill -9`, and even to pulling the machine's plug). This imposes some constraints on memory safety: in particular, many places in the file cannot be mutated, and the borrow checker cannot really detect anything. - Memory management cannot be done normally dropping, since the purpose of a database is the opposite of Rust's memory management model: the goal *is* to get a "dirty" file where not everything has been cleanly deallocated. Actually, the API is not safe in the Rust sense, and can cause data corruption if used improperly. The issue is explained in the docs, and similar libraries such as LMDB handle it using a flat file structure, with only one type of data stored in the database, akin to &amp;[u8], whereas Sanakirja can map u64 to another Sanakirja B-tree.
In Rust, you can have multiple types named `Rectangle`. This is so that you don't have to worry about "ERROR: Your code no longer compiles now that you've upgraded your dependencies because one of the crates you depend on decided to add a type with the same name as one of yours." `self` is about a function within a specific instance of `Rectangle` being able to see the rest of the stuff on that specific instance. Declaring `Rectangle` at the right scope is about `fn area (rectangle: &amp;Rectangle) -&gt; u32` being about to reliably know which `Rectangle` you're talking about when you might have more than one in your project. (eg. `std::error::Error` vs. `std::io::Error`) In a dynamic, duck-typed language like Python, you find out at runtime whether your assumptions about a given data type hold true. In Rust, the compiler insist that it all make sense at compile time, which means each function must be able to see the declarations of its argument and return types and they must match up with how the stuff inside the function tries to use them.
Now that I am actually thinking about when I’d be using each this makes sense. I realize I’m probably just over engineering the solution. Cheers!
Yes, it can be used, but what I really meant was don't do it unless you really know what you are doing. Even then, shouldn't the pin api (eventually) be used instead, at least where possible? I haven't used it myself, so I might be mis understanding want pin is for.
&gt; Another is the frequency reading and writing u64s by casting byte pointers, which my rustc didn't appreciate too much. Since 1.32 the standard library has `{from,to}_{le,be,ne}_bytes` for primitive types like u64.
As a pytorch user, I like what the code looks like in rust (esp. that of other examples in the same repo). Very nice work.
If you want to store a large struct on the heap. Rust will first create it on the stack, then memcpy it to the heap.
The link to the Tokyo Rust Meetup is wrong; it should be [https://rust.connpass.com/event/122377/](https://rust.connpass.com/event/122377/) Ping, /u/nasa42 
I wonder if allocating \`2\^k\` size chunks is easier than \`2\^k - 1\`, if so that might be more important than over-allocating by one slot.
Wait, if I have box&lt;foo&gt;, and I move it from a to b it does a memcopy? ``` let x = Box::new(5); let y = x; ``` I thought the point of box was that it’s a pointer?
How exactly helps copyless avoid that?
Thanks much for code! Actually working with negative bytes is the main problem here :)
Yes, but that keeps it as a u64, whereas sanakirja is serializing it to a file.
Thanks! It's still mostly work in progress but one of the goal is to keep things as simple as they are on the Python and C++ api. One difference is that there is an explicit 'variable store' to hold variables in this rust api, hopefully it's not too much added complexity (+ it makes things a bit more explicit).
\&gt; Are you using the same seed in your runnable code? yes &amp;#x200B; Thanks for all other info! I will play around :)
OP are your versions dependent on input param? {i32, i64, struct, etc.}
Interesting. Has there been any attempts to try to upstream this into std so that Vec::push just uses it?
Hmm. Sounds pretty antithetical to [reproducible builds](https://reproducible-builds.org/) if I understand its behaviour correctly.
We call a struct self referential not when it contains references to itself, but when it has references to something it *owns*. This is relevant because borrow checker does not have a distinction between *containing* and *owning*. It cannot see into the vector and understand that even if you move the vector then the references to its storage will still be valid. For example vector could store its elements inline (which would mean that moving it would invalidate references) - and there's nothing in vector's api thay says it doesn't do this. And rust doesn't even have a way to express it for that matter. However iirc this is guaranteed by the docs - but safe code will not be able to make any use of that.
I like that, thank you!
I'm curious how it will affect web assembly module sizes
Hmm. Interesting thing to have, but probably increasingly niche, given that the APIs presented seem pretty antithetical to [reproducible builds](https://reproducible-builds.org/) and the push for those is on the rise.
&gt;Parking_lot sounds cool. However, one goal of Sanakirja is portability, and relying on external dependencies is not always the best way to achieve this This would be more convincing if Rust want replacing their standard mutexes with parking_lot. :P &gt;What version of rustc gives problems? What kind of problems? I can't answer the first question beyond "the latest stable" since I'm on my phone, but for the second question, it was about casting a pointer to a stricter alignment. It's not the first time I've run into alignment messing things up; the first time I saw LLVM emit 8 separate writes because it couldn't tell that a pointer was aligned.
Ah! That makes a lot more sense now. Thanks for the explanation!
No, the input is always going to be an index: u32 into an array. The two implementations are getting the prefix for an array. One version does it linearly, the other does it using a Fenwick tree. There are cases when I want to use one over the other. 
As a side note of unsolicited advice. It's good to make peace early with the fact that the compiler likes to index things starting from 0, not 1, and that ranges are inclusive of the start value and exclusive of the stop value. This has a lot of nice properties when it comes to composing multiple ranges, and working with the length of lists. In this case, it would have prevented your off-by-one error (your "ai" can never throw scissors) and allowed you to write the less-cluttered \`.gen\_range(3)\` (but your match arms would be \`0\`, \`1\`, and \`2\` instead of \`1\`, \`2\`, and \`3\`). &amp;#x200B;
From copyless's GitHub: "We figured out a way to convince the compiler to eliminate the copies. This library attemts to make these ways available to Rust ecosystem, at least until the compiler gets smart enough". So it may or may not be a bug, that compiler doesn't do it by default.
I think I get it...I think this would be a good use case for traits one being Fenwick the other linear If I'm wrong link your repo so we can get to the bottom of this
https://doc.rust-lang.org/book/ufcs.html
Yeah, as another data point I use SIMD everywhere in Pathfinder 3. On x86 the overall speedup from SIMD is about 15%. It's not nothing, and I'll take it, but it's a sobering reminder that the benefit is limited for algorithms that don't have high data parallelism, low branching, and high arithmetic intensity.
I like it.
so i can do something like follows: pub trait Fenwick { fn get_low(&amp;self, index: u32) -&gt; f64; } pub trait Linear { fn get_low(&amp;self, index: u32) -&gt; f64; } impl Fenwick for SourceModel{ fn get_low(&amp;self, index: u32) -&gt; f64 { let low: u32 = fenwick::array::prefix_sum(&amp;self.fenwick_counts, index as usize) - self.counts[index as usize]; low as f64 / self.total_count as f64 } } impl Linear for SourceModel{ fn get_low(&amp;self, index: u32) -&gt; f64 { let mut low: u32 = 0; for i in 0..index { low += self.counts[i as usize]; } low as f64 / self.total_count as f64 } } but then my issue becomes using it. My use case is that I never call .get_low myself. Source model is passed to another method that calls another method which calls get_low. So how would I pass something that implements low to the original struct? here is the function sig for one of the those classes pub fn encode&lt;T: Write&gt;(&amp;mut self, symbol: u32, source_model: &amp;SourceModel, output: &amp;mut BitWriter&lt;T&gt;) { I cant have a generic that is **&lt;T: Write, S: Fenwick || Linear&gt;**? 
Why not `S: Fenwick + Linear`?
I thought that meant that S must implement Fenwick AND linear
Yeah - which u did for the example
Or better yet...just have a trait that both traits implement and use that instead so it can be or
Here is some context and real-world uses: - https://github.com/servo/webrender/pull/3362 - https://github.com/gfx-rs/gfx/pull/2705 - https://github.com/gfx-rs/portability/pull/178 What is confusing about the library is that the changes it does are seemingly non-semantical. But the generated code is different, because LLVM gets easier time optimizing it. What the library does is just ruling out any obstacles between value construction and placement. Obstacles can be conditional panics, including memory allocation scenarios.
@[oconnor663](https://www.reddit.com/user/oconnor663) &amp; @[newpavlov](https://www.reddit.com/user/newpavlov) so, here is what I've got with your help guys :) output from scala: SEED PHRASE: some long string that was generated by very random super generator in the middle of Universe phrase bytes: \[115; 111; 109; 101; 32; 108; 111; 110; 103; 32; 115; 116; 114; 105; 110; 103; 32; 116; 104; 97; 116; 32; 119; 97; 115; 32; 103; 101; 110; 101; 114; 97; 116; 101; 100; 32; 98; 121; 32; 118; 101; 114; 121; 32; 114; 97; 110; 100; 111; 109; 32; 115; 117; 112; 101; 114; 32; 103; 101; 110; 101; 114; 97; 116; 111; 114; 32; 105; 110; 32; 116; 104; 101; 32; 109; 105; 100; 100; 108; 101; 32; 111; 102; 32; 85; 110; 105; 118; 101; 114; 115; 101\] with 4 zeroes at start: \[0; 0; 0; 0; 115; 111; 109; 101; 32; 108; 111; 110; 103; 32; 115; 116; 114; 105; 110; 103; 32; 116; 104; 97; 116; 32; 119; 97; 115; 32; 103; 101; 110; 101; 114; 97; 116; 101; 100; 32; 98; 121; 32; 118; 101; 114; 121; 32; 114; 97; 110; 100; 111; 109; 32; 115; 117; 112; 101; 114; 32; 103; 101; 110; 101; 114; 97; 116; 111; 114; 32; 105; 110; 32; 116; 104; 101; 32; 109; 105; 100; 100; 108; 101; 32; 111; 102; 32; 85; 110; 105; 118; 101; 114; 115; 101\] blake2b256: \[-85; 23; 39; 28; 21; 97; 71; 34; -117; 94; 125; -21; -50; -79; -58; -45; -32; 51; -52; -46; 95; -10; 124; -32; -91; 119; 9; 85; 120; -119; 77; -89\] keccak256: \[-101; 121; 79; -64; -22; -75; 6; -3; -101; -39; -82; -48; -85; -9; 10; 90; -80; -4; 44; 13; 123; 123; 112; -116; -54; -107; 26; 33; -90; -7; -59; 76\] &amp;#x200B; and here is output from rust: SEED\_PHRASE: enact swing lemon bean drink airport orient fragile original exchange diet blade dice pottery radar phrase bytes: \[101, 110, 97, 99, 116, 32, 115, 119, 105, 110, 103, 32, 108, 101, 109, 111, 110, 32, 98, 101, 97, 110, 32, 100, 114, 105, 110, 107, 32, 97, 105, 114, 112, 111, 114, 116, 32, 111, 114, 105, 101, 110, 116, 32, 102, 114, 97, 103, 105, 108, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 101, 120, 99, 104, 97, 110, 103, 101, 32, 100, 105, 101, 116, 32, 98, 108, 97, 100, 101, 32, 100, 105, 99, 101, 32, 112, 111, 116, 116, 101, 114, 121, 32, 114, 97, 100, 97, 114\] with 4 zeroes at start: \[0, 0, 0, 0, 101, 110, 97, 99, 116, 32, 115, 119, 105, 110, 103, 32, 108, 101, 109, 111, 110, 32, 98, 101, 97, 110, 32, 100, 114, 105, 110, 107, 32, 97, 105, 114, 112, 111, 114, 116, 32, 111, 114, 105, 101, 110, 116, 32, 102, 114, 97, 103, 105, 108, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 101, 120, 99, 104, 97, 110, 103, 101, 32, 100, 105, 101, 116, 32, 98, 108, 97, 100, 101, 32, 100, 105, 99, 101, 32, 112, 111, 116, 116, 101, 114, 121, 32, 114, 97, 100, 97, 114\] blake2b256: \[-22, -7, 55, 1, -19, -111, -101, 28, 51, -97, -37, 3, 23, 127, -62, 124, 24, 89, -120, 38, -124, -111, -23, 5, 25, 36, -121, 109, -4, 14, 75, -120, 57, 5, -112, 71, 30, -12, 122, -87, -21, 29, -122, 63, 92, -62, -1, 25, 31, 45, 44, -1, 41, -90, -118, -111, -80, -46, 36, 75, 3, -26, 64, 33\] keccak256: \[32, -63, -100, -55, 41, 114, -127, -29, -37, -69, 119, -105, 48, 15, 109, -92, -48, -110, -123, -22, 102, -25, -48, 123, 66, -99, 92, 102, 49, -73, -47, -74\] &amp;#x200B; As we can see diff comes on first step :) - on phrase bytes. Here is rust code: use blake2::{Blake2b, Digest}; use sha3::Keccak256; fn main() { const SEED\_PHRASE: &amp;str = "enact swing lemon bean drink airport orient fragile original exchange diet blade dice pottery radar"; println!("SEED\_PHRASE: {}", SEED\_PHRASE); let seed\_bytes = SEED\_PHRASE.as\_bytes(); print\_bytes\_as\_signed("phrase bytes", seed\_bytes); let with\_4\_zeroes = \[\[0; 4\].to\_vec(), seed\_bytes.to\_vec()\].concat(); let with\_4\_zeroes = with\_4\_zeroes.as\_slice(); print\_bytes\_as\_signed("with 4 zeroes at start", with\_4\_zeroes); let blake2b256 = Blake2b::new().chain(with\_4\_zeroes).result(); print\_bytes\_as\_signed("blake2b256", blake2b256.as\_slice()); let keccak256 = Keccak256::digest(&amp;blake2b256); print\_bytes\_as\_signed("keccak256", keccak256.as\_slice()); } fn print\_bytes\_as\_signed(prefix\_str: &amp;str, unsigned: &amp;\[u8\]) { let signed = unsigned\_to\_signed1(unsigned); println!("{}: {:?}", prefix\_str, signed); } fn unsigned\_to\_signed1(unsigned: &amp;\[u8\]) -&gt; Vec&lt;i8&gt; { let mut signed = Vec::new(); for u in unsigned { signed.push(\*u as i8); } signed } fn unsigned\_to\_signed2(unsigned: &amp;\[u8\]) -&gt; Vec&lt;i8&gt; { unsigned.iter().map(|i| \*i as i8).collect() }
You've found a bug in hashbrown. Must be one of those "spins" on top of SwissTable that make it a non-direct translation.
Good catch. I didn't see this when I did the review. Perhaps open an issue?
# Compiling Standard Library documentation as fast as possible I am currently working on a PR for the documentation of Rust and I was wondering what the fastest / most easiest way is to compile the documentation of the standard library. Currently I am using the command: ```python x.py doc --stage 0 src/libstd``` But that still takes a few minutes every time I run it :(
How much scrap do you think a supply signal will sell for?
Somewhat offtopic: I'm halfway through [Lamport's video course on TLA+](http://lamport.azurewebsites.net/video/videos.html) and if there's one thing I could recommend when designing concurrent algorithms is to take this course. I knew it was hard having read Jepsen articles, but this is another level of amazing.
Yes this isn't really meant for use in libraries of reputable nature. I made it specifically to target a hostile environment of game hacking, where string constants are being targetted by anti-cheats as a way of identifying suspicious modules. Specifically the purpose of the obfuscation is to defeat automated mechanisms to detect the presence of suspicious strings.
Sanakirja, is that some kind of a code name, or is it the final product name?
That's probably what I'll do before I start reading if I can't find a script by someone else to download copies... but this is the end of my interest in Packt. (I'm not big on reading materials I'll visit repeatedly in my regular browser, bogged down by privacy extensions, let alone depending on having Internet connectivity. It's bad enough that the new "stitch everything together with JavaScript" rustdoc template broke docsets in the newest version of Zeal available of *buntu 14.04 and I'm really hoping they'll start working again when I upgrade to Kubuntu 18.04 LTS.)
Oh. That's actually really useful for things like relative pointers. It wouldn't require as much unsafe to initialize them since you can initialize them straight into the allocation (if only it would expose its own memory address so I could calculate the offset).
Great !! Does it generate consistent one way hash similar to \`HMAC-SHA512\`? I am using HMAC-SHA512 for my application.
Interesting, I change Blake2b256 realisation to one from [another library](https://docs.rs/crev-common/0.3.0/crev_common/blake2b256/struct.Blake2b256.html) and it gets me another result... ``` crev-common = "0.3.0" ... use crev_common::blake2b256::Blake2b256; ... let blake2b256 = Blake2b256::new().chain(with_4_zeroes).result(); ``` produce: `blake2b256: [-127, -128, -69, 24, 41, 10, 8, 52, 30, 72, -123, 70, 6, 98, 30, -91, -101, -24, -7, -127, -91, 46, -101, -128, 124, -51, 94, -88, -102, 124, 111, -109]` which is differ from previous rust result, but in the same time differ from scala result :D 
It probably has something to do with BTreeMap being sorted and HashMap not. With HashMap, you would just use the generic `Extend::extend` trait method, but with BTreeMap, it can be optimised by doing a merge algorithm that takes advantage of the fact that the inputs are sorted.
Thanks, that went 377k -&gt; 300k -&gt; 250k.
May be better cooperate with [https://github.com/Hoverbear/getset](https://github.com/Hoverbear/getset) ?
Thanks!
Plus `xbuild` is only suitable for projects that make use of `core` and `alloc`. It dropped the ability to build your own `std` which is one of the really interesting parts of `xargo`.
That crate looks really good, but I think is not really serving the same purpose. The point of my crate is having a way of accessing fields by type. Quite a niche situation compared to having old good getters and setters that are typically used for encapsulating some implementation.
Correction, it's clippy that has an issue with it: [cast_ptr_alignment](https://rust-lang.github.io/rust-clippy/master/#cast_ptr_alignment)
https://github.com/rust-lang/rust/issues/55514#issuecomment-446182349
Do you have MSVC and the necessary windows C++ libs that come with it?
Why not wasm-bindgen?
That said, I would argue that rust *isnt* approachable. Without reading the rust book learning rust is very hard. The Rust book is great tho.
I found another library, but fail to use it - my rust experience is to low :( [https://docs.rs/rust-crypto/0.2.36/crypto/blake2b/struct.Blake2b.html](https://docs.rs/rust-crypto/0.2.36/crypto/blake2b/struct.Blake2b.html)
Interesting question. It is the final name, without a doubt. There’s a story behind it, and I like it.
Maybe the beta channel is a good place to have those set of features accessible? 
I've read through that before but since this is a networked application and most of the inputs we're getting are messages from peers. (We're actually using this for tracking the state of a group of peers in the network, so each node is probably running dozens of these state machines in paralell.) So it's actually pretty are that we'll encounter an error on the inputs, they're only to be used for *truly* exceptional cases, not "hey you can't get this input in this state!" kind of situations. The state machines don't *actively* do anything, they're pretty self contained and only interact with the outside world through inputs and the `Effects` system. Being able to replay thousands of inputs later on to validate the current state is a pretty necessary requirement for here. I'm not worried about "oh what happens if we pass the state machine to function `do_thing` and it's in state P and the function is in state Q". So doing a super typed thing doesn't make any sense for this, using an `enum` of states or a single `struct` is pretty necessary. Also since we'll have to write the state to disk and read it from at a later point in time.
That's quality.
Tried to register, but it complained about an invalid email (using my gmail address). Tried to check if I had already used it to register, but the site said there was no record of it. Registered with a fake temporary email. Tried to change the email in my settings and got: \` \* Couldn't update email address (undefined) \`...
I would say the most common 80-90% of the language is approachable, and the rest is... very not. That probably makes it more approachable than most compiled languages though 🤷🏼‍♂️
Would be nice if there was some comparison of this vs using cargo web or bindgen. Looks cool though!
You're looking for /r/playrust.
&gt; "TypeScript will definitely solve my problems!" &gt; You can wait and hope that any of the big tech companies will solve your problems through TypeScript and other tools. &gt; "Hm, I don't know if this is an error with TypeScript or in the transpiled JavaScript? Or did I just disable the wrong ESLint option?" &gt; I would categorize many of the TypeScript and NodeJS pains as bad pain. Not because the tools are bad, but a language which was created in 10 days just gets you so far. It seems like you think of TypeScript as a red herring, but you don't ever explain why this is. You simply equate it to JavaScript and imply that it is more trouble than its worth. Could you maybe elaborate on that? If someone is seeking a more harmonious web development experience than JavaScript I would certainly recommend TypeScript before Rust, and I'm a pretty huge fan of Rust.
&gt; This would be more convincing if Rust wasn't replacing their standard mutexes with parking_lot. :P So less reason to use parking_lot when you can just wait for the uplift to std to be complete :p
The API is different than Vec::push. Copyless reserves a spot in the vector first and then has an API to construct a value into that spot. Vec::push reuqires the value to be constructed before making room for it in the Vec
inb4: it incrases
Rust is big compared to some, but still smaller than C++ and mass a lot more sense. It has an unusual learning curve though: C++ takes a little getting your head round, but then after a bit you can do things. But then after a bit MORE you realise that it's really hard to get things to work well and reliably, and you keep running into surprises (yay template error messages). Rust can be a little painful early on as you learn how to keep the borrow checker happy - you feel like it's fighting against you. But then when you get over that hump, you've got a great language that's working for you to prevent unexpected things happening.
I'd say there's nothing wrong with making them all options. Also, I'd remove the None variant from BookIdentifier and wrap that field in an option as well. I don't think there's an *easy* way to represent having at least one field present, but it's a matter of how far you're willing to go. You could make complicated enums, or you could not expose the fields and only expose getters, checked setters, and checked constructors. There's also a question of what is enough to actually identify a book. Does having just the publisher make any sense, or even just an author or title? It also depends on what you plan to actually do with Book. If you plan to store these in a typical database, you'll probably want a primary key. You might want that in your Book too, making a certain field mandatory. You could have the id be needed, or maybe require either a title and author or id for any given book. You can take it as far as you want, but there's no shame in just starting out with a bunch of optional fields until you figure out what you really need. Personally, I might choose to make the title, author, and id required. Depending on the interface, you could also have separate types for a checked book and a book that the user is editing the fields of. Then, when the user tries to save a book, you'd do a checked conversion from `UncheckedBook` to `Result&lt;LegitBook, BookError&gt;` and report to the user if they are missing some information.
Yes, I do. However, I have not put anything to do with that in the Rust toolchain library.
As long as the key remains secret, yes. 
Normally you shouldn’t have to. Try building using the developer command prompt instead of powershell. 
For me, it is still unclear, why the separation into two steps (alloc &amp; initialize) cannot be hidden from the user. 
When using the developer command prompt, I have get a different error: &gt; LNK1112: module machine type 'X86' conflicts with target machine type 'x64' This one is a lot more decipherable (although I am still not sure exactly why the mismatch is occurring). 
How is it different from the Transmission web interface?
Extremely cool ! Does it speak the same control protocol (so the android apps or transmission-remote keep working) ?
simplicity
Thanks. I built this library primarily to be simple to use and to create a technology agnostic . I cover less of the api currently, but I do cover the essentials. I also support custom elements.
It’s running the x86 vcvars batch file. You should be able to manually run the vcvars64.bat if it’s supported on your system. 
Are the elements rehashed once in a while if too many tombstones accumulate? Just wondering because all empty slots being tombstones would mean that the lookup would always have to traverse the whole table before being sure the element really isn't there, wouldn't it?
Looks like tech stack is a big one: [Transmission](https://github.com/transmission/transmission) depends on mostly C and jQuery, Tornado uses Rust and VueJS. It looks like the interface design is significantly different too -- if you click the link, there are some screenshots of the app so you can compare.
How to use this libraries to encrypt(encode) array of bytes? I don't find examples in docs :( lib1: [https://docs.rs/rust-crypto/0.2.36/crypto/blake2b/struct.Blake2b.html](https://docs.rs/rust-crypto/0.2.36/crypto/blake2b/struct.Blake2b.html) lib2: [https://docs.rs/hashlib/0.1.0/hashlib/blake2b/struct.Blake2b256.html](https://docs.rs/hashlib/0.1.0/hashlib/blake2b/struct.Blake2b256.html)
Not yet, but the feature is planned. We had to write our own library for Transmission so it's missing a lot of features still.
As /u/ErichDonGubler said the tech stack and UI are completely new and made with modern technology and usability in mind. Tornado also provides a number of other features, such as a full file-manager and RSS feed reader, which are slightly outside the scope of just Transmission.
&gt; This would be more convincing if Rust wasn't replacing their standard mutexes with parking_lot. :P Right, and that same post reads *"we would prefer to move more carefully and deliberately to measure performance, ensure stability guarantees can be upheld, and ensure that platform compatibility is maintained"*. This is more or less what I'm saying too! About alignment: each key and each value in Sanakirja is 8-bytes aligned, and I've spent quite a while on the design to ensure that.
So, I switched to the command prompt called "VS2015 x64 Native Tools Command Prompt". Here, I made sure to set RUSTFLAGS correctly `set RUSTFLAGS="-C target-cpu=native"`. Then, I executed `cargo run --release`, but I get an error right away: &gt; error: failed to run `rustc` to learn about target-specific information Caused by: process didn't exit successfully: `rustc - --crate-name ___ --print=file-names "\"-C" "target-cpu=native\"" --crate-type bin --crate-type rlib --crate-type dylib --crate-type cdylib --crate-type staticlib --crate-type proc-macro` (exit code: 1) --- stderr error: multiple input filenames provided When I didn't set RUSTFLAGS, things compiled, but I ended up getting the same error I noted in the title of the post.
Isn't this similar to placement new?
`Pin` doesn't change the lifetime system. `Pin&lt;&amp;'a mut T&gt;` is exactly like `&amp;'a mut T` except that you're not allowed to use any means (such as `mem::replace` or anything unsafe) to take ownership of the target, *unless `T: Unpin`*. A wrapper type (`Pin&lt;P: Deref&gt;`) is necessary because there may be other pointer types defined which implement `DerefMut`. Wrapping them conveys the same restriction: do not take ownership of the target. `Unpin` is implemented for almost any type. You'd opt-out by including a `PhantomPinned` zero-size marker. So it's the combination of a `PhantomPinned` field and a `Pin` wrapper around a pointer type which imposes the pinned restriction on a target value.
Shameless plug, my `boxext` crate provides a `Box::new_with` function that makes LLVM avoid memcpy too. https://crates.io/crates/boxext
I'm building a multi-view data structure. The entire idea is to store data in a queue but also allow arbitrary sections and orderings. In a parser for example you could use this to store every line in order, but also have the ability to fetch an iterator for specific lines (maybe those related to an error) without writing a filter() to do it after the fact. Or maybe what you're parsing has a meaning associated with it that's better expressed by a certain sort order, now you don't need to scramble your serialization by sorting it but can still gain the benefits of the sort.
Can the database be atomically snapshotted since it uses CoW? I've been thinking about writing an embedded database system similar to sqlite for Rust and of course it would use B+trees. I don't expect this project to become anything really, creating a database engine is a massive amount of work. But I might check out your B+tree implementation!
Wasm-bindgen requires you to have webpack, requires a special compiler component, has its own abstractions which detach you from what’s happening under covers, isn’t that approachable a code base in my opinion. Their tech also can’t be used for languages other than Rust.
I'm guessing that it has to do with `alloc` completing before the the stack gets prepared for the invocation of `init`.
Nah, not to my knowledge. Keep in mind it's like `2^n * (K + V + 1) + 8` or something like that so it's not like it's expected to snap into some really nice allocator-friendly size.
Have you seen the [web-sys](https://rustwasm.github.io/wasm-bindgen/api/web_sys/) crate? Maybe this would be better for you than using wasm-bindgen 'directly'?
web-sys is similar to my project, but mine is even simpler. If you look at my projects code you can see how simple it is. Also, I do custom elements a different way that I think makes more sense for wasm.
That all being said, it doesn't mean that you couldn't build linux distro with non-posixy userland. It just means that if you do so, it won't be compatible with lot of software, and you have to be fairly selective on what you can include in the distro. If you want to be fully self-hosting, then that might be bit bigger ask
This. People who still write JavaScript and refuse to even touch TypeScript are pretty dumb imo. TypeScript isn't fully a separate language, but rather an enhancement which solves many JavaScript woes in a seamless fashion, but of course you still have to have a deep understanding of JS and the APIs available in any given environment.
It would be nice if there was a way to let clippy know that (it's `deny` by default) and to prevent future contributors from messing it up.
Agreed. One of the selling point of typescript is that the runtime behaviour of TS is basically the same as JS. With the 1 major exception: decorators, which is an experimental feature implemented for angular. You can argue that google forced it in. And a minor exception: Enum, which is still quite straightforward. Unless you are using decorators, you can basically trust that if an error is a runtime error, it has nothing to do with typescript. TS is my favourite language, not because it’s the best in design, but because it brings (practical) sanity to a place dominated by JS.
The Rust compiler does very few optimizations, it's LLVM who ends up taking care of these. For example, the compiler can output a special LLVM attribute to say that the value is not aliased. Unfortunately, the compiler doesn't do it most of the time, because there are (or were) some issues with LLVM producing incorrect code. So it really depends on what LLVM is able to make out of your code.
I don't usually care much about name conflicts, but there's already an Android client for Transmission called [Torrnado](https://play.google.com/store/apps/details?id=com.gabordemko.torrnado).
Rust section: &gt; Last on our list is Rust, which neither grew nor declined but instead held steady at #23. This may be disappointing for its more ardent fans, which include some high profile and highly accomplished technologists, but Rust’s glacial ascent is relatively unsurprising. Targeting similar if lower level workloads than Go, a language itself that has plateaued in terms of its placement amongst these rankings, Rust suffers from the limits of a lower popularity ceiling while not receiving quite the same attention that Go did as a product of Google generally and people like Rob Pike specifically. By comparison, Rust’s ascent has been much more workmanlike, winning its serious fans over one at a time. It’s also worth noting that even if Rust never gets much beyond where it is today, it’s still ranking higher than well known languages such as the aforementioned Clojure and Groovy, as well as CoffeeScript, Dart or Visual Basic. Not bad for a systems language. 🦀😬🤙
You're right about the decorators but enums are actually just basic JS objects where their TypeScript key becomes the string key and the value is still the value--unless they're const, in which case TypeScript does do the job of replacing occurrences with values. I'm glad to see another happy TypeScripter. :) Plus the web APIs have gotten much better so large web applications are finally manageable.
&gt; but has less tricky concepts than Haskell. Why?
I made my first proc\_macro crate. The idea is accessing all the fields of a struct that share the same type, through an array, so it can be iterated over easily and without memory allocations. Here it is: [https://crates.io/crates/getters-by-type](https://crates.io/crates/getters-by-type)
&gt;99 out of 100 times we find the element and return early. I don't think that's true. Plenty of use cases where you're inserting an element exactly once (or not very often). 99 out of 100 seems implausible in general. I would be surprised if it's even &gt;50% across all hash table usage, but that's just a hunch. 
No, because tombstones don't count as empty w.r.t load factor. So if you're at e.g. 90% load factor then 10% of the elements are \*actually\* free, not just tombstones. Meaning it's very unlikely to walk for more than a couple of 16-elem groups before you find at least one empty slot. 
Cool! We also started with closures initially but then evolved into simpler API that looks very much like hashmap's Entry.
Special thanks to @KiChjang on IRC for review and corrections!
Also similar to "box" syntax but is available in Stable today.
I like this. When i heard of wasm support in Rust this is what I saw in my mind. Then I saw all the horrible things from the fragmented javascript world that bindgen uses like webpack etc .. I personally want rust on the web with wasm but I dont want anything to do with the crazy javascript ecosystem that changes popular frameworks more often than they change their underware. The JS world is a bad place and I hope with initiatives like this that we can have simpler ways of approaching webapps without 5 layers of hacks. 
This is fixed now. Thanks!
Are you talking about the game called Rust or support for the PS4?
Nice idea, needs a better name.
Seems like you're asking about Rust the game, in which case you want r/playrust.
I think that could be done in another derive. FieldNamesByType would be my naming for that one. I'm sure is not the best name either :P. What naming would you recommend?
Thanks to [lukaslueg](https://github.com/lukaslueg), [leshow](https://github.com/leshow), [r-darwish](https://github.com/r-darwish) and [Dowwie](https://github.com/Dowwie) for their (very significant!) contributions.
Are the plans to support transmissions existing HTTP API? This would allow organizr and sonarr support out of the box.
This listed "pain" in particular is extremely overstated: &gt; "Hm, I don't know if this is an error with TypeScript or in the transpiled JavaScript?" It's so trivial to distinguish ts errors from js errors I'd even consider it somewhat disingenuous to mention it as an issue. And I agree with you re: Rust and Typescript - the right tool for the right job.
struct-iter? I know your derive returns arrays and not iterators but I guess the intention is to iterate them right?
Hmm, tough to say without a runnable example to experiment with. Have you ever looked at the [DeserializeOwned](https://docs.serde.rs/serde/de/trait.DeserializeOwned.html) trait? From the serde page on [lifetimes](https://serde.rs/lifetimes.html): &amp;#x200B; &gt;**&lt;T&gt; where T: DeserializeOwned** &gt; &gt; This means "T can be deserialized from **any** lifetime." The callee gets to decide what lifetime. Usually this is because the data that is being deserialized from is going to be thrown away before the function returns, so T must not be allowed to borrow from it. For example a function that accepts base64-encoded data as input, decodes it from base64, deserializes a value of type T, then throws away the result of base64 decoding. &amp;#x200B;
Correct. That name conveys well the usefulness of the derive, despite being further away from the technical wording, and probably is also more attractive, since many people is alergic to the word 'getter' nowadays (blame that on Java's legacy). I'll think about it, thanks!
Yes, I didn't want to pick apart the blog, but that was the strangest claim. &gt; "Hm, I don't know if this is an error with TypeScript or in the transpiled JavaScript? Or did I just disable the wrong ESLint option?" * Error messages are generally clearly labeled and often come out of entirely different stages of the build. * TypeScript has very nice error messages. * JavaScript doesn't have compile-time errors at all. * TypeScript has its own linter, TSLint. I don't really understand where he was coming from with that.
Possibly yes, though it will somewhat change the meaning of beta channel and will tie semi-stabilization to 6 week release scheldule, but maybe it's not a bad thing since from user perspective beta does not feel too useful and semi-stabilization should be a thought-out decision.
I created this webpage entirely in Rust web assembly using canvas. I'm curious what people's performance is like on various types of browsers/machines. If you have time to give some feedback on your numbers on this website. I'd really enjoy seeing how canvas and web assembly could perform. This website runs on fairly bare bones bindings. If you are curious how this works check out the code: [https://github.com/richardanaya/ferris-count/blob/master/src/game.rs](https://github.com/richardanaya/ferris-count/blob/master/src/game.rs)
Something that I noticed, though, is that it has the same downside as placement new: it's not reliable. Most notably, it doesn't work on debug builds. I guess the same is true with yours.
Machine: pixelbook Browser: Chrome 1-1000 ferris = 60fps 10000 ferris = \~22fps 100000 ferris = 2 fps
100% agree with you, and that's part of why I wrote this library.
To add onto this, I put together a standalone example with DeserializeOwned. It doesn't use any network code, just spins up a single thread that takes a string and sleeps for a bit to give enough time for the main thread to add a new subscriber. &amp;#x200B; use std::sync::{Arc,Mutex}; use std::collections::HashMap; use std::thread; use serde::{self, de::DeserializeOwned, Deserialize, Serialize}; use serde_json::{self,Value}; pub trait MsgType {} #[derive(Serialize, Deserialize, Debug, Clone)] pub struct Point32 { pub x: f32, pub y: f32, pub z: f32, } impl MsgType for Point32 {} type SubMap = HashMap&lt;String, Box&lt;dyn SubBase + Send&gt;&gt;; type SafeMap = Arc&lt;Mutex&lt;SubMap&gt;&gt;; pub trait SubBase { fn callback(&amp;self, data: &amp;str); } impl&lt;T&gt; SubBase for Subscriber&lt;T&gt; where T: MsgType + DeserializeOwned { fn callback(&amp;self, data: &amp;str) { if let Ok(v) = serde_json::from_str::&lt;T&gt;(data) { (self.callback)(&amp;v); } else { println!("Can't turn incoming message to json {}", data); } } } pub struct Subscriber&lt;T&gt; where T: MsgType + DeserializeOwned { callback: fn(&amp;T), } impl&lt;T&gt; Subscriber&lt;T&gt; where T: MsgType + DeserializeOwned { pub fn new(callback: fn(&amp;T)) -&gt; Self { Subscriber { callback, } } } pub struct ReadHandler { join_handle: Option&lt;thread::JoinHandle&lt;()&gt;&gt;, subs: SafeMap, } impl ReadHandler { fn add_subscriber(&amp;mut self, topic: String, sub: Box&lt;dyn SubBase + Send&gt;) { let mut subs = self.subs.lock().unwrap(); if subs.contains_key(&amp;topic) { println!("Already listening to {}", topic); return; } subs.insert(topic, sub); } fn new(stream_val: String) -&gt; Self { let subs: SafeMap = Arc::new(Mutex::new(HashMap::new())); let subs_copy = subs.clone(); let join_handle = thread::spawn(move || { let subs = subs_copy; thread::sleep(std::time::Duration::from_secs(2)); if let Ok(v) = serde_json::from_str::&lt;Value&gt;(&amp;stream_val) { println!("Received {}", v["msg"]); match serde_json::from_value::&lt;String&gt;(v["topic"].clone()) { Err(e) =&gt; println!("Topic in msg malformed: {:?}", e), Ok(s) =&gt; { let subs = subs.lock().unwrap(); if subs.contains_key(&amp;s) { println!("To topic {}", &amp;s); let sub = subs.get(&amp;s).unwrap(); sub.callback(&amp;v["msg"].to_string()); } }, } } }); ReadHandler { join_handle: Some(join_handle), subs: subs, } } } fn main() -&gt; Result&lt;(), Box&lt;std::error::Error&gt;&gt;{ let incoming_json = String::from(r#"{"topic": "topic_a", "msg": {"x":1.3,"y":7.0,"z":3} }"#); let mut rh = ReadHandler::new(incoming_json); let cb = |p: &amp;Point32| { println!("{} {} {}", p.x, p.y, p.z); }; let sub = Subscriber::new(cb); rh.add_subscriber("topic_a".to_string(), Box::new(sub)); if let Some(jh) = rh.join_handle { jh.join(); } Ok(()) } &amp;#x200B;
Google Pixel XL Android Q Beta1 Firefox Nightly 1-100: 60 1000: 25 10000: 2 
Hello folks! I noticed that Struct `std::alloc::System` in Rust reference contains an example that atomically records the memory usage of a System allocator. I ran that example with manually using `libc::malloc` and `libc::free` to implement the `std::alloc::GlobalAlloc` trait and saw that 1300 bytes allocated before the `main` function. Is that some kind of minimal Rust runtime or anything else? Is there any doc about this?
Machine: iPhone X Browser: Chrome (Safari Webview) 1-1000 ferris = 60fps 10000 ferris = \~50fps 100000 ferris = 5 fps
Thanks a lot! I'll have a look tomorrow! I'm in Berlin time, so it's getting really late...
Yeah. But with enums you can’t just throw delete typescript stuff and expect the remaining JS to work. I agreed it’s still quite straight forward.
Closer to a reflexion system it seems.
Machine: OnePlus One 1-100 Ferris: 60-70ish 1000 Ferris: 20ish 10000 Ferris: 1-2 100000 Ferris: screen said 0, by stopwatch about 0.05
wasm-bindgen does not require webpack in any way. It was also built to be language agnostic, though it’s not currently being used as such.
if that's true their documentation isn't making it clear [https://rustwasm.github.io/docs/wasm-bindgen/examples/hello-world.html](https://rustwasm.github.io/docs/wasm-bindgen/examples/hello-world.html)
Yeah, the docs are written for the majority case. That’s the way that most people will end up using this stuff. But it’s 100% agnostic.
I feel like we have different concepts of "technology agnostic". I've grabbed at least 4 different languages that compile to web assembly, compiled a web assembly module, stuck it in a: ``` &lt;web-dom module="thing.wasm"&gt;&lt;/web-dom&gt; ``` and have instant access to the dom. What I see in wasm-bindgen looks to be like a code generator very tightly bound to the hip with rust build system.
I'd say it's not that approachable. Approachability is something I'd credit languages like ruby, python, or go with. Really easy to get up and running. The startup cost on learning rust is a lot higher, but that said, the initial learning curve on every language is tiny compared to how much time you'll use it over the years. My main problem with high learning curves is they slow adoption, which means fewer good crates, which means a worse experience for everyone. Rust is, however, one of the most readable languages I've ever seen. Readability is frequently confused with familiarity, but they're two very different things. What makes code readable isn't that it has syntax you're used to (though many will be), but that the rules are consistent, cohesive, and reasonable to follow. Even concepts that make the language harder to learn like borrowing and lifetimes are easy to read. I'm still new to the language, and I can sink a lot of time into figuring out what needs a dyn, impl, move, or lifetime, but once it's written it's incredibly clear what it is and why I needed it. I can follow third-party code too, so long as they don't get too up their ass with "clever" DSL in their macros. Even advanced concepts that seem far more complex in other languages, like concurrency and parallelism, are quite easy to read and understand. I refactored a synchronous SQL app into futures, and the new code was nearly identical, had equal complexity, but made it very clear the order of the async code. And unlike say, JS, which relies heavily on one specific implementation and convention (promises polled in the event loop), you get this clarity on any implementation. 
Machine: Kubuntu Linux 14.04 LTS 64-bit on an Athlon II X2 270 with 16GiB of RAM. Browser: Firefox 65.0.1 (Canoncial-provided build that hasn't had an update pushed to it yet) No clue what is causing it, but my Firefox clamps the displayed value to 10. Anything 1000 Ferris or below is 10 while 10000 Ferris wavers between 10 and 5, occasionally dipping down to 3.
So the problem of "too many open files" may be in the rust lib? My rust is on a ubuntu machine and imports the query traffic via nginx. I have adjusted the settings of nginx, but I am not sure whether rust lib and nginx will affect each other. So I want to try another web lib.
I also installed VS2017, and tried the same procedure out with its developer command prompt. No luck, as the same error appears.
Convert the url into the query syntax of the database, mainly to connect the api of the front-end web page call, using the RESTful api method. Therefore, when others test, they will ask thousands of requests at the same time to query the database. 
[here is the link](https://github.com/tantivy-search/tantivy)
opt-level=0 can be really bad. I usually set opt-level=1 for debug builds.
Here it is, no idea if its legit https://github.com/hyperium/hyper/issues/1422
I am curious that there are a lot of requests at the same time, I don't know which one can be handled effectively. At present, it seems that actix-web can support it.
The double lookup inside the Hashbrown code, still only computes the hash once. If you `get` and then `insert`, both methods will compute the hash, so you still need `entry()` to avoid that.
Oops thanks. I added the link in the post.
&gt; reading idiomatic Are you refering to https://github.com/mre/idiomatic-rust ?
I'm sure I have some bias (duh), but I find this description lacking. Anecdotally I know way, way more people outside of my Rust "circles" who have heard about the language, are excited about it, and would love to give it a shot if they can convince management or whatever other typical list of reasons that prevent people from using a new lang, than I did people who felt similarly about Go when it was first ramping up it's popularity. That says to me that Rust is fundamentally a more interesting / appealing language than Go, because if Go has "plateaued" it has done so with a massive artificial leg to stand on (Google). So if the idea is they may have similar appeal, I'd expect Rust to ultimately be much more popular, given it's winning all its fans without a gun to their head.
- Machine: OnePlus 6T - 1 Ferris: 60-70 - 100 Ferris: 60 ish - 1000 Ferris: 5 - 10000 Ferris: 5 - 100000 Ferris: 0
Should there be a fast path for when there have been no deletes at all in the hashmap? 
Lol I'm dumb
2017 MacBook Pro, 2.9 GHz Core i7, Radeon Pro 560 1-1,000: 60 10,000: 20-50 100,000: 4
I would agree with you back when Typescript was 1.x. After then the language evolved quite aggressively regarding the type features got introduced (to name a few, keyof and lookup types since 2.1 which is basically type family in Haskell, conditional types since 2.8, unknown type since 3.0, ). Its type system now is far from intuitive to Javascript users (or users of any language without a full-fledged OO type system). It's still convinent using it in some personal project but reading other's code in Typescript is increasingly challenging day by day.
When I read Web-first I hoped it would be webtorrent compatible. Unfortunately that doesn't appear to be the case.
What made you decide to use rouille instead of rocket/actix? They seem much more mature to me.
Okay but keyof and lookup types are both pretty intuitive once you see a couple examples and much like all the other features in TypeScript, they are a way to describe behavior that people already use in JavaScript, but provide an easy way to get compile time checking for such operations.
We've been building large, complex apps with TypeScript for over a year now, and I've never run into this problem. The only regular TypeScript-JavaScript oddity we run into once in a while has to do with the tsc compiler's type erasure, but this is rare and isn't a big deal once you get your head around what's going on.
1000 at 144 fps 10000 at 86 100000 at 8
I feel like there’s a better middle ground for memory management though. For most of the developers (applications, servers, web) memory management is fine abstracted away. What if explicit ownership was opt in?
Looking at this line - I'm not familiar with this ? syntax in the parameters. https://github.com/getsentry/sentry-cli/blob/88dd7e09622e1c29c18df9573bffd45a476595b2/src/utils/ui.rs#L48 Could someone point me to some documentation for the same?
that's fast! 
There's cargo-make which can invoke build steps/scripts to package a binary for your target 
What is the linker error? 
Described in detail here: https://stackoverflow.com/questions/55272754/error-lnk2019-unresolved-external-symbol-imp-shcreateitemfromparsingname-refe Also, look at the comment discussion. 
Take a look at https://github.com/rust-lang/rust/blob/master/src/libstd/rt.rs. I'd think that setting up unwinding would take a dynamic allocation. Most platforms also link a c-runtime startup object `crt0.o` which may also allocate.
I don't know about OP's reasons for choosing it, but it's the Finnish word for "dictionary".
I've got the following enums and classes. enum States { State1(Payload), State2(Payload), State3(Payload), ... StateN(DiffPayload), } enum Events { Event1, Event2, Event3... } match (state, event) { (State1(Payload), Event1) =&gt; SomeOtherState, (State2(Payload), Event1) =&gt; SomeOtherState, .... (State3(Payload), Event1) =&gt; SomeOtherState, // Handle other combinations of states that are different. } In the match, what's the best way to match the first three lines without repetition? I'd like to say something similar to (State1(Payload)|State2(Payload) | State3(Payload) , Event1) =&gt; ... Also this is a hypothetical example which I'm just trying to learn with - so please assume this is what I want to do.
By default, type parameters are expected to be `Sized`, which is typically what you want. That syntax just removes that restriction so the type doesn't have to be sized. See [`std::marker::Sized`](https://doc.rust-lang.org/std/marker/trait.Sized.html).
I think you’re missing thr windows 10 sdk. You can add it through the visual studio installer. 
What does \`\_\_nss\_passwd\_lookup\` mean on pprof and rust? I don't look up any passwords. I already have \`libc6-dbg\` installed and am using the go version of pprof.
Lucene is also an extremely fast general-purpose column store that performs very well on non-text data (which is why it's the backend for Elasticsearch). Where does Tantivy stand here? The readme makes it seem like it's focused on text.
Posted a few days ago, feel free to give your feedback with suggestions of fixes, cheers! 
Hey all, I wrote a simple Pong game in Rust as an example. Overall it went pretty well, but I reached a point where I was calculating some logic of a member of the game based on the game state, and I ran into some reference problems. I'm curious how you all might solve this part of the code. [https://github.com/richardanaya/pong/blob/master/src/game.rs#L226](https://github.com/richardanaya/pong/blob/master/src/game.rs#L226)
And the python framework tornado https://www.tornadoweb.org/en/stable/
It looks like you create a player (alongside some other game objects) when you initialise GameState::new(). Player.update takes a multiple reference to self and a reference to GameState. Would self.player.update(&amp;self) work instead of the memswap? You shouldn't need to create a new player (and other game objects) every update as the GameState struct already contains one.
"It is closer to Apache Lucene than to Elasticsearch and Apache Solr in the sense it is not an off-the-shelf search engine server, but rather a crate that can be used to build such a search engine." Are there any plans/projects which would fit the "off-the-shelf" search engine server on top of tantivy?
Via Google: https://github.com/toshi-search/Toshi
It's interesting how much Rust stands out from the trendline as far as StackOverflow is concerned. Is it because the community has a stronger affinity to github or a weaker affinity to StackOverflow? This sort of thing is so hard to measure either way. Anecdotally, Rust seems to be gaining traction that isn't reflected in this ranking. Rust was at 23 for Redmonk a year ago and I've heard of a lot more companies using it in the last 12 month than at that time.
Someone on Rust discord told me I was doing too much OOP, code is changed to do more logic in game state functions
Toshi is an independent project that aims at doing that. I don't have any plans on starting such a project myself as tantivy itself is super time consuming. If tantivy ever become popular and there seems to be a need for that, I'd love to go full-time, start a company and build that... But I don't think we are there yet.
For all of Google production this is what I said. It was measured when SwissTable was designed and true now. Of course you are entitled to your hunches. The 'plenty' of cases you are talking about are less than 1% in practice.
I wonder, are these things should be done by compiler some days or it is impossible to optimize code in such way and need help from human?
There are many ways to do such a fast path None of them worked out. Do you have a specific one in mind?
The question is a bit too large to be answered, but tantivy and lucene are very similar in that regard... But tantivy is missing some features. A couple of things though. Tantivy does not handle floating point Tantivy does not have range optimized int indexing. Tantivy does not have a geoindex. Tantivy does not optimize anything for terms that are unique.
It was a massive amount of work, and it is b-trees, not b+, because I needed multiple values per key. I don’t know what you mean by snapshot, but Sanakirja databases can be cloned by just increasing a reference counter.
Correct. Now if you guess what this crate does and where it was written…
The thing is that if you posted this in a popular sub like r/javascript or r/webdev, your comment would be massively downvoted, and I genuinely don't understand why. Also: JS devs seem to be hiding these layers of abstractions behind new "opinionated" tools with behavio defined by authors, so community must simply bear with it because it _Just Works (tm)_. For example: webpack and create-react-app.
The runtime seems pretty heavy ~ 370KB. Any plans to optimize for size?
I meant having a bool for the entire hashmap that says if a delete happened and if there has been no delete so far , use an insert implementation that merge the first loop and the second loop.
There's a minified version thats 60k not including gzip optimizations. [http://unpkg.com/web-dom@latest/web-dom.min.js](http://unpkg.com/web-dom@latest/web-dom.min.js) I have no current plans to optimize other than minification and better generation techniques. The value in this library from my perspective is it just magically makes your web assembly have access to dom with practically no effort and hope to keep that until there's an actual native solution to talk with dom.
Apparently Rust 1.33.0 [no longer automatically pulls in shell32.dll and its dependencies](https://www.reddit.com/r/rust/comments/avuwj3/announcing_rust_1330/ehi1gdb/) and it produced [what appears to be the same error](https://github.com/saurvs/nfd-rs/issues/14) in the `nfd` crate. There's a [pending PR](https://github.com/saurvs/nfd-rs/pull/15) for it which shows that the necessary change is to add `println!("cargo:rustc-link-lib=shell32");` to the `build.rs` for the crate in question. In other words, the problem is that the crate isn't declaring all its dependencies but it used to work anyway because the Rust standard library was pulling in a bunch of libraries in order to get access to `shell32.dll`'s command-line parsing functionality.
What if we just allow unstable feature flags to be used on stable releases, so library authors can try using them and report if they encounter problems? I guess packages using these features would have to be considered development releases, but maybe they would still get wider testing than beta/nightly...
Tried it. It doesn't pay off. This was something we thought would have worked because deletes in general are rare. Surprising as it may seem almost noone deletes from hashtables. I don't recall why exactly this didn't pay off though. It might have been code size/register pressure.
&gt; but it [loads half as many DLLs](https://i.redd.it/ggbipgsbadj21.png). Interesting. The shorter list *adds* `apphelp.dll`.
I just mean reading any code that’s considered idiomatic rust.
That's exactly the issue, and indeed, nfd was the crate causing me trouble too.
Machine: Android 9, Google Pixel 2 Browser: Chrome 73 1-1000: ~50-60 10000: 7-8 100000: 0-1
 iPhone X Safari 60fps up to 10,000 7fps 100,000
interesting. left to my own devices with what have now, I would have done the opposite i.e. a simple makefile that would invoke cargo-build to compile the android and iOS libraries (and desktop version) , then copy the libraries where needed and invoke the ndk / iOS build tools. I guess keeping everything cargo-driven makes it more coherent as a \*rust\* experience. 
The original SwissTable implementation reserves the very last bucket as a sentinel, which is required because of the way C++ iterators work. This means that SwissTable allocates space for a bucket that is never used. The Rust implementation does not need sentinels, and therefore can make use of the last bucket normally.
Cool, makes sense. Thanks for the answer!
Little cargo question: Is it possible to create a project that looks like: * \[**Lib 1**\] A first lib to do ***action\_1*** and ***action\_2***. The user can choose if he want the "***action\_2***" feature. * \[**Lib 2**\] A second lib to do ***action\_a***. * \[**Lib 3**\] A third lib that use all feature from **Lib 1** and **Lib 2.** * \[**Bin**\] A tool that use all libs and features. The goal is to allow the user to select what he want to compile (For example, he want only the **lib 1** without the ***action\_2*** feature and the **tool**). &amp;#x200B; Is it possible to do it ? Thanks!
lol. what. why? why would you not just insert 1-fffffff into a database?
I meant both "less concepts" and "less tricky" in that sentence. I believe this is because the intention of Rust is not to be an experimental platform to study programming language theory, but an industrial language. For instance, if Rust had the same philosophy as Haskell, the borrow checker would be called "the types of linear logic", and you would need to know about linear logic to go past the first tutorial.
yes, how?
I don't have a compiler available to me, but you shouldn't have to use sha1 (which would be wasteful due to duplicates), just loop over all numbers from 0x0 to 0x0FFFFFFF and format the integer as hex. for x in range(0x0..0x0FFFFFFF) { insert(format!("{:07x}", x)); } Be aware that this will generate millions of rows in your database, some simpler database setups might choke on that.
&gt; Since fundamental crates like serde or diesel started to work on stable Rust, we can see more and more signs of Nightly “demonization”. Rust: *allows users to switch to stable* Users: *switch to stable* Rust: surprised_pikachu.png Jokes aside, this observation means Rust has come far enough that it is viable for people to no longer stay on the bleeding edge. That leaves nightly with the experimentation model you describe, but also means the user base shrinks. So, my question about your otherwise fine-looking proposal is: Can I use semi-stabilised features from the stable toolchain? Because if that isn't possible, I don't think much will come of it since people won't switch compilers, as you observed. On the other hand, this looks a bit like Haskell's optional language features, and care would have to be taken making sure that the ecosystem doesn't get fragmented into multiple, worst-case incompatible flavours of Rust. For example, when Polonius eventually becomes semi-stable and some crates suddenly obey different borrowing rules, people would be forced to use it, and that would either bring back the problem where maintainers are expected to keep up with semi-stable changes at all times, or pressure the team into stabilising things early.
&gt;for x in range(0x0..0x0FFFFFFF) { insert(format!("{:07x}", x)); } Amazing! Will try that asap, thank you!
1-100: 58-62 1000: 37-41 10000: 4-5 100000: 0
How many rows would that be?
&gt; I meant both "less concepts" and "less tricky" in that sentence. Ah. That I think is primarily a matter of community, phrasing, and marketing. I don't think Rust is actually a language with fewer concepts than Haskell. Especially if you merely consider Haskell2010, subtyping, move semantics, and lack of type recursion make Rust more complex conceptually. This is to be expected as Haskell hides a whole lot more about the hardware than Rust does. The "lower" you get, the more detail oriented you are. &gt; I believe this is because the intention of Rust is not to be an experimental platform to study programming language theory, but an industrial language. Haskell may have started out as a research platform (see [A history of Haskell: being lazy with class](https://www.youtube.com/watch?v=06x8Wf2r2Mc)). However, it is an industrial language today. Sometimes extensions to Haskell are even driven by industry. The linear types proposal referenced below is a good example of that. That said, Haskell remains a good research platform and Rust is a beneficiary of that research. &gt; For instance, if Rust had the same philosophy as Haskell, the borrow checker would be called "the types of linear logic", and you would need to know about linear logic to go past the first tutorial. It's just called linear types, see [the linear types proposal for GHC](https://github.com/ghc-proposals/ghc-proposals/pull/111).
268'435'455 rows. According to my calculator.
I actually can't tell if you're messing with me or not. [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1c875dc43d70f20077cdb95f124e1fd2](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1c875dc43d70f20077cdb95f124e1fd2)
&gt;268'435'455 Perfect! thanks
I wasn't... Thank you
Thank you for your work! &amp;#x200B; We already use Tantivy in a tiny GIS application that is powered and supported by volunteers. Native float support might help to improve versatility. We currently map geo coords to integer for performing range queries. Works very well in this special case where the value range is limited. &amp;#x200B; I also plan to use Tantivy for a nextgen, stand-alone library back-end of [Mixxx](https://www.mixxx.org/). It is still in an early proof-of-concept stage, not yet published nor agreed upon.
I'm a beginner in programming
I think there's a bunch of things that are reflected in that trendline. * Rust has way more of its own support community than many languages: we tend to answer questions in a number of public fora, and to improve our documentation rapidly in response to confusion. * Some of these languages pretty much require StackOverflow to get anything done: one can write Rust by just reading how to do it and putting down code. I write quite a bit of Rust, and very rarely *need* StackOverflow for anything; I also write quite a bit of Python and, well… * StackOverflow is kind of self-generating: the more that's there the more gets added. Rust is pretty new to the StackOverflow game. *(Carefully walking the line of the rules of this sub…)* Do you notice any common characteristics of languages above the trend line *vs* languages below it? Are the languages on the trend line "in the middle" by your metric? (TIL that Objective-C++ is [was?] a thing. Huh. I had no idea.)
Thanks for taking care the time to reply :) and awesome work!
iPad air 2 (Safari) 1-1000: 58-62 10000: 17 100000: 1
Is WebGL support planned?
Not a man. :)
In contrast the ubiquitous Go vs Rust comparisons always miss the mark for me since they are very different languages. Go sits somewhere between "system" (C++/Go) and C#/Java. It's only closer to the former because it is compiled and not a VM. From a language perspective, it iss much closer the latter thanks to the GC and heavy runtime, despite having a very simple language model and type system. Go has found it's niche in network services because it is really amazing for relatively painless concurrency thanks to the goroutine concept. Rust will come closer with a stable async/await ecosystem, but still not compare since it will require much more attention and knowledge compared to Go where the runtime takes care of many issues like blocking calls transparently. Also the very simple type system is not that big of a restriction in networking code, whereas business logic heavy code would often be a better match for C#/Java style languages. They are inherently very different languages with a different target audience. 
yes
The point is that the compiler doesn't know anything about your requirements. Maybe the file being opened (and thus locked on windows) is used as a way to prevent other processes from changing it (or other shared resources) until now\_do\_lots\_of\_work() is done? The compiler knows nothing of that and that's why it can't optimize the closing of the file earlier. Instead of a file, the example could have used a MutexGuard, making it even clearer.
I think there are some additional factors here: * The heavy type system coupled with great compiler messages prevent a lot of questions since they often let you figure out a solution after some trial and error * Relatively extensive official documentation * Rust is usually picked up by experienced devs that already know multiple languages and can be more self-reliant * Rust does not have much in terms of a "killer product" (library) that would attract lot's of new developers just because they want to use it. Think React for the JS ecosystem. Often Rust is investigated and chosen due to the general benefits the language provides. I think users become quite knowledgeable in the core language before building something substantial with the library ecosystem and thus have an easier time with implementation
Currently working on an [Intel 8080](https://en.wikipedia.org/wiki/Intel_8080) emulator ([github](https://github.com/alexandrejanin/rust-8080)) I'm looking for a windowing crate, ideally one that allows me to draw pixels to the window directly (don't know if such a thing exists) I have used rust-sdl2 in the past, but while I like the API itself i don't like having to dynamically link libraries, install the development libraries etc.
This question kinda nerd-sniped me because I wanted to give a cohesive answer. So I stepped through a modified version of this code in visual studio and noted the call stack for every allocation. Keep in mind, this is for Windows and is going to vary on other OSes, but in general this is an outline of what happens before main() and why allocations are done: * Install a stack overflow handler in the main thread and ask the OS to guarantee a minimum stack size for the stack overflow exception handler, allocs occur when looking up the `SetThreadStackGuarantee` function address: * 24 bytes allocated: the module name has to be encoded as UTF-16 with a trailing null byte * 24 bytes allocated: the function name is allocated as a `CString` with a trailing null byte but not reencoded https://github.com/rust-lang/rust/blob/master/src/libstd/sys/windows/compat.rs#L18 * create the `thread::Thread` handle for the main thread * 4 + 5 bytes allocated: convert `"main"` to an owned string then converted to C-string with trailing null byte * 144 bytes allocated: three more function address lookups, to lock the `sys::Mutex` for generating a `ThreadId` * 16 bytes allocated: constructing a `std::sync::Mutex` for the implementation of `thread::park()` * 8 bytes allocated: constructing a `std::sync::Condvar` for the implementation of `Thread::unpark()` * 80 bytes allocated: allocating the `Arc` to contain all these https://github.com/rust-lang/rust/blob/master/src/libstd/thread/mod.rs#L1113 * 64 bytes for creating a `thread::LocalKey` to store this information I count 369 bytes total here. I ran the example on the Playground (Linux environment) and got 173 bytes. I don't know what system you're trying this on but 1300 bytes seems extreme. Did you remember to subtract from `ALLOCATED` in `Counter::dealloc()`?
It likely needs dedicated APIs or syntax. As other commenters have said: the unstable `box` keyword or the mythical `placement_new` feature.
Old laptop, Firefox stable 1-100: 60 1_000: 30 10_000: 3 100_000: 0
unfortunately, rust-nightly is the only supported rust version on BSD10 and PS4 runs BSD9
"One of the most notoriously difficult parts about Rust, and indeed one of its selling points, is enforcement of certain rules at compile time relating to how references work and what is allowed and what isn’t." A selling point indeed. I wouldn't say relying on a few clearly defined rules is difficult when the tooling is built in to do that for you: the borrow checker. Try fixing a circular reference in, let's say, perl, and find out why that can be notoriously difficult.
More likely, it could be used by the `vec!` macro - or _possibly_ by `Vec::extend`.
Thank you for this explanation. It seems part of the explanation to make it work is inlining the `init` function which does as little as possible. I was albe to easily reproduce it: https://godbolt.org/z/xW9lF6 There's only one call to `memset` which directly writes into the `Vec`'s buffer. :) BTW: I think your trait `VecHelper` should be an unsafe trait because in `init` you rely on `VecHelper` implementations establishing `len() &lt; capacity()`.
Thank you for pointing this out. :)
Yes. Call it "Reflection"?
Very nice! I've personally gone a slightly different road: Instead of using `proc-macro-hack`, implement their work around yourself! It's not so hard: Create an attribute macro instead of a functional macro. The attribute macro's job is to find and replace `macro!($tokens)` with a different output ([example1](https://github.com/CasualX/pelite/blob/master/macros/src/lib.rs#L25-L57) [example2](https://github.com/CasualX/obfstr/blob/master/impl/src/lib.rs#L245-L246)). Then create a `macro_rules!` macro in your main crate which uses the attribute macro, eg: macro_rules! f { ($rules:tt) =&gt; { #[$crate::f_attribute] const C: F = f!($rules); C }; } Here you define a `f_attribute` proc macro which will find &amp; replace function like macro invocations. You can do textual substitution or write a simple tt muncher equivalent for proc macros. I've done both. As you've noted, macros by example are _really really nice_! It would be awesome to mix both, to enhance the powers of macro rules with some procedural generated content. Nothing stops a proc macro from spitting out code which invokes a macro rules macro! [example](https://github.com/CasualX/pelite/blob/master/macros/src/lib.rs#L13-L16) All this works on stable of course.
Hi everyone ! I'm lost with Box. When should I use it ? Why should I use it ? What does it do (more than make my data Sized) ? I'm a bit lost.
/r/playrust
Is geoindexing something Tantivy will get in the future?
r/lostredditors this is the channel for the rust programming language. You are looking for r/rustgame
And via DuckDuckGo: [https://github.com/toshi-search/Toshi](https://github.com/toshi-search/Toshi)
If you ever do start that company let me know. I have some code I’d like to donate to that. 😉
Sorry if it doesn't answer your question. But I would say you would not find such structure, as peeking on channel does not make sense in same way as it does not make sense to have a way to peek if mutex is locked. 
I don't mind if peek must acquire a lock and then only be accessible while a handle to the lock is in scope. Eg: ``` { let x = r.peek(); // whatever } // lock released ```
For one thing, a lot of the SO answers on rust may be out of date. With all the language changes and Edition 2018, idioms that were common 3 years ago may no longer apply right now. For me what that practically means is that I ask a lot more of my questions on Discord and on urlo.
Oh sorry :D
Sidenote: I wish that as a community we figured out what chat platform we’re using. It seems like real-time discussions are spread out over IRC, Discord, Zulip and Gitter. It’s overwhelming.
That sounds more like you are trying to intercept the channel rather than actually peeking in it. Maybe primitive solution would be using two channels rather than one. Intercepting the workload in between them, if that makes sense. 
I would close the file and free the mutex guard explicitly, then. 
Well, it ain't 100000 that's for sure.
I prefer this subreddit over StackOverflow.
I don't think so. The capacity is enforced by Alloc(), after which the host vector is borrowed mutably until VecAllocation is either Init() or dropped. Also fwiw there is a playground link for Box helper at the end of Readme.
If you don't like templates you should check out go. And I don't think that C++ gained that many new features in the last 20 years. 
I suppose that will work. It does mean I'll have to do duplicate locking and moving the contents of the data-channel with peek() and then a 2nd time with get()
I really appreciate the work you and the rest of the Tantivy developers have done on this crate. Does Tantivy make it easy to index a document on more than one field -- for example, a document's Title and Body, rather than just the Body? And, if it does allow indexing on more than one field, is it possible to do weighted indexing / searching? &amp;#x200B;
https://rustwasm.github.io/docs/wasm-bindgen/examples/without-a-bundler.html
It sounds like `crossbeam_channel::bounded(1)` is very close to what you need. Consider filing an issue on crossbeam-channel to ask if it makes sense to add `peek()` functionality?
&gt; I don't think so. The capacity is enforced by Alloc() The important bit that I missed was that other crates can't create a VecAllocation directly. So, it's not possible for others to bypass your version of alloc as part of their own implementation of your VecHelper trait for some custom type that wraps a Vec. :-)
Ooh, this looks great. I look forward to part 2. My only pet peeve: I wish the examples used some variable names that had some friendlier names. I find it takes me 5 minutes to simply parse `a`, `b`, and `c` before I can actually start thinking about the underlying Rust logic. I feel like 2019 will be a good year for intermediate-level rustaceans like myself :)
&gt; I don't think Rust is actually a language with fewer concepts than Haskell. It's not necessarily the language so much as it is the library ecosystem. As somebody that tried to learn both Rust and Haskell, Rust was vastly easier to learn than Haskell even with prior FP experience in ML and no C++ experience. The language wasn't the problem, it was the library ecosystem and the abundance of "fundamental" packages with little to no documentation other than a bunch of category theory jargon. When I have to open 6 Wikipedia pages to decipher your documentation's opening paragraph and I still only have a vague idea of what your library does, there's a usability problem.
Device: OnePlus 5T Browser: Chrome - 1-100 Ferris: 60-70 - 1000 Ferris: 50 - 10000 Ferris: 5 
why is this query slower in tantivity than lucene " +"the who" +uk" ?
Oh I agree completely with your description of hackage, documentation, and usability.
I'm not trying to be snarky, but what does this have to do with rust?
The reason Go is often taken as a comparison is the similarity of timelines, not similarity of language.
Device: OnePlus 5T Browser: Chrome - 1-100 Ferris: 60-70 - 1000 Ferris: 50 - 10000 Ferris: 5 - 100000 Ferris: 0 (but still updates)
Windows 10 Laptop Intel i7-8750H @ 2.20 GHZ 6C/12T 40GB RAM Chrome 72 - 1 - 1000: 60 fps - 10,000: 18 fps - 100,000: 1 fps
Counter-point: chat discussions are ephemeral and diverse choice of channel trumps "following all" here. The project itself currently produces so much info that there's no one anymore who can even follow the project roughly while remaining productive. Even just figuring out _what_ parts of discord are important for your specific needs is hard. Easier ways to basically "subscribe all" would not help.
I don’t want to “subscribe all”. I just wish I didn’t have 4 different chat clients in order to ask questions in different rust communities.
I think what was meant by more complexity is: Having a new state (semi-stable) for features to be in, will increase complexity. i.e. What exactly is going to happen if a semi-stable feature get a breaking update? Who is going to put in the work for all the extra commit phases required for pulling features through this elongated pipeline to stability? Who even wants to implement this in rustc if it means more work for everybody? --- Even if all those things didn't matter, I'm still not convinced this is useful. It feels like premature optimization. The process of implementing Futures/async has things to be desired. But as far as i can tell, it is/was a unique problem. Features currently on the wish list are very different in terms of scope, design space, and community interest. 
This is a rust project?
Is it truly headless -- can it work without an X server, e.g.?
So from what I understand about these changes, does it make sense to keep readers around now like with writers?
If you start a patreon I will contribute. Or any kind of donations page. I would love to see an ES killer. Virtually my entire job is log search and performance and stability are very important to me.
This isn't some kind of new, serverless technology on the blockchain made of AI. Google chrome can be run in headless mode, exposing it as a service that is accessible through a well-defined websockets API.
That's the whole point of an opinionated formatted and you are trying to defeat it, so naturally it won't happen.
Something is taking time and `pprof` couldn't figure out what it's called. For whatever reason `__nss_passwd_lookup` is commonly the subject of this mistaken identity. (I haven't found details but I suspect it's like Missingno. in Pokemon usually evolving into Rhydon simply because Rhydon is index # 1.) Some Googling suggests this happens when a compiler (multiple languages) generates a function to implement a common operation when that generated function doesn't have a name; it's often some variety of `memcpy` or `memcmp` or so on.
These shouldn't be too hard to implement with two pre-allocated buffers of type T and use of the RCU pattern: * has two states: FULL and EMPTY. * sender can send type T to receiver * send blocks while state is FULL * recv blocks while state is EMPTY * receiver can peek(). blocking or failing is fine. returns &amp;T without setting state to EMPTY * It needs to be fast (no spin-wait!). * does not require T to have `Clone` * Does not need SPSC (precisely two threads interact with it) * Does not need more than 1 slot. This may be a little tricky: * Bonus: send() and recv() return error as soon as their counterpart is dropped. And this depends entirely on how select is implemented, and may be a performance challenge: * Need a way to select from a set of senders receivers (block until one becomes ready AND know which it was)
For a 0.2 I love the concept, but people definitely use dd with more arguments than you've provided. 
Like which?
Machine: 5.0.0-arch1-1-ARCH, other values irrelevant Browser: Firefox Quantum 65.0.2 (64-bit) (Mozilla Firefox for Arch Linux archlinux-1.0) LinkError: "import object field 'canvasrenderingcontext2d_set_fill_style' is not a Function" web-dom.js:4253:11 connectedCallback https://unpkg.com/web-dom@latest/web-dom.js:4253 
Buffersizes, Option to Copy to stdout/pipe
Block size for a start
Buffer size is ideally something we'd never actually have to care about, but the world is not an ideal place. I've never needed to copy stuff to stdout, have you? I admit it's the more unix-y way of doing things.
You can specify how large a chunk to copy with `--count`. Afaik we're past the days where the OS exposes underlying hardware block sizes for anything important.
The `conv` argument comes up for me more than I would like to admit
`Box` just puts a value on the heap. If you know how `Vec` works, it's kinda like that except you can only have 1 item in it instead of many. As for when it's useful, that depends. It's useful for situations where you want a value to have a stable address, or as you've mentioned, it's useful for working with dynamically sized data like trait objects. It's pretty common to use it less than `Vec` so don't be worried if you're ignoring it most of the time.
*Eyy, another year! * It's your **10th Cakeday** sanxiyn! ^(hug)
Yes, I appreciate the clarification but it doesn't exactly answer the question. Your clarification indicates my question is about chrome and not `rust-headless-chrome`. So for the sake of posterity, AFAICT: no, it's not headless in the sense that you can use it without X. `xvfb` is usually a tolerable workaround.
For what it’s worth, I agree with you 100%.
&gt; We’ve improved Pijul a lot since the last release (0.11, in November 2018). In particular, the new diff algorithms mean that Pijul is now a lot faster when recording. This impacts most operations, since Pijul automatically creates a temporary patch containing the unrecorded changes in the working copy before applying other patches to the repository. [diffs](https://crates.io/crates/diffs) is such a fantastic crate. Thanks so much for it. What else, if anything, do you see ahead with it?
Thanks! Is that a direction you want to go, though?
Congrats on the release! I recently started working on a log search server that uses Tantivy. My goal is to make it as easy as possible to set up and maintain, though not necessarily be able to reach the same scale as an ELK stack setup. I'd also like it to be small enough to run on a raspberry pi, although that might not be a good idea due to wear on the SD card.
Tweaking buffersizes can speed up things significantly depending on the kind(s) of disks you're operating on. Stdout is useful for reading say 16 bytes out of urandom or more commonly to pipe through gzip or openssl. 
A deeper issue I have with current rust frameworks for DOM is that a Rust dev has to see any JavaScript at all to be functional. This page does not look approachable to me and it looks like it has some trade offs which is why it’s not recommended out of the box.
I regret not putting a 50k option
I sometimes use `fdataconv=sync`.
When making a crate, I have a file src/snakecase.rs - what do I have to do to import this as Crate::SnakeCase ? 
Sorry, but you are wrong. Tuning block size for `dd` copies is very important and has very real performance consequences. The naive approach is to use the hardware block size (eg, 512 or 4k depending on the drive), but with larger queue depths on modern drives the throughput is greatly increased by using larger block transfer sizes: http://blog.tdg5.com/tuning-dd-block-size/ That isn't even to mention the effects that `direct`, `dsync` and `sync` can have on performance. The reason there's not a real `dd` alternative is because `dd` does it's job extremely well, despite having an idiotic CLI.
I use DD to stdout all the time. Examples: I'm gzipping the output, or base64 encoding it, encrypting it, streaming it over the network with socat, etc.
Thanks for heads up, forgot to fix a library to a version! should be working
I would also prefer an enum with structured information instead of a string. If I were to take guess it would be that returning an enum would make some users annoyed when that enum adds new variants, but that is just a guess.
Manually setting the block size on the devices I use causes a significant speedup. The USB stick I have writes almost twice as fast in dd with bs=8M
[removed]
&gt; If I were to take guess it would be that returning an enum would make some users annoyed when that enum adds new variants, but that is just a guess. There is a common pattern to work around this issue: Add a `#[doc(hidden)] __Nonexhaustive` variant to your error enum. This way, your users are forced to have a `_` case when matching the enum, which makes adding new variants a non-breaking change.
&gt; If I were to take guess it would be that returning an enum would make some users annoyed when that enum adds new variants, but that is just a guess. There is a common pattern to work around this issue: Add a `#[doc(hidden)] __Nonexhaustive` variant to your error enum. This way, your users are forced to have a `_` case when matching the enum, which makes adding new variants a non-breaking change.
Author of the README here. The goal of adopting Rust for that project has been to slowly start replacing some really bot rotted C code with something modern and maintainable. The project hasn't had staffing since last year, though, so it's got just the outer bits converted. The next thing to do would be to convert the hardware interfacing code to Rust. 
doesn't `cargo-make` output the build scripts in a format that is incompatible with any tool other then `cargo-make` ? I was looking for a _turn my crate into a make file_ tool, and I moved away from `cargo-make` as it seemed like the syntax was non-standard.
Hi all, feel free to ask anything about the CfP or the conference, I'm into the conference organization and pushing for a bright future for Rust :) If you have some interesting story about Rust that can be useful for anyone evaluating the adoption of Rust in a project, we are searching for you!
Very thanks for the explanation! I was wondering how the hack worked, but I hadn't had time to investigate yet. Now I have some material for a follow up :)
Unfortunately, you can't at this time. The \`|\` syntax is part of \`match\`, separating multiple patterns. Patterns themselves cannot have \`|\` in them. More information on the future of this feature here: [https://github.com/rust-lang/rust/issues/54883](https://github.com/rust-lang/rust/issues/54883)
Try looking at the other, more popular algorithms. Here's an example from rust-crypto's docs using SHA-3: [https://docs.rs/rust-crypto/0.2.36/crypto/sha3/index.html](https://docs.rs/rust-crypto/0.2.36/crypto/sha3/index.html). The code should be the same for using Blake2b. Incidentally, these are hash functions. They do not encrypt data.
You can do something like this: ``` #[derive(PartialEq, Eq, Debug, Clone, Copy)] enum State { State1, State2, } #[derive(PartialEq, Eq, Debug, Clone, Copy)] enum Event { Event1, Event2, } fn my_match(state: State, event: Event) { match (state, event) { (s, Event::Event1) if [State::State1, State::State2].contains(&amp;s) =&gt; println!("yay"), _ =&gt; println!("nay"), } } ```
Emmm, I already tryied `rust-crypto`. Question was - how to use mentioned libraries... I am novice in rust and don't understand what syntax should I choose to call functions it those libraries :)
proc-macro-hack notably uses `#[proc_macro_derive]` where as I'm using `#[proc_macro_attribute]`. Presumably because the former was stabilized first. Here's how the author of proc-macro-hack /u/dtolnay explains it: https://www.reddit.com/r/rust/comments/5x0j04/procedural_bang_macros_on_stable_rust_115/deegzrv/ I'm applying the attribute to a const then returning it (these attributes only work on items, not statements or expressions) which has limitations (such as being able to name the type of the expression generated). dtolnay appears to generate a macro rules (satisfying that the result is an item) and then invoking it. When I try that the compiler complains that generating macro rules is unstable pending macro hygiene. Not sure how he manages to generate a macro rules macro without hitting this error... Enjoy following up!
not exactly sure what this one means... cargo-make doesn't output build scripts. you define tasks and dependencies in a toml based file and cargo-make than execute them as needed. you can define how to install missing 3rd parties, which toolchain to use, how to setup the env before running the build steps and so on... you can define tasks that run command, scripts, rust code and so on...
Any feedback would be welcome
You might want to start with The Book to learn more of the basics: [https://doc.rust-lang.org/book/](https://doc.rust-lang.org/book/)
Yes, I already doing that, but this topic is for simple questions, so here is mine
Eventually `#[non_exhaustive]` will formalize this -- tracked in [#44109](https://github.com/rust-lang/rust/issues/44109).
&gt; I'm so used to maintaining my coding style by hand that my reaction to a code formatter that can't be configured to follow my style is to throw out the code formatter Note what I said at the beginning. &gt; I'm so used to maintaining my coding style by hand that my reaction to a code formatter that can't be configured to follow my style is to throw out the code formatter. The only reason I use rustfmt *at all* is because, with the help of unstable nightly features, it can be configured *close enough* to my style to make it worthwhile to run it once or twice a week, then revert the stuff I couldn't reconfigure.
The first link I gave you contains example code for almost exactly what you want to do.
I think I am misunderstanding the question, but you could rename it to `SnakeCase.rs`.
I was looking at this : https://github.com/chronotope/chrono/tree/master/src Trying to figure out how datetime.rs became chrono::DateTime 
It has nothing to do with any kind of pattern. It doesn't have to do with the size, or that an enum may add new variants. As I mentioned in the issue tracker, the _actual_ error type is defined in the `regex-syntax` crate. If the `regex` crate re-exported it, then the `regex-syntax` crate would become a **public dependency**, and this would be bad, because `regex-syntax` is an implementation detail of the `regex` crate. An alternative would be for the `regex` crate to copy the quite elaborate error types that are in `regex-syntax` and define a trivial conversion routine between `regex-syntax` errors and `regex` errors. But I don't see the point.
Isn't this what cp is for? ;-)
I'm really excited about [`RefCell::try_borrow_unguarded`](https://github.com/rust-lang/rust/pull/59211). 🤡
This works as well. Thanks for the example.
Thanks for the info.
I disagree that it's a false alarm because I went through the exact same thought process. I thought that this was an obvious memory safety bug in the first 20 lines and was prepared to dismiss the library as another beginner's folly until I read *very carefully* to discover the invariant. It needs a comment at least.
Thanks
You can re-export a module with a different name via `pub use`, I assume that's what chrono is doing.
I'm secretly relying on the support of other Rustaceans on the IRC channel to tell me that whatever I was claiming was incorrect or wrong (@talchas, @stephanyfx, @mbrubeck, @Mutabah, to name a few). The fact that they haven't mentioned anything during our session means that it is community-approved, so thank me not, thank the Rust IRC community!
`chrono::datetime` and `chrono::DateTime`, are totally different things and convention (enforced thorough lints) tells us instantly which is which. The snake_case/lowercase one is a type and the CamelCase one a module (of course snake_case could also be a variable or function; but this will always be obvious from context except in a use statement where it's the last path element; besides, variables as opposed to const or static items can't appear in module scopes).
The rankings only reflect StackOverflow and GitHub usage: - Not many companies put their code on GitHub, so company usage != GitHub usage. - The popularity on StackOverflow is measured by "# of Tags", which is a correlation of diversity of purpose rather than number of users asking/viewing.
This is a great article. Never knew about cascade, suspect I might be using that crate soon.
datetime.rs would contain a definition like: pub struct DateTime { ... } lib.rs would contain lines like this somewhere: mod datetime; pub use self::datetime::DateTime; This makes it accessible via `chrono::DateTime`.
Derive macros can expand to macro_rules macros.
I thought messing with "low/hardware level" storage details was the whole point of `dd`...
Yeah, I would still make the trait unsafe with a comment saying implementers must ensure there's extra capacity and that not doing so would invoke UB later. but I don't see a way of provoking undefined behavior using any safe code outside this crate. So, the security right now relies on the fact that only the copyless crate is able to create an instance of a `VecAllocation`.
Another alternative I've seen is to allow a `with` block that takes a value and any expression/statement that starts with a `.` is assumed to be as if called in the value. with(some_val()) { .chain(); .chain(); if some_condition { .chain(); } } Same as { let x = some_val(); x.chain(); x.chain(); if some_condition { x.chain(); } } Just a different take with different pros and cons. A different alternative could be to use something like `x.chain()_.chain()` which has the advantage that it doesn't get mixed up with range syntax.
Dell Precision 7520 (optimal: 1000) 1 ferris: 60fps 10 ferris: 60fps 100 ferris: 60fps 1000 ferris: 60fps 10000 ferris: 13-14fps 100000 ferris: 1fps Google Chrome 72.0.3626.121 (64-bit) Intel HD Graphics 530 / NVIDIA Quadro M2200 (per notebookcheck, most similar to Nvidia GeForce GTX 965M 2nd gen) Intel Core i7-6820HQ (2.70GHz, 4 cores, 8 logical) 32GB RAM Windows 10 Enterprise
20fps on Pixelbook at 10,000 ferris, 60fps up until that point
I think what he meant is that he can programmatically calculate the best block size
just make it smart and tell me if i am double sure when i try to dd my HDD/SSD instead of my USB pen , and im sold
It's peculiar that someone who seemingly hasn't used `dd` for more than one or two use cases started with a parable about "no exact replacement exist"ing. Unless you plan on matching all the features of `dd` (which I'm not sure is a worthwhile target), then I would recommend avoiding the comparison. `dd`'s interface isn't really that bad, and if the features you listed are all, then I think that wrapping `dd` in a small script would be pretty effective (to test for target existence, to print the progress bar, etc.). That's really the point of the unix system, is that even if the tool isn't exactly what I want, I can quickly tweak it into that.
I was very happy to have found the [memchr](https://crates.io/crates/memchr) crate... until I realized that I need quite the opposite thing. I have a `&amp;[u8]` and I need to find the first index where the element is NOT `b' '`, and the last such. Or, to say it in different words, I need to trim `b' '` from the byte slices start and end. I need it in a very hot loop and spent some time optimizing it myself, but I'm wondering if there's a better way. Does anyone know of a crate, or can point me to some kind of resource? If it matters, my general expectations are: The slices are mostly of length 8 or 10, they do start with blanks mostly, but most do not end with blanks. There are 2 CPUs that matter, and both can do AVX2 (probably not relevant, since simd doesn't seem to be the name of the game here). To avoid the XY-Problem: what I really need to do is to find out if the slice represents a valid float/integer, and I'm using the `lexical` crate to determine that, but to use it I need to trim the slice. Thanks for any pointers :)
If you return `self`, then this: let mut command = Command::new("foo"); command.arg("--bar"); if set_baz { command.arg("--baz"); } let result = command .arg("quux") .status() .unwrap(); turns into this: let mut command = Command::new("foo").arg("--bar"); if set_baz { command = command.arg("--baz"); } let result = command .arg("quux") .status() .unwrap(); It's ultimately something to be decided depending on how you plan to use the API. You could also mix the two approaches together and have: let mut command = Command::new("foo").arg("--bar"); if set_baz { command.optional_arg("--baz"); } let result = command .arg("quux") .status() .unwrap(); Where `Command::optional_arg` could have the type `fn(&amp;mut self) -&gt; &amp;mut self` or just `fn(&amp;mut self)`
I'm regularly hitting the problems mentioned with `chain_ref` and `std::process::Command`. [My latest example](https://github.com/assert-rs/assert_cmd/pull/72/files). I'm [unsure of a good way to improve the ergonomics](https://github.com/assert-rs/assert_cmd/issues/73). One benefit of owned-chaining is if a lot of expensive state can be moved into the built-version of the object. It gives the user the choice on when extra work is done for the sake of building multiple instances from a builder (by `clone`ing the builder). In contrast, ref-chaining requires everything all state transferred over to be `clone`ed. One downside not mentioned for owned-chaining is when you have a builder stored in a struct, it is rather annoying to modify the builder.
I've used actors in Scala (not Erlang) but they seem pretty close to the polar opposite of free parallelism and concurrency. You pretty much have to rewrite your code to be *explicitly concurrent* in order to use actors, don't you? For example, suppose you have code like let sum = a.iter().fold(0, |acc, x| acc + x * x); Assuming `+` is associative and `a` is sufficiently long, the above computation could be executed in parallel. Actually, depending on the type, I expect rustc is smart enough to parallelize it for free *somewhat*, via SIMD instructions. Using a library like [rayon](https://github.com/rayon-rs/rayon), you could parallelize it across cores with very little work. (See the example in the README)
So hold on, you want to have Cargo use old crates that work on your old Rust version? Why not just update Rust instead?
I would think there is no more efficient way of doing this than two for loops, one for skipping spaces at the beginning and one for skipping spaces at the end. However I wonder if you can change your code to use actual strings instead of byte arrays. You can pass a \`&amp;str\` or a \`&amp;String\` in to the parse functions of lexical without conversion, because both types implement \`AsRef&lt;\[u8\]&gt;\` since strings are stored as UTF-8 bytes. Then you can just use the \`trim\` or \`trim\_matches\` methods.
Crates have no way of informing Rust what version it should be compiled with, AFAIK the only thing they can tell is whether or not they are using the 2018 edition or 2015. However, the Rust version in Debian Unstable is 1.32, which is only one major release behind current. Are you sure you installed from Unstable?
**smh** What a profound display of misunderstanding the code you are translating from.
Great stuff! I will check this later. Did you made some comparison with minisat or glucose? Good luck for the rest of the implementation!
You wouldn't want to use a non-real-time GC for real-time applications of course, but assuming you use a real-time GC when you need real-time guarantees, why would it be a problem? 
Could the optimal block size be determined automatically, using increasing block sizes until performance starts decreasing? This would still incur a small startup penalty, but would make a sensible default.
Yes, I'm sure it was Rusr 1.32 instead of rust 1.33. After uninstalling the debian package I installed rustup and VSCode is happy now.
What was the command you ran for installing Rust? You probably didn't install any of the packages that Rustup also installs to aid auto completion and such.
One word "Stability" if u have any problems I suggest you direct then to the Debian Organization, they understand the thoughts in play better than myself.
Thanks! But it is only part of answer - how I can calc Blake2b256 (not sha3-256). Why [here](https://docs.rs/rust-crypto/0.2.36/crypto/blake2b/struct.Blake2b.html) they saying about struct, but not about how to use it? Also in your [example](https://docs.rs/rust-crypto/0.2.36/crypto/sha3/index.html) they return string, but how to get array(or vector) of bytes? 
I understand why Debian stable ships old Rust. What I don't really understand is why you'd want to use said old Rust for your own code, unless you're building a Debian package for it.
It was the code size increase that killed this idea. Bumping up the size of \`find\` just a little bit increases the chance it won't get inlined, and the cost of function call is higher than the savings from this optimization.
It does [not](https://github.com/abseil/abseil-cpp/blob/master/absl/container/internal/raw_hash_set.h#L555-L558). Layout(capacity + Group::kWidth + 1, capacity) `capacity` is 2^(k)\-1. The first argument is control bytes and the second element buckets. That's capacity + sentinel + extra group for floating windows control bytes, followed by padding to satisfy alignment for the elements, followed by capacity elements. There is no extra wasted slot at the end.
I only want to produce stable binaries, I don't want to risk a bug in rust causing there to be a bug in my application. I understand that rust is new and that it's expected that most things are unsupported on older versions. However this will not always be the case and it's important to be prepared for a future where the version of rust in Debian Stable will be good enough for most things.
thinks 
I used whatever the VSCode extension used, when that failed I used git clone and cargo build.
If you're worried about new Rust versions introducing actual miscompilations, that is _extremely_ unlikely. The Rust compiler has a ridiculously sized test suite, and basically every release also involves running `crater` - a tool that takes every single crate on crates.io, compiles them with the new compiler version, _runs all of their tests_ and records results. Errors in the compiler itself are therefore a very, very rare occurence.
I used apt to install rust, so I didn't get rustup. I suspect in the future Debian will ship with an alternative version of rustup that follows the systems directory layout. See perl on Debian for an example.
Would a macro like with!(value, { .chain(); .chain(); }); be possible on stable rust right now?
When VSCode was running rustup and claimed it couldn't be found I wrote a shell script that echoed the correct things give the arguments. This didn't solve the issue cargo was having with building the VSCode Extension.
Take it up with the Debian Organization, they do the same thing with gcc and I don't know why.
I write a lot of C++ and Rust. I need SO for C++ every day, I’ve never needed it for Rust. That’s a pro of Rust over C++.
I know exactly why - they have a very specific definition of "stable" (as in "unchanging", not "reliable"), and they stick to it very closely, which is good for what they want to do. I'm just saying _you_ don't have to stick to the builds of Rust provided in the repos if you're building your own stuff - just use rustup or build it from source or install from a third party repo or whatever.
Not really. Writing to physical storage is probably the most fickle operation you can do. There are a multitude of things that could cause two identical writes to take different amounts of time. The primary factor is caching. Writing to memory is far faster than writing to physical storage - even for NVMe drives. So the OS writes things to memory instead (as long as there's room in the cache), and then schedules writes to occur later so you aren't stuck waiting for your write to finish. The second biggest factor is scheduling. If your OS is doing a bunch of different tasks, it's going to take longer for your writes to complete because the OS has to spend time doing a) other disk IO, and b) non disk IO stuff. Caching can be disabled and you can force writes to complete before returning, which is great for data integrity, but kills your performance. You can change the scheduler to prioritize disk IO but then your system is going to grind to a halt. Not to mention each drive is different, so an 8TB Hitachi drive may have a different optimal transfer block size than an 8TB Seagate drive. Hell, two iterations of the same 8TB Hitachi drive could be different because they switched controllers between the versions. Storage is a pain in the ass.
I like the idea of a cascading invocation operator but the implementation of the syntax in op's `cascade!` macro doesn't seem super intuitive or rustlike. This with version is a lot easier to parse and interpret. Long term though it would be great if rust implemented some first class support so you could just chain like `value..chain()..chain();` I'm also curious how parsing this syntax would play alongside parsing range expressions or struct update syntax, both of which already utilize `..`
Well, that would make it easier to use, but not faster (I tried, that's where I'm coming from). Right now, I'm using two simple loops, but, yeah, I was hoping for something more, since I was surprised this little function mattered so much even in the highlevel-benchmarks, and I'm not even using it as often as I'll need to.
I don't understand why the expression on the empty string below evaluates as true. An empty string shouldn't be an uppercase because it's empty. Is this something to do with how Rust handles an empty iterator? fn main() { let message = ""; if message.chars().all(|c| c.is_uppercase()) { println!("uppercase"); } } https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=065cfec52037c8275c88c3e6ac8da022 
I like that `..` syntax though. Would be neat to add to Rust - though I suspect we're getting a bit of feature shock around here haha. 
Doesn't seem to be the case though: https://github.com/icefoxen/bcp/blob/master/src/main.rs#L130
Blake2b works the same way as SHA-3. You should be able to use Blake2b just by changing the names in the SHA-3 example. The reason for this is that they both implement the \`Digest\` trait documented here: [https://docs.rs/rust-crypto/0.2.36/crypto/digest/trait.Digest.html](https://docs.rs/rust-crypto/0.2.36/crypto/digest/trait.Digest.html) On that page you can also find the answer to your second question: you can get the bytes using the \`result\` method. Unlike \`result\_str\` which returns a \`String\`, the \`result\` method wants a \`&amp;mut \[u8\]\` to write the bytes to. So for example if you wanted a `Vec&lt;u8&gt;` containing the output bytes, you would use it like this: // Create a Vec&lt;u8&gt; full of zeroes that is exactly the right size. let mut bytes = vec![0u8; hasher.output_bytes()]; hasher.result(&amp;mut bytes); Since the output size of Blake2b is 64 bytes, you could also make `bytes` an array like this: `[0u8; 64]`.
Then there is literally no point to Debian existing. It's meant to be used by ppl who share the same values and I believe there are quite a few... Fell free to ignore that segment of your userbase though as we are a hardy and independent ppl. Thanks.
I definitely use other dd features. Stuff like sparse (fallocate can file punch holes in-place but sometimes you just want to speed up a block transfer), notrunc/append with bs= for working in fixed-record-size files, noerror (although dd_rescue does more). The one thing I've really wished for in dd is a way to transfer less data remotely, checksumming like rsync (but rsync doesn't like block devices). I made my own a while back, though. https://gist.github.com/ephemient/bfae492df93dcdb2fe17
I don't really know. I've tried to compile it to WASM, maybe incremental/interactive diffs would be the next step?
First of all, you're not _my_ userbase - I'm not affiliated with the Rust project in any official capacity. Secondly, I'm still not sure what your end goal here is, and how it's related to Debian's philosophy.
The docs for this method are here: [https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.all](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.all) It says that an empty iterator returns `true`. I find that a little surprising at first as well, but really neither `true` nor `false` makes much sense when the iterator is empty.
I've also come to enjoy https://crates.io/crates/tap
Debian will, and is, ship old versions of rust, but currently it seems as though the infrastructure for rust(cargo) is not equipped to handle that fact.
Getting back to this late, but there are definitely ambiguities. Vowels are important in Semitic languages. Roots are, uh, mostly consonantal, but it doesn't make the vowels less important. You can mainly tell what's going on because of context and they give you enough hints to work it out... bwt I dwnt thnk 'ts 'nhrntly dffrnt wth 'ndw'rwp'n Lngwgs. Yw mwstly r f'n 'f yw knww th' vwcbwlry 'nd th' cwnt'xt.
Forgot to write, the conference will be in Florence
Tap was what I was originally expecting to recommend when I started writing the article, but cascade ended up being so much better (both the specific Rust crate and the general pattern of method cascades) that I opted to not mention tapping 😉
Then you need to be more specific about what is "not equipped" - so you're trying to install the VSCode extension. Which version of VSCode? Which extension (there are multiple forks of it)? What exact errors are you getting?
No, I am not able to use it only with simple changes. Maybe you can test it? Why \`Blake2b::*blake2b*();\` want 3 parameters? Or why \`Blake2b::*new*()\` want one parameter (len) ? Why should I set it? Is it not known? \--- Also I try 3 different \`Blake2b\` in rust and had 2 different result (and all of them diff from what I've got in scala). In scala I just wrote: \`Blake2b256.*hash*(input)\` - input is Array\[Byte\] here
sure: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4b58a3c4d885b24a2cc53e85ddd8c263 lots of problems with this, can't take arguments, etc. but that's just a macro_rules! macro, i'm sure a full-fledged proc macro could do a lot more
Returning `Self` when using the builder pattern also requires that errors be deferred, because it is not possible to return a value associated with the operation (such as a `Result`). ```rust let foo = FooBuilder::default() .with_bar(bad) // Uh-oh... .with_baz(good) .build()?; // Fails here. ``` That's not so bad for client code, but sometimes it can be awkward to pile verification logic into a single `build` function. Arguably, that logic belongs in the _property functions_, not `build`. Maybe cascades could enable something more like this instead: ```rust let foo = FooBuilder::default() ..with_bar(bad)? // Fails here. ..with_baz(good)? .build(); ``` This also allows for more natural usage of the builder type when not chaining its methods.
Formal logic says that all statements about elements of the empty set are true, so this is indeed correct behaviour. 
I think you got it switched up. datetime is the module, while DateTime is a struct.
The issue was already reported and I followed that solution which was to upgrade. What I'm pointing about now is how when I ran "cargo build" it continued to build even though rustc was too old to complete. This is also an issue when selecting crates to download, currently I see no way for cargo to download the version of a crate that would actually build.
That's not something Cargo can do, then. Crates don't declare a minimum compatible Rust version, and even if they did, it wouldn't always be possible for Cargo to just use an older version of the crate when the application being built requires a newer one.
&gt; With references and borrows, however, it is not possible to upgrade an immutable reference into a mutable reference, at least not safely. I don't think it's ever possible to correctly upgrade an immutable reference to a mutable one, even with unsafe code. An unsafe call to `std::mem::transmute` will let you do it, but it is always undefined behavior. I think that's because the compiler applies optimizations that are only applicable to immutable references, so in upgrading to a mutable reference, you're breaking preconditions at the compiler level.
The argument passed to `Blake2b::new` is the length of the output in bytes, which can be between 1 and 64, the maximum size of a Blake2b hash. This number should match the size of your output array or `Vec`. It looks like `Blake2b::blake2b` is a convenience method that does everything in one step, similar to Scala, but also requires a key. 
Cargo should be able to fail when continuing would fail. It should suggest it's best guess as to a minimum rust version. I know not a lot of systems do this, but this would be a welcomed feature.
This could work if Cargo could be updated more often than Rust, but the two release in lockstep, so it's not like you can have a version of Cargo that knows what future versions of Rust will look like _and_ can apply that knowledge to guess required compiler versions for crates. The best thing it can do is try to build and then fail if the code is incompatible, because determining whether the code _is_ incompatible would effectively amount to the same thing.
&gt; is the length of the output in bytes I understand that, but why should I set it? Okey, I've found in scala Blake2b sources that size is 32, but result bytes array is not equals... Also, why \`Blake2b::blake2b\` want \`key\`? What is that key about :)? \--- Maybe you can give me advice about how to test this one [https://docs.rs/hashlib/0.1.0/hashlib/blake2b/struct.Blake2b256.html](https://docs.rs/hashlib/0.1.0/hashlib/blake2b/struct.Blake2b256.html) ? It is not the same :(
Crate versions should indicate the minimum version of rust they require and this should be included in the list of available versions Cargo downloads.
Crate versions having a fixed Rust version requirement is definitely a possibility. As for your second point, that's not going to work with how Cargo handles dependency locking. A "feature" that silently installs older versions of dependencies is very, very no bueno, no matter how good the intent.
I would be satisfied with Cargo to fail by default, someone else can always write a more powerful tool.
When are the workshop being held? Can I participate of the workshops without loosing any of the conference talks? Thanks for organizing this conference!
Then you might want to follow the discussion [here](https://github.com/rust-lang/rfcs/pull/2495).
I would actually really liked reading about why you prefer this to tap and in which situations
Any ideas on how better to name them?
Thanks. I even had that page open and didn't see that. This is really surprising to me, but coming from Python, many things are. 
That's helpful for understanding the design choice here, thanks!
&gt; all the ergonomic improvements in rust 2018 are really messing up my book that consists entirely of running face-first into compiler errors so i can explain concepts https://twitter.com/Gankro/status/1108568083260735489
You can make them take arguments, even with \`macro\_rules!\`. &amp;#x200B; https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=45faaf8ea417d322da11131fa7a66731
I want to answer this, but there are several different competing definitions of “real-time”. Can you provide an example of a real-time GC?
Thanks :) So far I only made some preliminary comparisons with the old code base to make sure I get the same performance (which I do now). I submitted the old version to [last year's SAT competition](http://sat2018.forsyte.tuwien.ac.at/index.php?cat=results) where it ranked just below glucose and minisat. Both of them implement some widely used techniques (e.g. bounded variable elimination) that I haven't implemented yet and that should improve the performance on many instances. In general I would say that regarding performance rust is at no disadvantage for a problem like this, even when performing a lot of bound checks.
Yeah, all options should be possible entirely as macros as it's only syntactic sugar.
Nope :) It's similar in the sense that it's used to copy data except it's not on a filesystem level, rather at block device level. Imagine an ISO for example, you can take the raw data from that and put it onto a block device and it replicates the filesystem from that ISO and all the partitioning too.
Your real world example does not convince me of the need for this honestly. The first solution that came to my mind was using an args function that could take an argument that implements IntoIter. Then you can supply any optional arguments via the args function and there will be no need for making chaining overly complex. Unsurprisingly that function exists in the Command API.
Derp. You're right; I had switched around what snakecase meant at first and neglected to re-match it with what it signified. Fixed now.
 $ sudo dd if=stuff.img of=/dev/sda ^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C $ ls -bash: ls: command not found Yeah I've never done that before..
The biggest concern is one of ergonomics: There's simply a lot of overhead to using tap. Compare a basic method chain to the equivalent with tap: let foo = Foo::default() .chain() .chain() .chain(); // Versus: let foo = Foo::default() .tap(|foo| foo.chain()) .tap(|foo| foo.chain()) .tap(|foo| foo.chain()); For each method invocation, you need to: * Invoke the `tap` helper function. * Wrap the method invocation in a closure. * Bind a variable name for the intermediate value. * Repeat the name of the intermediate value when invoking the method. In some cases you can simplify this a bit by taking advantage of UFCS: let foo = Foo::default() .tap(Foo::chain) .tap(Foo::chain) .tap(Foo::chain); But that's only available for some methods (i.e. ones that don't take additional parameters), requires you to repeat the type name (which can sometimes be pretty long), and requires that you wrap the method you want to invoke in `tap(...)`. Functionally, cascaded method invocation operator in Dart does the exact same thing that `tap` does, but in a far cleaner, more succinct way. While the `cascade` crate doesn't quite manage to be as clean as the functionality in Dart, it seems like a substantial improvement over `tap` to me. &amp;#x200B; Hopefully that clarifies things 😁
I didn't expect the trends shown in that doc, I basically expected speed to increase until your buffer was Big Enough and then mostly flatline... Not decrease! Thanks, I'll add an option for this. And play with the scripts in that article too, probably.
That looks way too much like old JavaScript to be a sane idea Imho…
At long last, Rust will have the expressive power of Visual Basic 6! https://docs.microsoft.com/en-us/dotnet/visual-basic/language-reference/statements/with-end-with-statement
I expect this was a joke, but now I'm actually tempted. It could see if the destination file is a mounted device and refuse to touch it...
A different syntax would need to be determined, since `..` is already the range operator.
Great Post, I was searching to find the difference between these libraries and this helped me understand it a little better. Also: Pettines is fun
A case where returning `self` by value is necessary is when you have builder methods that change the type of the builder.
I have added a few corrections to the post. Thanks again to the fan-freaking-tastic Rust community!
Would returning `self` work in that case? If you're returning `self` directly, doesn't that mean that the method *isn't* changing the type of the object?
Thanks for the detailed answer!
If you only have a single receiver, you can create a trivial wrapper around a crossbeam channel that adds peak by caching the value output by recv...
I'd be very interested in trying this for a crossword workload (see the [Phil](https://github.com/keiranking/Phil/commits/master/third_party/glucose-3.0) repository, which is C++). I tried a bunch of solvers a year and a half ago, and glucose significantly outperformed the rest. I can make some cnf files if you're interested.
Look at the `faster` crate aswell. 
Nice write-ups. I've written my own SAT solver in rust but not that interesting. It's worth noting that VSIDS is no longer the leading heuristic. [MapleSAT](https://sites.google.com/a/gsd.uwaterloo.ca/maplesat/) has recently beaten glucose by changing to an online ML heuristic (the hybrid LRB/VSIDS heuristic performs well too)
Yeah this is definitely one of the nicer features of Dart and it would be great to see it in Rust.
Yes. You probably want to create and keep a single reader per index in your process. 
How was that data gathered? How was it normalized (e.g is it measured in terms of total number of inserts across google, biasing the results to a few heavy users?). It almost certainly isn't anything near that extreme for my own code, so I'm a bit suspicious that you're reporting something *that* high for other code. 
The idea is to use tap to insert a "ref-style" chain in a "move-style" chain: ```rust consume(Foo::default() .chain_move() .chain_move() .tap(|foo| { foo .chain_ref() .chain_ref(); }) .chain_move() .tap(|foo| log!("I have a {:?}", foo)), ); ```
I figured it was possible but I didn't have time to figure it out. Thanks!
It is called tantivy... This query is slower because lucene's algorithm for this query is better. Lucene knows how to compute it as the intersection of the words +the +who + UK and postfilter the results by removing the doc that do not match the phrase query. Tantivy computes the intersection of the phrase query and UK. Computing the phrase match or not is the most expensive operation so lucene's approach is better. That being said this is the only "white box" query in this dataset. I imagined it precisely to emphasize the impact of this lack of optimization in tantivy. (+The +who returns a lot of more results than the phrase query)
Yeah, I plan to implement that at some point, but as the branching heuristic it is only loosely coupled to the rest of the solver I went with the simpler one first, it's easy enough to replace/extend later on. Same reason I'm only doing luby restarts for now.
the `conv` argument is super important. Namely `sync`, `fsync`, and `noerror` since they let you ensure the write occurs, and lets you ignore errors which you know will _happen_ but don't matter for esoteric reasons. 
If you have cnf files I can add them to my benchmark set :)
&gt;but coming from Python, many things are. Python and pretty much any other programming language makes this choice. As stated, it comes from formal logic. You can try it out in python with the all function on the empty list. &gt;&gt;&gt; lst = [] &gt;&gt;&gt; all(lst) True The exact equivalent to the rust code you wrote I think would be something like this: &gt;&gt;&gt; lst = [] &gt;&gt;&gt; all(x.isupper for x in lst) True As you see, you may put any kind of condition there and it would evaluate to true.
&gt; i'm more interested in webgpu support Why? I mean, WebGL applicability is wider - think games, visualization.
Having your functions return `std::result::Result` is the standard method. It's like using Option, but you can return a meaningful error type instead of just None when the function fails.
Apple is not going to support webgl 2.0
not to mention the classic [BitTornado](https://en.wikipedia.org/wiki/BitTornado)
This is sort of irrelevant but if all you're doing is, for example, copying an image to a disk, you should just use `cat`. cat some.img &gt; /dev/sdb It's harder to get wrong and it's faster than `dd`.
This was the series that really made Rust click for me, glad to hear it's being maintained. I recommended it for anyone struggling to grasp ownership. 
Old Javascript had a lot of issues, syntax wasn't the biggest one. Just an alternative.
What a hilarious quote. Thanks for sharing.
That's a good suggestion - we'll mark the trait as unsafe.
It *is* a false alarm, since the library is safe. Adding a comment would be good though, I agree on that :)
To build up on this, you might run into trouble when you have different error types. There are better ways, but you can start with `Result&lt;T, Box&lt;Error&gt;&gt;`.
This reminds me of [Kotlin's scope functions](https://kotlinlang.org/docs/reference/scope-functions.html), which are fantastic. They're a consequence of the language having extension functions and lambda with receivers, so I don't think it would be so easy to replicate them in Rust, but at least it's something to think about.
Or require a --force at least!
The “All issues labeled...” links at the bottom are broken, both 404.
Probably rouille's unflinching simplicity and lack of magic.
It's equipped perfectly fine. If you have an old compiler, and you want to compile Foo written in Rust, then you need to go look for the last version of Foo that compiles on your version of the compiler. That's the trade off you make with Debian. The rest of the world moves forward, but you don't get to. If you want to use an older compiler, then you need to use old libraries and old applications, written in Rust. This problem is more pronounced in the Rust ecosystem than, say, C, because Rust is young and is still moving (relatively) fast compared to C, so it's much easier to notice. But the basic steps are the same: you need to stick to using older software.
Yes, this is an important failure mode that isn't very good today. There is an RFC open for adding something like this.
That's hard to do if Cargo downloads the latest version of something on you.
Maybe you have an older compiler, but you want to build ripgrep. But ripgrep tracks the latest stable release of Rust. So if you want an older compiler, then you also get an older ripgrep. For example, this would work: $ git clone git://github.com/BurntSushi/ripgrep $ git checkout 0.8.0 $ cargo build Cargo won't download new versions of crates because ripgrep has a lockfile. If you're building a library, then you'll need to specify version constraints in your `Cargo.toml`. It won't be playing nice with the rest of the ecosystem, but version constraints exist for a reason. If you need them, use them.
RIP in pieces to the paragraph where I vented about `struct Iter&lt;'a, T:'a&gt;`
Awesome! Thanks for letting me know. That kind of comment really help me stay motivated :). &amp;#x200B; Is this openfairdb? To be honest, tantivy is really suboptimal for geostuff and range queries. It will eventually get better. Your Mixxx project sounds super nice too!
Sure why not? In my experience, businesses are more likely to use exotic tech for analytics than for their core business search engine. 
Ha ha :) Thanks!
Thanks! I actually created a Patreon a long time ago, but instantly felt too shameful to put it up. 
I just checked, RLS does have a lockfile.
\&gt; Does Tantivy make it easy to index a document on more than one field -- for example, a document's Title and Body, rather than just the Body? &amp;#x200B; Super easy yes. Here is an example code. [https://github.com/tantivy-search/tantivy/blob/master/examples/basic\_search.rs](https://github.com/tantivy-search/tantivy/blob/master/examples/basic_search.rs) &amp;#x200B; \&gt; And, if it does allow indexing on more than one field, is it possible to do weighted indexing / searching? &amp;#x200B; Actually not easily yes. Right now, ranking is BM25 which means that the weighting of each field is automatic. Fields that are shorter (e.g : title) will be considered more important. &amp;#x200B; In the current version, you need to write quite a bit of code to tweak this score and boost a specific field. In the next version it should be much easier.
Hey! Long time no see :) Is your code opensource? I'd love to see that. &amp;#x200B; I've had some bad experience using tantivy on Arm 32 bits. I forgot if it was because the index had files too large or if it was something deeper. Is your Raspberry PI is 64 bits? I'm interested in your experience with tantivy on ARM.
That should be quote for next TWIR
Yes, now you have to find the last version of RLS that compiled with your Rust compiler.
Ah, that's very cool! I hadn't thought of using \`tap\` like that. This helps to bridge the gap between ref-style and move-style chains, but I still think returning \`self\` (whether by value or by ref) is a fundamentally flawed way of implementing method chains.
Pixel 3xl 1000 60fps, 10000 10fps
Surely there would be a more ergonomic way without introducing new syntax?
I would highly recommend just sticking with the [failure crate](https://github.com/rust-lang-nursery/failure). - It is part of the Rust language nursery. - Has excellent documentation. - Can be used to easily handle cases where you have lots of different error variants (coming from other crates perhaps) which you don’t need to handle differently. - Lastly, there are plans to have std::error adopt a similar interface (I’ll have to dig up that RFC). All in all, it makes error handling much more uniform. 
Yeah definitely! I want to contribute more but my spare time has been limited. Right now it's in a private repo, I'm going to open source it once it can understand NGINX logs and has a basic API to call. I'll probably ask you some questions in gitter before then. I haven't quite gotten that far yet haha but I'll definitely let you know. I have the RPI3B which is 64 bit. It might be worth testing on the new AWS arm instances if you want to test on arm.
They are also a consequence of the language having implicit `this` - something that Rust doesn't have.
I added the \[Patreon link\]([https://www.patreon.com/fulmicoton](https://www.patreon.com/fulmicoton)) 
I don't really follow why the extra "else" statement is necessary for owned values? Why do this: ``` let foo = Foo::default(); let foo = if some_condition { foo.chain_move().chain_move().chain_move(); } else { foo }; consume_ref(&amp;foo); consume_move(foo); ``` When you can do this: ``` let mut foo = Foo::default(); if some_condition { foo = foo.chain_move().chain_move().chain_move(); } consume_ref(&amp;foo); consume_move(foo); ``` Personally I find this much simpler than the cascade example. I do agree with you regarding builder methods returning a reference though.
Same. It helped me more with lifetimes which I was seriously struggling to wrap my head around.
...though I tend to use ddrescue for that since, if I encounter an error, I can take the disc out, try various methods to resolve the problem and, in between each one, let ddrescue retry the failed blocks so that, even if not all the disc is readable at the same time, I may still get a complete read.
Also this one, which has tap\_mut(): [https://git.myrrlyn.net/myrrlyn/tap](https://git.myrrlyn.net/myrrlyn/tap)
Agree. Always allow the user to have the final say, even if what he wants to do seems very stupid.
Tap's also handy for inserting debugging into the middle of an expression, although I guess `dbg!` does that now. Also, I often only need one tap. Cascade seems overkill for that, although I'll bear it in mind.
How can I convert a string to a CIDR using the Cidr crate? The from_string trait gives me: error[E0599]: no variant named `from_string` found for type `cidr::AnyIpCidr` in the current scope and I can't find what I'm missing.
There is an extra ` ; ` here: let foo = Foo::default(); let foo = if some_condition { foo.chain_move().chain_move().chain_move(); // &lt;--- } else { foo };
&gt; I've never needed to copy stuff to stdout, have you? That's not the line of logic you'd want to take if you want to compete with dd. * dd is useful precisely because it allows users to have as much control over low-level details as possible. * dd is useful because it doesn't try to force its users to use it in a certain way or makes assumptions about what its used for.
I tried the oldest tag on github and it was no good.
I just use `cat` (or `pv` if you want progress) for the basic copy block devices to each other use case. It's just as fast or even faster than `dd` on modern Linux and no where near as obscure. 
I'm assuming you are looking to implement all the arguments of `dd` eventually. For instance, I can create a 100MB random file by: ``` dd if=/dev/urandom of=./random_file bs=1m count=100 ``` Not sure how to accomplish the same with bcp. I use the block size argument quite a lot for a variety of things. 
iPad Pro 12.9 2nd gen Safari 1-1000: 58 10000: 47-50 100000: 5
If you prefer, a more precise phrasing would be: consuming `self` and returning a new value that is almost, but not quite, the same.
This is a really cool idea, but adding a dependency to Cargo.toml for transient use when debugging something is a bit unfortunate. I wonder if we could get something like this into std someday.
I like to use typical dog and cat names, but that might make talking about lifetimes rather heartbreaking 😢
There's no reason an appropriate statistical method couldn't account for changing circumstances; network congestion control algorithms do so all the time, in far more challenging circumstances.
If I have a `mut Vec&lt;Box&lt;dyn Trait&gt;&gt;`, how can I get an `Iterator&lt;Item = &amp;mut dyn Trait&gt;`? There are some lifetime issues I fail to figure out how to work around [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=30c1d897356b689dd0ec394d65258514)
&gt; I'm assuming you are looking to implement all the arguments of dd eventually. I'm not, that's the point. I'm looking to implement the *useful* ones. From various comments in this thread, my definition of "useful" is pretty unimaginative though, it seems. ;-) You'd currently do the as your example in `bcp` with `bcp --count 104857600 /dev/urandom ./random_file`. 
`Vec` already has an iterator implementation so you shouldn't have to do anything extra: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f6c3e9b8ac1025ad87aaf891c611326a
Just change your definition to this: fn iter&lt;'a&gt;(a: &amp;'a mut Vec&lt;Box&lt;dyn Trait + 'a&gt;&gt;) -&gt; impl Iterator&lt;Item = &amp;'a mut dyn Trait&gt; { a.iter_mut().map(|item| item.deref_mut()) }
I ran into something like this recently myself. I was trying to demonstrate how the borrow checker interacted with moving stuff into threads, and NLL figured out what my "incorrect" code was trying to do anyway. *shakes fist*
Thanks! I forgot about the argument, I was trying to specify the return lifetimes. I kinda get it, but still it puzzles me though. For example, why does this compile: [fn (&amp;'a mut Box&lt;dyn Trait&gt;) -&gt; &amp;'a mut dyn Trait](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2a78b830d9852d9a216cda55899268eb) Should this not fail with same error message?
Congestion management algorithms are most useful when you have a limited amount of a resource that's continually desired by multiple consumers. This *is* the case for many networking appliances. You have (generally) a 1 or 10 gb link that you want to saturate while serving your resources fairly (not accounting for load balancing across systems). That's not really the same situation you have with storage (imo). For a single copy, you're not going to have enough samples to get a good measurement. And if you want to track it over multiple copies you absolutely can by reporting back to some storage io tracking daemon. But if you're moving enough data around to be doing this, you're also not using `dd`, you're using some custom software that handles calculating optimal block sizes or you have enough money to buy a bunch of nvme drives that it becomes less of an issue.
Linux, Firefox 60 (ESR), Intel HD Graphics 530. 60 FPS up to 100, 20 FPS at 1000.
What GPU?
Hmm, just realized I can't really do that in my real code, since the `Vec` is actually nested in a `self` struct: [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ab16a123d63a78b13f315a2fd5092015) Is it possible to do it here?
- Machine: Windows 10 nvidia 980ti i7 - 1-10,000: ~70 - 100,000: 10
Rx 480 8gb
I think I've got a working example of what you want here: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=56db1107b036553fdda3fb73ff2d5b16
Yeah, but making the struct generic over the lifetime is not good. I edited the comment already with the solution I found. BTW, since in the actual code it is not just the `impl Iterator`, but `Box&lt;dyn Iterator&gt;`, it gets even more complex: iter_mut&lt;'a&gt;(&amp;'a mut self) -&gt; Box&lt;dyn Iterator&lt;Item = &amp;'a mut (dyn Trait + 'static)&gt; + 'a&gt;
Ah cool. glad you found something that works!
`Box&lt;dyn Error&gt;` in modern parlance. :)
You could add it as an optional dev dependency and put it behind a feature. 
The trait is FromStr with method from_str, and you need the trait in scope to call from_str. Alternatively, the FromStr trait is used to enable the parse method on str. One of these will work. let x: Result&lt;AnyIpCidr, _&gt; = "ip".parse(); let x = "ip".parse::&lt;AnyIpCidr&gt;(); // if there is already enough context to infer the type let x = "ip".parse();
I wrote this module (under src/state.rs) which contains a bunch of public types and functions that's meant to be used as a library. When I compile I get lots of compiler warnings about being unused - do I need to write tests making use of all of the types to make the warnings go away? For a lot of the types, in real world it might not be possible to do so (require other components/bringing up the project) etc. What do people do here?
Yes, this would be a very reasonable thing to do.
I'm pretty happy with builder methods that take and return `self`. I'm not as happy with the ones that take and return `&amp;mut self`
is `tap` from ruby? i dont know of any other language that refers to this as `tap` other than ruby
Can someone please sanity check my thinking with regards to `move`-ing types with the `Copy` trait? My line of thinking is: * 1. The idea of `move`-ing something (a variable) is to transfer ownership. The idea is that the recipient of the move is then the one who has to drop it. * 2. The `Copy` trait refers to lightweight types that in C# I would think of as value types. The idea being if you assign a variable to another variable that is `Copy` (or make a call with that variable), that variable is copied. To me, the idea of a `move` then only seems meaningful to non-Copy types (i.e. types that implement the `Copy` trait). But the rust book seems to allude to moving Copy types also. And possibly I am conflating the terms (i.e. I'm thinking possibly in the idea of a filesystem move vs copy, while there is a subtly different semantic meaning here). In Ch 4.1 under `Ownership and Functions` it says the following: | Passing a variable to a function will move or copy, just as assignment does But there is also a code snippet that says: &gt; let x = 5; // x comes into scope &gt; makes_copy(x); // x would move into the function, And in 10.2 under `Fixing the largest Function with Trait Bounds`, there is an error that says: | error[E0508]: cannot move out of type `[T]`, a non-copy slice Which I understand - if you allowed a move here then the slice could be potentially invalid (it has lost ownership of one of its elements). But the phrasing kind of suggests to me you can move items out of a Copy slice (actually it even suggests you can ONLY move items out of a Copy slice - but again semantically speaking is this really a move?) And in Ch 13.1 under `Capturing the Environment with Closures` it states: | using vectors instead of integers, because integers can be copied rather than moved; note that this code will not yet compile The _can_ here maybe even suggests some sort of duality here. Am I right in thinking that when `move` is refering to `Copy` types it is basically a copy? Or is there something more nuanced going on? Am I misunderstanding the idea of a `move`? For C# folks, is the of `Copy` trait being analogous to C# value types a safe line of thinking or is this going to bundle me up later? 
Copy types are moved just like any other type — but a Copy-typed location (variable, field, etc.) can still be accessed just like normal after being "moved out of". IOW, a copy is left behind.
If you're getting those warnings then your functions are not accessible outside of your crate. Check that you have `pub mod state;` in `lib.rs`.
I don't know what to tell ya. Either you're using a compiler from before RLS even existed, or there is something else going wrong. But I can't tell because I don't have any details from you. You're essentially paying the Debian tax here. It's valuable in certain circumstances, but there's never going to be a way around it in an ecosystem that advances quicker than Debian itself.
Can't you just use `App::new()` instead of `App::with_state(state)`?
If you're looking to improve on `dd`, have more reasonable/memorable size suffixes (`b` should be bytes, not blocks, dammit) and convert to the "new-style" (since the 1970s) argument syntax. In general, the `dd` argument patterning is not ideal: things are lumped under the `conv` and `flags` arguments semi-randomly. You might take a look at `sox` for some ideas about how to handle complex descriptions of input, output and conversion in new-style argument syntax. That said, `sox` is also kind of a mess. I'm not honestly sure what the best approach to command-line programs that can't be broken up and take complex arguments is. `pcap` uses a DSL described in the `pcap-filter` manpage: this has its own perils. Maybe with pipes now being pretty efficient and cheap the right thing to do is to break up `dd` into three separate programs that communicate with some kind of structured datastream? One can imagine `filein | fileconv | fileout` being a thing; one can imagine leaving `fileconv` out of the pipeline when conversion is not needed; one can certainly imagine having `filein` have an option to just emit a raw datastream, and having `fileconv` and `fileout` do some kind of magic-number thing by default to decide what kind of data they're looking at. Maybe this is making a bad solution even worse, though. If you want to remove something, `conv=ebcdic` and `conv=ibm` don't come up much anymore. (I honestly couldn't tell you the difference without looking it up.) Most everything else I've used at one time or another.
I pretty commonly use a specific `ibs` and/or `obs` and/or `cbs` when dealing with stuff in `/dev`. For example, I sometimes read a specific number of blocks of a specific size from my [ChaosKey](https://altusmetrum.org/ChaosKey) for testing purposes or just to understand read efficiencies. By the way, 512-byte blocks aren't just a PDP-11 thing: it's still the fundamental transaction unit for modern block devices. If you try to lay down data on a raw disk that's not written in block-aligned and block-multiple chunks, it may or may not do something sensible.
The JS problem was that the meaning depended upon what properties were on the object and which variables were in scope (including the global scope). A `with` expression that avoided those issues would be fine.
Isn't that pretty much how the peekable iterator works? It does seem like the simplest solution.
Like Python's `breakpoint()`!
[https://blog.burntsushi.net/rust-error-handling/](https://blog.burntsushi.net/rust-error-handling/)
Guess that's a small typo in "EXAMPLE 01: COMPILES" which actually doesn't because of `let _c: &amp;b;` with `&amp;b` obviously not a type.
u/Akira1364, is that you?
I'm aware of the issue you encountered, and the solution I like is a `chain_if` method: FooBuilder::new() .with_if("a parameter", condition) .with("another parameter") // etc. 
This could be easily done in Rust with postfix macros.
There are a couple "shouldn't compile" examples in the book now that do because of NLL :)
Thanks, I've edited my comment.
**Summary** WebAssembly is a runtime that lets languages beyond JavaScript to execute in frontend web applications. WebAssembly is novel because most modern frontend applications are written entirely in JavaScript. WebAssembly lets us use languages like Rust and C++ after they have been compiled down to a web assembly binary module. Language interoperability is only one part of why WebAssembly is exciting. The execution environment for WebAssembly modules has benefits for security and software distribution and consumption as well. In previous shows, we’ve given an overview of WebAssembly and explored its future applications as well as its relationship to the Rust programming language. In today’s episode, we explore the packaging and execution path of a WebAssembly module, and some other applications of the technology. Syrus Akbary is the CEO and founder of Wasmer, a company focused on creating universal binaries powered by WebAssembly. Wasmer provides a way to execute WebAssembly files universally. He joins the show to talk about the state of WebAssembly, and what his company is building. **Transcript** [Transcript (pdf)](https://softwareengineeringdaily.com/wp-content/uploads/2019/03/SED784-Wasmer.pdf)
If you're aiming to rewrite/improve dd in rust: *preserve all the original dd cli api *possibly extend it with capabilities from GNU ddrescue. http://www.gnu.org/software/ddrescue/ddrescue.html It can be **confused** with another similarly named tool **dd_rescue** so be careful. *possibly extend it with capabilities from sg_dd http://sg.danny.cz/sg/sg_dd.html *possibly extend it with capabilities from ddpt http://sg.danny.cz/sg/ddpt.html 
&gt;Can I use semi-stabilised features from the stable toolchain? No, in this proposal you can't. Either way you will have to update compiler to get newly semi-stabilized feature, so I don't think temporarily switching to beta/nightly will be a big problem in this context.
&gt; What exactly is going to happen if a semi-stable feature get a breaking update? Your build will break, as it will do today on nightly feature breaking changes. The only difference is that probability of such event should be much-much lower for semi-stable features. Note that you will not be able to use semi-stable features on stable channel. &gt;Who is going to put in the work for all the extra commit phases required for pulling features through this elongated pipeline to stability? The teams responsible for stabilization. I don't know about the exact process for lang features, but for lib features you will just change attribute from `#[unstable(..)]` to `#[semistable(..)]` and will add a tag to tracking feature, that's it. &gt;But as far as i can tell, it is/was a unique problem. I don't think so, even today we have const generics, GAT, specialization, all of which can influence design of crates quite significantly and quite complex, so extensive ecosystem testing is preferred. And who knows what feature will be developed in future?
gpt-2 finish this
**OUTPUT (courtesy of u/tehdog):** &gt; **`--null-data` is needed in this case because the input data is binary and without it or `-a` ripgrep silently ignores the input file.** &gt; The value in `--input` is the same for both the binary output as for the unquoted data. If no input file is requested, `--unquote-file-name` is needed, for instance `--put-file-name`. Note `--version` and `--version-name` may be omitted by the user as well. It is not possible to have multiple versions or multiple user data. Some programs may only accept one input file and some will ignore the other one. Beep boop, I'm a bot.
Is there a RFC for this? I never thought of this before but now I'm convinced that this would be a great addition to the language, and not too complicated either.
Would love an article on this about lifetimes
No.
At such short lengths, I'd be pretty surprised if techniques like that found in `memchr` would be useful to you. For example, take a look at what happens when the haystack is too small to fit into a SIMD vector: https://github.com/BurntSushi/rust-memchr/blob/d70ea8a1dedd89be028e11f09c99423d4393e26f/src/x86/sse2.rs#L115-L123 --- A simple byte-at-a-time loop! A byte-at-a-time loop is _probably_ your best bet here. The only other thing I can think of---at such short lengths---might be to load 4 bytes at a time into a `u32` and then use bit tricks (similar to what you can find in the [fallback implementations of memchr](https://github.com/BurntSushi/rust-memchr/blob/master/src/fallback.rs)) to try to detect the first non `' '` byte.
Another approach to crossword construction is to use constraint programming, which in my very biased opinion makes for clearer models. Unfotunately, I'm not in a position to make a comparison with Phil, but I do have a link to a complete example model for crossword construction if that is a topic of interest: * Description of Gecode crossword model, chapter 22 of Modeling and Programming with Gecode [https://www.gecode.org/doc-latest/MPG.pdf](https://www.gecode.org/doc-latest/MPG.pdf) * Gecode crossword model code: [https://github.com/Gecode/gecode/blob/master/examples/crossword.cpp](https://github.com/Gecode/gecode/blob/master/examples/crossword.cpp)
I really love this series, helped me learn so much about using rust in the right way.
A great example for a toxic community for sure 
Can you provide any benchmarks comparing functional and imperative approach (in this exact case)?
That's going to come in handy if I ever need to copy [a lot](https://en.m.wiktionary.org/wiki/bcp) of data between drives.
Thanks for your answer, I saw that loop in memchr, but figured there might be some optimization that's not appropriate for the general usecase I'm missing :) I'll definitely try to come up with something using bit tricks, that looks pretty feasible (though it might not be, seeing my usecase is the opposite of what most of this code does). Your SIMD idea seems scary, I'm not sure if I want to try that :D
False makes less sense, at least. For it to be false, there must be a char that was not uppercase. Since there are no chars, as it is empty, this is impossible.
Fingers crossed for anonymous product types 🤞🤞🤞
It has been some time since I last looked at constraint programming solvers like this, but in my experience they were a lot slower than SAT solvers for the problems I was trying to solve. Of course that's a biased sample of problems. I know they can beat SAT solvers for constraints that are hard to efficiently encode into CNF, but extending SAT solvers with other constraints is possible (cryptominisat handles xor clauses, monosat handles many different kind of complex constraints and I plan to add a constraint API to varisat in the future). I agree that it's much easier to model problems using the higher level APIs they provide compared to manually encoding the problem as a CNF formula. Luckily there are SMT solvers that allow for higher level problem descriptions but then use SAT solvers to solve them (sometimes by encoding the whole problem into CNF, somtimes by combining SAT search with specific theory solvers). &amp;#x200B;
Indeed, very scary. I do not recommend it. Although, glibc uses it in their implementation of memchr.
I would recommend the \`Failure\` crate as well. It greatly simplifies error handling.
My background is in constraint programming solver construction, but I strongly believe that one should use the best solver for the task at hand. However, I do think that modelling is better in a constraint prgoramming style, and I like using MiniZinc ([www.minizinc.org](https://www.minizinc.org)) for that. In MiniZinc, one can write a solver-independent high-level model, and then run those models using various backends, including CP, MIP, and SAT solvers. &amp;#x200B; Would be really interesting to run a comparison of the SAT model in Phil vs. the Gecode model.
Yeah, that's what I'm doing, but then what? He's responding that request with the DB. 
how do I sage a thread on reddit
Something for a far far far future compiler guidance annotation probably. There is some advantage in being the only implementation of a compiler i guess.
I ran benchmarks on the two versions with a string of 10k lines. They were roughly the same often alternating between being slower and faster between runs. I wouldn't try to use any benchmarks like this to generalize which style is slower or faster. The functional style gives rust more info to work with and can at times result in faster code than the imperative style. 
This is Rust, arguably idiomatic Rust, there should not be any different. 
I had a go at two examples from the article: struct Cat; fn main() { // `cat` _owns_ the value let cat = Cat{}; // `pet` is an immutable _reference_ to what `cat` contains let pet = &amp;cat; // `rosie` is an immutable _reference_ to what `pet` contains, which is a reference to `cat` let _rosie: &amp;pet; } And another: struct Cat { name: String, } fn rename(cat: &amp;mut Cat) { cat.name = "jelly"; } fn assign(cat: &amp;mut Cat) { *cat = Cat { name: "dot" }; } fn main() { // create a mutably owned struct let mut pet = Cat { name: "jam" }; // pass a mutable reference to `rename`; can only create &amp;mut from a mutable value rename(&amp;mut pet); // create a new cat let mut pet = Cat { name: "polka" }; // modify it in-place reassign(&amp;mut pet); }
Amazing city! 
How is the coverage of the Vulkan API as compared with MoltenVK on Mac? I know MoltenVK is missing a few features, such as events and geometry/tesselation support. 
MoltenVK recently got tessellation support (with a few caveats). Other than that I think gfx is pretty much on par.
I guess I'm a little confused with rust modules. Why can't I just declare a file src/foo.rs and rust by default make all the public types/functions accessible for other modules? Why do I have to declare them in a src/lib.rs?
Avert your eyes. 
I guess this would depend a lot on the usecase, functional programming tends to be very heavy on the allocator, which may not be a problem if most of your logic operates on the stack anyways. But even then it would affect cache locality 
The flatten illustration! Yesss! Such a useful visualization.
What is the big deal about returning self?
Tuples are anonymous product types. You're thinking of anonymous sum types.
Hello, I want to write a backend using WebSockets (ws-rs) and Serde (Serde-JSON). I thought about a struct that stores the deserialized payload, and some additional metadata. But the Lifetimes / Generics are in my way :( pub struct ParsedMessage&lt;T&gt; { session_id : String, method: String, payload: T } impl&lt;T&gt; ParsedMessage&lt;T&gt; where T: Deserialize { pub fn new(sid: String, method: String, payload: Deserialize) -&gt; Self { ParsedMessage { session_id: sid, method, payload } } } Do I need to `Box&lt;T&gt;` and where do i have to put the Deserialize-bound, or is this the wrong approach? Thanks for any help!
I believe the difference is that the other function's return type is too complex for lifetime inference. Because you put it inside `impl Iterator`, Rust can no longer infer the `'static` lifetime from the argument's `Box&lt;dyn Trait + 'static&gt;`. Because this function returns a simple reference, Rust is smart enough to know that you mean `&amp;'a mut (dyn Trait + 'static)`. In fact, it's smart enough to infer _all_ of the lifetimes: fn foo(x: &amp;mut Box&lt;dyn Trait&gt;) -&gt; &amp;mut dyn Trait 
I wish rust could perform the move into the closure automatically, seeing as line index was never used again in that scope. 
Hi, so I just wanted you to know that I loved reading this. But I'm still a beginner so I can't do much. I am studying and hope to help the ecosystem as soon as I can :) 
`Deserialize` is a trait, so you can't really use it as a bare argument like that the way you can interfaces in other languages. You need to use a generic type that implements `Deserialize` instead. That's exactly what you're doing with `impl&lt;T&gt; ParsedMessage&lt;T&gt; where T: Deserialize`. Try using `payload: T` instead of `payload: Deserialize`.
I agree that Futures are awful but they aren't a secret elephant in any room. Everyone knows and agrees that they are very hard to work with. Learning how to use them amounts to walking over hot coals. I do not value the experience of learning how to use them but they were better than waiting for Rust contributors to deliver async-await syntax. Relational database resultsets are mapped to Rust types by slicing the raw binary result provided from the database. Serde plays no role here nor should it, unless json strings are the returned type. rust-postgres and its ecosystem have been great tools for my work. I shed use of ORM/query builder when I migrated from Python (I used sqlalchemy) and regret not using parameter binding sooner. Regarding Arrow, Wes McKinney and company acquired datafusion. Datafusion will influence thinking about query engines but probably won't stand alone forever (in my opinion, but not Wes's, at the moment). Arrow is going to have its own query language and it will be language agnostic and glorious.
Whoops, my bad!
Sounds like copyless tricks would make it into std.
Rust does by default make the public items in a module accessible to other modules. It's just that modules themselves also have visibility. This allows you to separate functionality into different internal modules but expose them as a single simpler module, or to pick and choose which items you wnat to expose from internal modules. If you put `mod state` in src/lib.rs, then every module in the entire library can access it with `use crate::state`, but anything outside the library cannot. If you put `pub mod state` instead, then code outside the library can also access it with `use your_library::state`.
I know that further down in the thread kuviman finds the correct answer, but in case anyone else is learning lifetimes and reading this comment I just want to point out that while this answer does compile it doesn't actually work. The type `Vec&lt;Box&lt;dyn Trait&gt;&gt;` is the same as `Vec&lt;Box&lt;dyn Trait + 'static&gt;&gt;`. So the correct signature here is: fn iter&lt;'a&gt;(a: &amp;'a mut Vec&lt;Box&lt;dyn Trait&gt;&gt;) -&gt; impl Iterator&lt;Item = &amp;'a mut (dyn Trait + 'static)&gt; The error message hints at this. Your answer is equivalent to writing this: fn iter(a: &amp;'static mut Vec&lt;Box&lt;dyn Trait + 'static&gt;&gt;) -&gt; impl Iterator&lt;Item = &amp;'static mut (dyn Trait + 'static)&gt; While this does compile, it's largely impossible to use. [Here's an example that demonstrates the problem.](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ed94e78b02df80ebab03f3e9bf2a1391)
I feel this should start as a macro-crate, that explores the options and shows the validity/power of the whole thing. Once it's better understood how it works we can talk about integrating it into the language. There's warts I can see but can't quite solve yet. How do you recover what's in the with? Does the block return?
Is the specific use of futures really that widespread for database programming in Rust? Why?
Author here, thanks for sharing!
afaik gfx still doesn't support MSAA (I still need to send in the PR for that, so blame me), not sure about MoltenVK.
Databases are IO, which you end up waiting for. Async makes sense there, especially in the OLTP case (almost all web-based use cases). So the widespread futures use is sound, and encouraged. It's OLAP cases where you sort of don't get to benefit from futures, because IO and CPU are your biggest constraints, and you're often doing the kind of work where you need to wait for all dat data before you can do anything.
Very nice. But I didn't get how did u run it on AMD CPU based Mac.. hackintosh ?
I totally agree. Data and database interaction is 90% of what I do every day. 
How many other people say "mut" as "mutt". I've always pronounced it "mute". Seems to make a lot more sense to me as a contraction of "mutable".
That's the next post in the series :)
What does this mean? Does the function exists or not? And if it does, what trait should I import? error[E0599]: no method named `to_owned` found for type `std::sync::mpsc::Receiver&lt;std::string::String&gt;` in the current scope --&gt; core/src/common.rs:92:18 | 92 | let rx1 = rx.to_owned(); | ^^^^^^^^ | = note: the method `to_owned` exists but the following trait bounds were not satisfied: `std::sync::mpsc::Receiver&lt;std::string::String&gt; : std::borrow::ToOwned`
Yes! I've been dabbling in rust off and on for some time, and I've found documentation for many crates to be quite poor. It is \*extremely\* frustrating. I'm the type of person who learns by doing. I tend to learn a language by (after doing a little reading) picking some project that I would normally write in a language I'm familiar with, and attempt to do it in the target language. The rust experience for that is very very poor. Since rust is not a batteries included language, that requires me to research and pick a crate, or more often a hodgepodge set of crates appropriate for the task. I then have to try to make sense of how to use them all, using often minimal documentation. I spend most of my time - not learning rust - but trying to figure everything else out. Of course \*that\* is complicated by the somewhat foreign terminology, conventions, etc of the rust world, in addition to unfamiliarity with the language itself. My experience thus far has been that picking up a new language has not been all that difficult. Rust has been a very different experience for me.
Thanks for the insightful feedback :) &amp;#x200B; Fair point on futures. The part re. rows, my point is that the focus is on serde, and perhaps going off-topic with \`serde\_json\` was poor of me, but my point's that in general (outside of databases), the focus is on serialising record-by-record. Take JSON and CSV parsers' performance when you have to deal with the \`StringRecord\` and \`Value\`. I don't think that serde takes a task that runs at 100 and makes it run at 80, but it adds little overhead going from \`&amp;\[u8\]\` to a predetermined type, than guessing the type (what I think serde\_json does) or returning a \`String\` which you then convert to your type. So serde makes the 100 to be maybe 105, but dealing with the 'rawish' data yourself adds more overhead. \`rust-postgres\` yes, but postgres is not the only database out there. That's what we often don't consider. I'm considering potentially landing into trouble for recommending that we ship our multi-application thing with MSSQL and Postgres, just because I can be able to complete my work on postgres. I convinced my leadership that Rust would give us good performance, but we have clients who are already shelling out a lot of money on our request for MSSQL licenses. In this case, the fact that Rust support with postgres is good, doesn't really help me. &amp;#x200B; I do mention that I contribute (in the "I submit PRs here and there sense", I'm not alluding to having an official 'Contributor' status, which is a thing in Apache) to Arrow, so I'm a bit clued up on the library, and follow a bit of what's happening in the overall direction of Apache Arrow. You mention computation yes, but the data mostly resides on disk, and that's actually part of my contribution to the Rust implementation, working on some IO so we can use DataFusion to query more data sources. Regarding documentation, I've tried here and there, and honestly; it depends on whose repository one's contributing to. Some PRs (from other people) end up sitting open with no attention for months. I'd think that almost anyone here who's struggled with using a library has found answers in PRs that were rejected or are still open. This is why I said I'm not sure if "open a PR" is always the best answer. If one were to thoroughly look at successful OSS, they'd probably note that they become successful because: &amp;#x200B; 1. They originate from some company, or are backed by some company that decided to open-source such software 2. They are backed by some foundation 3. They were posted by someone who's been working on a library for a while, on Reddit/HN, and people start using them and contributing. Sometimes getting a group of people together for something informal (I was deliberate in calling it informal) is better than 1 person trying to move mountains. The, "there is no 'we'" is what ends up leading to many crates that aren't maintained, because 2 or 3 can't gather to create something together. &amp;#x200B;
It doesn't care if you are on real Mac or a hackintosh, or what CPU you have. The instructions are all the same: - for a regular app that links against Vulkan loader (from SDK), you can provide `VK_ICD_FILENAMES` pointing to gfx-portability ICD json - for Dolphin, provide `LIBVULKAN_PATH` pointing to our `libportability.dylib`
Heh, I did specifically say in the blog post that I hope the crate would be unnecessary at some point. So, knowing that, we shouldn't put it into std.
where do we not support MSAA? gfx-rs itself certainly does. Not sure about the portability layer, but this isn't a bit effort to expose.
I thought it was more backends, but apparently it's just the Vulkan one: [https://github.com/gfx-rs/gfx/compare/master...CryZe:vulkan-msaa](https://github.com/gfx-rs/gfx/compare/master...CryZe:vulkan-msaa)
Ohk. This is very interesting. Should bring more consistency, compatibility and much easier to maintain for developers.
Before you read the rest of my post, hang in there. Once a lot of things start to click, you become much better. I'm getting the hang of generics, which makes me fell like I've got some super-powers :) ___ To give the authors credit, it's not that it's (always) poor, but that sometimes the level where they are, isn't where people picking up the library are. Rust's gifts to programmer-kind is also its curse. Amazing type-casting and awesome documentation support make us take a lot of things for granted. Here's an example of that thing that I said I've been stuck with all day: ```rust = note: expected type `futures_state_stream::AndThen&lt;tiberius::stmt::QueryResult&lt;tiberius::stmt::StmtStream&lt;std::boxed::Box&lt;dyn tiberius::BoxableIo&gt;, tiberius::query::QueryStream&lt;std::boxed::Box&lt;dyn tiberius::BoxableIo&gt;&gt;&gt;&gt;, [closure@src\database.rs:37:68: 39:18], proto::rolemanagement::v1::Role&gt;` found type `proto::rolemanagement::v1::Role` = note: required for the cast to the object type `dyn futures::future::Future&lt;Item=proto::rolemanagement::v1::Role, Error=tiberius::Error&gt;` error[E0277]: the trait bound `proto::rolemanagement::v1::Role: futures::future::Future` is not satisfied --&gt; src\database.rs:34:9 ``` Someone who uses futures regularly will take a look at this, and say "oh, you need to add ` + Something + SomethingElse + 'static`". Yet I spent hours negotiating with the compiler. I even find lying to the compiler and saying I'm expecting an `usize` to produce more helpful message, cos then she says "No Nev, you're lying, I saw `Arc&lt;Box&lt;(Future&lt;Item=You, Error=Get&gt;, dyn My::Point::Here&gt;&gt;&gt;`
I support this. Using mssql, postgres and db2 every day makes me really feel the same pain points as the author here. I think the `postgres` crate is a great example of an implementation that could serve (and to some extend does it seems) as a guideline for other crates. I'm only familiar with `tiberius` (which I have used the most) and `postgres` but I have also briefly used `diesel`, `rusqlite` and the `odbc` crate, but while most follow the same API and patterns as the `postgres` crate they do differ quite a lot. Tiberius for example is async only. I think it would be an asset for the ecosystem to have a standard, or some best practices for these kind of API's. Helping document a already sparsely documented db driver using old versions of crates in the futures-ecosystem is pretty hard, and a lot of work for something that might see some significant changes in the near future depending on how (and if) each crate owner plans on supporting the changes to come. 
&gt; Take JSON and CSV parsers' performance when you have to deal with the `StringRecord` and `Value`. I don't think that serde takes a task that runs at 100 and makes it run at 80, but it adds little overhead going from `&amp;[u8]` to a predetermined type, than guessing the type (what I think serde_json does) or returning a `String` which you then convert to your type. I feel like I'm missing some context here. What does this have to do with databases? Regardless, either you know the type in which case serde can generate code to handle the deserialization so there's no guessing or you don't know the type and you're just operating on a `Array(Object(value_hashmap))` like data structure and there's still no guessing about the types. If the parser sees a `[` it's an array, if it sees a `{` it's an object, if it sees a `"` then it's a string, etc.
oh yeah, Vulkan is trivial. Looking forward to that PR :)
I am not sure if I truly understood your question. Do you want to create a Cargo package that is a library and which contains the local subcrates `lib_1`, `lib_2`, `lib_3` which you re-export alongside their features and also a binary called `tool` (→ `[[bin]]` section)? Then [this site about the Cargo manifest](https://doc.rust-lang.org/cargo/reference/manifest.html) got you covered (→ `[features]` section). --- If I got that right, here is a possible root `Cargo.toml` (the sub-manifests not shown): [package] name = "project" version = "0.1.0" publish = false edition = "2018" [[bin]] name = "tool" path = "tool/main.rs" required-features = ["lib_1", "lib_2", "lib_3", "action_1", "action_2"] [features] action_1 = ["lib_1/action_1"] action_2 = ["lib_1/action_2"] [dependencies] lib_1 = { path = "lib_1", optional = true } lib_2 = { path = "lib_2", optional = true } lib_3 = { path = "lib_3", optional = true } It does not seem to be perfect: To build `tool`, you need¹ to write `cargo build --features "lib_1 lib_2 lib_3 action_1 action_2" --bin tool`. ¹: You could leave off the `bin`-flag (and smh, `tool` using the functions `action_1` and `action_2` I defined, builds just fine without the required features `action_1` and `action_2` … weird, a bug?). Or you could define →default features. Or maybe, `tool` shouldn't be bundled that way but just included in a →Cargo workspace, dunno. I hope, I could be of some help.
Hey /u/cfsamson, I'm actually stuck on trying to use tiberius for the first time. Do you mind helping me out with some details? Depending on which country you live, I might not have enough cash (purchasing parity), but I can pay you a bit for 30-60 minutes' guidance over the weekend if you're free. I live in South Africa btw, and I don't think it'll take that long, I mainly need learn how to CRUD in my use-case.
I tend to pronounce it möt, like book
I agree with you, it’s better to get a group with the same interests and then try to pull the project with others, because as you say, many libraries have potential but their authors moved to another project and forget about them, you may also post this link in the rust discord, so you can reach more people, and or create a discord for that purpose, so you can gather people and discuss various topics about databases there, and lastly, you may try to contact the repo owners of libraries that you find interesting and see what’s going up and if you can be added as contributor (if they don’t care more about it the project, so you can at least review pull request, etc)
Fair point. The typical use-case in (csv, serde\_json) is to convert data to a struct, which is much faster than the alternative as you also mention. Now, if you want to write such data to a database from a struct, you already know the fields and their types. &amp;#x200B; If you try to write something general that picks up an unknown csv file and creates a new table out of it, the performance is sloooow. I normally use Apache Spark for this (it outperforms Oracle, Microsoft, MySQL, Pg standard tools), but I was interested in trying to use Rust for that, because Spark eats all my RAMS away. If I inspect the table and create structs, where I then go: csv &gt;&gt; \_de &gt;&gt; struct &gt;&gt; \_ser &gt;&gt; database::Row &gt;&gt; save, the performance is almost night-and-day compared to if I don't know the data structure. So there's the relevance, but I do concede that I went a bit off-topic.
It sounds like MIR-level optimizations could really benefit in this case, since it isn't semantically making difference, but LLVM can't rip through the abstractions.
It's not that it doesn't work it's an issue where rust currently doesn't work well, there is even an rfc about it. https://github.com/rust-lang/rfcs/pull/2495
Yes... I referenced that RFC in another comment. I'm more or less pushing back against the notion that everyone should be bending over backwards to support Debian-style conservativeness.
In Rust, trait methods are different from inherent methods. The syntax for calling trait methods directly is ugly (to say the least) so there is a bit of sugar which notices when a method call can't be dispatched to an inherent method and substitutes a call to a trait method if possible. So you've called `rx.to_owned()` . - `mpsc::Receiver` doesn't have an inherent method `to_owned`. - The compiler attempts to dispatch to any trait method `to_owned` - One trait method matches: `ToOwned::to_owned`. However the trait isn't implemented for the type. There's a blanket impl which automatically implements `ToOwned` whenever a type is `Clone`. However, `mpsc::Receiver` does not implement `Clone`. It's a Multi-Producer *Single*-Consumer channel after all. It doesn't support more that one consumer, so there's no sensible way to clone `Receiver`. Assuming that `core/src/common.rs` is your code you probably have to rethink the design and possibly use another primitive. (I would guess that Crossbeam might have a multi-consumer channel.) Or, if there really is only one receiver, you want to transfer ownership and shouldn't use `to_owned`. `to_owned` duplicates data as necessary to create a new thing independently owned. It either calls `clone` or does something else allowing it to duplicate dynamically sized data, such as duplicating `str` data into a new `String` buffer. 
The error message is Rust trying to be more helpful than just telling you “that type doesn't have that method.” It's telling you that the method `to_owned` is probably supposed to be from the `ToOwned` trait, but `Receiver&lt;String&gt;` does not implement that. If I had to guess what's really going on, you might be trying to call `to_owned` on one of the strings from the `Receiver` but forgetting to call `recv()` so you're accidentally calling `to_owned` on the `Receiver` itself.
I mean, doing it that way at runtime will never be as fast as custom deserialization code generated at compile time. There's probably advanced tricks you could do to speed it up like infer a schema from the first `n` rows then generate a program that your program can interpret to drive the deserializer in a more typed way. The more advanced version of that would be to JIT actual machine code to drive that process. How much slower are we talking about? 20%, 80%, 2x, 10x? How many rows per second can you process with a static mode vs a dynamic model?
I'd imagine they're referring to the GPU vendor rather than the CPU vendor, though putting it right next to the core count does leave it rather ambiguous. 
I'm making a Lisp, and of course, Lisps need garbage collection. Is it a bad idea to just wrap everything in `Rc`? Would using rust-gc be significantly better? Is this premature optimization?
That is an important part of producing a *build system*?. Gcc, Python and to a lesser extent Perl all do this. I'm pointing out the issues rust will face if it continues without change. Bitcoin core and Gnunet are examples of projects that have failed to take Debian-style conservativeness into account and they may have suffered because of it. One solution is to reject any attempt by Debian to Package Rust, these requests should always be honored by the Debian Community.
Making a Lisp. It's not exactly innovative, pretty much just Scheme with some syntactic sugar for common constructions, as well as first-class extern objects. The eventual goal is to have a language that can easily talk to other languages through its Rust intepreter, be it through the C ABI or higher-level bindings. Not ready for public release yet.
The Debian community is free to package whatever they want, license permitting. However, in doing so, it's also their responsibility to resolve issues they've created because of their packaging. The tools work in the packages provided by the upstream project. If Debian's version of those packages do not work, then you have a packaging issue on Debian's side. Furthermore, even if you get the Rust team to agree to follow Debian's "conservativeness" for official Rust projects, the wider ecosystem is almost certainly not going to go along with it. You're going to be in exactly the same situation you are today as soon as you try to install a 3rd party package.
With an Intel i9-7900X and RTX 2080 (although I'm not sure my gpu is being used at all here) on Ubuntu 18.04, I'm getting these results: &amp;#x200B; * 1-100 -&gt; 60 (although sometimes flickers to numbers in the hundreds) * 1,000 -&gt; 33 * 10,000 -&gt; 3 * 100,000 -&gt; 0 (a few seconds between each frame) &amp;#x200B; Really interesting that my machine is performing so slowly compared to the phones listed here. I ran on the latest versions of both Firefox and Chrome, similar results for both &amp;#x200B;
I like your ideas, thank you! I'll spin up something with the code that I have, and make concrete measurements. It might only be during the coming week, but I'll try to do it this weekend.
&gt; I even find lying to the compiler and saying I'm expecting an usize to produce more helpful message, cos then she says "No Nev, you're lying, I saw Arc&lt;Box&lt;(Future&lt;Item=You, Error=Get&gt;, dyn My::Point::Here&gt;&gt;&gt; Just FYI, in languages with very strong types like Rust, this is often a totally valid thing to do. Haskell even has a feature for this called "typed holes" where you can write a `_` in place of a value and the compiler will tell you the type that should be there.
I sort of say both lol. No clear pattern to my speech on this one yet 
The Debian Community will respect Developers wishes to not package software, even though they don't have to. Bitcoin, for example, requested that their software never again transition to Debian Stable after their network was inundated with thousands of obsolete clients. Though I think that a little overboard for Rust, it's an option if keeping compatibility is deemed impossible. Looking at other build systems it's already been proven to be possible, it just requires a little bit of infrastructure.
Actually, writing good documentation for Future/Tokio-based crates is not super easy in my experience. One issue is that once you understand how futures work, the libraries often become much easier to use, and you often end up explaining Tokio again and again instead of explaining your crate. On the other hand, I acknowledge that in order to get experienced at Tokio, you need to start playing with examples. Another issue is that crates might need to expose more of the protocol they implement when using Tokio. In a synchronous implementation, I feel this can often be hidden more easily, as the return types might be more explicit. This has been an issue for me for instance when documenting [Thrussh](https://nest.pijul.com/pijul_org/thrussh). I actually wrote two database-related crates: - `pleingres`, which I'm using quite happily to power [nest.pijul.com](https://nest.pijul.com). It is another interface to PostgreSQL, which I started for fun before the more serious libraries got support for Tokio. Unfortunately, since migrating wasn't easy, I turned it into a more serious project. A few weeks ago, I made it work on stable with procedural macros to send requests from a \`struct\`. I can provide support if needed. - `sanakirja`, which is actually a database backend. I believe Sanakirja could become a sort of pure-Rust equivalent of Reddis (we're not there yet), usable both in RAM and in memory-mapped files.
Real list programs will quite often have circular references, so yes `Rc` is a bad idea.
&gt; I believe “correct” in an engineering sense should be that the specification is designed to fulfill intended function, guard against foreseeable risks, be ethical, and be logically consistent by itself (formal), and the program is proven to implement the specification within industry standard assumptions. Seems to me this is still well, well beyond the level to which most software is written.
That's what I figured. I'm looking into [`rust-gc`](https://github.com/manishearth/rust-gc) as a more robust replacement.
Send me a chat, don't worry about money. I'll see if I can help.
Remember to wear your hazmat suit before entering.
Is there any way to do something similar in situation when the thing in the inner loop may fail (e.g. conversion from string to int)? With the outer function returning Result with Vec inside?
The copy operation is just a special case of the move operation in which a copy remains at the old location. The Copy type also guarantees that the Drop operation does nothing. Together these rules make it possible for generic code to "move" or "drop" a value without knowing whether or not the concrete type is `Copy`-safe. Unfortunately some documentation uses "move" incorrectly to exclude copying. This happens is a very specific situation: when discussing code which wouldn't be defined if the type is not `Copy`-safe. fn main() { let x = vec![1, 2, 3]; let equal_to_x = move |z| z == x; println!("can't use x here: {:?}", x); let y = vec![1, 2, 3]; assert!(equal_to_x(y)); } The excerpt let equal_to_x = move |z| z == x; println!("can't use x here: {:?}", x); is well-defined if `x` belongs to a type which implements `Copy` (although the compiler violates the principle of least surprise). If the concrete type doesn't implement `Copy` or if `x` is parametrically typed, then the code doesn't mean anything - and the compiler rejects it. The least-surprise violation is noticed here: https://stackoverflow.com/questions/44390366/when-does-a-closure-take-ownership-of-its-environment-without-the-move-keyword fn test2() { let x = "abc".to_string(); thread::spawn(|| { foo2(x); }); } fn foo2(x: String) In this case the call `foo2(x)` moves from `x`, the compiler doesn't prove that `String: Copy` (because it isn't), and the compiler concludes that it must capture `x` by value. fn test1() { let x = 1; thread::spawn(|| { foo1(x); }); } fn foo1(x: i32) In this case the call `foo2(x)` moves from `x`, the compiler proves that `i32: Copy`, concludes that `x` (in the closure) may be a borrowed location, borrows `x` from the parent scope by sharable reference, introduces a lifetime parameter, infers that this lifetime must be `'static` to satisfy the signature of `thread::spawn`, notices that the local variable `x` within `fn test1` doesn't live forever, and raises a lifetime error. In the future, a smarter compiler might resolve this one of two ways: - after determining that `x` can't be borrowed, it backtracks to the closure and captures `x` by value. - after determining that `x` must be borrowed forever, it backtracks to x, tries to convert it to a `static` allocation, succeeds, borrows it forever, and gets the equivalent of `|| foo1(*&amp;(1))` , which hopefully can be further optimized to `|| foo1(1)`. Currently it's necessary to disable capture-by-borrow using the `move` keyword. And there's reluctance to add too much lifetime-extending magic to the compiler.
I'm not saying that copyless should go as is into std, I'm saying whatever methods in std it is providing alternate definitions for should just be changed to use the copyless ones, assuming there is no interface change necessary. That would get us the benefit immediately.
I'm not saying that copyless should go as is into std, I'm saying whatever methods in std it is providing alternate definitions for should just be changed to use the copyless ones, assuming there is no interface change necessary. That would get us the benefit immediately.
How about thread pools? I'll be honest I've not done any db work in rust just yet, but I didn't see any mention in your post as to using a connection pool such as r2d2
Well, copyless is essentially a work-around for placement new which has been languishing for a long time. I'm not clear on what is blocking placement new, personally; syntax maybe?
At least one of my Rust programs was among the first Rust applications to ship in Debian, so I'm quite familiar with what's involved. I just think the way you are approaching this is off.
That's possible, I have a low social IQ.
I always use \`let () =\`, because the thing I'm getting back is never a unit (unless I've seriously messed up), and it obviates the need for a type annotation. 
Reminded me of an experience from last week: trying to just pull rows out of a legacy database that broke from the well-traveled path (no primary key, etc.), gave up and decided to export to csv via a script, which I can then handle easily with serde/csv crate.
`tokenizer` is a submodule of `interpreter`, so `self` is an alias for `tokenizer::`, not `interpreter::`. &gt;Also I thought importing Lexer into main should be possible since I declared it as a public module in mod.rs? The compiler is telling you `Lexer` is a private struct. Did you mark it `pub` in the module? &gt;If I add a `mod tokens;` into tokenizer.rs I only get one error message and that's: Because `tokens.rs` is a submodule of `interpreter` as its in the `interpreter` directory. If you want it to be a submodule of `tokenizer`, then declare it as pub mod tokens { // stuff goes here } Or put `tokenizer` in a subdirectory of `interpreter` with its own `mod.rs`. 
Is this a replacement for the official book? And if so should I instead do this? 
&gt; I agree that Futures are awful but they aren't a secret elephant in any room. They aren't awful, they are _unergonomic_. I'm sure you didn't mean it this way, but calling them awful comes across as insulting to the folks that poured so much time and energy into creating a zero-cost abstraction for futures in rust.
Some people like it better, they're both fine. The Book is better edited, more comprehensive, and maintained.
`self` indeed means current module, which in your case means tokenizer, which alongside with tokens are submodules of interpreter. So your import should be: use super::tokens::Token; or use crate::interpreter::tokens::Token. Similarly with use statement in main, you're missing one level of indirection. Also, you could just drop `self::`, if you're using 2018 edition and rust 1.33. There's a nice tool to help you visualise your module hiererchy: https://github.com/regexident/cargo-modules
I'm pretty sure that ImmutableJS uses it, too
Well I misread the chapter in programming rust then, damn. Thanks, works like a charm now. Also thanks for the link, may come in handy as this project progresses
incremental as in [interdiff](https://github.com/twaugh/patchutils/blob/master/src/interdiff.c)? would it be pushing the scope of diffs to include that? we tried looking into porting it, but it was too complex for us.
I have many thoughts on this topic, but I'll try to keep things brief and then go into details if necessary. - I _completely_ agree that interacting with async Rust database libraries are a pain. - I think one of the primary reasons _why_ it's a pain is that database connections are generally not multiplexing, which means that you can't issue another request until the previous one finishes. This in turn means that you have to consume the connection (i.e., take `self`) and return it when the response future comes back, which is a pain to deal with. - [`futures_state_stream`](https://docs.rs/futures-state-stream/) can help a little with this. - `async`/`await` might help a lot with this, since `Pin` _should_ let db libraries now have a signature like `fn&lt;'a&gt;(&amp;'a mut self) -&gt; impl Future + 'a`. We're probably still some way away from that though. - I've started writing [`tower::Service`](https://docs.rs/tower-service/) wrappers for common database connection libraries [here](https://github.com/jonhoo/skyline/). They do _not_ consume `self`, which should make them _much_ nicer to work with. They also interact nicely with the rest of the growing [`tower`](https://github.com/tower-rs/tower) ecosystem. It's still in relatively early stages, and has some known problems, but I'm working on incorporating them into this [Noria DB benchmark](https://github.com/mit-pdos/noria/tree/master/noria-benchmarks/vote), so in theory they should be in a usable state in not too long. The primary issue with this approach (and w/o async/await) is that all arguments now have to be owned.
Hmm, interesting discussion. While I agree with everything you've said, I also feel like I *have* to provide method chaining in my API, because 1. It's pretty much a standard convention everyone already expects, and 2. I feel like telling users to go get a second crate for chaining in order to make *my* library ergonomic to use is a tough pill to swallow. I'm not sure how this can be solved, but I am all ears.
enardo be my dad
This looks cool! Not having used a debugger / IDE in ~8 years, is there a good way to get this working on OSX in a nice UX? I definitely can't integrate it into my editor haha.
"Awful" is depends on context. Are Futures good for lower level stuff than what 99% of devs need? Sure. Probably. Are they awful in the context of 99% of devs looking to do async programming? I would say definitely.
Databases is my high pain point, so much, that I try build a relational language, because I claim NO MODERN LANGUAGE ON ERTH IS GOOD ENOUGH. So, the good news is that Rust have it hard, but is not alone. That is why is important to learn some lessons about this. &amp;#x200B; This are my ideas (apart of build a language!): * Ditch futures. Futures are not a core aspect of the language, and will be, forever, [a leaky abstraction like in ALL the languages](http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/) that bolt over any kind of async/parallel stuff. So, yeah, is important to be compatible, but is orthogonal. A lib must be tangential to futures or similar. Like you say, if I wanna use a database lib, NOT assume I need futures * This also cross with async: If your lang is not async from the start, async dependant libraries are pain. Is good to have async *optional.* * We need something like the python database api * We need a good way to map database results to structs. But despite being type safe is nice, having a HashMap like container is also required. Sql is dynamic and not amount of structs and types will be enough. * Is important to support dates, decimals, enumerations, and embebed data like arrays/json * Look how some micro-arm are made (like dapper). I think them hit the sweet spot * Making full ORM or heavy interfaces like diesel are nice, but is not what we must build first. The FIRST layer MUST be dynamic. NOT assume everyone need a fully typed database layer. * Making query builders and similar is nice, but not depend on it. Send sql strings with parameters. End * Make nice to do parametrized queries * The library must be integrated with logging * Convert from/to son, cvs, etc is not necessary to be on the library. BUT convert to SQL is! (ie: dump sql scripts) &amp;#x200B; This is some of the basic stuff...
I come from Java and Javascript land. I feel MDN/npmjs.org Javascript docs or to a lesser extent Oracle's Java documentation are intuitive at showing how you can use the API. Docs in the Rust ecosystem don't feel intuitive; the only exception being the official TRPL book.
What is the best way you're found to debug Rust code? I've been using `dbg!` but I feel like it's lacking in some ways and doesn't amount to much more than a JS `console.log`. Is there any more full-fledged debugger? Or would you say it isn't as necessary in Rust because of the strictness of the compiler?
When you use spark, don't you have a crawler or something that infers the Schema before the Spark job actually runs? In AWS's Spark they have a tool that does this so you don't have to guess what you want in the actual job. 
yea but i think that came from ruby too, iirc. 
Is there any equivalent of Gitea written in Rust? I'd love to see a Rust project for a self hosted Git server.
Yeah, I'm in the same situation :( It's a chicken-and-egg problem: * In order to stop building method chaining into a crate's API, everyone needs to be familiar with the other options for chaining. * Crates need to not have method chaining built into their APIs before folks are gonna start adopting alternative solutions. Personally, I'm going to stop building method chaining into my crates and I'm going to try a couple of things in order to ease the burden for users: * Explicitly recommend an alternative (like cascade) in my documentation. * Demonstrate usage using the alternative in the examples (both examples in the documentation and crate examples). So far this has been okay for one published crate, though it only sees light usage and I don't know how well it would go for a more popular one.
This might be what you're looking for: [https://stackoverflow.com/questions/26368288/how-do-i-stop-iteration-and-return-an-error-when-iteratormap-returns-a-result?noredirect=1&amp;lq=1](https://stackoverflow.com/questions/26368288/how-do-i-stop-iteration-and-return-an-error-when-iteratormap-returns-a-result?noredirect=1&amp;lq=1)
I hope you don't mind me asking, but it's there any particular reason why you're using tokio and futures 0.1 when you're using the nightly async await anyways? From a very cursory glance, it doesn't seem like you're using anything that isn't available in futures 0.3 and romio. I'm trying to be less dismissive of futures 0.1, but my impression was that it's mostly used to avoid nightly or because you need something that isn't available yet for futures 0.3.
\`cp /dev/cdrom /path/to/cdfile\` &amp;#x200B; Works pretty well.
Yah, that does seem really unusual! :o
This way of defining it helps keeps the implementation natural. "Iterate looking for false. If you find one, short-circuit and return false. Otherwise if you make it to the end, return true." In that implementation, the empty set is naturally true. Doing otherwise would make it a special case. This approach also keeps the inverses clean. "All true" is the opposite of "any false". But again if the empty set was defined to be false, that inverse relationship wouldn't be there anymore.
The rust community seems to have a fetish for async. I don't know why, maybe many rust developers have come from javascript. They think async database access will be faster, 'because blocking'. Databases will always be blocking because storage is blocking by nature. Its ironic since rust is known for its threading.
Ah, dang, it appears you can only view all the issues in an organization if you are a member of that organization :( Not sure how to fix this.
You can get a full-fledged debugger for Rust using LLDB. For example in VS Code you can use vscode-lldb to set breakpoints and everything: [https://github.com/vadimcn/vscode-lldb](https://github.com/vadimcn/vscode-lldb) I would say that while there may be a few things you don't need to debug that you would in other languages, in my opinion a debugger is always useful for any language including Rust.
Playing with Rust for few weeks, I've decided to implement minimal wrapper for lazy values as an experiment ([plaground](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=41926645af77055d7520077bec9dd07f)). I was trying to get the smallest possible representation possible and support for FnOnce in relatively ergonomic way, but this meant using some nigthly and unsafe features. Did I make some "beginner mistakes" in unsafe code and is this representation generally safe for single-threaded use?
YES PLEASE. But also maybe a small recap of the system you are gonna write in, like type, size and stuff that make easy to recognise your root/data partition from your USB stick you want to load an image. Asking for a friend.......