&gt; the library is called `libz.so` and it is installed god-knows where. It sucks. Constantly have to google "where is libz installed?". I have better things to do. Using `pkg-config` for C libraries is pretty standard, at least in Debian. $ cat foo.c #include &lt;zlib.h&gt; #include &lt;stdio.h&gt; int main() { printf("zlib %s\n", zlibVersion()); } $ CFLAGS=`pkg-config --libs --cflags zlib` $ gcc $CFLAGS foo.c -o foo $ ./foo zlib 1.2.8 Anyways, I agree that using crates in Rust is *much* easier. 
tui looks promising, I will check it out, thanks!
Yeah, same format! There are some conversion tools but basically all of them will be lossy since ftl encodes more data than any other l10n format. We're providing parser/ast/serializer in js, Python and now getting it into Rust to help people add support in their tools. I know there has been an effort to add ftl to Pootle, but I expect it'll take a while to get more tooling pick it up. For now we're working on proving the paradigms and solutions via Pontoon.
I got a skeleton algorithm working: https://github.com/PeterReid/unicode_skeleton/blob/master/src/lib.rs The highlight is: assert_eq!(skeleton_chars("ùî≠ùí∂·ªøùï°ùïí‚Ñì").collect::&lt;String&gt;(), "paypal"); 
oh, i see it's still something hardcoded up to N elements. I guess something like a builtin count!() or enumerate!() macro would be nice for this kind of stuff.
Nevermind, just gonna restart it's development as [Dotto](https://github.com/feddasch/dotto). Sorry for making your head spin that much :P
[This fixes the 1st version](https://play.rust-lang.org/?gist=cf74b2a866706aa15195758afd8b8bf2&amp;version=stable) - No need to change the struct here, it's just the borrow stuff you didn't need. (And read_to_string() takes a mutable reference) [This](https://play.rust-lang.org/?gist=8ca34de260e381579dae045f299ab34a&amp;version=stable) is a better version of (2). As you see, you can just put the trait bound on `T` when declaring the struct, then call `Read` methods on its body. The tradeoff is pretty much static vs. dynamic dispatch as you stated, with dynamic dispatch preventing inlining optimizations, hurting cache locality (in a vec of many elements for instance), and adding a layer of indirection for each method call. Use it only when static won't do. Edit: Also Result does not need to be in scope here, rust imports it for you.
Awesome project! 
As /u/_Timidger_ pointed out, that is his project. Smithay on the other hand is a collaborative project, started by /u/levansfg, dev and maintainer of the [wayland-rs libraries](https://github.com/smithay/wayland-rs) and myself, dev of [fireplace](https://github.com/Drakulix/fireplace). I am seeking to build a new better working rust-based foundation for exactly that project, which is currently mostly unmaintained, because its current foundation (wlc) is kinda bad. So while way-cooler could be build on top of smithay and fireplace could just go the same route of using [wlroots](https://github.com/SirCmpwn/wlroots/), both projects are not actually related.
Ah, thanks for the tip! Yeah, ignoring is probably a reasonable option here.
For me it's unable to find the other header files in the project. In your case it works because you are using headers that are installed on the system.
&gt; We're providing parser/ast/serializer in js, Python and now getting it into Rust to help people add support in their tools. Nice! The three main languages I care about. :)
Yeah you are right. But why doesn't you set the include path for bindgen? 
&gt; In Rust as it is now, if a Trait is necessary for using a library, it should be in the library prelude. User code still has the option of not importing it. Then I think we pretty much agree - all that I'm argue is that fact that some traits should be in the library's prelude - or in other words, the fact that you need a prelude that will contain these traits - is a fault in Rust as it is now.
You may need to specify include directory via adding "-I" argument to clang args I guess.
I think your first example will work if you do a similar iterator trick: fn compute_recursively(v: &amp;mut [Foo]) { for i in 0..(v.len() - 2) { let mut cursor = v[i..].iter_mut(); let a = cursor.next().unwrap(); let b = cursor.next().unwrap(); let c = cursor.next().unwrap(); do_something(a, b, c); } } 
Best of luck! Hope it works out.
Could you provide a bit of info or a link to what session-typed channels are? Haven't heard of that before. Thank you.
I am a programming language aficionado, so when I found a language developed in a completely open manner, I watched for a while and then jumped in. The mostly really helpful compiler supported me very well on my first steps, giving me the courage to try things I'd normally left to others when dabbling in a new language. The inconceivably awesome community further lifted me up with great mentorship, friendliness and technical acumen. Now I feel I can take on projects in Rust that would have appeared daunting in Java (a language I know and work with for the better part of two decades).
What you've defined isn't quite a Monad, since Bind should be able to return a value of a different type wrapped in the same Monad. But this is impossible until associated Type constructors lands, I think. In any case, this is a cool experiment, nicely done.
Thanks! That helps a lot. I read that trait bounds on structs may be a bad idea. After reading some more, it sounds that it's only a bad idea for certain cases.
An official Docker image for the server would be cool! 
Wow. Thanks. :) I'll give that a try as soon as I clear out some more important work. (My hosting provider just announced they'll be changing their pricing model come December 1st, which means I now have a deadline for some changes I kept putting off, and I also need to make sure that I have replacement contact solutions for everyone I know on AIM when it gets shut off on December 15th.)
I wouldn't worry about dynamic dispatch of `read` if you're reading from anything other than memory, because you've got at least a syscall happening. Seems like a silly optimization. The second pattern could probably use the new `impl Trait` feature if you're targeting a recent enough compiler, and should if you can.
Yeah you can't do Monads without Associated Type Constructors (ATC) landing. [This post](https://m4rw3r.github.io/rust-and-monad-trait) goes into this in a lot of detail, but the short answer is without ATC (well really Higher Kinded Types but ATC subsume this) you can't make things work generic over any type of Monad. Even then, another question is if this abstraction makes sense for Rust. Maybe it does. Maybe it doesn't. Who knows. Only time will tell.
OP you might want to also look at [frunk](https://github.com/lloydmeta/frunk) to see how they do some of this stuff! Though I think they only implement Monoids for some types and not Monads. It's been awhile since I last looked.
Oh heck yes. I think it is a phenomenally good thing for Wayland to have more choices than the big desktop environments and their embraced-extended implementations. (I've found that Wayland as it is implemented today is a way for Qt or Gnome to work without X. But if your client just speaks the core protocols without a toolkit it doesn't actually work.) On that point, does Smithay have an opinion on the client-side/server-side decorations debate, or is that a detail which front-ends can figure out?
From what I understand(and I'm not that familiar with Servo, or the insides of browsers in general) Servo is fast not because it's written in Rust because of it's concurrent rendering - which I guess **can** be attributed to Rust. But you can't do concurrent JS because JS code is assumed to be single-threaded - so you won't get much speed from making the JS engine concurrent...
Compared to 35.3% C and 6.6% C++!
There's [neon](https://github.com/neon-bindings/neon), which allows you to write Node FFI libraries in Rust, and AFAICT supports working with Electron.
See this code from Concurr: https://github.com/mmstick/concurr/blob/master/src/client/redirection.rs It's possible to determine if your program has received a piped or redirected input, and handle that accordingly.
Damn I thought this would be about linux containers/bsd jails made in rust.
I also have https://github.com/brendanzab/approx which has some handy traits. Nalgebra integrates with them as well, so it's already in the dependency tree.
That library looks like just what I've been looking for! That said, Monoid is possible since it doesn't need anything higher kinded, that is, if T is a Monoid, Monoid operations always keep you in T.
Really nice!
Care to elaborate why the current macro won't suffice? I just did a quick search for floating point comparisons and it was the first crate to show up and seem to do the job. Willing to switch if there's enough reason to.
We aren't planning to write a JS engine in Rust for Servo, but the SpiderMonkey team has been writing a new JIT in Rust called Cretonne. I believe it will be used first for WebAssembly. So that's not quite the answer that you wanted, but I think replacing IonMonkey (the fast JS JIT) is a ton of work that doesn't benefit as strongly from Rust as other parts of the system. It may happen some day, but we have a lot of lower hanging fruit to pick first.
So does the compiler figure out when an option needs to be tagged? Or should their signatures actually be different?
https://docs.google.com/presentation/d/1q-c7UAyrUlM-eZyTo1pd8SZ0qwA_wYxmPZVOQkoDmH4/edit#slide=id.p This is the original sheet, and the "can't be null" text is a link to https://doc.rust-lang.org/core/nonzero/struct.NonZero.html which differentiates between Options that need to be tagged, and ones that don't.
~~I think `Box` is wrong. `Box&lt;T&gt; where T : ?Sized`, so the compiler actually disallows boxing any type that specifies its size must be known, so e.g., you can't box any type that `: Clone`, since `Clone : Sized`.~~ ~~I'm also pretty sure `&amp;T` doesn't require that `T: Sized`.~~ Edit: nevermind, I was both misunderstanding the chart and incorrectly generalizing trait object safety. 
Thanks!
I am trying to get started with rust. When I go to the github page of the rust book, it is something that is supposed to be built using a rust tool called mdbook. When I tell cargo to install mdbook. it tells me error: struct field shorthands are unstable (see issue #37340). Searching on-line tells me struct field shorthands are stable since 1.17, and I installed 1.20 and updated it to nightly to no avail. What now? 
You're misreading it. Those annotations just mean "this is how it works *when these conditions hold*." If you look elsewhere the sheet also contains the unsized cases.
This is not showing what's possible, but rather the in-memory representation of some of the possibilities. Box&lt;T&gt; for an unsized T can certainly exist, _but it isn't represented the same_ as `Box&lt;T&gt; where T: Sized`. This is why there's both `Box&lt;T&gt; where T: Sized` to show the representation of sized boxes, and `Box&lt;Trait&gt;` / `Box&lt;[T]&gt;` (the two main unsized things) to show the representation of unsized boxes.
If it's possible to do the mutex without the allocation, why does the std mutex do an allocation? (just curious)
an oracle engineer did just that (linux container runtime) https://github.com/oracle/railcar
There's also vagga
[This issue gives some context](https://github.com/way-cooler/way-cooler/issues/248)
Sometimes I see people `use`ing in a function rather than a the top-level of the module. What's the benefit of this? Just to minimize namespace pollution? // not `use`ing here... use std::io::{BufRead, BufReader}; fn main() { // but `use`ing here instead use std::io::{BufRead, BufReader}; // ... }
What is a Slim lang?
If you specify both, a path and a version, in you `Cargo.toml` (eg. `somelib = { version = "0.2.3", path = "../somelib" }`, Cargo will use the path dependency during local development and the crates.io dependency when your project is installed from crates.io.
You don't actually have to compile it yourself, you can install it using rustup component add rust-docs And view them using rustup docs
`float-cmp` doc has a link with an explanation: https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/
Ahh, thanks. I'm too used to thinking of local configs and public configs as separate branches, I guess, and expected something more complicated :)
Yeah something like that. I personally do this only for enum variants: enum Foo { Bar, Baz, Qoux } fn do(f: Foo) -&gt; i32 { use Foo::*; match f { Bar =&gt; 0, Baz =&gt; 35555, Qoux =&gt; -1 } } Otherwise, I find `use`ing in the middle of the file not that readable
It seems to be a template language. All I know are listed here: https://github.com/flosse/rust-web-framework-comparison#templating
Wow, I didn't realize that wlroots was so near completion! (Though I must say I'm not so surprised.) Fully agreed, it's really interesting to see, from smithay's perspective, which design choices were made by weston or wlroots. I feel the design problems we encounter are, in the end, quite different.
Well, I see /u/_Timidger_ and /u/Drakulix have already both given pretty good answers. The projects are indeed not directly related, apart from the fact that we keep an eye on each other and have some occasional chats about it. My original idea with smithay was mostly to make a wlc equivalent, but in Rust rather than in C. Then, stuff happened (wlc got abandoned, wlroots was born, libweston also), and /u/Drakulix decided to join me on this track. I guess it was good luck for the project, as I doubt I would have managed to go this far alone.
Indeed, this is kind of "where it all started". :)
There's always a bigger and faster turbofish.
Consider the the implications of such a subtle bug.
&gt; But if your client just speaks the core protocols without a toolkit it doesn't actually work. Well, it's not that bad. Mostly you really have to use the `xdg_shell` protocol extension. But apart from that... For example, I've been working on the wayland backend for winit. And while it does still have [some very rough edges](https://github.com/tomaka/winit/issues/306), overall it does work too bad. :) &gt; On that point, does Smithay have an opinion on the client-side/server-side decorations debate, or is that a detail which front-ends can figure out? Currently, this question is higher-level than what Smithay does. For now, basically, Smithay will tell you "there is this window with this content" and it's up to the compositor using Smithay to decide what to do with this information. In the future, when we integrate higher-level abstractions, my overall philosophy with smithay is that it should make as few choices for the compositor. As such, we'll likely have an optional integration of the KDE protocol extension for border negotiations (or better yet, if this extension gets standardized at some point?), but in the end, it'll be the compositor itself that decides what to do.
&gt; waveform Personally, I can't derive much info from waveforms, while I can derive a lot of info from spectograms. E.g. I can quite easily discern music playing, a loud bang or a voice. This doesn't work as well for me with waveforms.
Because of moves. At least on Unix, there's no guarantee that memcpying a pthread_mutex_t from one place to another is legal; it could theoretically be implemented with internal pointers or something. (I'm not sure what things look like on Windows.) Since moving an object implies there are no other references to it, thus no locks, it would be sufficient to destroy the old mutex and initialize a new one in the new location, but Rust doesn't support move constructors so that isn't possible. Nor is there a reasonable way to disallow moves, at present.
Does Rust have something similar to the C-style `union` which allows multiple types to occupy the same storage space in memory?
&gt;so e.g., you can't box any type that `: Clone`, since `Clone : Sized `. I'm pretty sure you mean something else here, because the way you phrased it, it's simply false. I can have a `Box&lt;u8&gt;` without any issue, and `u8 : Clone`.
In this particular case, you can tell that nothing in the module (apart from perhaps submodules) is likely to do buffered reading.
That's what enums do. Rust does also have a [union](https://blog.rust-lang.org/2017/07/20/Rust-1.19.html), but it's primarily intended for interop with C, and requires unsafe to read and write.
No, according to the diagram above, an `enum` has to store a "type" variable along with the actual data stored. That is very, very different to what a `union` is.
Also &gt; `Box : ?Sized` can't be right. `Box` should be `Sized`, it is the size of the pointer.
I think there is a misunderstanding: I never talked about channels. Session Types are, simply put, types that encode the full flow of a session. They can certainly be combined with channels, but need not to. You may find an explanation [here](http://munksgaard.me/papers/laumann-munksgaard-larsen.pdf). You can use State Machines without Session Types, they are just a way of fully encapsulation the State Machine description in a single type for easier inspection.
I think it would be interesting to track this similar to http://www.arewewebyet.org/ ; i.e. have a page with the library you are a looking for (the language it is written in) and the possible alternatives in Rust (or none if it doesn't exist).
It would be much more reassuring to know that the language running in your browser is running on a much more memory-safe and type-safe engine, though.
Pro-tip: before trying to engage with the social medias, it helps to do *absolutely any research whatsoever*. Like, f'er example, looking at the subreddit you're posting to. You do not, after all, go into an Alcoholics Anonymous session to drink whiskey without your wife finding out.
Can somebody write a program(not sure what to use exactly). That outputs value.joke from here http://api.icndb.com/jokes/random . I tried following this https://rustbyexample.com/std_misc/threads.html and using crates like curl or hyper. And was able to get the data with curl but couldn't parse it.
I see! Thank you a lot! 
Will try (and think about) that as soon as possible! Thank you! 
Ups.
TIL: &gt; cargo check
Thank you!
Thank you!
It's a confusing error message, but you just need to mark `has_dot` as mutable.
 / \ / \ O O / \ / \ \ / O O \ / \ / \ / 
Wait. Shit I thought I had has_dot marked mut. Thanks.
Yeah, enums in Rust are like tagged unions, which are what you usually use `union` for in C/C++ anyway. If you're asking for the ability to interpret the same data as different types bit-wise, that's horribly unsafe and so Rust is obviously against it, but there's always [std::mem::transmute](https://doc.rust-lang.org/std/mem/fn.transmute.html), I guess
I want to have a public trait, but its functions to be private. That is not allowed by default - if you declare a public trait, its functions also are public. How should I implement this? Context: I am writing an interpreter for a statically typed language. I want to mark some rust types as convertible back and forth to some builtin types of the interpreted language. Ideally it could be something like this: // I want this trait to appear in signatures of some public functions, // like `add_foreign_fn` below, but keep its member functions private. pub trait Type { fn internal_type() -&gt; InternalType; fn into_internal(self) -&gt; InternalValue; fn from_internal(val: InternalValue) -&gt; Self; } impl Type for i64 { ... } impl&lt;A: Type, B: Type&gt; Type for (A, B) { ... } impl&lt;A: Type&gt; Type for Option&lt;A&gt; { ... } // Now I would be able to typecheck foreign fns // and automatically insert necessary type conversions. pub fn add_foreign_fn&lt;A, B, F&gt;(f: F) where A: Type, B: Type, F: FnMut(A) -&gt; B, { ... } I don't want to make member functions public because I don't want to expose internal representations of types or values.
The difference is invisible at the Rust semantical level, only visible if you inspect the representation (looking at the size or raw of bytes of it). Their signatures are the same, "Option&lt;T&gt;", and since the compiler makes separate versions for each specific T, the difference is handled at that time.
And there's `union` in Rust too.
MaidSAFE may be hiring too. They were not too long ago, at least, and more recently were still talking about expanding to new premises etc.
&gt; I think integration testing uncovered issues with OSX over native-tls as well (different sonames). My goal is for `rust-musl-builder` to provide a step-by-step walkthrough for avoiding all problems of this sort. If there are any special rules for OS X, such as "Always build binaries on OS X version $FOO", then I'd love to add them to my TravisCI docs! (Even though I `rust-musl-builder` focuses on Linux and musl-libc, it attempts to provide TravisCI setup instructions for other platforms.)
TIL that Elite is written Rust ;)
Then you might also like [`cargo watch`](https://github.com/passcod/cargo-watch).
I've added some docs to cargo source if anyone's interested, in commits [446ed6e], [873a081] and [7da5c8]. I actually went to cargo to try to fix a different issue, but I felt that I had to read the source code of all the parts I could possibly touch, so that I knew what I was doing made sense as part of the whole, and didn't increase complexity by using different patterns. Hopefully anyone following me can just read the summaries and avoid having to understand the details. In the mean time someone else fixed the problem I was going to fix :) [446ed6e]: https://github.com/rust-lang/cargo/commit/446ed6e440f175b14f6d42cc941c44801028a40f [873a081]: https://github.com/rust-lang/cargo/commit/873a081019d8efb1906ae44c9afaffeec21f7db5 [7da5c8]: https://github.com/rust-lang/cargo/commit/7da5c847776913c3a499d62753903b9ae8451639
I'm a bit confused by something: the design document states: &gt; Importantly, the context does not contain any writable state. But later on in the pseudocode it shows that the cache lives inside the context (indeed I can't see how else you would easily add the caching). Does that quote mean "writable state apart from the cache" or am I missing something?
`elite.pchooooo::&lt;Rust&gt;();`
Recommend you use [Reqwest](https://github.com/seanmonstar/reqwest) for querying the API and [this json crate] to recover the joke; both are very easy to use. You won't need threads or serialization for this (but if you ever do, Serde is the effective replacement for rustc_serialize)
Really nice. In my opinion it is worth adding to the rust book.
Just to make sure: You know the book is [available online](https://doc.rust-lang.org/book/) yeah? 
Does this support streaming? If so - how legal is it?
Thank you.
You could expose an empty, public marker trait and have it as a bound on the private one: pub trait Convertible {} impl Convertible for i64 {} trait Type: Convertible {} impl Type for i64 {}
Waiting for this cheat sheet for a long time. Thanks!
Yes, it is an implementation of the Spotify protocol, which --- as far as I know --- is not open. Regarding the legal aspects: 1. It only supports Premium subscriptions. So you cannot bypass ads. This means it probably won't annoy Spotify that much, since users pay for the service, just that they're using another client. 2. In theory, it could be used to capture the music. But this also works with the official client using audio loopback drivers. 3. See disclaimer in the wiki: "Using this code to connect to Spotify's API is probably forbidden by them. Use at your own risk." 4. The actual legal aspects differ depending on the country you're living in, no matter what Spotify's terms state. Reverse engineering are likely to be forbidden in the US, while in some European states that's not a problem.
This is amazing. I was looking into writing a DJing app backed with Spotify but I couldn't figure out the streaming. Thanks for making me discover this.
The best part about NonZero is that it's enough to have one NonZero field in a struct, and that struct can already be optimized: sizeof Vec&lt;u8&gt; == sizeof Option&lt;Vec&lt;u8&gt;&gt; [Playground link](https://play.rust-lang.org/?gist=ef95d60f2032f9163026b069187c45e6&amp;version=stable)
To be clear, when there is a tag, the data is stored all in the same place just like a union, so enums are like unions but with a little extra. Rust gives you both for a reason.
There is an open issue about it, but it's tricky: this diagram implies internals that aren't always guaranteed to hold, so while it's useful, it could also cause people to write unsafe code that's incorrect.
The bit where is says `String = Vec&lt;u8&gt;` makes me uncomfortable. Maybe it's just talking about the internal representation being isomorphic, but it looks like it's saying they're literally the same thing. It at least needs a caveat about UTF-8.
Your body of 2 is strictly worse in my opinion. It has major tradeoffs that aren't being considered. By putting the bound on the struct, you will need to state that bound *any time* you are attempting to take an instance of that struct, regardless of whether you need it to be `Read` or not. This tends to be pervasive and means any struct that has this struct as a field will need the same bounds, and then any mention of that struct will require these bounds, and so on. This is why (for example) `HashMap` has no bounds on its parameters, even though nearly every operation on it requires that the key be `Hash + PartialEq`. Typically the only time you should have a trait bound on a struct definition is if a field of the struct needs to be an associated type of the type parameter. [Example 1](https://github.com/diesel-rs/diesel/blob/0ce9179facd7defaef70b3b3563ba1c69dd27d48/diesel/src/query_builder/ast_pass.rs#L220-L234) [Example 2](https://github.com/diesel-rs/diesel/blob/0ce9179facd7defaef70b3b3563ba1c69dd27d48/diesel/src/connection/statement_cache.rs#L194-L200)
If you want to get slimmer than node.js, what about using Rust? It has a very nice server writing community already, and its improving as we speak.
Since struct layout is always undefined, it seems like someone trying to write that unsafe code would be wrong regardless of whether they've seen this diagram or not
There's an open bug for this. https://github.com/rust-lang/rust/issues/41790
That's stellar
Here's my experience of getting into Rust. It had been somewhat on my radar due to its memory model being novel. Every language is trying to solve the problem of shared mutable state. Most languages do this by removing mutability, but the fact that Rust decided to do this by removing sharing was interesting to me. I remember there being an episode of the changelog with /u/steveklabnik and Yehuda Katz that got me much more interested in it. Fast forward to Rust 1.0, I had been working on a project that was a 3d rendering engine in C++ (I normally do Ruby but I had become the 3d rendering engine guy at my company for some reason.) This was the third engine I'd written, but the first one in C++. I'm really bad at C++. I tried porting it to Rust on nights and weekends just for fun (As a consultant, I couldn't ship the Rust code in good faith at that point in the language's lifecycle). I had a segfault that I was having a hard time tracking down. The Rust code wouldn't compile. I fixed the bug in the C++ code. So I was sold on it for anything I would have used C/C++ for at that point, but I don't use C or C++ for anything. However, that was when I also started to realize how awesome Rust's type system was. I was very into Haskell and Scala at the time, so this really piqued my interest. So I started to ask myself: "Can Rust work as a higher level language?". To me, higher level means web. The first thing I wanted to do for a web framework was and ORM, so I wrote [diesel](http://diesel.rs) to see if I could make something in Rust that felt as high level and productive as Ruby. (I was originally going to make a whole web framework but then I had a baby instead) The answer was a resounding yes, and for me Rust has become "The Haskell I always wanted". Memory safety isn't a selling point for me, since for me it's a given. Rust wouldn't be less interesting to me if it were garbage collected (though I have found that its ownership rules improve code and help express intent). Speed is nice to have, but also not a big selling point. However, I've found that its type system is extremely powerful and extremely expressive. But the language has an element of pragmatism that I wanted in Haskell. (Scala had that same pragmatic element, but has many other issues). As for what's unique: - The combination of 0-size types and an optimizing compiler gives you some really interesting guarantees about your program. [I gave a talk about this recently](https://www.youtube.com/watch?v=wxPehGkoNOw) - RAII is "enforced". You can't accidentally forget to free memory or close a socket. - Thread safety enforced at compile time, and represented by the compiler.
Why is writing to an union unsafe?
Oh, hm. I must be confusing something then. I ran into [this case](https://play.rust-lang.org/?gist=cc5e61ee6a37a21dae1e145911bb620f&amp;version=stable), where you cannot box a trait if the trait `: Sized`. Can someone please explain to me why this doesn't work, but boxing other regular types that `: Clone` works?
Ah, yes, sorry, I fixed it.
But this is backwards, no? Now in my function the bound is `Convertible`. If I put functionality in trait `Convertible`, I have the same problem as before. If I put functionality in trait `Type`, I don't get anything useful out of bound `T: Convertible`. What I would need is have the private trait as a bound on the public one - so users of my crate would not be able to implement public trait because they can't reach the private one. But that code gives the same warning for having a private trait in public interface.
Because of [this](https://play.rust-lang.org/?gist=d9dc3208cd85397791a99360fd910788&amp;version=stable). That is how hard to find bugs sneak into programs.
That's why reading it is unsafe. If getting a pointer is safe, writing to an union should also be safe, shouldn't it?
You also don't _need_ to use a `Box&lt;FnMut&gt;`. If you need to call it by dynamic dispatch, use `&amp;mut FnMut` [like this](https://play.rust-lang.org/?gist=f3f035aaa88e5b14413ec6cc79bbc08a&amp;version=stable). If you want to call it via static dispatch, consider making the function type generic [like this](https://play.rust-lang.org/?gist=516aadded2971f8ba17e77dc41a94c73&amp;version=stable).
Perhaps [bus](https://crates.io/crates/bus) is what you are looking for.
Wow those are really a lot better than doing Box. But why don't they need the move keyword?
Hmm. Each T could have multiple Monoid instances, and in Rust the only way to impl the same trait in multiple ways is with a type parameter. So to do Monoid in Rust, we'd need to have some way to distinguish different functions in the type system. (Unless by Monoid you mean the collection of different traits std::{ops::{Add,Mul}, iter::Extend}.)
This is [trait object safety](https://doc.rust-lang.org/book/second-edition/ch17-02-trait-objects.html#object-safety-is-required-for-trait-objects). For one, you have to be sized when the type is used by value, as with the return value of `clone()`.
It's because passing the closure by-value or by-reference the way I did lets it be known that `has_dot` is borrowed by the closure for proper amount of time. `read_while` may do anything with its `Box&lt;FnMut`, including let it outlive the calling function's scope and thus outliving `has_dot`.
Writing to a union containing only copy types is safe. It becomes unsafe when you have non-trivial drop code in the union's fields: [example](https://play.rust-lang.org/?gist=db6e6751600720da83c28728a4817776&amp;version=nightly). Notice how it segfaults even though it never reads the contents of the union. This is triggered because when you write to a union, it assumes the previous value to be of the same kind (if you write to foo.x, it assumes the previous .x value is valid) and drops it. So when foo.b is assigned to Bar, it assumes the previous value is a Bar and drops it, which results in 'dropping: 1' being printed because Bar happens to have the same memory representation as a u32. When it assigns a vec to foo.c, it assumes there was previously a vec in there and tries to drop it, which causes a segfault. To make it not trigger the drop code, you'd have to do it like this: foo = Foo { c = vec![1,2,3] } which is safe, but means some structs may potentially not be dropped. 
Nah, your bound is still `Type`, the point is that in order to implement `Type` for `T`, there must exist `impl Convertible for T`, and only your crate can do this. You can see this used in [byteorder](https://github.com/BurntSushi/byteorder/blob/master/src/lib.rs#L144)
I have [a few suggestions](https://play.rust-lang.org/?gist=5acc2e54feb3284dbe64c99f24110a1c&amp;version=stable) that'll (imo) improve style but maintain behavior.
If the bound is `Type`, then you probably got the `pub`s wrong in previous code sample. `byteorder` is different, because it doesn't try to hide trait methods. It just needs to prevent others from implementing a trait, whereas what I want is to hide trait items to prevent leaking implementation details. [Here is a sample on playground based on byteorder (which does not compile).](https://play.rust-lang.org/?gist=71383eeac5bc918c84be515c18c299ea&amp;version=stable) Maybe I'm simply not seeing the solution, maybe you can provide a full working sample?
[removed]
1. Try not removing DirectX 9 2. Maybe you were looking for /r/playrust
Yes, but it makes it easier/more likely that it'll be written in the first place.
If you search github for 'cargo-' you'll find a lot of good ones.
Then maybe it is backwards. BTW not the one who wrote the previous code example
Hi. I appreciated your answer. I've reworked it and even though the very last loop I wrote threw me a "you cannot borrow x mutable because the iterator borrows it immutable" problem, now it's running, but with superfluous copy operations on items. I'll try to slowly work these away when I've got a bit of time. Thanks again!
Whoops, didn't notice that :)
So I'm a bit mired in the Entry, OccupiedEntry, VacantEntry APIs. These aren't traits - every data structure has their own structs for these. I have my own handrolled data structure (a [trie](https://github.com/sevagh/fs-trie)) that I want to implement Entry/OccupiedEntry/VacantEntry for. Is there a document that specifies the required methods of Entry/OccupiedEntry/VacantEntry? A spec? Is it a struct, a trait, an impl? Do I just go by the source code of (for example) the std::collections::HashMap Entries?
So, what I meant is that, if you have a Monoid instance for i32, all of the Monoid operations take and return i32. And this holds for lists too: the Monoid operations on LinkedList&lt;T&gt; can only return LinkedList&lt;T&gt;. But for Monads, you need a way to keep the container type but change the element type. That is, you need to be able to take a LinkedList&lt;A&gt; and a function mapping A to LinkedList&lt;B&gt;, and get a LinkedList&lt;B&gt;. I think you're right about type parameters for multiple instances, but I don't know if there's a good solution to that i.e. Haskell has the same problem. Usually the solution is just to make a Ring trait that has both operations.
Thanks for your feedback! I've created [an issue](https://github.com/fengalin/media-toc/issues/50) with your suggestion. I though about spectrograms too, however they are more CPU intensive and in most cases, I think a waveform is good enough to find a silence or the end of a sentence. I'll probably add this as a toggle option. This should be quite easy to introduce in the architecture.
Oh, wow, thank you for that explanation!
Oh, wow, thank you for that explanation!
That's all [String is though internally](https://github.com/rust-lang/rust/blob/master/src/liballoc/string.rs#L294-L296) All it does is add a wrapper around it, adds some methods to deal with Strings, and allowing it to be UTF-8 compliant. For the sake of this diagram it makes sense to say this as this is about the layout of data types, not what they do.
Try /r/playrust
Yes, you can basically follow the docs or sources of hash- or btree-map entries.
There's a hack to work around the "no private XX in public" errors: just reexport them from a module. Like this: mod foo; // Type, InternalType and add_foreign_fn are in here pub use foo::add_foreign_fn; // Notice how "Type", "InternalType" etc are not reexported Don't know if this is supposed to be allowed but it seems to work :-)
I have a rayon question. I was looking at the tutorial and saw that it just says to import rayon::prelude::* and then I can use par_iter() on my vecs, but I am getting: no method named `par_iter` found for type `&amp;'a mut std::vec::Vec&lt;T&gt;` in the current scope How do I implement the par_iter trait for this type? Thanks!
Thanks.
I can't get that to work - to be able to reexport the function I need to make it `pub`, and then I need to make trait `pub`, and then I'm back where I started. I think the solution I will go with is just wrapping stuff I don't want to expose in opaque structs.
The trait is pub but it's in a non-pub module, and you don't reexport it, so it still won't be usable to other crates.
It isn't.
It isn't unsafe to write to a union.
This seems to just wrap a call to the ruby gem
Yeah, you're right there. Boxing a *trait* that requires `Sized` doesn't work. However, boxing traits is very different from boxing regular types. It has special semantics (see the other comment about trait objects).
Quote from the article I linked (emphasis mine): &gt;Since we can interpret the data held in the union using the wrong variant and Rust can‚Äôt check this for us, that means reading **or writing** a union‚Äôs field is unsafe: I don't recall reading in the 1.20 release article about it now being safe to write.
We also have a few more here in Awesome Rust: https://github.com/rust-unofficial/awesome-rust#template-engine
Hmmm. It appears I am wrong. Thanks for enlightening me!
I think this might have something to do with `saturating_add`. With `wrapping_add` it seems to vectorize.
Interesting. I wonder if this is a limitation in LLVM, since C/C++ don't have support for saturating adds?
Good to know about the rocksdb crate. I see several other people saying the same thing, and I'm sure RocksDB itself would be fine for my application; it's just a matter of if the parts I need have been wrapped and if that wrapping is sound/ergonomic, if it introduces extra copies, and so on. I'll keep an eye on sled‚Äîlooks great. If you ever write up something about its data format, I'd be very curious to read it. For the application I mentioned, I want something that either lets you do a customizable page size, or uses chained overflow pages to not waste up to a page per value, or both. I'm also curious how the zstd compression works‚Äîif it compresses individual values or whole pages, and if the latter, how it splits the input to get roughly full (but not overfull) blocks post-compression. I skimmed your references but they don't seem to talk about that, and I get the impression you've made enough of your own implementation choices that none of them are exactly describing sled. I'd also be curious to read more about the log and concurrency aspects.
This is great and I'm definitely saving this, but can anyone explain Cell and RefCell to me? Or point me to an article that does?
TIL!
I'd guess it's this. I'm wondering if there's some way to improve upon it still, for example compuing .overflowing_add in blocks of 8/16/32 values and then saturating those that have the wrap around flag set...
Personally I'd go with the first one, but if you have things that aren't copy, the second one _can_ work. The main problem is that 'b is _supplied by the caller_ at the moment - so you can't have it just be inside the closure. To fix this, we need to specify "for any 'b lifetime, &amp;'b T is PartialOrd&lt;&amp;'b T&gt;". This is possible with the `for&lt;'b&gt;` syntax. Full example: pub fn max_subarray&lt;'a, I, T&gt;(mut i_iter: I) -&gt; T where T: std::ops::Add&lt;Output=T&gt; + std::cmp::Ord + std::default::Default + Clone + 'static, for&lt;'b&gt; &amp;'b T: std::cmp::PartialOrd&lt;&amp;'b T&gt; + std::ops::Add&lt;Output=T&gt;, I: Iterator&lt;Item=&amp;'a T&gt; { Instead of declaring 'b on the method, we declare it within the bounds - meaning &amp;'b T has to be Add for _any_ 'b possible. There's a bit of documentation on the `for&lt;'a&gt;` syntax here, but it's a somewhat niche part of the language: https://doc.rust-lang.org/nomicon/hrtb.html.
(Disclaimer: I'm assuming you're targetting x64 and just compiling with -C opt-level=3) I'm not sure why it's failing to vectorize this loop. The vectorized version of it is extraordinarily simple: MOVDQU a, [pointer_a] PADDUSB a, [pointer_b] MOVDQU [pointer_a], a increment pointer_a and pointer_b by 16 However, I haven't been able to make the stable compiler emit a vectorized loop. It might just be that LLVM's vectorization doesn't know about vectorizing saturating instructions as they're so rarely used. 
Author here. I wrote a thing. It's a bit rough but it might be useful to other people.
Yes, x64. The problem is this function is a hot path. I'm converting some C code (with intrinsics), and the rust version is much slower because of this problem.
very thank you, much cool! Looks like a lot of work even though you're trying hard to convince the reader this was rushed out in a few minutes ;) No this looks like a good amount of work and even better i really think that could help some people to make an informed decision and jump into the topic really quick. Please take my upvote :)
You could link in C code for just one function like this -- even C code that is just using inline asm of the compiler. It's less portable of course, but the cc crate makes it easy to do if you want to.
After a bit of investigation of rustc internals, it seems that saturating_add is implemented in terms of checked_add, which then is implemented in terms of overflowing_add, which is an LLVM intrinsic. However LLVM also has saturating_add intrinsics. It might very well be that LLVM has trouble reducing all the control flow in saturating_add and checked_add back to a saturating add. It might be worth submitting a patch to rustc to just directly use the saturating add intrinsics instead of handing LLVM two layers of control flow abstractions.
I'll give that a try. It is a very small function, so it should be easy to test.
You sure like your generics! :) You can take it one step further by accepting `I: IntoIterator` instead of `I: Iterator`. Note that all iterators implement `IntoIterator`.
Segfault results from `drop()` **reading** from the union.
Great article, this is the most informed opinion on the subject at this point in time! Also cements current plans of "keep using iron while waiting for Rocket to hit stable Rust".
It seems like saturating addition is not LLVM's strong point currently, as there is not even a saturating add intrinsic, so your only hope would be the optimizer figuring out the specific intended behaviour from the pretty huge LLVM IR dump that rustc gives it to do a simple saturating add.
Awesome, that fixes it. TIL. Is there an appropriate authority to bounce this instance to in order to help make it more 'discoverable'? I had seen this before, but since I didn't understand it, I didn't think to use it. It would also have been nice if the compiler advised it, but I don't know how tough that is.
Unfortunately into_iter isn't implemented for fixed-size arrays yet. https://github.com/rust-lang/rust/issues/25725 Otherwise I'm not sure what you mean by this?
Basically, runtime borrow checking. You can pass an immutable reference to a RefCell, to a function for example, then inside that function borrow the contained item mutability from the RefCell. [see the rust book](https://doc.rust-lang.org/book/second-edition/ch15-05-interior-mutability.html)
Shouldn't this actually get simplified at compiletime to a constant array? Simplifying the example yields interesting results when looking at the generated assembly. This looks like it allocates space for both arrays: fn main() { let a = [100u8; 1024]; let b = [200u8; 1024]; let c = a.iter().zip(b.iter()).map(|(ai, bi)| ai.saturating_add(*bi)); println!("c: {:?}", c.sum::&lt;u8&gt;()); } Whereas this looks like it computes the value at compile-time and doesn't need to allocate the arrays: fn main() { let c = [100u8; 1024].iter().zip([200u8; 1024].iter()).map(|(ai, bi)| ai.saturating_add(*bi)).sum::&lt;u8&gt;(); println!("c: {:?}", c); } Although this looks identical: fn main() { println!("c: {:?}", [100u8; 1024].iter().zip([200u8; 1024].iter()).map(|(ai, bi)| ai.saturating_add(*bi)).sum::&lt;u8&gt;()); } This seems like a lot of variation in final assembly output that's actually somewhat undesirable, because it suggests that making source code more verbose actually impacts its optimization.
In the imageproc crate, drawing a line requires the trait bound "GenericImage" from the image crate. [It looks like any ImageBuffer should have GenericImage implemented for it.](http://docs.piston.rs/imageproc/image/struct.ImageBuffer.html#impl-GenericImage) But I think I am misunderstanding, because I am getting the error: draw_line_segment(&amp;rendered_strokes_buffer, (stroke.start.x as f32, stroke.start.y as f32), (stroke.end.x as f32, stroke.end.y as f32), stroke.color); | ^^^^^^^^^^^^^^^^^ the trait `image::image::GenericImage` is not implemented for `image::ImageBuffer&lt;image::Rgb&lt;u8&gt;, std::vec::Vec&lt;u8&gt;&gt;` How would I implement this trait?
Under __First Pass__ all crates.io links except for conduit's point to https://crates.io/crates/
Sorry, that example isn't exactly how my code works. It's more like this: fn accum(a: &amp;mut [u8], b: &amp;[u8]) { for (ai, bi) in a.iter_mut().zip(b.iter()) { *ai = ai.saturating_add(*bi); } } In any case, there's no constant arrays involved.
I was referring to your specific example in the first post - this was just something I noticed about the compiler in general.
Hi, I'm glad I helped you discover it :) ..but are you aware of djay Pro by Algoriddim, which does exactly what you described? It's available for Windows and Mac, iPhone and Android.
Project Euler problems are expected to take a few seconds at most on a poor machine with many problems being able solvable with paper and pencil with enough math knowlage. Unfortunattly if you need more speed to get your solution quickly your algorithm isn't good.
Is this happening like right this very second? How much time do you have to prepare, in other words. :P Also, is the audience familiar with everything since C++11, or are they more old school?
* Rust has a standard project/package management system. So no hundreds of arbitrary rules for building 3rd party packages, all are handled by cargo. I love it. * Rust has traits, C++ hasn't gotten concepts just yet. * RLS is a great tool. Clangd is still in development, however RLS provides cutting edge linting, formatting, definition checking across many editors. * Does not suffer from the burden of source-level C compatibility. To me, inheritance, polymorphism, overloading, std::move, templates, R-value references, std::unique_ptr, parallel STL are the essence of C++. However, C++ has [many issues](https://yosefk.com/c++fqa/). I'd address them and show how Rust takes a different and more sophisticated approach.
No.
Thank you very much for your input! I did not even know about Clangd - very interesting. :) Also thanks for the link to the C++ FAQ, it looks very valuable.
There is also this: https://github.com/flosse/rust-web-framework-comparison It's a pity that the author skipped out on Shio. I've been using Shio and it's been excellent. Very lightweight and async by default.
Threads without data races was a big selling point for me coming from C++
I have about one month of preparation. ;) The audience is an open-minded, university centric C++ community that is also very interested in Rust and other new technologies.
&gt; I really wish there was a document that would explain those "magic" concepts like this. What did you think of the linked book chapter?
I ran into this, though with the `mut` versions of the draw functions. What I ended up doing was a re-borrow, something like this: draw_line_segment(&amp;*rendered_strokes_buffer, ...);
&gt; focus on how both languages can learn and profit from one and another I don't mean to rain on your parade or fanboy too hard over Rust, but I don't think that's going to work. The rest of your talk sounds promising (and I'd rather like to watch a recording); but not this. Here are the biggest things I wish C++ could learn from Rust, but won't/can't * No header files. Rust's module system has its weaknesses, but it it's way better than nothing. * Don't coerce between types. Changing this would break enormous amounts of code, so it won't happen. * Put error handling in the return value instead of using exceptions. This won't be done because the C++ standard library uses exceptions, and too many people depend on that. * `#[derive(Debug, Clone)]` makes my life so much easier, I wish C++ had a way to do this. I doubt it could be a language feature, but maybe somebody could write some crazy library. * A centralized package ecosystem and a really good ubiquitous build system that pairs nicely with it. The difference in dependency management between Rust's default and C++ is night and day. I miss only one thing from C++. Availability of (idiomatic) libraries. I thought I'd miss more of my tools, but between RLS in VSCode and `rust-gdb` I can manage projects of a few thousand lines just fine.
&gt; I really wish there was a document that would explain those "magic" concepts like this. relevant rust-lang/rust issue: https://github.com/rust-lang/rust/issues/44061
First of all: Thank you for your critical input! Actually in C++ there is a huge momentum towards nearly all of the points you stated. Let me summarize point by point: - No header files: The hopes are high that C++20 will add modules to the standard led by Gabriel Dos Reis. As far as I can remember they will more or less deprecate header files in new C++ projects. - Don't coerce between types: It is possible to use a more type-safe approach in C++ even today by not using the primitive types directly and use thin wrappers instead. The problem is that nearly nobody is using it that way. - Put error handling in the return value: Again, there is a lot of movement around this field with newer standards, I think it is named std::expect or something like that that acts similar to Rust's Result. - #[derive(Debug, Clone)]: Look into Herb Sutters Metaclasses proposal. - A centralized package ecosystem: This is exactly why Gabriel Dos Reis worked on modules in the first place - as a base for a package and build manager.
Then I might invite them to first imagine a language where move construction, rather than copy construction, is the default, implying that every piece of data is uniquely-owned by default (no need for `unique_ptr` to make it so) and moved by default (i.e. `std::move` is implied). Then build upon that by noting how, since everything is uniquely-owned, RAII can be safely used to reclaim everything allocated on the heap. Then finally build upon that by noting that references are first-class, and also prevent mutation of the referred-to memory for as long as the reference exists. This probably gives a good foundation for understanding Rust from a modern C++ perspective, though obviously by this point the devil is in the details (i.e. Rust has no user-defined move constructors, and no *implicit* user-defined copy constructors, and so on...). But beyond that, there's a *lot* of potential ground to cover... I think that Rust's ability to statically prevent data races might be something that C++ programmers might find cool, in that it takes the fangs out of thread-based concurrency, but the exact mechanisms of how it achieves that might be too much for your talk depending on how long it is.
Hello, Steve! I'm [still trying](https://news.ycombinator.com/item?id=13583961) to grasp as much Rust as possible. I actually can't wait for the Second Edition to be finished. The issues I have with the chapter are these: * It doesn't spend enough time explaining `Cell`, which is, as I understand it, *the* type to use when talking about interior mutability. * It doesn't explain well what's the difference between `RefCell` and a simple `Rc`. At least, it isn't immediately obvious to me from the sentence &gt;Unlike `Rc&lt;T&gt;`, the `RefCell&lt;T&gt;` type represents single ownership over the data that it holds. I would rewrite it into something like &gt;Unlike `Rc&lt;T&gt;`, which counts the number of owners of the data, the `RefCell&lt;T&gt;` type counts the number of dynamic borrows. You get the idea. * It doesn't go into details about *how* this is achieved (this is where you have a chance to explain `Cell` better). I understand that you might not want to just outright say that this is a struct with a value and a counter, but at least write something like &gt;*You can think of* `RefCell` as a struct with a value and a counter. I think the issue here is the same as the one I've mentioned in the HN comment back then. The chapter still seems aimed at non-programmers or at least non-c|c++|go-programmers. I'm not saying that the Book is therefore bad, it simply may not appeal to me and like-minded individuals as much as it could.
It's cool; we've been working on this chapter lately, including talking about this kind of thing, so it's very relevant. Thanks!
Thanks, I‚Äôll try this when I get back. This behavior seems like it would be a glitch, no? Is there a reason for it?
Another (perhaps a bit silly) way of thinking about `RefCell`: it's just like a `RwLock` that panics instead of blocking.
I think /u/steveklabnik1 would be the best authority on documentation matters - I'm not sure about compiler error messages. I definitely agree that it could be more discoverable. On the other side though, it is a fairly niche topic, so I don't know where it would be good to introduce... The only place I've ever used it myself is in writing a custom serde Deserialize implementation.
Fixed, thank you!
This is going to come off as very negative and maybe combative. Please don't get me wrong, I don't hate C++. I just have no reason to go back to it in a world where Rust has the things I wish C++ had, and where C++ has both the things I want and old crotchety ways to do the same thing. So if C++ does add all these things, now we have: * Modules _and_ header files. So sometimes you still have to deal with header files. * Coercion, except where you avoid the basic language features (isn't that tantamount to saying you're using a broken language ?) * I can use `std::expect`, but an exception still might pop out of almost any line of code. And how would the standard library even migrate to such a thing? Or are we just going to assume nobody uses _the standard library_? Or will the standard library both return this new type _and_ throw exceptions to preserve backwards-compatibility? I've seen some of the proposal for metaclasses, and they do look promising. I just really dislike that the C++ solution to "We only have bad ways to do x" is always "Here's another way to do x. And we promise never to break all the old ways." I personally am not excited to juggle three different ways to report errors.
Indeed, and that unsafe read is triggered implicitly from writing to the union, so writing must also be marked as unsafe.
This is the tricky part with this kind of thing; almost *nobody* needs it, but then, when you do, you *really* need it.
&gt; A centralized package ecosystem: This is exactly why Gabriel Dos Reis worked on modules in the first place - as a base for a package and build manager. This is very interesting. I was under the impression that C++ will never be able to change the status-quo. At this point there are so many variables to count: for example Qt with its Qbs and Qmake, it uses meta object creation as well. It's going to be very difficult for them to move over and conform to the new centralized package management. *(My bet is that they'll once again show the middle fingure to the C++ standard and keep reinventing the wheels).* I've read about the proposed `build2` system but I'm genuinely interested to see how it plays out. If C++ gets a standard package management protocol/tool it would be absolutely amazing. But at the same time there will be a lot of projects that will have to migrate their structure to comply with the new standard.
&gt; Rust has traits, C++ hasn't gotten concepts just yet. They were recently merged into the C++2020 draft, by the way.
Not only for cell/refcell,but also for smart pointer suchas Box,&amp;[],Rc,Arc,Mutex...etc.
this subreddit isn't about the game. this subreddit is for the programming language which predates the poorly named video game. If you don't read the rules before interacting with a community, don't be surprised if you get banned.
I got banned from the Rust programming language community.
I have no idea what you did, but clearly you did something. Acting this way indicates that the behavior is a pattern, not an exception.
also, /r/rust is not an official channel anyways.
Are there many people still using Slim? Seems like I haven't heard or seen much recently, but maybe I'm just in the wrong communities to hear about it.
What's the use case?
Does imageflow properly account for sRGB's gamma curve when doing any operations on images? Proper sRGB support is the make or break feature for any image editor for me.
I strongly agree to all of those points: It is more or less a similar story to why I moved to Rust for my personal projects. :) However, I find it even more important to spread the knowledge about this. Also, C++ will continue to play a very important role in the future and so it is also equally important to stay on par with the best you can possible get out of it. " personally am not excited to juggle three different ways to report errors." - Made my day! :D
I probably won't personally use rocket for my API server, but hyper directly with some routing layer. I dunno if one exists yet but I'd love a compile time router that generates a true of matches for segments.
I'd argue these are all frameworks not servers
Hey guys, excited to publish my first crate. It is just a simple timer for frames in game. It has no dependencies and is more or less drop in. I am open for suggestions to add to it, and I will be maintaining it.
You should name the source file hi.rs
Once you have a good grasp of how `&amp;` and `&amp;mut` and `Sync` all work by themselves, I find it helpful to compare the different "interior mutability" types in those terms: - Cell - `&amp;self` -&gt; `set()` the interior (no `&amp;mut` from `&amp;self`) - Contents must be `Copy`. - Never `Sync`. - RefCell - `&amp;self` let's you get `&amp;mut` of the contents - Never `Sync`. - RwLock - `&amp;self` let's you get `&amp;mut` of the contents - `Sync` if the contents are `Sync`. - Mutex - `&amp;self` let's you get `&amp;mut` of the contents - Shared references to the interior _also_ take the lock, so only one can exist at a time. - Always `Sync`.
Correct me if I'm wrong, but can't Cell contents be used with types that just implement clone? I remember reading that somewhere I thought. 
I can name it hi.gay and it'll work. The point is that it blindly wrote over my source file.
The executable name is the source file name minus the extension, by default, on Unix like systems. As the other person said, the extension *should* be `.rs` to ensure proper behavior. This sounds like user error.
To use the git repo url when the crate is imported via its own git url, but use the local path when it's imported via `path`.
Wrong. In this case "hi.gay" will work just fine (I just tried it). As long as it has an extension, it will work. I just tried "hi.", and that works fine.
again, *it's stripping the extension*. Yes, *this* works with whatever extension, but *many* other tools will break with the wrong extension, whether source code editors, formatting tools, etc. I am not wrong.
This seems like consistent behavior. Rustc writes the target file as the source file minus the extension by default, and that's what it did here. It overwrites `hi` in all other cases too, it's just that usually the file is a binary file - a previous compilation. It would be less than helpful in the majority of cases if it _didn't_ overwrite text files similarly. This is rustc behaving as expected for an unconventional situation.
A subset of the operations can be used with non-Copy types, but I'm not sure Clone can be used as a substitute for Copy without additional guarantees: Cell works with no memory overhead (Cell&lt;T&gt; is the same size as T) and little runtime overhead (it "just" inhibits optimisations, it doesn't introduce extra explicit operations) because it never allows anyone to obtain a reference to the contained T. To use Clone, the clone method needs a reference to self (i.e. the T) which breaks the invariant and can lead to memory unsafety like dangling pointers (essentially, if there's a cycle, a bad clone could mutate the Cell containing the T being cloned).
&gt; I really wish there was a document that would explain those "magic" concepts like this. https://manishearth.github.io/blog/2015/05/27/wrapper-types-in-rust-choosing-your-guarantees/ This used to be in the book, but isn't in the new one.
No, you're wrong. You said it MUST have .rs to ensure proper behavior. I proved that wrong.
But: rustc hello.foo.bar error: invalid character `.` in crate name: `hello.foo` error: aborting due to previous error Why not make "hello" invalid as well?
"Ensure" proper behavior. Ensure is the key word there. You can have things work occasionally with the wrong file extension, but proper behavior is not ensured, it just *happens* to work in an unsupported configuration.
This is another example of improper behavior caused by not adhering to the documented correct ways of using the compiler. Why is it so hard to just use the proper file extension? There is no logical situation I can think of where you would be unable to use the correct extension. It's not that much to ask.
/r/playrust
I find it is useful for debugging a dependency. Make a local fork and hack on it.
This is a compiler, which requires a lot of things to follow exact specifications, such as the syntax of the code inside the files you give it, the accuracy of any paths you provide to it, and a host of other things. It's not a high level tool like Firefox which has to tolerate all kinds of errant behavior. If you deviate from the specification, don't be surprised when the results are unsatisfactory. The majority of Rust programmers never even use the `rustc` command directly. They primarily use the `cargo` and `rustup` tools, which are the more developer-friendly frontends for Rust related tasks.
A basic sanity check that it‚Äôs not going to overwrite the root source file would not be terribly difficult, and seems fairly sensible to me.
I'll file an issue. It's not about me--I just want things to make sense.
Source?
Why? :P
woohoo, agreement. Something like "input file must have a dot to automatically name output" would be fine.
For reference, [implementation of `saturating_add`](https://github.com/rust-lang/rust/blob/b7041bfab3a83702a8026fb7a18d8ea7d54cc648/src/libcore/num/mod.rs#L567-L575) is actually fairly complex.
I recently picked up my side project Indigo which is my take on a UI framework in Rust. It works using a procedural macro plugin for a markup like UI and a WPF like two-pass layout system. It uses WebRender for GPU accelerated rendering. This week I got hit testing and scrolling implemented. For hit testing, I leveraged WebRender's new support for built-in hit testing. Scrolling is also using WR's API. I also worked a ton on the styling macro to support multiple styles and visual states (basically input events trigger visual state changes (hover, pressed, et all) and a restyle, the style system then chooses a defined style for the visual state, and if need be rebuilds the display list.). The cool thing about the style system in my opinion is that it is strongly typed, so you can define things like you would in Rust using enums :). This week I plan on implementing a Grid layout system to allow for more than just vertical/horizontal stacking. I'd also like to tackle image primitives and fix text, it's horribly broken. The code is in a really ugly, hacky state, but I'm working towards getting it somewhere I can be proud of so that I can throw it up on GitHub. Here's a [screenshot](https://imgur.com/a/D7LS4) and [video](http://www.videosprout.com/video?id=c9690822-d709-494f-82ac-5005704a49d9) 
It's possible to implement a custom `saturating_add` implementation which LLVM will know how to optimize. fn saturating_add(a: u8, b: u8) -&gt; u8 { let result = a as u16 + b as u16; if result &gt; 0xFF { 0xFF } else { result as u8 } } fn main() { let mut a = vec![100u8; 1024]; let b = vec![200u8; 1024]; for (ai, bi) in a.iter_mut().zip(&amp;b) { *ai = saturating_add(*ai, *bi); } println!("a: {:?}", a); } 
I'd agree. It seems like common sense that something which defaults to generating its output filename would guard against the generated name being equal to the input name. I know I do that in all of my creations which support that sort of thing.
And you could add errno, too, to that list.
I'm not a game developer so I'll probably never use your crate but welcome nonetheless!
It doesn't lie to the compiler. It uses UnsafeCell inside, that the compiler knows, which is crucial for the cells to be sound.
[removed]
This is really awesome, and I look forward to seeing how things develop!
This is really awesome, and I look forward to seeing how things develop!
Most unix tools don't do this. Prime example being "foo some.txt &gt; some.txt" which most likely will end you up with an empty file,. 
I'm writing a compressed datastore for order book data - almost finished. Still need to add user authentication. https://github.com/rickyhan/tectonic
If it's the one I'm thinking of, I remember trying it out and being distinctly underwhelmed. I'm probably not going to do much better - in fact I'm probably going to do much worse - but it's still something I'd like to try.
You could use something like this to make proofs about generated code, but that's probably not the approach you'd want to take for proving soundness of compiled rust. You'd probably want to verify the compiler itself. You might want to have the compiler generate some proofs or proof obligations about the generated code. At that point it might be useful?
The app looks awesome! This looks like a lot of work.
Not the same situation. 1. The shell has no idea whether `some.txt` as an argument is an input filename, an output filename, a string to be sent to `syslog`, or anything else. 2. Checking whether `input_filename == output_fd` isn't as easy as checking whether `input_filename == output_filename`. (If `cat` didn't do it, I'd never have learned that it was possible and, for all I know, it requires Linux-specific APIs.)
It would be reasonable to make "hello" invalid, but I'd argue it's also reasonable to leave it as valid. You have to realize that `rustc` _is not a standalone tool_. It _can_ work from user input, but the majority of cases have it working alongside other tooling calling it (cargo or another build system). The reason `hello.foo.bar` is invalid is because `rustc` strips up to one extension, and takes the remaining as a module qualifier. `hello.foo` is not a valid module name, and thus the filename is also invalid. This doesn't make sense if you consider `rustc` alone, but it completely fits if you realize `rustc` is just one part of the equation. Files don't stand alone, they link together - they represent rust projects. `hello.foo.bar` being invalid is a necessity for this, while `hello` has no similar reason to be.
Strongly agree, with special emphasis on *modifying* multithreaded code. With Rust I can make (and have made) major changes to multithreaded code, both my code and other people's, with complete confidence. I'm not saying that I won't make mistakes, and break the code - but my mistakes will be logic errors, which are relatively easy to detect, rather than race conditions, which is the class of errors that I most fear.
It's more a FrameLimiter, since you can't access the frame duration. Maybe frame_end can return a value indicating if it slept x ms or if the frame was longer than 16ms?
I just copied this from the documentation let mut res = reqwest::get("https://www.rust-lang.org/en- US/")?; and got this error the `?` operator can only be used in a function that returns `Result`
I just deleted the ? and it works fine.
Soooooooooo just like jshint/eslint or **any other compiler** for compiled languages? El oh el
I find it interesting and sometimes illuminating to read the standard library‚Äôs source code. In this case https://github.com/rust-lang/rust/blob/master/src/libcore/cell.rs The core of all this is `UnsafeCell`, which seems to do nothing other than wrap a value in a new type: #[lang = "unsafe_cell"] pub struct UnsafeCell&lt;T: ?Sized&gt; { value: T, } It‚Äôs that `#[lang]` attribute that makes this type special and tell the compiler to apply a different set of rules to it. (At least making the type [invariant](https://doc.rust-lang.org/nomicon/subtyping.html), I don‚Äôt remember if there‚Äôs more.) Everything else is implemented in libraries. `Cell` and `RefCell`, but also `Mutex` and `AtomicPtr` (anything that does interior mutability through `&amp;T`) are all built on top of that. And you can read exactly how, `cell.rs` is a bit long but not dense.
I wish you tried some ""advanced"" features, like using a database. In my opinion that's where friction really starts to appear, and that's specifically why I wrote rouille in the first place, because with rouille it's very straight-forward. 
Looks like some version mismatch. Maybe you need a newer version of the openssl crate? Did you check which version of openssl you have installed? Maybe there is already an open issue. 
There doesn't seem to be an open issue. The ssl crate gets pulled in through dependencies I think, Which I've tried to modify to be the newest version. Arch liked to have the latest version of everything (&lt;3), do that is probably the reason. Is there a way to patch a crate such that all dependencies of it get resolved to a given version? I've seen replace/patch, but those seem to require fitting versions still, and manually downloading and setting the path seems very cumbersome
Author here, let me know if you have any questions or comments :) Thanks for spreading the word OP
Also to make use of a headless API for scripting the web
&gt; the type to use when talking about interior mutability You're thinking of UnsafeCell, not Cell. UnsafeCell is the *only* way to introduce interior mutability which is guaranteed to produce correct code. Cell and RefCell use it internally, as do other types. The difference between Cell and RefCell is that Cell doesn't give out mutable references to the contents of its internal UnsafeCell (meaning to set its contents they must be Copy) so doesn't need run-time borrow checking; RefCell does.
The API may not be great, but interconnects (infiniband) understand it and its performance is much better than IP-based protocols available over Ethernet
I am trying to implement hierarchical facetting in tantivy (a search engine crate: https://github.com/tantivy-search/tantivy).
Evil perhaps but very clever
Also Swift IIRC
Conclusion: Yes.
Maybe try first with a test project and the newest openssl version. After that, you could use patch to patch the dependency. Just look into the cargo documentation, they describe how to overwrite a dependency. 
No question, I prefer compiled languages. The feedback loop is huge. But a lot of languages don't have the combination of not forcing me to have everything in my head and also letting me drill down and optimize later.
Yes, I plan to do a comprehensive answer once I have figured out the best way to solve this for my application - probably sometime this week. If someone posts a good answer before that, then even better :)
That's pretty impressive considering that Rust is not the easiest language to learn.
What's the difference between PEG and CFG? Are there any advantages between the two?
I would love for someone to use it to write something that I can replace [Fidelify](http://fidelify.net/) with: A Spotify client that passes audio through a VST plugin so I can apply room correction etc.
To do it properly though you have to do more than just check whether `input_path != output_path`. You have to do what [`same_file`](https://github.com/BurntSushi/same-file) does and compare the file ID numbers.
I played around with writing bindings to allow Rust to be loading in Godot as a nativescript. The incomplete buggy result is https://github.com/Thinkofname/gdnative-rs which ended up being pretty hard due to the lack of documentation for nativescript currently. Most of the basics work, you can write classes, [expose properties](https://i.imgur.com/FB66C34.gifv) etc. I managed to port the 'first game' godot example from gdscript to rust: https://github.com/Thinkofname/rust-godot-example ([looks like this](https://i.imgur.com/yl5xVPB.gifv)) which seems to work for the most part, the editor does tend to crash randomly when the nativescript is reloaded though.
It might be nice to have it be clever like that, but it‚Äôs not necessary for it to be *useful*. The main time I‚Äôd expect this to be triggered would be when invoking `rustc` on a *nix platform with no output file specified, and an input file without an extension.
Sidenote: we had a Rust Evangelism Strike Force infiltrator in the conference: [It was a great talk](https://www.youtube.com/watch?v=lO1z-7cuRYI) 
If you count asserts and return codes, Rust also has more than one way to report errors.
Normally the compiler can assume that a variable will never be mutated while you hold a shared reference to it. The important special rule for `UnsafeCell` is that disables that assumption.
I thought Rust has no move construction? Doing foo = Foo::new() will always call foo.drop(), not in C++. Please correct me if Im wrong.
You go for it! Are you making it open source? I'd like to try it :) Anyway in my experience djay Pro is stunning. It goes even further suggesting potentially matching songs from the Spotify catalogue based on the currently playing track, its BPM, key and genre! I am not here to advertise djay Pro or Spotify, but I just found out that with a Spotify Premium account you can unlock djay Pro to a lite version which has less effects than the full versions but has no time limits (the free demo is fully featured but you can't use it for more than 15 consecutive minutes). Maybe the one you refer to as distinctly underwhelming is VirtualDJ and for what I could see (youtube videos) it did not look so great. Maybe it was just not ready for it. In fact I just learnt that Spotify integration has been removed since april 2017 from VirtualDJ. There were technical problems and there are threads in the forum about restoring this capability: http://it.virtualdj.com/forums/219703/General_Discussion/Spotify_missing.html https://www.virtualdj.com/forums/217233/General_Discussion/_NOTE__Spotify_integration_in_VirtualDJ.html
why what?
Yep, great article, and it should (in my opinion) be in the book.
This is exactly what I did and is the reason I've made this post, so that other people can have the answer faster.
The problem is is that it's very much an independent post and doesn't fit in the flow of anything. The book is an actual book with a lot of thought to the organization. The older book was just an online book and was less organized; this allowed for more chapters like this but was less good overall because of it. It's a tradeoff.
This topic was discussed here several times. TL;DR: Rust is still missing few features that HFT people need.
Diverting from making a single player game out of [PlanetKit](https://github.com/jeffparsons/planetkit) to figure out basic networking for it, after realising that the design of multiplayer and load/save for single player will inevitably influence each other. I'm currently wiring up the bits that will let me send messages over TCP or UDP, depending on the desired reliability/latency trade-off for that kind of message -- rather than trying to reinvent reliability on top of UDP. So, e.g. "attempt to craft item X" would go over TCP, but "I'm now looking over in this direction" would go over UDP.
Also sum types. They enable the best error handling approach I've ever seen.
If they like to keep up with the latest C++ standards, say "We have modules (crates), concepts (traits), and reflection (derive macros are still ugly, but better than nothing). Today". Also mention the [C++ Core Guidelines](http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines), many of which Rust code follows (such as RAII, marking owners and non-owning references differently, etc.) and (unlike in C++) the compiler enforces such rules. If they are more of the "C with classes" type, talk about an extensive and well organized standard library (including network sockets, which are still ugly in C++), the great package manager and how easy it is to install libraries, how easy it is to use things such as `serde`, and maybe C compatibility. If they are all about template metaprogramming or really complex inheritance webs, walk away. 
I'm a bit surprised that you put Iron into the "survived" group. Last time I checked it has been in a "we can't publish crate updates"-limbo since beginning of the year. So unless that changes, or Iron is rebooted under another name, it is slowly dying.
The common solution for multiple instances/implementations problem in Rust (as well as in Haskell and possibly others) is to introduce a newtype and implement the trait on that. In case of numbers, this would mean a separate `struct Sum(i32);` and `struct Product(i32);` types with distinct `impl Monoid`. Similarly for `bool` with `Any` and `All`, etc.
I'm not sure if I understand the problem correctly... can't you use the `src/bin/` directory? You can do: * src/bin/first.rs * src/bin/second.rs Then `cargo build` will generate * target/debug/first * target/debug/second
The reason I want this is that I'm making an `oreutils` crate (https://github.com/Manishearth/oreutils) that will consolidate tools like ripgrep, exa, fd, etc. Makes it easy to install rust utils on new machine.
I tend to disagree. Lots of times I used programming books as a reference, not as a tutorial, where each chapter should be connected to the previous. In the best case, the book should work both as a tutorial and a reference, but if this is too hard to achieve, I prefer reference-style books, simply because a tutorial is something you read only once, and a reference is something you use all the time. Maybe there should be two books. The Tutorial Book, introducing Rust to high-level-language-programmers, and The Reference Book, going into details about how this and that feature is implemented and when (not) to use it.
Seems like the official line from the Cargo team is that Cargo is not really designed for installing OS utils, and that that use of `cargo install` is discouraged / reserved for dev tools like `diesel-cli`. It would be more work, but you might be better off going with `homebrew` / `snap`. Another advantage of this approach would be that it could include tools written in languages other than rust. For example the `micro` text editor [0] is an excellent modern replacement for `nano` written in Go that would seem like a good fit for such a collection. [0] https://github.com/zyedidia/micro
&gt; The long compile breaks give me time to focus on TV. Is this QOTW material?
See this issue from the abandonned fix-rs crate for a discussion why it's not ready just yet: https://github.com/jbendig/fix-rs/issues/1
[This SO answer](https://stackoverflow.com/a/5501886/5436257) explains it better than I probably would. 
To me, RAII is the essence of C++, and I'm happy to see Rust turning it up to 11.
I did not mean to come across as evangelically arguing for or against the importance of any particular feature. It is indeed unproductive. I guess it did sound that way, now that I re-read my comment. I wanted to point out that features like const generics, procedural macros, HKT stuff, etc. actually enable the language to do new amazing things that were previously impossible, while features like the `?` operator and NLLs are "just" usability/ergonomics improvements. Doesn't mean they are unimportant. I completely agree with you on the points about how important these kinds of improvements are. Honestly, Rust's focus on ergonomics is one of the things I love most about the language. Things like the `?` operator make Rust one of the most pleasant languages to code in IMO. NLLs will make it even better and will also reduce the learning curve, as you said. Big kudos to the people working on this kind of stuff. We are all excited about different things, out of everything that is coming to Rust in the near future. If anything, it shows how amazing the Rust project is. I am really looking forward to 2018 when most of this stuff lands.
Like what?
Isn't that kinda what Rocket does? Or, if not, could be relatively easily converted into doing? If I'm misunderstanding your meaning, do of course correct me.
Not much else is that extreme as rust to the extent of restricting shared mutability.
&gt; except where you avoid the basic language features (isn't that tantamount to saying you're using a broken language ?) No, it's saying you're using a language that has evolved. Java still has raw types. They compile, because of backwards compatibility, but they get you a warning, and in practice, people don't use them any more. Sure, that's not as good as starting from scratch without the initial mistake. But it does mean that you don't have to start from scratch.
Interesting! What can be done to speed up builds for rustc itself? I've done some work on the AVR backend in LLVM, and recompiling rustc afterwards is a *pain*. A full LLVM compile takes me about 30 minutes; incremental less than one. A rustc compile takes about 4.5 hours, and seems to only ever use one CPU core.
Super lazy lately. This week I'm jumping straight [back into my Gameboy emulator](https://github.com/simon-whitehead/chemboy). I had a really interesting/dumb bug that I ironed out which revolved around me casting a sprite position (signed) as a `usize`, which caused some calculations to fail and graphical bugs to arise. Was fun to track down. Anyway this week I'm finalizing the GPU cleanup before bug hunting a bug I have when playing Kirby. When I get to a specific part of the first level, my emulator crashes with an out-of-bounds error when accessing a vector.
Which raw types in Java are you talking about? Like `List` instead of `List&lt;T&gt;`?
I dare somebody ~~to make something more useful than this.~~ to help me improve it. extern crate json; extern crate curl; use std::io::{stdout, Write}; use curl::easy::Easy; use std::str; fn main() { let mut easy = Easy::new(); easy.url("http://api.icndb.com/jokes/random").unwrap(); easy.write_function(|data| { let jsonres = str::from_utf8(&amp;data).unwrap(); let jsonres = json::parse(jsonres).unwrap(); println!("{}",jsonres["value"]["joke"]); Ok(stdout().write(data).unwrap()) }).unwrap(); easy.perform().unwrap() }
&gt; No header files I *think*, that's at least what I draw from discussions with /u/ubsan on that topic, that modern C++ doesn't use a distinction between header and non header files any more, but puts everything into the "header" file, and its only due to history that some projects still have the distinction. This solution is not really nice (same issue as in Rust: large compile times) and its basically forced upon you if you want to do advanced template metaprogramming. That's not special to C++, it also happens in Rust if you use templates, its just done automatically for you: generic code is not compiled for you, instead its stored as MIR and gets fed to LLVM inside the final leaf compilation unit, because only there you know about the actual type. The upcoming modules proposal for C++ will probably have to do similar things if generics are involved as well. &gt; Don't coerce between types Sadly some people inside the Rust community have never seen C++, never seen how horrible doing too much coercion is, and they actually want more coercions inside the Rust language. There has been a proposal for doing automatic coercions when comparing numbers of different types, and it only got postponed due to deadlines, so its not off the table yet. Sadly that proposal is only seen as start by those people. 
I've never done much work on the compiler, but the few times that I have, this is hardly the experience that I've had. I don't remember the compile times off the top of my head, but iterative development cycle was less than 5 minutes, I'm quite certain. One thing that might help is to only build the stage 1 compiler while you're developing iteratively with the `./x.py test --stage 1` command.
Yes, exactly. Java only got generics in 2004, eight years after it was released, and six years after it got a standard collections library. Before generics, if you wanted a list of strings, you would write: List strings = new ArrayList(); // please only put strings in this And hope for the best^1. With generics, you would write ^2: List&lt;String&gt; strings = new ArrayList&lt;&gt;(); And then trust the compiler to enforce that. Note that that is exactly the same List and ArrayList - generics were retrofitted to existing classes. However, because of backward compatibility, it had to be legal to use a generic class without type parameters - this is known as a "raw type". The compiler will warn about it, but allow it. The retrofitting of generics was done imperfectly, IMHO. Backwards compatibility meant that raw types had to be allowed, but they could have been restricted to existing pre-generics features, with an "allow raw types" marker used in just those cases, rather than cursing the language with them forever. We also got headaches like the fact that the method used to check if an object in a set [is not generic, and will accept any type](https://docs.oracle.com/javase/8/docs/api/java/util/Set.html#contains-java.lang.Object-), so you can still write: Set&lt;Integer&gt; numbers = new HashSet&lt;&gt;(); numbers.add(10); boolean lol = numbers.contains("10"); // no warning! ^1 You could also use a [checked list](https://docs.oracle.com/javase/8/docs/api/java/util/Collections.html#checkedList-java.util.List-java.lang.Class-) to enforce type checking at runtime ^2 Actually, this code uses the diamond operator, which was only added in 2011, seven years after generics!
Sorry for straying offtopic, but can someone give me an explanation on why we actually need reference counting? Fulll disclosure, just a noob, both in rust and programming in general.
Best answer yet. I've tried it. You are so right.
https://www.reddit.com/r/cpp/comments/6ngkgc/2017_toronto_iso_c_committee_discussion_thread/
It depends on if the union field is Copy or not.
I've tried it. If the union field is Copy, you are right.
&gt; A centralized package ecosystem: This is exactly why Gabriel Dos Reis worked on modules in the first place - as a base for a package and build manager. How modules would help? The difficulty with creating source-based package manager is that there are just too many build systems. The difficulty with creating binary package manager is that there are too many compiler/platform/build-options combinations to provide all possible binaries. Modules solve neither of those issues.
This is great!
Sometimes, there's no way to know how many owners a bit of data will have, or when exactly in time the existing owners will go away. In those two cases, you need reference counting.
moves are always a memcpy, there's no way to override that, yes.
Nice! That makes a huge difference.
Things like graphs.
Being consistently first place in speed.
Aren't Haskell and OCaml used in HFT as well? I've even heard of Java and C# being used in that space...
"So that narrows it down to:" ... rocket is missing in the list.
Diesel seems to work very well with Rocket, due to its inherently strongly-typed nature (and both use code-gen at compile time to this effect). I haven't used Rouille. What might it offer me over Rocket in this respect, out of interest?
Report an issue [here](https://github.com/rust-lang/rust/issues).
It looks to have been going on with updates to 3 days ago. Turned out to be [a bug in crates.io](https://github.com/rust-lang/crates.io/pull/1107) which was fixed and merged so just a matter of time now.
Why does Rust uses unsigned integers for collection sizes and indices? In C++ community it is widely acknowledged that using unsigned integers for these purposes in the standard library was a mistake. Why did Rust designers chose to do the same?
Thanks. Will do. Agreed it seems like undefined behaviour. I guess the question is then: should it be defined? If it allows reading uninitialised memory (which I think it does) then it seems like it needs fixing... 
This is previously reported at https://github.com/rust-lang/rust/issues/10184 and https://github.com/rust-lang/rust/issues/41799.
I didn't know that because honestly I didn't check! 
Thanks. I tried searching for them before posting but didn't spot them.
After a week of playing with Vim settings I'm back to [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis). I'm reviewing past couple commits and implementing conversion factors that will work with different types (integer, float, ratio).
I've raised an issue for the ICE here: https://github.com/rust-lang/rust/issues/45134. It looks like the other behaviour has tracking issues for them already.
This problem requires platform-specific solutions to perform well. Wakeup latency is easily several milliseconds, and at 60fps you only have a 16ms frame time. On BSD and Os X, all your timer functions are nicely optimized for short sleeps, but other OSes aren't that way. On Linux, waking up to handle input is faster than waking from a timer or timeout, *microseconds not milliseconds*. Your best bet is `timerfd`. In general anything that works with `epoll` will be faster than things that don't. I haven't researched Windows enough to find the fast timers. My guess would be DirectX, but even then, the minimum time you can safely sleep is probably a few milliseconds. All things to think about before 1.0.
`const fn` and custom allocators were the two I remember as most important.
I've been trying to fix [issue 18510](https://github.com/rust-lang/rust/issues/18510) which is an unsoundness bug in the compiler that's almost three years old. Basically, if you have a Rust callback from C and that Rust callback panics (and you don't catch it), then it's undefined behavior. It's my first dive deep into the compiler, which means `bcx` `ccx` `hcx` `tcx` and all kinds of `id` on top of that. That's how it looks like to a compiler newbie anyhow :-) But after some experimentation I think I have a draft implementation/fix of the issue, and I'm waiting for feedback / mentoring. 
I actually tried using Diesel for the original program that inspired me to do this... And after two hours of trying to shove a struct into a sqlite db I got fed up with it I bailed and did it the way you see, since really I just wanted to play with public key crypto a little. Between Diesel being a WIP and thus unpolished, me not being super experienced with ORMs, and Diesel seeming to offer a lot more features for "this is a big persistent database I am going to engineer" than "I want to shove a struct in a file", it was just way too much work, even with one of the devs very kindly helping me out on gitter. Making Diesel work is going to be a whole nother article, I suspect. 
I appreciate this article for introducing me to the Canadian term "gong show".
I appreciate this article for introducing me to the Canadian term "gong show".
Anything else than Result is considered unidiomatic and shouldn't be used. Panics are for programmer errors only.
Are there any good benchmarks for how much slower runtimes ThinLTO generates compared to compiling the program as one unit?
See also: https://github.com/BookOwl/fps_clock
Which could also be an argument for having a compendium to The Book. Much like The Nomicon.
Great to hear that!
I attempted to build a project of mine that, somewhere down the tree, depends on `syn`: cargo 0.23.0-nightly (e447ac7e9 2017-09-27) rustc 1.20.0 (f3d6973f4 2017-08-27) nightly-x86_64-unknown-linux-gnu Compiling syn v0.10.8 Compiling shared_library v0.1.7 Compiling memchr v0.1.11 Compiling rand v0.3.17 Compiling phf_shared v0.7.2 Compiling xml-rs v0.6.1 rustc: /checkout/src/llvm/lib/Transforms/Scalar/GVN.cpp:1933: bool llvm::GVN::replaceOperandsWithConsts(llvm::Instruction*) const: Assertion `!isa&lt;Constant&gt;(Operand) &amp;&amp; "Replacing constants with constants is invalid"' failed. error: Could not compile `syn`. warning: build failed, waiting for other jobs to finish... error: build failed
Yes, I've heard of all of these but C# being used, and I'm sure that I just haven't ran into a C# person yet.
Thanks; I originally wasn't intending to do rocket but by the time I got to the end it seemed like a waste to leave it out.
Check, I was a bit confused initially because the conversation was about C++. Java has some more warts, e.g. you can assign any array to an `Object[]`, but you will get a runtime exception when you store the "wrong" type (compared to the original array type)... also the typesystem [is unsound](http://io.livecode.ch/learn/namin/unsound) :)
Its quite possible rocket does this already. I haven't looked too extensively as it doesn't work on stable.
Same for me, I reported it there: https://github.com/rust-lang/rust/issues/45131
&gt; &gt; &gt; The rest is IIRC an undefined behaviour since the destination integer type cannot hold the rounded value. Safe Rust shouldn't trigger UB even in this case. Because of that, this is a compiler bug too.
I published a crate [`ref-cast`](https://github.com/dtolnay/ref-cast) to make this safe.
People's definitions of "High frequency" can vary by several orders of magnitude. Some HFT shops have a *max* tick-to-trade latency of about 15 microseconds. To consistently achieve these latencies, you are bypassing the linux kernel scheduler, which means you're directly polling nics. Also, the FIX protocol is likely out of the question in this case, instead, most exchanges have their own protocol, that is based on something that can be de/serialized mostly with cast-fill structs. Others talk about high frequency at the 10 ms tick-to-trade level, at which point you can use your linux kernel's scheduler and TCP stack. For, that, you're free to use any language which can create static buffers. You're probably also issuing trades and receiving market data in FIX. For the latter case, vanilla Rust would be a prime candidate. It is missing some protocol, fix support was mentioned, but Nom would be an amazing choice for building a safe, correct, and fast Fix parser. Serde would provide great benefit and convenience as well, for turning data structures into market orders. For the very low latency HFT, You'd likely have a bunch of unsafe code, since you'd be dealing directly in buffers of different types and copying between them. Buffers for DMA on the nic, memory pools for your custom data structures, trees and lists built by direct pointer manipulation. One could, theoretically build compile-time abstractions around these to help guarantee safety. Rust would still be a good choice for this, IMO, but it would take a lot of custom development, and you'd lose the benefit of the borrow checker in most cases. 
Oh and I forgot: thank you very much for this.
I think you should use -C target-cpu=native
While I agree about that Tokio is not usable yet, I am not sure if that really can be used as an argument against Rust. I do not think there is any popular equivalent to Tokio in C++, the C++ aync libraries are more like mio. And I am not sure making the mio event loop multi threaded is such a good idea for this kind of applications since that adds synchronization overhead.
Are you already using [https://github.com/mozilla/sccache](`sccache`)? I haven't tried sccache yet -- `rustc` compiles for me are ~2-3 hours (but that's rustc stage1 only, no libstd). IIRC if you use `RUSTFLAGS='-C codegen-units=N'` you can increase the parallelism. Also IIRC that has some impact on the binary size (results in larger binaries).
TL;DR It's for cases when the borrow-checker is not smart enough and/or the concept of single owner doesn't really apply.
Rustc won't use that instruction anyway, but to even be able you need the right target feature or target CPU flag.
Intoiterator is always more general as an argument bound than Iterator, accepts the same types as before plus a few more.
Rust has overflow checking (in debug builds), and no implicit type conversions between primitive types. I think that takes care of most of the downsides of unsigned sizes and indices, so the upsides remain: you don't have to deal with negative values.
Great reply, fantastic summary! It seems Rust is getting there.
Working on [tarpaulin](https://github.com/xd009642/tarpaulin/) some more. Last week I closed 9 issues and got another release out, I also opened 2 issues. This week I'll be closing some more issues and starting on my design of branch and condition coverage. Trying to make something nice and flexible so I can progress up to MCDC which to my knowledge no open source tool offers!
It's probably worth your time to watch [Niko Matsakis's talk](https://www.youtube.com/watch?v=lO1z-7cuRYI) from C++Now this year. It had a very similar goal of introducing Rust to C++ folks. Also, it's definitely not trash-talking C++.
Depending on the Chromebook, you can flash the firmware with a simple script (check /r/chrultrabook) and you'll be able to install Windows or Linux (UEFI only btw).
I can't speak to the original motivations, but my understanding is they're largely considered a mistake in C++ due to the implicit conversion rules and invoking undefined behavior. Rust has neither of those in this situation. Safe Rust also has other protections for the class of bugs caused by the C++ rules. Specifically, Rust will panic on overflow in debug builds (as well as issue warnings if it can prove an overflow will happen), and it has bounds checking on indexing by default (including in release mode, unless it can prove the access is safe). The only argument I've heard that could make the design a mistake in Rust as well is that `unsigned` is about _representation_ and not _semantics_. That is, "unsigned" does not necessarily _mean_ "non-negative", only that negative cannot be represented. Like `2usize - 3usize` is semantically `-1`, but that cannot be represented by the `usize` type that is the result of the subtraction. I'm not sure yet how much I agree with this line of thinking when applied to Rust as it seems to be closely tied to the implicit conversion and UB rules which, again, are absent. Maybe there's value in a `NonNegative` type which can yield a `PossiblyNegative` type for mathematical operations. I'm not sure what that would look like in practice. It could be great. It could be worse than just using unsigned and signed types with explicit casts. So was unsigned for sizes and indices a mistake for Rust? Maybe. I don't think so right now. Time will tell. At the very least the situation in Rust is much better than in C++. Yes, you can still write the same kinds of bugs that happen in C++, but at least you usually have to be more explicit about it with `as` and `unsafe`.
The binding being mutable is necessary, but sufficient for creating a mutable borrow. For example, if this was allowed: let mut v = vec![]; let a_borrow = &amp;v; mutate_the_thing(&amp;v); // your function it would be unsafe because two borrows exist, at least one of which is used mutably. The creation of the *borrows* triggers the borrow checker rules, not the *bindings*. So Rust would have to figure out that by `&amp;v` you actually meant `&amp;mut v`, at which point it can just require you to write it down in the first place. The reverse direction is safe; having an exclusive thing and using it in a non-exclusive way is fine.
If you could get that working with `cassowary-rs` that would be a really awesome UI framework.
I‚Äôm planning on looking into it. I‚Äôm unfamiliar cassowary layout but it would certainly let me rip out a fair amount of code and focus on other things. 
`&amp;mut T` is a different type from `&amp;T`. There is implicit casing from `&amp;mut T` to `&amp;T` but not the other way around. Also, consider this: let immutable_borrow = &amp;some_variable; mutable_borrowing_function(&amp;mut some_variable); This code is illegal, because mutable borrows must be exclusive. How about this: let immutable_borrow = &amp;some_variable; mutable_borrowing_function(&amp;some_variable); Should this be allowed? Mutablity is one of the things that should be explicit. Implicit mutability is easy - you don't even need a `mut`(or `const`) keyword - if you don't mutate it it's immutable. But that's not very helpful to you or to the compiler, so they made it explicit wherever it's used. 
I've seed FP used in other finance stuff like arbitrage where correctness matters more than speed (compared to HFT), never heard of it being used in low latency HFT though...If you've got examples I'm curious
The compiler certainly could do this; it does for the auto-borrow of method calls. But I like the `mut` at the call site because it reminds me that the callee may mutate the location I loan to it. The second example is allowed because it's safe. From the perspective of the type system `&amp;'a mut T` subtypes `&amp;'a T`. It's not good style though.
So, why doesn't rustc just do this?
/u/nasa42
This seems to be the fastest solution. I even have a version which uses ARM intrinsics. That should cover the popular platforms, I think.
You can also do [--keep-stage 0](https://forge.rust-lang.org/x-py.html#other-flags) to avoid recompiling that. Anytime LLVM gets updated is usually the longest as it takes forever to compile all of it. Usually between 1 and 2 hours.
I use Haskell for work where we do financial applications and backtesting. I wouldn't use it for HFT though personally. Unsafe code with Haskell using pointers and the like for low level manipulation is a huge pain to deal with -_- I'd rather use Rust or C/C++ instead which is made for that kind of stuff.
Yes, UB here is a known bug as mentioned in another comment: https://github.com/rust-lang/rust/issues/10184
How about `croreutils`? Is it for `cargo install`? If there were a `cargo install --all` you could submodule in all of the crates, and put them in a workspace. Or you could do it in build.rs, trivially by just having it run `cargo install` for each tool, or in a more DAG-respectful way by parsing your Cargo.toml to get the depended-on versions. That does provide the distinctly surprising behaviour of installing binaries on someone's system when they do a `cargo build` so maybe it is not such a good idea after all...
Fair point. Most FIX libraries provide memory pools and message/messagegroup pooling for high-throughput systems. I feel the author might have struggled to get the pools implemented in a satisfying way. Also, if rust ends-up being slower than C++ there is zero incentive to migrate. I had to fight hard to bring rust in our shop (stat-arb stuff, much slower than HFT).
Can I... buy you a soda or something? As I see it you ignored directions (using rustc directly and not Cargo) something unexpected happened and now you're looking for someone to blame? If you did the same thing with power tools, yikes! Yes, I agree that rustc could have a sanity check that the root source file has an extension. It *could* be filed as a bug, but I'd guess that implementing the feature yourself and submitting a PR would be better received. Overwriting the source is never right, but it's not going to happen to any project that knows what it's doing, so pretty low priority. You'll have poor luck with "well, `hello.gay` works" as an example. It has the subtext of "hey remember that homophobia is funny, lol" which isn't allowed on official (and many unofficial) Rust discussions.
For what it's worth, I made a [parser](https://crates.io/crates/itchy) for the ITCH protocol (similar to FIX) with `nom`. It was straightforward and quick to implement and benches around 7M messages/second on my crappy laptop. So yeah Rust is perfect for this kind of library.
so, I know someone did a FIX parser in nom recently, and that it's on github, but I can't find it again :( I've definitely seen some interest from finance oriented people, but it was mostly for custom protocols
Java might even be the preferred language for low-latency trading these days. Not for sub-microsecond response but it can do single-digit microsecond. Turn off the GC, use off-heap datastructures, reuse your objects and restart your program every night. The benefit over C++ is simply that it is much easier to make code changes and much harder to create terrifying errors in Java. Since Rust does all this and gives one much more control over data layout, I think it has huge potential in this space.
It'd be helpful to make sure we're talking about the same things: https://rust-lang-nursery.github.io/rust-cookbook/basics.html#ex-std-read-lines these two? Both seem to compile and run for me. What error do you get?
There are a couple of cases of UB in rust and anyone writing rust should read through the issues. There aren't a few, but some are not-too-hard to trip over. Most of them seem difficult to run into, some less so: https://github.com/rust-lang/rust/issues?utf8=‚úì&amp;q=is%3Aissue%20is%3Aopen%20label%3AI-unsound%20
Make sure to expand the code before copy/paste examples. I run to the [first recipe of the basic section](https://rust-lang-nursery.github.io/rust-cookbook/basics.html#read-lines-of-strings-from-a-file) and it [runs correctly](https://play.rust-lang.org/?gist=d95278cc7a87e26b1986e366ea98206e&amp;version=undefined). The Rust Cookbook is active, it is part of the [impl period](https://github.com/rust-lang-nursery/rust-cookbook/milestone/1) and hacktoberfest.
I can't get cargo fuzz working. I created a fresh library project, in it a new fuzz target, but if I try to run it I get: Running `rustc --crate-name arbitrary /home/domantas/.cargo/registry/src/github.com-1ecc6299db9ec823/arbitrary-0.1.0/src/lib.rs --crate-type lib --emit=dep-info,link -C debuginfo=2 -C metadata=910df6b303cd4b0d -C extra-filename=-910df6b303cd4b0d --out-dir /home/domantas/Projects/fuzz-test/fuzz/target/x86_64-unknown-linux-gnu/debug/deps --target x86_64-unknown-linux-gnu -L dependency=/home/domantas/Projects/fuzz-test/fuzz/target/x86_64-unknown-linux-gnu/debug/deps -L dependency=/home/domantas/Projects/fuzz-test/fuzz/target/debug/deps --cap-lints allow --cfg fuzzing -Cpasses=sancov -Cllvm-args=-sanitizer-coverage-level=3 -Zsanitizer=address -Cpanic=abort` rustc: /checkout/src/llvm/include/llvm/ADT/StringRef.h:236: char llvm::StringRef::operator[](size_t) const: Assertion `Index &lt; Length &amp;&amp; "Invalid index!"' failed. error: Could not compile `arbitrary`. Caused by: process didn't exit successfully: `rustc --crate-name arbitrary /home/domantas/.cargo/registry/src/github.com-1ecc6299db9ec823/arbitrary-0.1.0/src/lib.rs --crate-type lib --emit=dep-info,link -C debuginfo=2 -C metadata=910df6b303cd4b0d -C extra-filename=-910df6b303cd4b0d --out-dir /home/domantas/Projects/fuzz-test/fuzz/target/x86_64-unknown-linux-gnu/debug/deps --target x86_64-unknown-linux-gnu -L dependency=/home/domantas/Projects/fuzz-test/fuzz/target/x86_64-unknown-linux-gnu/debug/deps -L dependency=/home/domantas/Projects/fuzz-test/fuzz/target/debug/deps --cap-lints allow --cfg fuzzing -Cpasses=sancov -Cllvm-args=-sanitizer-coverage-level=3 -Zsanitizer=address -Cpanic=abort` (signal: 6, SIGABRT: process abort signal) error: could not build fuzz script: "cargo" "build" "--manifest-path" "/home/domantas/Projects/fuzz-test/fuzz/Cargo.toml" "--verbose" "--bin" "fuzz_target_1" "--target" "x86_64-unknown-linux-gnu"
I've recently been building an API using Iron, and I've found it to be pretty hit or miss in terms of "the usual" libraries. I tried (and failed) to implement an existing CORS-Iron package, which resulted in me hand-writing CORS functionality into the app. On the other hand, it's just my style of web framework. I'm not very interested in frameworks like Rocket that are Macro driven, I'd much rather have a very obvious request flow. This flow I've been able to build out in Iron with minimal headaches, so overall I'm quite happy.
And actual support for single process just for this purpose
In the future, anyone out there looking to track down reported instances of memory unsafety bugs in safe Rust code should start with the I-unsafe tag on the issue tracker: https://github.com/rust-lang/rust/labels/I-unsound . I personally seek to make sure that every open memory safety issue that I see has this tag, no matter how small. If you see a memory safety issue that doesn't have the tag (please make sure the issue has a working test case so I can confirm it), PM and I'll personally tag it and nominate it for discussion.
Been reviewing PRs during Hacktober fest for my library [github-rs](https://github.com/mgattozzi/github-rs) which is super easy to contribute to if you want to knock those out. More than happy to mentor. That and working on something not ready to announce yet but is related and hopefully can help advance the community libraries in this space as a whole further
There's some discussion of implied borrows here: https://blog.rust-lang.org/2017/03/02/lang-ergonomics.html
So I'm not the only one who's eyes glaze over when I read the word "middleware".
`let mut foo = "bar"` means: "This value is able to be mutated" `&amp;mut foo` means: "The value `foo` references can be mutated through this reference"
It's a pretty nice tool! Looking at the source code, I see that the formatting is rather inconsistent, I suggest you run `rustfmt` whenever you save the file (or before committing). You should take a look at `cargo-clippy`, it's a really sweet linter that highlights potential issues in your code (requires rust nightly to build - you can use rustup to switch between stable and nightly easily). Some suggestions: 1. Make functions as generic as possible, use slices instead of Vecs in function arguments. Using `&amp;Vec&lt;T&gt;` forces the caller to allocate a Vec from whatever container they have, which is wasteful. Use `&amp;[T]` instead, and use `vec.as_slice()` when calling the function (which is a free operation). As `String` is similar to `Vec&lt;char&gt;` (in contrast to `&amp;str`, which is like `&amp;[char]`). `&amp;String` is unnecessary. Use `&amp;str` instead for the same reason as using slices above, and call `string.as_str()` when you call `collect_files` You can see how that is wasteful in the recursive call to collect_files, you call `path.to_str()` (a free operation), only to construct a string with `String::from` later, which allocates and copies the string. 2. You pattern match on an Ok result and discard the result, there are many useful methods to check if a value is a particular variant of an enum, like `Option::is_some`, `Option::is_none`, `Result::is_ok`, `Result::is_err`. 3. You collect program arguments into a Vec, only to remove the first element and then iterate over the rest. You can use `for arg in env::args().skip(1)`. I suggest looking at iterator documentation, iterators are really powerful.
Does Rouille support async?
So is this really "undefined undefined" as in anything can happen or do you still always get a valid u8 but where it fetches it from is not defined? As in can it actually try to load it from memory not assigned to the program? The last time I read about this bug it was LLVM that simply used the contents of the last-used register or something like that if memory serves.
It says the Result expected two arguments but got only one. Not at my computer right now, I'll post the actual error later
&gt; They are both just ways to lie to the compiler. Cell is a way to pretend that a value is immutable and then treat it as mutable. It uses unsafe to mutate the insides. This is needed for cases when the compiler cannot prove that a value is mutated safely. I disagree with this, nothing is being lied to. The truth of the matter is that "mutable reference" is a horrible lie and misnomer. The correct term would be "exclusive reference" vs "shared reference". A reference just needs to be exclusive when it needs to be, as in if it permits things that would go horribly wrong if another reference existed. While some of this is mutation, not all mutation needs this and a lot of things that need it aren't mutation. `Cell` and `RefCell` are just data structures like any other that permit certain things and they are safe in that they only allow operations through a shared reference that are safe with a shared reference. In this case mutation; there is nothing inherently unsafe about mutation through a shared reference‚Äîonly specific types of mutation aren't. There are a great number of mutations within the standard library that happen through shared references.
&gt; They are both just ways to lie to the compiler. Cell is a way to pretend that a value is immutable and then treat it as mutable. It uses unsafe to mutate the insides. This is needed for cases when the compiler cannot prove that a value is mutated safely. I disagree with this, nothing is being lied to. The truth of the matter is that "mutable reference" is a horrible lie and misnomer. The correct term would be "exclusive reference" vs "shared reference". A reference just needs to be exclusive when it needs to be, as in if it permits things that would go horribly wrong if another reference existed. While some of this is mutation, not all mutation needs this and a lot of things that need it aren't mutation. `Cell` and `RefCell` are just data structures like any other that permit certain things and they are safe in that they only allow operations through a shared reference that are safe with a shared reference. In this case mutation; there is nothing inherently unsafe about mutation through a shared reference‚Äîonly specific types of mutation aren't. There are a great number of mutations within the standard library that happen through shared references.
This is all-the-way UB.
Put simply, it's because of explicitness. We spend more of our time reading code than writing code, so when you are scanning through your code, you'll immediately know where mutable and immutable borrows are happening, without having to hunt down function declarations.
Because nobody considered SIMD. I may try submitting a pull request later, but I don't really have time to properly benchmark improvements after trying once to optimize `saturating_mul` and making it slower actually.
Good discussion on why Cell requires Copy: https://stackoverflow.com/questions/39667868/why-can-cell-in-rust-only-be-used-for-copy-and-not-clone-types
Is the end goal something akin to GEGL?
You might be missing the `error-chain` setup? click the button with two arrows so it expands to the whole example
Yikes, those ones involving `match` are concerning. I could imagine myself tripping over them in "ordinary" code. You don't even need match guards for the data races. I really think all of these should be made more prominent in the community.
&gt; I could imagine myself tripping over them in "ordinary" code. Same, which is why I avoid them.
Just FYI, there is [another rust project named tectonic](https://github.com/tectonic-typesetting/tectonic), (a TeX/LaTeX engine, so the applications are pretty different).
ThinLTO should have reduce/remove the downsides to using a high codegen-units value.
It uses threads+sync I/O. Its performance is addressed by the author in &lt;https://github.com/tomaka/rouille#what-about-performances&gt;
You're not wrong but the practical distinction isn't huge. I suppose they're "server frameworks." ;-)
&gt; I really think all of these should be made more prominent in the community. Keep in mind that all ways to trigger UB in safe Rust are bugs that will be fixed, even going so far as to break backwards compatibility if absolutely necessary. AFAIR, something like a third of all open soundness bugs will be fixed in the act of porting the borrow checker to operate upon rustc's MIR (current borrow checker operates upon the AST); this includes all the bugs with match arms. I'm told that this should be done by later this year. The float-casting bug and the infinite loop bug are both due to LLVM behavior, and both have fixes in the works. I'm not going to complain if someone wants to structure their code to minimize the chances that they hit any soundness bugs; Rust is for people who don't want to take risks, after all. But I think it's a bridge too far to tell people to stop using certain constructs just because bugs exist, because all received wisdom of that nature ends up persisting forever and we'll end up with people still avoiding match guards in 2037. :P
LLVM is the only thing I change, though; I never touch any actual rustc code. I'm not sure exactly which parts of the Rust compiler are affected by that. Theoretically, it seems to me that only parts that affect code generation (for AVR) should need to change. Since I build LLVM as shared libraries (to speed up incremental builds there), is it perhaps even possible to avoid rebuilding rustc altogether? Either way, this isn't a huge issue for me, as I can generate a .ll file with rustc, work on solving the backend issue (which can take a lot longer than 1.5 hours!), and then validate the fix in rustc after the work is done. With ninja and building LLVM as shared libs, rebuilding changes in LLVM is very quick. (I just tried removing a debug print -- recompiling and relinking every affected exectuable took 12.9 seconds!)
[removed]
One of the central theses of Rust the language is that you can build safe abstractions from those low level resources you refer to with low-latency HFT, and then benefit from the borrow checker when using them in your "business logic." I'd be curious to know if anyone has successfully been able to do so.
I'm not able to access that page as I'm getting 500 internal sever errors. This could be useful in proving properties of some components of systems or applications written in assembler, such as proving that assembly language routines in a kernel which are used for hardware access can only affect what they are supposed to affect, or that assembly language routines used for constant-time operations in crypto libraries satisfy certain properties. Coupled with something that was able to map proofs on top of high-level Rust semantics to the properties that need to be true of the compiled code for those proofs to remain true, it is possible that this could be used to verify those same proofs against the x86 machine code. You might be interested in the [technique used to prove generated code correct in seL4](http://ts.data61.csiro.au/projects/TS/compiler-correctness.pml), which was to decompile the machine code into logic expressions, and compile the C code (which had already had formal proofs of correctness verified against C semantics) into logic expressions, and then prove that those two logic expressions were equivalent using an SMT solver. One thing you might notice is that this analysis omits verification of the hand-written assembly routines and volatile access to hardware, so it might be the case that it would be useful to do formal proofs against those pieces of code. On the other hand, this seL4 verification work was done against ARM, and using a different theorem prover Isabelle/HOL, and they did this work even though it was possible to compile seL4 with a verified compiler CompCert because it was too difficult to prove that the model of C they relied on written in Isabelle/HOL was equivalent to the Coq model that CompCert uses. So for this work to be extended, they would probably need an Isabelle/HOL implementation of x86 semantics, not a Coq implementation. So, the sort answer is yes, it is possible that this work could be used as part of some formal verification work of the compilation of Rust code. But there are a lot of other things that depends on, one of the biggest being an actual formal semantics for enough of Rust that you can start writing formal proofs of Rust code.
I have heard about Erlang used in HFT. 
I don't see the connection between the bugs eventually being fixed and the problems we have now. Developers should code to avoid unsafety, regardless of whether that unsafety is LLVM's fault, something fixed in the future, or otherwise. Avoiding that unsafety means avoiding patterns like match arms - this one seems incredibly easy to run into if a value gets moved into an arm.
Please add issue to github. Cookbook is being improved actively
&gt; I do not think there is any popular equivalent to Tokio in C++, the C++ aync libraries are more like mio. Huh? Boost Asio is an incredible mature library, and is pretty much the de facto C++ async I/O library. It's not the fastest thing in the world (the design has a few locks in it that could be called "unnecessary"), but it's certainly not slow. That being said, you're right that you wouldn't use Asio for HFT. But you wouldn't use async at all, because that's slower than pinning a thread to a core and spinning.
There's plenty of C#, just not in the high-frequency parts. HFT has quite a few pieces that don't need to be that fast.
I know of only one prominent HFT shop that uses Java for this. Almost everybody else uses C++. Java just isn't a good fit for it, and if you try to turn off the GC entirely, you can't use lots of libraries which is one of the main reasons of using Java in the first place. Java can be fast, but optimized C++ is usually faster.
I have not used Asio myself but I counted it as more low level, occupying the space between mio and Tokio. But now that I think of it its API is pretty far from how epoll and kqueue works so it makes sense to compare it to Tokio.
I would use one of `slice`'s `split` methods here: fn compute_recursively(v: &amp;mut [Foo]) { for i in 0..(v.len() - 2) { let (a, fs) = v[i..].split_first_mut().unwrap(); do_something(a, &amp;fs[0], &amp;fs[1]); } } fn do_something(a: &amp;mut Foo, b: &amp;Foo, c: &amp;Foo) { *a = Foo(b.0, c.0); } I really don't like `for i in 0..(v.len() - 2)`, but at a glance it appears that you can't get `&amp;mut`s with `slice::windows()`... :-S
I agree. This is the main reason I use MPI. I do most of my work using HPC clusters where the sys admins have carefully tuned the MPI installations to take the best advantage of the available hardware.
This is a fantastic start. Unfortunately I tend to make use of parallel IO and interprocess communicators which aren't yet implemented. However, if I do start using rust regularly, I would definitely try to help out getting these going...
Printing hello world does not encompass the entirety of Rust's behavior. 
When writing a procedural macro derive, is it possible to tell if the input struct takes a type as a parameter? Something to be used like [this](https://play.rust-lang.org/?gist=ca892138e82a4f434ec0fc6ff1fa4e54&amp;version=stable). I'm revamping a little library I wrote a while ago for using higher order types in rust, and would like to provide a macro that derives the `HigherOrder` trait for structs like `Vec` and `Box`.
There was some progress on Rust as a GDNative binding here: https://github.com/GodotNativeTools/godot-rust . Not sure if this helps, but I'd love writing Rust with Godot! 
Are we sure it's best to have these lines so hidden? It seems quite easy to miss that the page doesn't show full examples if someone links directly to the specific cookbook from IRC or another medium.
I think this is worth discussing over in the Cookbook repo, like u/jaroslaw-jaroslaw suggested. We could look at ways to either make it more obvious that what you're looking at is a snippet or give you a copy all button or something.
You should have fairly smooth sailing then. Every so often I've fielded questions from pure Haskell folks who are shaky with their `while` loops, but if you're okay with local mutability, Rust is too. 
In general it's to see if it's safe to do some operation on the data. For example automatically freeing a heap allocated object when no references to it remain.
A few weeks ago, I volunteered to take over maintenance of the [Criterion-rs](https://github.com/japaric/criterion.rs) project, so I've been writing some documentation as a way to learn more about the codebase (and because it kind of needed it) in between fighting with Travis-CI and Appveyor. For those who don't know about Criterion-rs, it's a statistical benchmarking tool which can automatically detect and estimate the magnitude of performance improvements/regressions. It's not currently on crates.io but I hope to start building releases and getting community feedback soon. I've written a [user guide](https://japaric.github.io/criterion.rs/book/criterion_rs.html) in the form of an MDBook. If anyone could take a look at it and let me know if there are any questions left unanswered or areas that should be expanded or typos or whatever, that'd be very helpful.
This is always a tension: what do you show, what do you hide. It's tough.
Cell no longer requires Copy except for the `get` method.
Yeah it doesn't help that I'm a total newbee to this type of language. I'm intermediate with python and beginner level at c#, never touched c or c++ before
That sort of info should be on the API docs for the type.
Yup! It‚Äôs a thing that‚Äôs at least shared among the book-style documents, and hopefully will end up in the API docs as well, where currently it‚Äôs even less discoverable. Sorry it tripped you up! It has for others in the past.
&gt; reflection (derive macros are still ugly, but better than nothing) I would never call derive macros "reflection". I guess it's like compile-time reflection, but it's nothing like what most people think of when they here "reflection" (specifically, being able to inspect types at runtime). In fairness, in C++ it's probably best known as RTTI, not "reflection".
&gt; every piece of data is uniquely-owned by default (no need for unique_ptr to make it so) Well that's just wrong, though. Value types are also uniquely-owned in C++. The equivalent for `unique_ptr` in C++ is `Box` in Rust. Rust does have the borrow checker, which C++ doesn't, but that just lets you know if there's a violation - the data is still uniquely owned in both cases.
[I made a for of rust-openssl that statically links it](https://github.com/valarauca/rust-openssl) You might have fiddle with your other deps to accept this, the `build.rs` is a little hacky but it works just fine. You'll need to manually load root certificates tho.
Chapel is a specialized HPC language for writing compute-intensive code for massively parallel machines. Not comparable to rust in any significant way. As an aside, I have yet to see any real-world uptake of Chapel in the HPC world. I'm not saying it won't happen, but I suspect the very tight focus means it will be difficult to build a significant user base or a user community outside Cray itself. 
It doesn't work for the largest integer size, although it could be special cased for that.
Most languages you've probably used before have garbage collection, when you're done using a variable, the language runtime releases that memory. In a language like Rust that's meant to be used for systems programming, having a runtime like that is unnacceptable. In this case you need a way to release memory. Rust chose to give the progammer Rc and friends, which count the number of borrows a certain type has, and when the count reaches 0, releases the memory. It should be noted that you often don't need Rc, if the compiler can figure out when to release some memory statically (and you're borrowing with &amp;), you don't need the additional cost that comes with counting references.
Well, yea, the end goals aren't comparable but I noticed they were doing a few things that remind me of rust (more than C anyway) like type inference, generics, iterators, though they're implementing synchronization as primitives and not using quite the same ideas about thread safety. Not sure it will get beyond cray either but that might depend on whether they get some sort of GPGPU support that makes that easier. Seems that some people at ATI are/were looking into it.
I guess it depends on who this is catered to. I'm super new and I've never touched c or c++ before. . I didn't realize the two arrows showed hidden code at all until people told me that here. After that it took me a while to figure out to import the error crate with a "-" rather than a "_" in the toml file, I'm still not sure why it's different in the cargo.toml file and the main.rs file . So if this is catering to total newbies I think it would be a good idea to not only list the full code, but also a shot at the cargo.toml . Even for intermediate users I wonder if the hidden code is really that obvious to them. 
See [here](https://www.reddit.com/r/rust/comments/6ln5du/mut_mut_why_is_this_required/djvfjc0/) for a rather long explanation that is a bit aggressive and ranty.
It may be more interesting to compare it specifically to Rust with rayon.
I don't like this either, but I think I managed to build a standalone binary with musl. Either way, this made me appreciate go much more 
I'm not talking about standalone binaries.
Continuing work on my little graphing crate for Rust. Unfortunately being bogged down by university, but hopefully at the end of this week I'll be able to solidify the API and begin to actually implement the rendering of graphs and charts! https://github.com/saresend/Grust
Compile-time reflection is what C++ may be getting in the near future, they call it "static reflection". So it would sound right to someone who follows proposals to the C++ standard. 
(With these types of questions it's always difficult not to seem overly dismissive, so my apologies if it does.) Well, sure, it's a modern language, so it has modern features. But the things you list are features you'd expect in every statically typed language from an era when compiler writers can assume their compiler will run on a machine with more resources than a PDP-12. Heck, even C++ and C#/Java backported a kind of type inference (though not a Hindley-Milner kind, since those languages' type system doesn't work like Hindley-Milner). [Almost](https://golang.org/) all modern statically typed languages have generics, the only real design decision tends to be whether to monomorphise at compile time (e.g., C++ and Rust), at classload time (C#.net), or never (Java). Finally, iterators are a good idea for abstraction in general, and become even better when your language semantics allows the compiler to do smart stuff with it. Rust of course has it to offer laziness in an otherwise eager language, and to help writing code in which lifetimes and scopes are cleanly linked. Chapel does it because (iterating over) large homogeneous data structures is the core of their language. The similarity between both languages kind of ends there, though. You can find artificial similarities (overlap in some keywords, like `use` or... `if` and `for`, I guess), but the main ideas of both languages differ widely. Rust wouldn't be (post-1.0) Rust without an ownership model, Chapel doesn't have it (it requires manual deletes, and though this might be fixed by now, at some point the backing store of strings was leaked by Chapel). Rust prefers explicitness, Chapel does implicit type coercion. Chapel's iterators are what Rust calls "coroutines". Chapel is a [PGAS](https://en.wikipedia.org/wiki/Partitioned_global_address_space) language, Rust doesn't even try to offer this feature. **So in conclusion**: I don't think they have similar **goals**, Rust's ability to go low-level aims at either offering low-footprint/low-overhead (such as for embedded systems or device drivers), or offering high-control/low-overhead, such as for runtimes (think browsers, RDBMS'es, game engines). In contrast, Chapel's ability to go low-level comes from the need to be low-overhead/number-crunching (think slapping billion-element arrays together til the ~statisticians~~ data scientists are happy). The **major differences** are that Rust revolves around an ownership model (plus pragmatic choices for performance and ergonomics that lift it beyond a mere research language), while Chapel revolves around being a PGAS language (plus pragmatic choices for performance and ergonomics that lift it beyond a mere research language).
**Partitioned global address space** In computer science, a partitioned global address space (PGAS) is a parallel programming model. It assumes a global memory address space that is logically partitioned and a portion of it is local to each process, thread, or processing element. The novelty of PGAS is that the portions of the shared memory space may have an affinity for a particular process, thereby exploiting locality of reference. The PGAS model is the basis of Unified Parallel C, Coarray Fortran, Split-C, Fortress, Chapel, X10, UPC++, Global Arrays, DASH and SHMEM. In standard Fortran, this model is now an integrated part of the language (as of Fortran 2008). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
In general terms, I think that using a reference to a long-lived value that might have been created via lazy_static! seems like a perfectly sensible approach. That might not necessarily need to be an argument to your structs' methods. It's possible to hold references in a struct and access them as a field. This can make life a little bit tricker with lifetimes.
In terms of the MySQL crate (which I am not familiar with), it might be useful to see how its reverse dependencies have handled working with it: https://crates.io/crates/mysql/reverse_dependencies
Part 1 of [Rust in Action](https://www.manning.com/mcnamara) is done, now on to the really fun parts of the book. Looking forward to seeing if I can explain bits, bytes, pointers and CPU instructions in a novel and compelling way.
That's neat. What features are you using for clustering? Straight tokens or incorporating some dimensionality reduction such as topic modelling?
Very! interested in this. How does the actor system work behind the scenes? What is the API surface going to look like? I'm very keen to get an actor system into a project I'm working on using futures, and might be building something very similar to this library.
Diesel is actually getting close to 1.0 according to a [recent tweet](https://twitter.com/sgrif/status/917190508610383872). I recently implemented a pretty standard server in Rocket + Diesel and found the overall experience of macro-driven strongly-typed development to be fun and accurate, and the performance is "production grade" with zero extra effort. I'm not sure I would be happy going back to Flask + Alembic at this point. Main problem with Diesel is that the API docs are very hard to read due to the liberal use of generics, and the user docs sorely need an update with less explanation and wayyyy more examples. I imagine this is on the 1.0 roadmap. But, once I understood how it works, the strong typing of Diesel was really helpful.
&gt; Java can be fast, but optimized C++ is usually faster. No doubt, and a lot of this revolves around one's definition of 'H'. I once worked for a market maker similar the one in the video; they rewrote their whole stack from C++ to Java and it actually got much faster due to much improved cache-friendly architecture. We referred to our particular dialect of Java as 'Java--' because as you note, it did not look like normal Java code and did not use much of the standard ecosystem - but there is a whole separate ecosystem built for just this purpose, see e.g. [OpenHFT](https://github.com/OpenHFT). Still, trading firms value rapid development just as much as any startup (there was a constant fear that our competitors were smarter, better equipped and mere days away from "eating our lunch") and the real value of Java over C++ was nary a pointer in sight.
You probably want to use [`Stdout::lock`](https://doc.rust-lang.org/std/io/struct.Stdout.html#method.lock) here to speed things up.
Other than Assembly, C is probably the only low-level language still popular today. This makes the division of languages to "C" and "not C" actually meaningful. Both Chapel and Rust are in the "not C" category, which means that compared to C Rust has many similarities with Chapel - but so does any other modern language. If you don't compare to C and look only at the "not C" category - which is a huge category containing a large variety of languages - Rust and Chapel are very different.
When I looked at those it seemed abandoned and pretty bare bones. Given the API had changed a lot since then I decided to just start from scratch. Thanks though
*ring* doesn't use inline assembly for constant-time operations. It uses out-of-line assembly and/or C code. If you use a special C compiler you can get additional guarantees of constant-timedness from that C code. In theory such a special Rust compiler could be written too, but it's harder because Rust is a more complicated language, and also Rust doesn't have a language spec.
It was probably called CRYPTO_memcmp in BoringSSL. In order for it to be guaranteed constant-time, you need to use a special C compiler. Otherwise it is just "almost definitely constant time." If we were to redo it in Rust, it would be less likely to be constant time for some reason.
[removed]
There are a few ways to do this depending on what you want. If you want a single, global-within-process pool with synchronisation (this is closest to Java, but Java does it implicitly) then use `lazy_static!` with a `Mutex` or `RwLock`. If you want one pool per thread, allowing you to remove any kind of synchronisation, you can use `thread_local!`. The overhead of synchronisation is low compared to the overhead of making a SQL connection though, so this probably isn't necessary. If you want more control over which function gets which pool, you can pass the pool around. You'd probably want to have your functions be methods on some kind of "config" object if you find yourself passing data in function parameters often to improve ergonomics. In general, idiomatic Rust isn't about having some perfect way to do something, it's about acknowledging the tradeoffs inherent in each method and making the choice based on what would be most performant and ergnomic in your application.
&gt; Of course, an even better approach would be to have a 95% Rust, 5% C/assembly implementation of TLS that required no external dependencies and which supported cross-compilation using a custom build.rs and the gcc crate. That's almost exactly what Rustls (100% safe Rust) + webpki (100% safe Rust) + *ring* (mostly assembly language, a bit of C code and even more Rust code) is.
&gt; Running that gave me 51.3MiB/s. Faster than the version, which comes with my system, but still way slower than the results from this Reddit post that I found, where the author talks about 10.2GiB/s. Did you just compare benchmarks from different machines? The **slowest** version they ran(illumos) gave them 100MiB/s. Since the fastest version they ran was a hundred times faster, I'm going to assume the illumos version is unoptimized. And yet - it was still twice as fast as your optimized Rust version. I'm going to assume they just had better hardware...
That's the thing: I have pretty decent mid-2014 Macbook Pro. Something's really odd with my setup. Maybe one of you could run my program and see how fast it is. I would happily update the article with new benchmark data.
I think you missed [this post](https://www.reddit.com/r/rust/comments/4wde08/optimising_yes_any_further_ideas/)
The main thing I started looking at was what newer languages were trying to do to make concurrency safer and easier. My understanding is that rust is primarily about safety, and safer/easier concurrency primarily comes from that. Some things like go seem to put a higher priority on more massive concurrency with things like goroutines. I think what made me associate chapel with rust is that I was conflating parallelism with concurrency, but especially when parallelism means parallelizing operations on massive matrices and such it becomes pretty obvious they're not the same thing.
Oh wow. Thanks! I'll add it as a reference to my article.
Update: Tried the version of /u/nwydo and it's faaaast. ``` cargo run --release | pv &gt; /dev/null Finished release [optimized] target(s) in 0.0 secs Running `target/release/yes` [3.07GiB/s] ``` Will update the blog post accordingly.
Just tried it and it's fast as hell (see thread above). Thanks!
Oh, great remark, I totally missed that nuance in my comment. To link it back to what I said earlier: "parallelizing operations on massive matrices" is what I implied with "number-crunching", whereas "safer/easier concurrency" is ‚Äì in a way ‚Äì where I was getting at with "runtimes". With Chapel-like loads, you'd typically have a relatively small amount of code, that is applied to a gargantuan amount of data. What counts there, is getting the utmost performance out of your code; your programmer can be expected to keep the control flow in his/her head, since the application level control flow isn't *that* complicated (by making the accidental complexity of parallelism/data-accesses/checkpointing/HPC-stuff the language's responsibility). So the language focuses its attention on helping the programmer access the right data at the right spot, and tries to prevent the programmer from writing code that accesses data that isn't at the right place, or in the right shape. For Rust, such loads would be exceptional, and hence the language doesn't really help you there. Partitioned address spaces are not a language feature in Rust, and have to be handled with application logic (or, of course, specialized libraries). With "runtimes", on the other hand, the amount of code becomes larger, and the control flow is largely dictated by the user-provided data (for some obvious examples: html rendering, css, etc. In general: when your data is a graph instead of a linear data structure). The programmer cannot be expected to keep the control flow in his/her head, and the data cannot even be trusted. Of utmost importance, then, becomes safety: prevent corrupt data from breaking the system, prevent failing threads from taking down the entire system (compare with Chapel: if one thread crashes, the other threads have probably computed something wrong too, since they were running the exact same program), prevent the programmer from expressing control flow that doesn't make sense, etc. Chapel doesn't need fancy constructs to prevent the programmer from querying a database connection that isn't opened yet; in a typical Chapel program, there won't even be a 'proper' database, and if there is, it will be used in a rather straightforward way; if any run of the program accesses the database wrongly, all runs do; so it becomes a trivial debugging job.
[r2d2](https://github.com/sfackler/r2d2) is a crate that implements a generic connection pool. Looking over it‚Äôs examples and implementation might help understand how this particular pattern translates to Rust.
The thing with benchmarks is to see how one piece of code performs against another, all of the code being compared needs to run on the same machine. Running code on an Intel 2nd gen Core i5 is going to be different from running it on a 5th or 6th gen. Probably not much different, but enough to skew results.
Did you compile with `--release`?
I've been trying to figure out how to make [Sucredb](https://github.com/arthurprs/sucredb) more useful in practice. I think adding support for other redis data structures as CRDTs is the best option. Hopefully I'll make some progress later this week.
Yep. It's mentioned in the post. cargo run --release | pv -r &gt; /dev/null Compiling yes v0.1.0 Finished release [optimized] target(s) in 1.0 secs Running `target/release/yes` [2.35MiB/s] 
I think that you're looking for the "if let" syntax: if let Some(n) = opened.iter().position(|&amp;x| x == current) { opened.remove(n); }
mm I'm a rust noob myself, but I think you could avoid the map as such: opened.iter().position(|&amp;x| x == current).map(|n| {opened.remove(n);}); You don't need to specify that you want to return `()` there, because that's what the semicolon does. 
&gt; What I want to do is do something if I find something and just do nothing if it's None, I don't care about the result of `opened.remove(n)`. Is there a better way to do this? Yes, there is: [if let](https://doc.rust-lang.org/book/second-edition/ch06-03-if-let.html). if let Some(n) = opened.iter().position(|&amp;x| x == current) { opened.remove(n); } Also, in general `{ do_something(); () }` is the same as `{ do_something(); }`. A block that ends in a statement (instead of an expression) returns `()`. &gt; First off, is it a good practice to use reference as key on a HashMap I'd say *usually* not. Can you give a few more details about your use case? &gt; is it better to do `tgscore &gt;= *gscore.get(n).unwrap()` or `&amp;tgscore &gt;= gscore.get(n).unwrap()` ? At least for `==`, Clippy suggests the former (see https://github.com/rust-lang-nursery/rust-clippy/wiki#op_ref). So I'd say that. &gt; Finally, is there any context in which shadowing a variable is better than making it mutable ? That depends on your idea of "better". If it's a long function where you want to shadow / modify the variable only once at the beginning, my recommendation would be to use shadowing as it stops you from accidentally mutating it later on. But I don't often see code where both are possible anyway. Usually shadowing is used to create a variable with the same name but a different type, which makes mutability a non-option; or the variable is modified in a loop and used afterwards, making shadowing a non-option.
You're looking for /r/playrust
&gt; First off, is it a good practice to use reference as key on a HashMap or should you always use the value ? Due to my background I tend to always use value which lead me to do a lot of .clone() and I am not sure it's the better way to do it. It's better to use references - but that doesn't mean you always **can** use references, because they may not live long enough(or the compiler won't be able to prove they live long enough). I'd just use references and let the compiler scream at me if they don't live long enough. Also keep in mind that you can use values with `insert()` and references with the index operator.
TWiR is now official! All future newsletters will now be sent from twir@rust-lang.org! Aw yeah! üéâ
Personally I wouldn't worry about that proto abstraction stuff. Just set up a core, call some functions which return futures, and pass them to the handle. The example on [the tokio homepage](https://tokio.rs/) is pretty good.
Tokio-core is the way to go. It will probably be easier to use. Just run a future like the following in an event loop: ``` rust loop.run(tokio_io::io::read_until('\n', Vec::new()).and_then(|mut line| { let calculation = calculate(&amp;line); line.clear(); let buffer = serialize(&amp;calculation, &amp;mut line); tokio_io::io::write_all(line).map(|_| ()) }).map_err(|e| println!("{:?}", e))) ``` 
You keep acting like it's a given that you can develop more rapidly in this special Java (where you probably can't leverage Java's impressive library ecosystem) than in C++. It's possible your company could have just rewritten their systems still in C++ and gotten better performance. Pointers aren't evil. I work for one of these companies and some of the components I work on are in Java and I would love to be able to use raw pointers.
Cool. Seems you forgot the easy and fast way: using just hyper! It is easier to learn than any other and might be as productive as the others. Plus, you get the coolness of running an asynchronous web server, which can be more than 1000 times faster than its synchronous friends.
I'm seeing gradual decrease of the content in TWiR. Is this just because everyone is focusing on impl period, or something else happens?
Nothing... Facets are defined by the user for each document. 
hi, a few comments here: - it seems your first loop is looking for the node with the highest score. why not use a max_by_key ? - why hashmaps and not bidi-arrays ? - why is closed not a hashset or bidi-array ? - remember: 'remove' comes with linear cost as contains on vectors - instead of a lot of prints you can maybe convert each line to a string and join them together - iterate on neighbours instead of creating the vector for example: ``` #![feature(conservative_impl_trait)] #[macro_use] extern crate itertools; // Iterate on neighbors of any node. fn neighbors&lt;'a&gt;( mat: &amp;'a [[i32; WIDTH]; HEIGHT], x: usize, y: usize, ) -&gt; impl 'a + Iterator&lt;Item = (usize, usize)&gt; { iproduct!( max(x - 1, 0)..min(x + 2, WIDTH), max(y - 1, 0)..min(y + 2, HEIGHT) ).filter(move |&amp;(nx, ny)| mat[ny][nx] == 0 &amp;&amp; (x != nx || y != ny)) } ``` then using it is easy: ``` for n in neighbors(mat, current.0, current.1).filter(|x| !closed.contains(x)) { ``` 
I'm currently writing a simple Tetris clone using SDL2 and [sdl2 crate](https://docs.rs/crate/sdl2/0.30.0). The question is about lifetimes and program architecture. I am using SDL2_ttf to draw text onto surfaces. From these I create textures. In the sdl2 crate, the `Texture` struct has a lifetime bound on the lifetime of `TextureCreator` it has been created by. It works like this: struct TextureCreator; impl TextureCreator { fn create_texture(&amp;self) -&gt; Texture { // create texture and put into the struct, rustc elides lifetimes :) Texture { raw_texture: 0, _marker: PhantomData, } } } struct Texture&lt;'tc&gt; { raw_texture: i32, _marker: PhantomData&lt;&amp;'tc TextureCreator&gt;, } Now I want to have a sprite which owns said texture with additional information: struct Sprite { x: i32, y: i32, texture: Texture, // error: expected lifetime parameter } This in turn requires lifetime bounds on the sprite and everything, which owns one or multiple sprites. My first idea was to add an `Rc&lt;TextureCreator&gt;` to the `Sprite` struct to have the `TextureCreator` live at least as long as the sprite. But I have yet to find a way to tell rustc, that the lifetime the `Texture` is bound to the field containing this `Rc`. Is there a way to prevent this "viral" lifetime bound on the `Sprite`, perhaps by reworking my architecture without `Sprites` owning their textures? Or should I just accept adding the lifetime bound everywhere?
Interesting, do we currently pass a special flag to the C compiler to get what we need here? Or are we relying on behavior that's not guaranteed?
How to beat yes in python (at least on my machine) #!/usr/bin/env python3 from sys import argv import sys out = "y" if len(argv) &gt; 1: out=argv[1] out = "\n".join([out]*10000) outb = bytes(out,"utf-8") while True: sys.stdout.buffer.write(outb) My yes runs at 8.48 Gib/s over 1 minute My yes.py runs at 8.93 Gib/s over 1 minute Obvously my yes.py is not a good version because of start up time. Maybe that should be measured as well in a 'yes' benchmark.
Be sure that you truly want one instance per process. If so, the equivalent of a singleton is a static of some kind. `lazy_static` isn't bad, but I wanted a specialized single thread version (for talking to glfw). So I wrote one in less than 100 lines. There's a thread pool in `rayon-core`. I haven't looked at it in detail, but I do know it aims to be the *only* thread pool in a process. That is, you don't necessarily need a thread pool for web clients, a thread pool for database connections, a thread for logging. (On Linux you *do* need a separate thread pool for file i/o.) Anything that can be written non-blocking can be managed by someone else's event loop. Whether or not that's a good idea depends on the application and OS. In theory though, it's the stable Rust 1.x answer to N:M threading.
Behind the scenes is probably the worst part haha. As I mention in the article, actors map to operating system threads. https://github.com/insanitybit/derive_aktor/blob/ThreadBranch/src/lib.rs#L301 That code is what generates actors. As you can see it's basically just a loop and 'try receive'. There's some logic for killing actors in there as well. Basically the macro generates a message for every public method it encounters. So for a block like impl Foo { pub fn bar(b: bool) {} pub fn baz(i: u32) {} } You get a message enum FooMessage { BarVariant{b: bool}, BazVariant{i: u32} } And a route_msg method, which destructures that and routes the arguments to the right method. The generated actor method then wrap the arguments in those variants and pass them across the queue where they get routed.
Thank you for your advises :) . I didn't know that you could change the type when shadowing a variable, now it make more sense. 
Okay, I still need to read more about the lifetime of reference I think :) .
I think Rust might have reached the end of its honeymoon period as the darling of /r/programming and hackernews. It had to happen at some point, and it might not be the worst thing - some people will probably take it more seriously now the hipsters are finding new languages and frameworks to jerk about.
I'm trying to figure out how to make [Sucredb](https://github.com/arthurprs/sucredb) useful. I think the first option is to provide more redis datatypes as CRDTs. So I hope to make some progress on that this week. 
You want /r/play_rust please check before posting :)
Ohh dang wrong group thank you üòÇüòÇ
Thank you for your comment :) . By bidi-arrays, do you mean a double linked list ? Or a bidimensionnal array ? The main reason is that I used the na√Øve pseudo-code from Wikipedia without thinking (for now) of better way to do it. Closed is a list of "Point", that is why I used a vector. A linked-list might be better, but I don't think using a hashset or or a bidimensionnal array would really fit, no ? I will try to use your comment, I think it's a good way to learn more :) .
Was "[faster compile times for release builds with llvm fix](https://github.com/rust-lang/rust/pull/45054)" successful, or not? The [related issue](https://github.com/rust-lang/rust/issues/44655) and the [ticket at the llvm-side](https://bugs.llvm.org/show_bug.cgi?id=34652) seem to imply it is not, but it's in "Updates from Rust Core".
That's impressive, but on my machine that only runs at 2.08GiB/s. Maybe I should get new hardware. ü§î
how is it different from limn?
Already discussed on r/unix: [How is GNU `yes` so fast?](https://www.reddit.com/r/unix/comments/6gxduc/how_is_gnu_yes_so_fast/)
Another fast implementation (in C) https://github.com/maandree/yes-silly
Java is ok for throughput but getting consistently low latency from it is extremely hard.
I think it's caused by [thinLTO](https://internals.rust-lang.org/t/help-test-out-thinlto/6017) on the latest nightly causing rutsc to hit [llvm assertions] (https://github.com/rust-lang/rust/issues/45131).
The noalias information is available now in Nightly with either -Z mutable-noalias or -C panic=abort. https://godbolt.org/g/nX9fg1
Open source projects often have an ebb and flow. Commits are down during holidays, as an example. On top of that, there isn't a huge volume of posts in general about rust, so even 3 or 4 more posts looks like a huge week, and 1 or 2 less looks incredibly light. I wouldn't read into it.
You'd be surprised by how much FORTRAN code is still used in high performance computing.
I've noticed the same thing. I think to some extent the language development has slowed down now that a lot of the easy sources of friction are smoothed over with `?`, `if let`, macros 1.1, and so on. Now that a lot of the low-hanging fruit has been plucked people are focusing more on the hard stuff, which is slower, less exciting, and less user-visible. The same goes for libraries: we're past the phase of "tokio is going to be awesome!" and "nom is going to be awesome!" and "gfx-rs is going to be awesome!" and such, and are in the phase of "these things ARE awesome but they still need a lot of elbow-grease applied".
I haven't looked at the cookbook recently, but I believe that they all rely on `error_chain`.
If you're interested you could take a look at [my A* implementation](https://github.com/rhys-vdw/a_star) from when I was first learning rust. I'm still pretty noob too, but might be interesting.
All signs point to less successful than originally anticipated, hence the followup. No reason not to include it in the roundup of recent activity, though.
 linked lists are very often not the right choice for anything. I mean bidi arrays. When you have hashset or hashmaps if the key space is very small it is often better to replace them with arrays. for example if you have an hashset with at most 256 values you can replace it with an array of 256 booleans. as for closed : when you choose a data structure you need to figure out what operations are taking place on it. remove on vectors is very expensive while its cost is low on hashset. 
Yeah, an interesting tidbit is that Python's numpy gives you a way to allocate arrays in C or Fortran style, so you can interop with the vast amounts of existing C or Fortran scientific/compute libraries/frameworks.
Neat! You should also post this at /r/rust_gamedev.
Maybe you had your power cable disconnected? macbooks have lower performance when charger is unplugged.
Maybe you had your power cable disconnected? macbooks have lower performance when charger is unplugged.
Maybe you had your power cable disconnected? macbooks have lower performance when charger is unplugged.
&gt; Maybe I should get new hardware Just so you could run `yes` faster?
Hi, So limn looks really neat! I saw it for the first time in their post the other day. There are a few differences: - I use a JSX like syntax plugin to get something which *feels* like declarative markup but actually gets transformed into Rust syntax at compile time. Limn seems to build the UI graph in traditional Rust code. - Limn uses cassowary-rs, while I use a more traditional two-pass layout system. The first pass is a depth first recursive call where components declare their preferred sizes. The second pass the controls are given their actual sizes and positioned on screen. Layout is encoded using a "Layout" property, which can be put on any component, and it dictates how the children of that component will be laid out. So far I only support linear stacking in horizontal / vertical directions without wrapping, I am working on a Grid type layout as well as one which wraps to the next row/column if the items do not fit in the container's block. - Limn has their input situation all figured out (mouse, keyboard, etc), in my UI framework things are very basic yet. Events don't bubble, basically the only supported event is a mouse move event which gets piped to WebRender, which returns to me an identifier so I know which hit-testable component was closest to the mouse at any given point. Integrating that with an event bubbling/tunneling apparatus isn't yet implemented, there's a lot of cool things happening in Rust around async which I'd like to fully sink my teeth into before I start. - I use Rctree to represent a tree of trait object Components, which are then folded into a tree of primitives (Components have an inherent trait method they can override to dictate this, the default is Rect) which are then passed to layout and eventually the graphics module for display list building. - We both seem to use some macro magic for styling, my property system allows for an open set of properties (though at the moment this is just an implementation detail, style/layout/graphics are aware of a set of known properties they check for, any custom properties are as of now of little use) stored in a TypeMap structure. - My text system is horribly broken because I haven't abstracted out text layout yet. I have a hacked together DirectWrite implementation but it has a lot of missed corner cases, but WR's text API doesn't really do much for you here, you need to determine the positions of the glyphs yourself. Limn has very rich support for text from what I can see, including alignment and wrapping. The screenshots look nice :-). - I don't support images/ellipse yet, nor do I support rounded rectangles. I do support borders in various styles though :). - I do my scrolling in WR, limn does their scrolling in framework and just re-runs layout. This allows them to have scrollbars since WR doesn't support them for now. In general though, limn is much more complete. My UI framework is more like a prototype for my own crazy ideas at the moment :) 
Thank you! I'll definitely check this out.
Oh very good point, will do! Thanks!
Thanks for the insight! I'll check those out and very true I just want to make sure I'm aware of the common Rust practices whenever possible.
Non-Type Generics is the second bigger blocker as far as I am concerned. When you want to touch as few cache lines as possible and prefer avoiding the heap, you start relying a lot on arrays of a great many sizes, build data-structures on top of those arrays, have functions to manipulate those arrays, etc... ... and today in Rust it's just painful. Possible, certainly, with traits and macros and whatnot, but painful. --- The first bigger blocker is the difficulty in incrementally converting an existing C++ codebase ;)
&gt; Some HFT shops have a max tick-to-trade latency of about 15 microseconds. I'm going to cite Carl Cook's talk here: the best play at around 2.5 microseconds^1 in software. Consistently. Below that, you have to look toward FPGAs or other hardware solutions^2 piloted by software, in which case the requirements for the software part are somewhat relaxed; somewhat. ^1 *Wire-2-Wire: that is, between the first byte of the inbound packet reaching the port and the first byte of the outbound packet exiting the port; this therefore include all the hardware handling before the data even gets to the CPU and after it leaves it.* ^2 *I actually wonder if anybody out there is using another hardware solution than a FPGA, just putting it there for completeness.*
Rust is as suitable as C. There are two reasons, for me, why C++ will be more difficult to encroach on: 1. Rust is difficult to integrate with modern C++ and its heavy reliance on templates, 2. Rust does not yet have non-type generics, and HFT loves fixed-size arrays.
2.5Œºs switch-to-switch is absolutely tiny. To do that consistently without jitter is a feat. For that, I'd guess that a shop is leveraging a nic with on-board FPGA to help accelerate unpacking, routing and filtering of inbound data before it makes the data available to the process. E.g. the data arrives on a multicast to multiple machines, each with separate responsibilities, and the nic does the "routing" by filtering relevant data. https://www.optiver.com/ap/en/job-opportunities/na-379 . seems to indicate that they're using custom FPGA solutions to get to wherever they're at. 
Thanks. It is slightly simpler. I don't like loops like `for i in 0..len(v)` either since they have poor performance.
Presumably if `yes` is slower, then so is every other operation on the computer.
Also, at around 36 minutes, he references sending non-executable orders outbound to the nic. So likely the nic is filtering and "routing" via FPGA outbound as well. Rather neat trick to keep the cache hot for the hot path. 
I'm sure Optiver uses FPGA, I'm not sure if they use them as NIC. There are commercial solutions for user-level network cards which are really good, so it's not necessary to have them built in-house. And yes, 2.5Œºs is really a very short amount of time :) At 4GHz, it's 10,000 CPU cycles.
Feel free to ask any questions about WR here, lots of us who work on it are around.
&gt; Whenever a shape is translucent, you need to blend the colors of the two shapes. And in order for it to look right, that needs to happen back to front. That isn't true. You do need to render them in a consistent order, but it can be either back to front or front to back. With premultiplied alpha (which you should be using anyway to get correct filtering), blending is associative so you can render it back to front or front to back and get the same result. Not sure it really helps in this case since you'd want to change either stencil or depth writes for opaque vs blended objects, so it may make sense to split them up in two passes anyway, but it's possible to render translucent stuff front to back. 
I'm skeptical about this game engine comparison. It's true that games do render at much more consistent frame rates than web pages, but that's at the enormous cost of constantly running the GPU. On mobile, you can *feel* the heat radiating as soon as you run even a simple 3D game. Also, I think the comparison is a bit incongruent in another aspect: game devs are largely aware of how their rendering pipeline works and design specifically to cater to it. Webdevs (at least for the pages I visit) don't seem to show any awareness at all of layers and compositing, much less design their CSS to cater to that paradigm. But in the alternate universe where they did, this does seem like the more sensible model to me. So basically my concern is... just how much GPU power does WR consume to get these consistent, high frame rates compared to the traditional compositing paradigm?
I'm curious, is energy consumption on the whole anticipated to be better or worse with webrender?
That's not really what it's for. `map` is probably closer semantically.
Why is the ? Operator not working here? let r: Request = serde_json::from_str(&amp;input)?; They use it in the Example: https://github.com/serde-rs/json let p: Person = serde_json::from_str(data)?; But i get a Error: cannot use the `?` oprustuperator in a function that returns `()`in this macro invocation use: nightly-x86_64-pc-windows-msvc (default) rustc 1.22.0-nightly (4502e2aa9 2017-10-03)
No, other languages have a different map function too. Inspect/each/foreach are all used when logging values in the middle of an iterator, so the return value of println doesn't matter. With map, the values of the iterator would be mapped to `()` (the return of println), which is not intended.
I'm not aware of any language where `foreach` has those semantics. Which language exactly are you talking about?
This is great here. Thank you for tackling such an important issue.
For its intended purpose, the simplest implementation is fast enough by a huge margin. Coreutils should include the straightforward version, not some convoluted uber optimized solution for absolutely no gain. Optimizing `yes` may be a good learning experience but it shouldn't go into production.
&gt; But i get a Error: cannot use the ? operator in a function that returns ()in this macro invocation It's hard to say without seeing the rest of your code, but it seems like your function returns `()`, not `Result`.
JavaScript has [`.forEach`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/forEach) (although it doesn't really have iterators, instead all the methods are on the array). JQuery, underscore, and lodash (JS libraries) all use `.each`. Java has [`.forEach`](https://docs.oracle.com/javase/8/docs/api/java/lang/Iterable.html#forEach-java.util.function.Consumer-).
I sure hope these innovations will still allow me to use my browser on my laptop with GM45 GPU by supporting OpenGL 2.1 ‚Ä¶
The `inspect` method is lazy, while the foreach-like methods are not, so I don't think they're much alike at all.
Yea usually when I see `forEach`, it's has slightly different semantics. OP, `peek` or `tap` is the method I usually see with these semantics https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#peek-java.util.function.Consumer- https://ruby-doc.org/core-2.2.0/Enumerator.html#method-i-peek http://underscorejs.org/docs/underscore.html#section-111
Hmm, good point. Though I don't think many other languages' iterators are lazy.
Good point. I stand corrected. In my case, I was just looking to log the contents of a collection.
Just curious, why is energy consumption important for you?
I'm not Cldfire, but: laptop battery life
It has nothing to do with whether or not it consumes the contents of the iterator. That's just a side-effect of `.forEach` being a method on a data structure which can guarantee that iteration without side-effects is possible. In most languages, `for` came first and is a C-style looping construct, while `foreach` (which is often a keyword) came later as a "for each item in something iterable" construct. (I haven't checked, but it wouldn't surprise me if Python popularized the latter with its `for item in iterable:` syntax, which uses `for item in range(x, y):` for C-style looping.) Providing non-consuming iteration is impossible in the general case, because the iterator interface allows for arbitrary code, which may not be able to produce elements non-destructively. (ie. It could be a "matching files I've just deleted" iterator with no "dry run" code.) The closest you can get is an iterator adapter which caches the output of the iterator in something like a `Vec&lt;T&gt;`... but that will trigger the potentially-irreversible action on the first pass through and just replay the results on later passes.
Both of those examples return `void` though? They don't return an iterator of the same elements. 
I don‚Äôt work on it, but what I‚Äôve heard is ‚Äúexpected better‚Äù, as you‚Äôre doing the same thing, but more efficiently.
Right now we're too far from shipping to give anything close to a reasonable evaluation of that goal, though.
Until devs start adding *more* things that they couldn't afford before...
^ this.
You need to use a C compiler that guarantees constant-time behavior for certain operations. I'm not sure there is one that I can link to.
I'm revisiting a little library I wrote awhile ago for higher order types in rust. I've learned a lot since I wrote the library, so I'm trying to polish it up a bit. In particular, I'm working on reformatting the library and writing some procedural macros to derive traits like `Functor`, so you can do things like [this](https://play.rust-lang.org/?gist=335c12901e2fa6958cbf7e255f90630f&amp;version=stable)!
We currently require OpenGL 3.1, though with some engineering we could add in more fallback for older systems. I have no idea if OpenGL 2.1 is possible though. For unsupported configurations we'll have to fall back to software rendering, which in Servo we've assumed we'll do through LLVMpipe or some similar mechanism.
You need to know much less about the pipeline with WebRender because all operations are nearly equally fast. Currently some operations in browsers are optimized in the compositor (changing position or opacity), but the rest are not. This creates performance cliffs. Also, games will typically use most of their frame budget since they are designed to scale up quality with available hardware. We've got gobs of power to spare even with mobile GPUs. Frame times for WR are mostly low single digit milliseconds on most web content. We need to measure and optimize for battery of course. In other words, we create a workload of a similar shape to a game, but not anywhere close to a similar size.
I think you'll have to rely on the CPU fallback. The minimum target for WebRender is Sandy Bridge integrated GPUs.
I don't see it. The list of RFCs and FCPs is just smaller because the RFC process was in overdrive for the two months leading up to the impl period, and now that the impl period is upon is the RFC process is supposed to be on the backburner--that's sort of the whole point. :) The blog posts section looks small this week, but it seems that it's arbitrarily not including a lot of the blog posts that I've seen in the past week (and as a mod, I see pretty much all of them, and I haven't noticed any decline). The updates from Rust core looks as big as it ever does, and that list is always arbitrary based on whatever the Rust devs think is cool (Github graphs on the rust-lang/rust repo corroborate that there's been no slowdown to activity). And the list of new contributors (fourteen of them) is actually way higher this week than our recent average (five, in my unscientific estimate).
Ok you must not be repainting *everything* each frame then - surely you're not getting render times like that if you're redrawing text nodes each frame? In which case I don't understand your claim about certain style changes no longer being more efficient than others: if you are caching the text texture, then anything I do which forces a reflow (or worse yet, a glyph redraw) will indeed be far more expensive, no?
Is web render in nightly?
Glyph atlases will be retained and selectively updated. We don't cache whole paragraphs of text rendered to pixels, though the layout system would hopefully be caching all the shaping info at an earlier stage of the pipeline. WebRender starts with a display list where we know the position and z-ordering of all the objects. The objects themselves are pretty simple and high level (box, shadow, text, image, etc) and map to CSS concepts. Games do loads of other things like materials and lighting in the shaders that we don't do any of. For the case where the CPU would have painted a single pixel, WR will certainly use a bit more power than CPU rendering with a compositor. For the case where a large portion of the screen changes, CPU renderers might miss their frame budget spending &gt; 16ms drawing a frame, where WR will complete that task in 4ms. We might light up part of a GPU for longer than compositing would have, but we just saved &gt;12ms of CPU compute. The power hypothesis is that the saved CPU compute consumes more power budget than the extra GPU compute we added. If there was no further JS code to run (app is idle) then we can go back to idle state after 4ms, instead of after 16+ms.
It is, but not enabled by default. See https://mozillagfx.wordpress.com/2017/09/25/webrender-newsletter-5/ for instructions on how you can play with it in Firefox. Note that Firefox is adding support incrementally, so not everything is rendered with WebRender yet (as in Servo). It should continue to improve in speed as they convert more display items.
Do you know what the requirement is on Windows, assuming ANGLE is translating to Direct3D? Would it be D3D 9 or D3D 11?
Off topic, but having acquired some very inexpensive (on Amazon) SoC-based NVRs I went looking for an alternative OS/firmware for them and really didn't find anything. It would be cool to have a secure OS for them or at least a main application written in Rust and a little more control of how things are written out to disk. I have photos of the board if you have any idea what it might be.
On FF nightly, the memory consumption with WR seems to go higher and higher as time passes by and I browse image heavy websites. If I turn texture cache debugging on, sometimes I see duplicate images across different boxes in the debug view. It seems like entries in the cache are not being properly evicted over time, or as tabs are closed 
Lower(ish) activity can be attributed to the relatively major pending changes in the language feature set, which is may be putting a lot of Rust endeavors on hold. ~H~ Enthusiast youngsters moving on may have nothing to do with it.
So there are still badly-behaved CSS properties then, right? Stuff like changing the text to bold or changing its font size are still bad. I say this not because I expect you to solve this problem, but because front-end designers being ignorant of such things is the reason the current paradigm is problematic in the first place. &gt; We don't cache whole paragraphs of text rendered to pixels, though the layout system would hopefully be caching all the shaping info at an earlier stage of the pipeline. So you do compute the text flow each frame then? That's impressive! And indeed, it should alleviate some of the current "cliff cases" like changing a text container's dimensions. &gt; For the case where the CPU would have painted a single pixel, WR will certainly use a bit more power than CPU rendering with a compositor. For the case where a large portion of the screen changes, CPU renderers might miss their frame budget spending &gt; 16ms drawing a frame, where WR will complete that task in 4ms. I'm worried that "single pixel" change might be pretty common -- the blinking text carat comes to mind! In any case, I don't mean to advocate CPU rendering; I'm all for doing all possible painting on the GPU. Rather, I'm questioning why the compositor had to vanish just because we're moving more work to the GPU.
Since Firefox will cease XP support, I believe we can rely on DX11. As far as hardware requirements, I think D3D 10.1 is what is needed. There will likely be more clarity around this as we get closer to release.
This should be fixed when https://bugzilla.mozilla.org/show_bug.cgi?id=1331944 lands.
Yes. We currently depend on ANGLE to gives us OpenGL ES3 support. ANGLE requires D3D10.1 level hardware for this. We plan on expanding this to at least D3D10 level hardware after the initial release.
Thank you, I had the CSS features enabled but not webrender. I will play with that for now.
This week had alexcrichton contribute an insanely nice array of PRs for codegen-units and ThinLto. michaelwoerister adding [red/green change tracking for incremental compilation.](https://github.com/rust-lang/rust/pull/44901) Feels like a really good week!
I think you are conflating layout and painting. WR does not change how layout works. If you change something in the DOM that affects their layout, layout will be rerun (hopefully just partially). After layout, WR must paint the result. Before if the result was a lot of pixels needing to change, CPU painting would take a long time *as well* and perhaps cause you to miss a frame. If only a few things changed, you'd be ok. With WR, small and large changes take the same, small amount of time. You are right that problems remain, but those are problems that Webrender caused, just ones that WebRender, alone, can't address. For this specific one (relayout) we have Servo's parallel layout engine, and the hope is that the CPU time gained by WR will absorb a lot more relayout issues. Before, web devs needed to understand both how their DOM updates could affect layout, and how they could affect rendering. We've eliminated one of the things they needed to know to get good performance. Additionally, before compositors could only handle a few CSS transforms (opacity and transform changes; basically anything that amounts to manipulating a layer). WR can handle them all. Have you ever used the layer debugger in a web engine? Or the little boxes that tell you what was repainted? Those are no longer needed. (See http://blog.atom.io/2014/07/02/moving-atom-to-react.html for some examples) You are correct that the "single pixel" case might get worse, though blinking carets can be handled by a one element cache of the old display list. The thinking was that it wouldn't be a big deal, and the upside is that all web content will be painted fast. If it is a big deal, we'll have to address it, but it's a lot easier to fix that than to solve the memory bandwidth limitations of filling pixels on a CPU :)
Hooking Rust to the Intel DPDK/SPDK would seem to be an interesting combo. If you were really ambitious, then you could replace even the lower level user side consumer of the development kit "drivers".
Is my interpretation of about:support on Nightly correct that enabling [OpenGL accelerated layers](https://bugzilla.mozilla.org/show_bug.cgi?id=594876) is a prerequisite for having WR on Linux? And what's the actual issue here? Is it GTK not liking this, or X11 issues, drivers, or something else?
Hi, serde_json::from_str(data) returns a result. I can use a MATCH on the result, but not the ? operator. So i assume that i missed a "use" of some kind of trait ... I have no idea ... unitl now i always worked arround the ?. The code: https://pastebin.com/gguiGGTZ 
&gt; I can use a MATCH on the result, but not the ? operator. Yup, and that makes sense! See here: fn handle_client(mut stream : TcpStream){ This function does not return a `Result`, only `()`. Therefore, you can't use `?`, as you can only use `?` in functions that return `Result`s, as `?`'s semantics are "return `Err` if this thing is an `Err`", and you can't do that with a function returning `()`. Does that make sense?
Just wait, if the trend continues, one day rendering a website with 3 lines of text will involve more computations than your human brain computes in its entire lifetime.
Oh! Thank you! The outer function has to return a result! ... Thank you! Awesome!
Yup! Glad we got that worked out :)
Could you run the code from the reddit post too?
Correct, WR requires hardware acceleration, and hardware acceleration still needs to be manually force-enabled on Linux. You identified the correct bug about doing that by default; you can look through the open bugs in its "Depends on" list to see which problems will need to be investigated before that can happen.
That's also true, but it's better style to use the imperative control structures if side effects are your goal. 
The most direct way to solve this problem would be to create an `Rc&lt;TextureCreator&gt;` like you were thinking, and then use the [`owning_ref`](https://kimundi.github.io/owning-ref-rs/owning_ref/index.html) crate to build an `OwningRef&lt;Rc&lt;TextureCreator&gt;, Texture&gt;` that "carries the `Rc` along with it". That said, I think reaching for `owning_ref` is kind of a red flag. I have no idea what `sdl2` code looks like in general, but do other people tend to propagate the lifetime parameters?
And what did you choose for front end?
Is it possible to specify some dependencies as internal so that `cargo doc` doesn't generate documentation for them?
And that‚Äôs why people think that browsers have slowed down: because most frontend web developers and web content creators are lazy. (In a bad way, not a good way.) I think most people with this impression would be surprised at just how badly Firefox 1 would perform on the web as it is now (though you‚Äôd probably need to polyfill a bunch of features that have been added since to get it to work at all).
You may be able to add `#[doc(hidden)]` to the `extern crate` to hide them in docs.
It better not be Lorem Ipsum
Tried that, doesn't work. It only hides `pub extern crate`s from the "Reexports" section. Looks like documenting dependent crates is happening before even parsing my crate, so I guess I should add something to `Cargo.toml`
Thanks for writing this. It's been a deterrent to use Rust for projects because there's no clear preeminent web framework. The existing comparison articles don't do a very good job at doing this (https://github.com/rust-unofficial/awesome-rust). When I'm starting a project, I want to choose a framework that's easy to use, well-documented, secure, and is going to be supported for the foreseeable future. It's not terribly helpful to have a dozen frameworks listed with no indication of popularity or how good they are, because it's going to take a non-negligible amount of time to sort through them and figure out which ones are dead or not. From the outside looking in, it looks like there's a lot of churn right now for web frameworks - lots of ones that people are starting but never finishing, or in various states of partial completion. What I'd ideally want to see would be the different niches (minimalist likely intended for serving static content, complete likely intended for full blown web apps) and which framework is the best. As in if a friend wanted to use Rust for production but would only be willing to try two frameworks, maybe three tops, before he gave up and went back to Python / Java / Ruby, what would would I suggest? I have a similar complaint about UI frameworks in Rust - Backend (Gtk, Qt, etc) is a factor, but I just want to know what the community is passionate about so I know what to invest my time in.
Dupe comment; you should delete this.
Surely the turbofan illustration is some kind of jab at chrome?
A better question is why is it not important for you?
It is true that it possible to front to back alpha blending and with enough head scratching I suppose one can figure out how to support the rest of the supported blending operators. But that wouldn't be very useful in webrender's case. The important thing is that blending isn't free so having a first pass that has blending disabled gives a nice boost. The other reason to separate the two passes is that webrender could not draw font to back with the z buffer enabled (read+write) if transparent primitives are part of the pass, and the z-culling is a really important optimization in webrender. 
Actually there is also a copy to clipboard button which copies the whole expanded content but it might be as hard to find as the expand button ;)
Hi cookbook maintainer here! Sorry to have tripped you and thanks for taking time to report it! I admit that visual and UX side of things is lacking at best at the moment. There is a [cookbook issue](https://github.com/rust-lang-nursery/rust-cookbook/issues/327) regarding this very problem. Also we are gathering ideas in [design overhaul issue](https://github.com/rust-lang-nursery/rust-cookbook/issues/246) feel free to add any additional thoughts, suggestions or pain points to that thread. 
I use Firefox on Android. Every milliwatt hour matters.
You *can* do a front to back blend, but IIRC it's not as simple as using premultiplied alpha. You need to use destination alpha which isn't very common. It's called the "UNDER Operator" in [this article](https://developer.nvidia.com/content/transparency-or-translucency-rendering)
Using `r2d2` with `diesel` is the best way to go with interfacing with databases in Rust.
For "not C" I was thinking of languages that are "not C, compile to native machine code, and have little or no runtime overhead". I doubt chapel really fits the last criterion though. That narrows it down a little more and rules out anything bytecode VM based like Java, Clojure, Erlang, elixir, or things with large run times even when compiled like Common Lisp. Basically C and rust fall into the category of "things that could be used to write code for a small microcontroller" (I think?) 
thanks for long explanation! we still dont have clear winner for gui library so there is still plenty of room to experiment!
Yes. I mean you could render each image to an off-screen buffer and use *its* alpha channel for the next image. Using destination alpha is exactly the same as that but more efficient because you use the rame buffer as your intermediate target (you're blending the frame buffer on top of the new image, which makes the dest alpha your alpha).
Maybe in addition, but it's primarily an ongoing metaphor that the whole Quantum project to revitalize Firefox is like changing out a plane's engines while it's in flight.
&gt; On mobile, you can feel the heat radiating as soon as you run even a simple 3D game. You can feel the heat radiating as soon as you run a _poorly written_ application, be it a 3D game, 2D game, or not a game. A lot of game developers for instance come from the "as fast as possible" mindset and will happily try to render 600 FPS on a device that can only display 60 FPS. That doesn't help. A lot of game developers don't understand how to efficiently use a GPU or how to efficiently use the CPU even (especially in the mobile world) and will happily run the whole device at 100% speed. Note that mobile and iGPU scenarios even run into funny counter-intuitive cases here; in the attempt to be fast these apps run _slower_ as the system down-throttles when it gets too hot. Which is mostly irrelevant to your point, but to me humorous. For anecdotal evidence, I've played (at the time) cutting-edge 3D games on older phones where I'd be able to play for 4+ hours on battery and simplistic 2D games on the same phone that would drain the battery in 1 hour. A GPU is much the same as the CPU. Doing lots of work is totally fine. The key is doing lots of work as quickly as possible and then idling so the hardware can switch down into a lower power state for most of the time. For most graphics works, the GPU is capable of doing the same work faster than the CPU would, and smart GPU usage of course does the work even faster than naive GPU usage. tldr: Doing that work on the GPU means not just better performance, but also better battery usage and lower heat output, so long as it's done correctly and intelligently.
With `mix-blend-mode` it‚Äôs possible to do blends other than additive blend; but I don‚Äôt know enough of the math to tell if that would break the associativity.
Do these crates work with MySQL? Or are the specific (or unspecific) to a DBMS?
That is not a better question.
So it's an one-to-one threading model? That might an issue if it's a large actor pool with a lot of idle time. Would you allow it to become M:N worker threads?
Yes, exactly. I absolutely want to move to an M:N model, but I spent some time on it and decided to move on to other areas since I wasn't making progress. My idea was to have a group of 'scheduler threads' that receive futures representing the actors queue loop, and execute the futures. I have a branch in derive_actor that shows my first pass at it - a single actor basically dominates processing until it has no messages, so it's totally non-viable in its current state.
This is nice. What about the final comparison (performance, binary size and stability?)
Ah that makes sense. carlleche and I might before working on bringing [kabuki](https://github.com/carllerche/kabuki) back from the grave with a thread pool scheduler. It is pretty old but there is some stuff there that you might be interested in.
Pre-multiplied alpha can do alpha blending an additive blending and any mix of the two (basically, artificially set alpha=0 to get additive, leave it at the actual alpha to get alpha blending.. Interpolate between the two to get an in-between version). For other blending modes you'd have to switch to some other kind or blending, and probably lose associativity (except for things like multiply).
Thanks for the work you put into maintaining it! I suspect that after I get used to the conventions it'll be ok, probably just trips newbies up
I'd be interested in seeing that, certainly. The whole scheduling thing I do right now is a total hack.
You can do `cargo doc --no-deps`, I'm not sure if it affects reexports from other crates or not.
I think the key is that to write 'battery efficient' code, you just write 'fast code', they're basically the same. The phone can only save battery by shutting shit down, it can't be like 'okay only 30% on the cpu now' - so unless the gpu is taking MAD power the speed up should mean the CPU is on way less &amp; you save overall? I mean, this is a bit hand wavy but after a quick google found this - http://www.nvidia.co.uk/object/gcr-energy-efficiency.html "GPUs are inherently more energy efficient than other ways of computation because they are optimized for throughput and performance per watt and not absolute performance,"
&gt; apps run slower as the system down-throttles when it gets too hot It would be interesting to see if there is a throughput gain on phones at various levels of throttling. I'm leaning towards it not though. &gt; A GPU is much the same as the CPU. Doing lots of work is totally fine. The key is doing lots of work as quickly as possible and then idling so the hardware can switch down into a lower power state for most of the time. Do GPU's switch to a low-power state in-between frames? i.e. is there a difference in power consumption between 100 calls in 1ms and the same 100 calls in 16ms?
Yeah, I'm curious. Sent you a PM.
ffmpeg? [[wrapper](https://crates.io/crates/ffmpeg), [sys](https://crates.io/crates/ffmpeg-sys)] I see that there are Rust bindings for it. I haven't used either of these Rust crates, but [ffmpeg supports mpeg-ts](https://ffmpeg.org/ffmpeg-formats.html#mpegts) 
It competes with GNU coreutils `yes`, but the gnu one is still better: 5.72 GiB/s with the Rust one (release mode) 6.49 GiB/s with GNU Coreutils `yes` version 8.28.
&gt; So basically my concern is... just how much GPU power does WR consume to get these consistent, high frame rates compared to the traditional compositing paradigm? Can you describe specifically the scenario you think is problematic, and how power usage would be worse under WR than under a compositor?
&gt; Stuff like changing the text to bold or changing its font size are still bad. How are they worse under WR than under a compositor?
Can you mention how GNU yes performs on your machine?
&gt; References as keys All keys must have the same type. Because lifetimes are a kind of type, you need a situation in which borrowck can identify a single lifetime for which all references are valid. This means any function that will make changes to the map needs to know that the references it adds will live long enough to be usable as keys. And the only way a function knows about lifetimes longer than the function itself (I call them "external lifetimes") is with lifetime parameters. Since keys are usually small pieces of data, it's quite a lot easier to just copy them into the hash map. And it's easy for the computer too, likely faster to just copy. References are pointers and following them to the target every time you compare costs an additional memory access. With small data, pointers are relatively expensive. 
You can also underclock the cores, which will lose performance but save more battery. Some initial Servo tests showed that by turning off turboboost on desktop intel chips, we lost 30% of the performance but saved 40% of the power. And of course, Servo parallelism closed the performance gap. Net win was 40% less power. Keep in mind this was an early experiment and we have not done rigorous testing. We wanted to do a similar test on mobile, but without modifying the Android kernel we wouldn't be able to underclock the cores.
https://github.com/0freeman00/tokio_simple_server i have written small setup fot simple tcp server, you can check it out.
desktop
What if I have multiple windows open in the background. Will the GPU be overloaded constantly rendering all these?
Question from even bigger noob. I'd like to know if I read this right. opened is a container. iter gets an iterator for that container. the position function takes a lambda. I assume position is "for each position in container"? The lambda is capturing current and checks x at that position if they are equal. position returns an option and we catch the Some part of the tuple in the if let. If there is something there, its put into n and remove from opened in the {}? Did I get it? :)
Can you explain the 'a? I've seen this in rust docs, but still kinda lost on the concept. Has it something to do with timelines?
I too was looking for this information
Can somebody make it so that the object isn't outputted? https://pastebin.com/MP7PLsVX
That is incredibly impressive! Wow, thanks for sharing :)
&gt; I think you are conflating layout and painting. Ah, I think you're right. I was incorrectly using "painting" to refer to both of these steps together. What you're saying is that CSS which triggers layout changes (changing the dimensions of a text container) are still pathological, but CSS which only causes a repaint (e.g., background-color) was previously pathological but is fine under WR, correct? &gt; You are correct that the "single pixel" case might get worse, though blinking carets can be handled by a one element cache of the old display list. Apparently I was also ignorant of the correct spelling of "caret." Anyway, from what I understand now, I'm optimistic about your current approach!
&gt; But I like the mut at the call site because it reminds me that the callee may mutate the location I loan to it. Exactly this. I like this about Rust. C++ programmers sometimes use a style to always pass mutables by pointer, so they have a &amp; at the call site. But if you start passing pointers everywhere. You probably already have a ptr at the call site. So its moot. Awesome that rust forces this
They aren't. I was responding to this: &gt; You need to know much less about the pipeline with WebRender because all operations are nearly equally fast. Currently some operations in browsers are optimized in the compositor (changing position or opacity), but the rest are not. This creates performance cliffs. Animating CSS properties which cause layout changes should still be pathological, and so making the web fast is as much about publicizing an understanding of how the browser computes what to display as it is writing a good display engine.
Well intuitively, any persistent animation that affects only a small portion of the screen (e.g., flashing caret, small SVG, etc.) and has been given its own layer, no? Which should be a pretty common case for idle pages, so my concern is that WR, like games, will have a fairly high power idle state.
Not to my knowledge. If you do one, please use nom, I'm interested in seeing more container parsers :)
Ah, position is returning the position in the container where the lambda returns true, duh:) And some isn't a tuple, its the Option enum, which can have data. N in our case.
I was able to fix it on my own. And I understand what happened and why. And now I think I can read rust documentation. Here is the improved version. I think I single-handedly made the most useful program in existence.
Gotta go so no time for proper response but basically its manual control of the lifetime of the item (reference usually). Particularly crops up for guarenteeing the value the reference points to is still alive in all situations that the reference itself is in scope -&gt; no dangling pointers. If rust can't work it out automatically for you, gotta do it yourself with that notation. https://doc.rust-lang.org/book/second-edition/ch10-03-lifetime-syntax.html
I'd say I'm intermediate or better in Rust. I would have looked at this and immediately known that there was missing code, but I had no idea that you could click the "expand" button.
All of these are great reasons why this crate needs to exist - in all my games I just used `std::time` and `std::thread::sleep`, if these details were encapsulated in a crate then I could get access to these kinds of optimisations for free.
Maybe have fn run_frame&lt;F: FnOnce(f64)&gt;(&amp;mut self, frame_fn: F) { self.frame_start(); // I don't think you expose delta-time right now, this is just an example f(self.dt); self.frame_end(); }
&gt; For anecdotal evidence, I've played (at the time) cutting-edge 3D games on older phones where I'd be able to play for 4+ hours on battery and simplistic 2D games on the same phone that would drain the battery in 1 hour. Definitely, some of the clickers/"idle" games out there are absolute CPU hogs despite doing very little.
Tell that to C++
This was really amazing to watch! But really, you should train yourself not to say "uhm" every 5secs, this can get a bot annoying.
Lazy? I'm not a web developer, but I still think it's not a fair view of a group of people. The fact that they don't need to undestand how to run code efficiently means that they can implement more features and UI with better user experience. Performance suffers, but the value of the web is growing rapidly in great part because of them.
&gt; As an aside, I have yet to see any real-world uptake of Chapel in the HPC world. Very little "real-world" programming is constrained by processor speeds. CPUs are *insanely* fast and commodity hardware means they get faster every year without doing anything. Also Amdahl's law bounds what concurrency can actually achieve. I could easily see Chapel being used by some of the labs I've worked in before, though. At least for newer code.
Really great to watch. I'd love to see a video showing the transport between the debugger and the emulator
&gt; Rust of course has it to offer laziness in an otherwise eager language, and to help writing code in which lifetimes and scopes are cleanly linked. Laziness and iterators are pretty much orthogonal. I'm pretty sure Rust uses them because they're a zero-cost abstraction that still recovers some of the benefits of lazy linked lists. 
Some FORTRAN libraries for scientific computing are faster than their C counterparts.
Is WR using glutin or something else to create a window etc? Or is that not WR's domain at all (and if so whose domain is it)? I tried to look in WR's Cargo.toml but did not find an obvious answer.
Anyone who sets up a UI and doesn't check if it's using .5% CPU or 50% CPU is lazy, yes. The value of the web gets an even better boost when a developer makes a page that reliably has a good interface, instead of sometimes bogging down.
Thanks for the pointer to `owning_ref` :) I browsed through the issues on the `sdl2` crate. [It seems I am not the first hitting the `Texture` lifetimes](https://github.com/Rust-SDL2/rust-sdl2/issues/667). The reasoning behind this is, that an SDL-Renderer (of which parts are abstracted away in `TextureCreator` of the `sdl2`-crate) *and* the `Drop`-implementation of `Texture` both manage the lifetime. If the SDL-Renderer / `TextureCreator` was to be dropped before the actual `Texture`, dropping the `Texture` would lead to a segmentation fault :( It looks like there are some solutions: * propagate the lifetimes * use `owning_ref` as you suggested * enable an unsafe feature in the `sdl2` crate to have textures without lifetimes * use a resource manager and reference the textures using strings or other keys, [like in an example from the `sdl2`-crate](https://github.com/Rust-SDL2/rust-sdl2/blob/master/examples/resource-manager.rs) I think I'll go with the `owning_ref` crate for now, because it seems like the most painless approach for my simple use case. I hit a wall when trying to propagate lifetimes because I have a struct (my `GameScreen`), which is reference counted (thus resulting in a `'static` lifetime). The TextureCreator, to which the lifetime of the GameScreen is bound to, would have to outlive this.
Laptops and mobiles.
Yes, that was it. I tried with older nightly, and everything works now. Thank you!
Turbofan? Good name! When I fire up Chrome the laptop fans go turbo!
This tends to happen if you call methods or access fields of a variable _before_ its type can be inferred ('before' here means the code that textually comes before the access). It seems like code after the access doesn't help type inference for some reason. This is more glaring with closures, if assign a closure to a variable before passing to a function ([playground](https://play.rust-lang.org/?gist=7177e881653d9e5bfd4bd715676fedd0&amp;version=stable)).
Also, depending on how paranoid your team is, that means that you have to do null checks on things that you know should never be null -- some idiot might pass null in after all. 
Just as a side note: Reading that entry on my phone (Firefox Mobile) was annoying as hell, since at some places scrolling just jumped around very unpredictably :/
/u/AngriestSCV, what kind of machine is that? And what OS?
&gt; I assume I'm expecting it to be cleverer than it is and that, as the error points to, the first usage of it has to solidify what it is. Basically, yeah. The error message given&amp;mdash;[E0619](https://doc.rust-lang.org/error-index.html#E0619)&amp;mdash;has this in the docs: &gt; Type inference typically proceeds from the top of the function to the bottom, figuring out types as it goes. In some cases -- notably method calls and overloadable operators like * -- the type checker may not have enough information yet to make progress. This can be true even if the rest of the function provides enough context (because the type-checker hasn't looked that far ahead yet). In this case, type annotations can be used to help it along.
 $ gyes | pv -r &gt; /dev/null [842MiB/s] (The GNU yes is aliased to "gyes" on mac)
&gt; probably just trips newbies up That is very (if not most) important part of our demographic! I imagine part of new users tripping over the UX will just say "bah humbug!" and never look back. Missing the opportunity to use our resources. So this is a very important issue to address.
As far as I understand it‚Äôs the responsibility of whatever is using WebRender to provide a GL context, which can be created with glutin (like in Servo) or something else.
IIRC FORTRAN compilers can make use of the fact that the language forbids aliasing.
Alternatively you can not spend trillions of calculations waggling shit around and detracting from what is otherwise a text only page.
&gt; UI with better user experience. You see, that's the problem. Pop-overs, menu bars that cover 1/3rd of the screen with blank space, images that fade in rather than just being an &lt;img&gt;, parallax scrolling, overriding scroll wheel and other UI input etc. None of these things improve the UI even slightly, they just make it slower and less accessible.
I'm more into learn with it then reviewing. It's very very cool. I was thinking on something similar this days to use in aggregation with the AI lib I'm developing in Python: github.com/leopepe/GOAPy I will try to review and make any comment if pertinent. :) 
Wow you went much farther than me :) . I will definitely take a look.
&gt; We don't cache whole paragraphs of text rendered to pixels What stops WR from doing this when appropriate? More generally, what stops it from "pre-compositing/pre-painting" the layers on a page that *aren't* going to change and caching the result of that operation, so that fewer layers are involved in the final blending step? (This assumes that the final blending is associative, of course. But that's true in many cases.)
Just saw this comment. I am also working on a project called Grust which is in-memory graph database for Rust. I should go change the name. :)
It's an unfair representation to suggest that's all that front end developers do. 
Are there any plans to "fix" this, or is this the expected semantics of Rust?
Oh right, I forgot four million trackers, chat windows that run an n^2 algorithm on every character entered in the conversation history, and requiring websockets for features that worked perfectly well without even any javascript. Oh yeah, there's also unnecessary hover-over menus that don't align properly so they vanish when you try to move your mouse into the submenu in the deeply nested menu with one item at each bottom node.
I mean, okay? So what you're saying is... 'we can do website more minimally, so therefore that's the standard we should live up to, so therefore let's not bother improve web browsers'? Seems a pretty counter productive point, the fancy animations CAN add things to a page and it's design. For an example of something that just seems surface level but can really help users understand the page's flow and navigation, take a look at 'hero elements' - https://github.com/lkzhao/Hero That's an iOS example but there are similar things for webapps. Again, not really sure what you're trying to say, seems like you just want to jam the 'webapps are too slow!' mentality into anything you see.
Unfortunately the peril of code reuse is the temptation to never read the source and just assume that the library author did the legwork for you already. This crate uses `std::thread::sleep` in the most straightforward way. That in turn [calls nanosleep repeatedly until it always wakes late](https://github.com/rust-lang/rust/blob/master/src/libstd/sys/unix/thread.rs). *How* late depends on the system, I need to write a test case.
That may already be true on a modern computer, even with basic HTML. A lot of work goes into text rendering, correct styling, OS-level interrupts and context switching, etc.
&gt; Webdevs (at least for the pages I visit) don't seem to show any awareness at all of layers and compositing It's a real issue to learn this though. Understanding what the browser is doing is a real black art filled with old wives tales on what's fast, what's slow, and why. You also have the impact of reflows intermingled with rendering.
Yeah, it wasn't really rehearsed, and I'm a few months removed from actively developing it so sometimes it took a bit of time to recall my thoughts. Talking to myself into a mic is also a pretty new and awkward experience for me, definitely need to work on it :)
I'm just about to start writing my own NES emulator, do you have a repo on GitHub ?
Yep, links to the GitHub repos are in the video description, but I'll put them here as well: - https://github.com/bgourlie/rs-nes - https://github.com/bgourlie/rs-nes-debugger-frontend
Oh yes, often watching videos I tend to forget that it must be really awkward to speak alone in front of a mic if you're not used to!
Thanks !
I don't know how you'd know that something is not going to change. That said, I believe we may be able to reuse more of the intermediate state created on the GPU side over time. I think the reason we haven't yet is that there is lower hanging fruit to optimize still.
Yeah, if you look at the commit history you can see an earlier version where the algorithm is still coupled to the graph. I split it up because I was planning to apply it to a different problem but then I got distracted...
I can't speak with full authority here, but I'm pretty sure the behaviour of the type inference is not strictly specified. It should obviously not degrade and some features interact with it (such as associated types), but improval can definitely happen. Some of the features for the currently ongoing ergonomics initiative can be considered in that realm.
I believe in any case where you would overload the GPU with WR, the CPUs would have been screaming even more without it. In theory, we shouldn't have to process anything non-visible, so you'd have on the order of a full screen's worth of things to paint no matter how many open windows you had. You'd also have to assume that each window was not idle and had page content that was stressful for WR. I'm sure you could construct such a case, but it doesn't seem very likely to occur naturally :)
In my case it is to not have to shut down Firefox when working on my laptop in places where I have limited or no ability to charge it. Firefox is the by far the largest battery drain unless I am compiling a large project.
It is unfair to suggest that's all that front end developers do. But nobody suggested that. The comment you're replying to listed things that are done, without quantifying how often they are done (let alone that the author claimed that it was what front end devs do full-time). There is no mention of anything being "all that front end developers do" in this entire comment thread. The closest to it is the grandparent-comment, which states that "most frontend web developers and web content creators" are lazy. Lazy is defined there as being something completely different from the remark you were replying to, though. So don't get offended by something nobody even said. Instead, if you identify as a "front end developer", take the feedback you were provided with: people dislike it when an application bogs down their computer. People dislike 'fancy' user interfaces that slow things down. People dislike 'fancy' user interfaces that get in the way. People dislike 'fancy' user interfaces that mess with accessibility features. People dislike trackers (for privacy concerns, for performance, for battery life, for use of metered bandwidth, etc.). People dislike user interfaces that use 'fancy' but fallible features where more stable features would do just fine. This might be as hard to swallow for a front end developer as the obsolescence of bit-twiddling knowledge for desktop applications is for back end developers, but it regardless is the truth.
I've put my actual code on pause for my game http://github.com/agmcleod/ld39, and have been working on design and a project plan. Been thinking through various details like upgrades, costs, etc. As well as new systems and mechanics to introduce. I feel like I can start figuring out a roadmap, and figure out the code architecture so I don't have to refactor for each change I make.
Eventually, Cargo is going to grow support for "public" vs "private" deps, I'd imagine this would tie into that in some ways.
&gt; I can't speak with full authority here, but I'm pretty sure the behaviour of the type inference is not strictly specified. Yup: https://github.com/rust-lang/rfcs/blob/master/text/1122-language-semver.md &gt; ### Underspecified language semantics &gt; &gt; There are a number of areas where the precise language semantics are currently somewhat underspecified. Over time, we expect to be fully defining the semantics of all of these areas. This may cause some existing code -- and in particular existing unsafe code -- to break or become invalid. Changes of this nature should be treated as soundness changes, meaning that we should attempt to mitigate the impact and ease the transition wherever possible. &gt; &gt; Known areas where change is expected include the following: &gt; &gt; ... &gt; &gt; * Some details of type inference may change. For example, we expect to implement the fallback mechanism described in RFC 213, and we may wish to make minor changes to accommodate overloaded integer literals. In some cases, type inferences changes may be better handled via explicit opt-in.
Exactly
hey u/kibwen, cool article and project! I've recently begun work on a [library of audio analysis functions](https://github.com/meyda/meyda-rs) in Rust (it's a port of a JS library I co-wrote). It's sadly not quite ready for prod yet, but I thought you might find it interesting ‚Äì and if these functions are useful to you, perhaps we could figure out a way of publishing the finished ones as a crate.
I might have ported it to Redox: https://github.com/redox-os/rs-nes Pretty awesome!
OS: Laptop with Linux 4.13.4-1 with Arch Cpu: Intel i7-6700HQ Ram: 16 Gb of DDR4 at 2133 MT/s. It is a referbished and discontinued MSI gaming laptop I bought about 9 months ago. The CD drive broke after one use and I had already voided the warranty to install a ssd (It came with a 5400 RPM plater drive). I'd recomend against a MSI laptop. Keep in mind that gaming laptops are now worth the cost (neither are Macs though) and if I didn't use my phone for a home internet connection (meaning there isn't internet at home if I'm not there) I would probably just ssh to a desktop with a netbook. 
&gt; https://chat.redox-os.org/files/p8q3h86h83nm7b7jz4w4fczjac/public?h=kfVAXfoWmcOCkbpotnk57uoSz6ldRvWKN-ihJaomAxg That is super cool!
Browsers already composite the entire screen if only a small part of it changes. WebRender only adds a small amount of vertex shading time.
Not technically *written* in rust but I figured it would be useful for a lot of Rustaceans (plus, we all like tooling that yells at us so).
You might also want to also look at my A* implementation from the `pathfinding` crate (https://docs.rs/pathfinding/0.2.2/pathfinding/fn.astar.html, you have a link to the source there).
[removed]
I've also been looking into rust/elm as a stack! I'll have to check this out. I think they work well, with the common focus on safety, and having ideas from ml.
I'm actually not the author, I just posting this because I thought it was interesting. :) I think the author might be /u/tzaeru.
I don't identify as a front end developer and I'm not offended. And that's not what I said either. But I still think this discussion is at least skirting the edges of negatively stereotyping a whole subgroup of developers that is at the forefront of what's possibly the fastest changing development area in history. Even if you ignore JavaScript framework churn (which is a reasonable concern), webbrowsers are changing with significant regularity. 
Thanks for the updates! I appreciate the amount of work that it must take to make the shoutouts list, but it's very valuable to be able to get a summary of what's going on behind the scenes. :)
No please, you‚Äôre welcome to keep the name! I‚Äôll likely change it down the road as I figure out exactly what direction I want to take this library
https://en.wikipedia.org/wiki/Jevons_paradox
**Jevons paradox** In economics, the Jevons paradox (; sometimes the Jevons effect) occurs when technological progress increases the efficiency with which a resource is used (reducing the amount necessary for any one use), but the rate of consumption of that resource rises because of increasing demand. The Jevons paradox is perhaps the most widely known paradox in environmental economics. However, governments and environmentalists generally assume that efficiency gains will lower resource consumption, ignoring the possibility of the paradox arising. In 1865, the English economist William Stanley Jevons observed that technological improvements that increased the efficiency of coal-use led to the increased consumption of coal in a wide range of industries. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Again, this is a very negative stereotype. I feel this is not worthy of the rust community which tries to be inclusive and not become a segregated echo chamber. Do webapps do inefficient things sometimes? Sure. Do backends contain unmaintainable low level code sometimes? Of course. But let's refrain from slinging poop. 
&gt;Anyone who sets up a UI and doesn't check if it's using .5% CPU or 50% CPU is lazy, yes. [Only 50%? It should be higher!](https://www.theverge.com/2017/9/26/16367620/showtime-cpu-cryptocurrency-monero-coinhive)
Why not just use a raw pointer? #[repr(C)] pub struct SignalBuffer { len: usize, } 
Release early!
That doesn't help with creating a SignalBuffer on the Rust side with a given len, right? I found a way by using their signal_buffer_create method instead, so for now I'm good.
Awesome! Thank you Does that mean that an idle page with no changes will not be triggering webrender?
Spacemacs is trying to find your rust source code to use for `racer` autocompletion. As the error says, you need to set an environment variable to point to your `rustc` installation. In fact, it looks like it is already set, but presumably pointing to the wrong place. Which is strange because it looks like it is pointing to the correct place. You will need to dig around a little to find out where the source is actually located (if it is installed at all - if not, rustup can do it for you). The [racer docs](https://github.com/racer-rust/racer) have more information. Spacemacs is my preferred Rust IDE, once properly configured it works very nicely!
Thanks. I'm giving it a try atm, it seems to work well enough out of the box. So far I'm only missing a few things, like ctrl + w from IntelliJ to iteratively select elements. I'll report back. üòÉ
I'm not proficient in Rust, but doesn't `max(x - 1, 0)` underflow for x == 0, as x is usize?
I don't have exact numbers at hand, but the Python version, packaged with cx_freeze, was around 50 megs, vs. the Rust version a few megs. Performance is also slightly hard to compare in that I didn't do a 1:1 copy, but suffice to say, you don't really see it near the top of the process list anymore, no matter how many active connections there are and how many inputs you are listening to. But a few things were done quite a bit smarter.. Stability is also well better. While doing the reimplementation in Rust, I actually found out several potential race condition bugs that were happily causing crashes once in a blue moon in the Python version! I could maybe write a small post about a few of them.. None were particularly complicated and are pretty easy to spot and understand once pointed out, but yeah, it's way too easy to get sloppy having no compiler backing you up.
Release often!
That might be very relevant, yes. :) I'm hoping to add at least somekind of a beat detection and some spectrum data, for which I'd happily use pre-existing functions. My long term goal, on my hobby hours, is to try and keep this project at least slightly active, so it can be used as a basis for plugging in a library that makes it slightly easier to get some audio stuff out than manually combining PortAudio, individual analysis functions, etc.
Diesel works with mysql
I see, thank you!
Well, on one hand, sure, "no open source contribution too small", but on the other hand, each trivial crate added to crates.io *does* increase the noise level. So ask yourself: would you personally take a dependency on somebody else's crate that provides such functionality, or would you just copy-paste the code? Is documentation good enough? Do you intend to maintain it long term?
If you are planning to release it **eventually**, you should release it **now** as version `0.0.0` to reserve the crate name. Don't worry about pollution - until you make an official release it won't get many downloads so it won't appear in the first page of searches(unless they are specific enough) and won't annoy people.
When you think it will be useful to someone else!
Alternatively, you can set in your ~/.spacemacs a variable to point to your rust directory, like so: ``` (rust :variables racer-rust-src-path "/path/to/rust/src") ``` That would go in your `dotspacemacs/layers`.
I started developing my own NES emulator but found that the documentation for the PPU was much less detailed than that of the CPU. What resources did you use when writing your PPU module? I should mention that I was attempting to write the entire emulator only referencing documentation and no pre-existing code.
If I just read the `main` function, it looks like `path` could be a `String`. Even though there's enough information available to infer the type, a human reader has to specifically notice it. So that specific example has poor readability. I like the current inference rules. They're a little bit dumb, but if I come back to my code later and need to read it *I'm* a little dumb too. 
As soon as you're willing and able to handle bug reports and pull requests from strangers.
I liked the comparison against other languages for its brief and pragmatic nature.
Depends what you want it *for*. The fast version is good for filling up disk, e.g. to overwrite old data. Not as good as randomized shredding if you're really worried about someone analyzing your disk platters, but not useless.
&gt; I actually found out several potential race condition bugs that were happily causing crashes once in a blue moon in the Python version! I could maybe write a small post about a few of them.. Please do! It'd be amazing to have more blog posts about detecting real-world race conditions using Rust, especially in a language like Python where the risks of concurrency aren't as outwardly apparent compared to e.g. C.
/dev/zero?
As a follow up to this, do I have to create seperate impl My_Struct {...} sections for each pool of fn's required by different Traits, or can they all just live in one big impl statement?
The size is not a problem I think. I have implemented basic functionality and it's already a few hundreds lines of not-so-trivial code. Rust macros can be... surprisingly difficult to use due to some of their limitations.
Have a look: https://github.com/matthiasbeyer/is-match A crate with exactly one mini macro. Thus: release early, release often (like others have said already)!
When I have this problem I just need to do `rustup component add rust-src`.
Rust doesn't have a direct equivalent to C structs that use unsized arrays. I believe their memory layout is supposed to be equivalent to struct signal_buffer { pub len: usize, pub data: [u8; N] } except that N is a dynamic value that isn't part of the type signature. If your buffer is always the same size, you could hard-code that N to get a single struct with the correct layout.
I've run into a similar issue trying to make NES games. The nesdev wiki tends to be one of the best resources. Like this page dedicated to the PPU: http://wiki.nesdev.com/w/index.php/PPU One resource linked from there is the Visual 2C02 page: http://wiki.nesdev.com/w/index.php/Visual_2C02 It's a hardware simulator for the PPU based on extracting the transistor network from real PPUs. It should allow you to introspect the ground truth on how the PPU operates at a fundamental level.
I've started working on Rust debugging. First I'm finishing up some patches I have to make it possible to inspect trait objects in gdb. Then, I am going to start work in earnest on Rust support for lldb.
My criteria is very simple; when I want to re-use code from another project. That's it! Release early, release often.
sounds great!
Just wanted to join the choir and say "awesome work!" I'll be pouring over your code for inspiration. 
Implementing the PPU was a battle of attrition. What makes it particularly tough is that the most comprehensive source of [technical documentation](http://wiki.nesdev.com/w/index.php/PPU) is really difficult to comprehend if you lack a high-level understanding of how the PPU works, and it wasn't until I ran across [this blog post](http://www.dustmop.io/blog/2015/04/28/nes-graphics-part-1/) that I found a good high-level explanation with visual aids. It's a 3 part blog post, and I highly suggest reading all of them. Even if you're not creating an emulator, it's a super interesting read. The other thing that makes implementing the PPU difficult is that it takes a long time before you actually produce anything tangible. The approach I took to overcome this was to implement one small thing at a time, in its own module, and write tests to verify correctness the best I could according to the documentation. This gave me a feeling of accomplishment since I could consider some small part of the PPU "done" while also feeling reasonably certain I had implemented it correctly. I also leveraged StackOverflow and the NesDev forums when I had a question I couldn't find an answer to: - [Confusing byte format in APU docs](https://forums.nesdev.com/viewtopic.php?f=3&amp;t=15846) - [Sprite evaluation timing clarification](https://forums.nesdev.com/viewtopic.php?f=3&amp;t=15770) - ["If rendering is enabled..." clarification](https://forums.nesdev.com/viewtopic.php?f=3&amp;t=15708) - [Is it possible to procedurally determine the number of cycles a particular instruction takes on a 6502?](https://stackoverflow.com/questions/41255275/is-it-possible-to-procedurally-determine-the-number-of-cycles-a-particular-instr)
I wonder how many of these games accessed out of bounds memory accidentally but it worked because the compiler didn't correct it. Basically wondering if using rust creates problems because games can have unsafe code.
But Elm has nothing like traits/typeclasses so it really limits your abstraction capabilities. When researching the options, I also considered Elm at first, but then I got into PureScript because it has a more advanced type system and better JS FFI, and The Elm Architecture‚Ñ¢ is very restrictive, and Elm can only be used for full webapps, not components. GHCjs apparently isn't as mature as PureScript and generates huge amounts of js, whereas PureScript generates lean code.
Will rustc ever get smarter so that it can infer such cases?
Yes, it translates everything into linear RGB (with color profiles applied) before blending any pixels.
https://play.rust-lang.org/?gist=54d7f85cf287d6bb2b7488425f346980&amp;version=stable
There is both an official docker image and a docker stackfile for running it behind NGINX. imazen/imageflow_server_unsecured exposes the server directly, and the stackfile for Docker Cloud is https://github.com/imazen/imageflow/blob/master/ci/docker/hub/proxied/stackfile.yml 
Imageflow is now mostly Rust. The jpeg and png codecs are still in C, but that's likely going to change. The plan is have a decode path that is 100% Rust, and extremely resistant to malicious files. 
Ok so I still have to impl Trait for Struct twice, but I can just reference self.method inside both implementations
Relevant: https://xkcd.com/676/ -- even mentions Gecko!
Alternately: https://play.rust-lang.org/?gist=d9940efbfcac24509f69fba67ee40905&amp;version=stable
Same here, I really like it, though I'm not sure if I can get as productive as I'm in VS Code, for example.
That makes sense, thanks. I assumed it would install it into the right place so it can be found.
If the solution with rustup doesn't work, I'll try that. Apparently I've ruined my spacemacs by installing source code pro, though. :P
Thanks, I'll try that if I can manage to uninstall Source code pro... apparently it killed my spacemacs.
Why `Printable::summary`? It feels more natural to have `Printable::print`. https://play.rust-lang.org/?gist=a4014d27889ab40bf1a57463595592a5&amp;version=stable Traits are not interfaces. Trait methods are not requirement but features the trait adds to the type. `impl Trait for Type` doesn't add methods to the type, but it adds implementation of the trait specific for the type.
I am a web developer, and I feel this is a fair characterisation of the industry‚Äînay, of human effort altogether. The fact of the matter is that as what you depend on get faster (computers, browsers), what you build gets slower (software, web sites/apps)‚Äîin some cases this is because it‚Äôs now feasible to do things that weren‚Äôt formerly feasible, which is what you‚Äôre describing; but in most cases it‚Äôs because you can simply stop putting in effort that was *necessary* before. Thus, considering websites, you commonly end up with several hundred requests from a page where a hundred of them are foolish, a hundred of them readily reducible (do you really need three versions of jQuery, one of mootools and two of React? and why didn‚Äôt you concatenate all these JavaScript files together or even minify them?) and perhaps fifty were actually useful. (*Perhaps.*) This is not something that is specific to the web; it happens everywhere. Just as a population will grow to the capacity of its food source (and often a bit beyond), bloat will grow until it becomes just *too* slow.
Yeah you're right in practice. I was just giving an example in which two traits required implementation of the same function.
Its possible I guess - I haven't studied the source of many games myself so I couldn't tell you. My actual OOB issue though was a bug in my emulator. At some point (not long after the first "boss"), the first level of Kirby starts to become vertical instead of horizontal. As you jump up and the "camera" moved, thats when my emulator crashed. Anyway it turns out my calculation for the `SCY`/background scroll on the Y axis was incorrect. So thats why this bug only occurred when the Y scroll happened ... and that only happened as the level became vertical. Fun stuff! :D
"Move fast and break things"
Wrong subreddit
You're probably looking for /r/playrust, this is the subreddit for the Rust programming language
I did something similar a while ago. In short, I think this is the right representation: #[repr(C)] pub struct signal_buffer { pub len: libc::size_t, pub data: [u8; 0], } You would then have some accessor function: impl signal_buffer { pub fn data_slice(&amp;mut self) -&gt; &amp;mut [u8] { unsafe { slice::from_raw_parts_mut(self.data.as_mut_ptr(), self.len as usize) } } } Does that help?
Depends who's supposed to own the SignalBuffer. I believe it is generally on c side (so the create method). In this case go for a raw pointer and convert it to a `&amp;[u8]`, else convert it to a `Box&lt;[u8]&gt;` if you don't plan of modifying it's length.
I've done a bit of testing with Rayon and it's definitely the way to go for applications performing these transformations often. I added a benchmarking script here: https://github.com/DaveKram/coord_transforms/blob/master/examples/benchmarking.rs Rayon does really well as the number of transformations scale.
&gt; Budgie Linux Maybe you meant Ubuntu Budgie or Solus?
Running your code under with `RUST_BACKTRACE=full` in the environment means that you automatically get a stack trace whenever a panic occurs.
1. When using Process::Command().new().spawn() does checking the status() happen after the child process has finished executing? 2. What if I check command.status() after child.wait()? 3. In the following code, does the message get printed to the screen after sleep interval because the child blocks? (I'm a bit confused here) let mut cmd = Command::new("sleep"); let mut child = cmd.spawn().expect("Failed."); let status = cmd.status(); println!("cmd status: {:?}", status); child.wait(); https://play.rust-lang.org/?gist=87fe3306f76bd3147ea1a468228332cf&amp;version=stable
Each trait is a self-contained set of methods. It's like having a set of metric wrenches and a set of inch wrenches. You can write generic procedures that work with any kind of wrenches, but to actually use them you need the type of wrenches that match the type of machine you're working on. Well, sorta. If the real world worked like this analogy you'd have `&lt;Bike as Wrenchable&gt;` wrenches and `&lt;Car as Wrenchable&gt;` and so on and on. Rust doesn't try to do complex inheritance hierarchies, so you wouldn't likely have the direct equivalent of metric and inch wrenches. (However, look at discussions of "specialization" for similar features being planned for the future.) The name of each tool (each method) is specific to its toolset (trait). `Wrench::tiny_precise` is a completely different kind of tool from `Hammer::tiny_precise`. And remember, you need a specific `&lt;Car as Wrench&gt;::tiny_precise` to work on a `Car`; this is defined in the `impl Wrench for Car` block. As such it's poor design in Rust to have the same method with the same meaning appear in multiple traits.
An idle page won't trigger layout or rendering.
It does this: let mut cmd = Command::new("sleep"); // spawn child A let mut child = cmd.spawn().expect("Failed."); // spawn and wait on child B let status = cmd.status(); println!("cmd status: {:?}", status); // wait on child A child.wait();
This is a feature of Rust, not a limitation. If two traits happen to have an associated function with the same name, why should they be forced to have the same implementation? In almost every case, the contracts on the two functions would be different‚Äîif they're the same, then why do you have two traits?
You need this: #[repr(C)] #[derive(Debug, Default)] pub struct signal_buffer { pub len: usize, pub data: [u8], } Currently it's not possible to allocate one of these with the stable standard library. I would probably use libc malloc. It *is* possible to create and use slice pointers via an undocumented hack. If you create a `*mut [c_void]` with a length parameter and transmute it to a `&amp;mut signal_buffer`, then `size_of_val` shows that rustc knows the correct size of the allocation { i.e. `sizeof(size_t) + n * size_of(uint8_t)` } 
Hello all, I made some docker images for cross compiling static Rust binaries using musl-cross, hope you'll like it. I am not sure about the openssl cross compiling, so I opened a issue here: https://github.com/messense/rust-musl-cross/issues/1 , would be great to get some help.
I don't think you're crazy, but only because we have cargo. Usually my fears about combining languages in a project is that getting all the package management and building gets very fiddly. Cargo should be able to effortlessly handle all the Rust management for your project. --- If/When you add Rust to the project, you might want to replace some of the C++ bits with it. I remember somebody here was working on an audio player in Rust, maybe they know of some useful libraries like JUICE...
I am trying to validate if actix architecture makes sense, so I am building SockJS server. SockJS is a javascript library that abstracts away real-time browser transports. It uses different server connection types (web sockets, long polling, event source, etc). I hope it would be enough. here is repo https://github.com/fafhrd91/actix-sockjs Any help or feedback would be very appreciated.
Also, what do you mean by "but it doesn't help if the error is coming from a crate that doesn't use it"? Crates that panic when they shouldn't? As I understand it, `error-chain` is specifically designed so that you can plumb it together with dependencies that expose their own errors... it just might not be as well-documented as would be desirable.
Do you plan to implement something like `context.watch` in Akka? (to detect if another actor has terminated)
in actix you have to use `Address&lt;A&gt;` if you need to communicate with actor, it has [`connected()`](https://docs.rs/actix/0.1.0/actix/struct.Address.html#method.connected) method
another option could be special message, something like: use actix::prelude::*; struct Disconnected&lt;A&gt;; // actor struct MyActor {addr: Address&lt;A&gt;}; impl Actor for MyActor { type Context = Context&lt;Self&gt;; fn started(&amp;mut self, ctx: Context&lt;Self&gt;) { ctx.watch(addr); } } impl&lt;A&gt; Handler&lt;Disconnected&lt;A&gt;&gt; for MyActor { fn handler(&amp;mut self, msg: Disconnected&lt;A&gt;, ctx: Context&lt;Self&gt;) -&gt; Response&lt;Self, Disconnected&lt;A&gt;&gt; { } } &amp;nbsp; and watch method could be defined like: impl Context&lt;A&gt; { fn watch&lt;B&gt;(&amp;mut self, addr: Address&lt;B&gt;) where B: Actor + Handler&lt;Disconnected&lt;B&gt;&gt; { ... } } I need to think about that :) should be easy to implement.
Fucking sweet! Thanks for posting! I‚Äôm trying to get Vagrant boxes working for macOS, Windows, and FreeBSD to help compile non-Linux Rust targets, but it looks like Vagrant stuff is buggy and dying compared to the new hotness of Docker :/ If anyone DOESN‚ÄôT get tons of errors with Vagrant boxes for these operating systems, please post your stuff!
Is there an option to panic, or similar function like panic, that guarantees a stack trace is printed? Sucks to have to wrap every Rust app in a shell script to ensure stack traces.
&gt;another option could be special message In Akka, you receive a `Terminated` message when the monitored actor is... terminated. It is very useful when you distribute a task in multiple child actors, and want to detect if any of them fails. Another common use case (in my code) is to create an actor to clean up resources when a set of actors are terminated. The `connected` method is useful for debugging, but for supervision you may need support from the actor library. TBH, supervision is one of my favorite features of both Akka and Erlang, and it is frequently ignored by other implementations :(
I am not sure how this could be related to rust implementation. we don't have exceptions, so if actor get terminated it could be panic and in general way, handling panic is not good idea or actor itself stops execution and in that case it can send response for the message. whole supervision idea needs some thoughts in rust context.
I haven't worked with `error-chain` a lot, so please correct me if I'm missing something or just wrong, but from what I understand, its while philosophy of "keep all errors" is realized through its error types' [`chain_err`](https://docs.rs/error-chain/0.11.0/error_chain/example_generated/trait.ResultExt.html) method, which does the plumbing that composes new errors with their causes, so that by the time it terminates, you have a `Backtrace`, something like the back traces you'd get on a panic. But if one of your dependencies doesn't use `error-chain`, then whether or not you wind up with the same full context depends on whether the crate author was a good citizen and composed their errors correctly with a `cause`. And even then, I think you'd be missing the full back trace if the error originated deeper than the crate you called into. I could see this being problematic for complex dependency chains.
Yeah, but panics should hopefully be the rare case. I'd love to be able to print stack traces like that inside regular error log lines.
Edit your .spacemacs with another editor. Change Font to whatever monospaced font available in your linux machine
That makes sense, if you want to define two variations of the same function based on the Traits required of it. If two Traits require the same function it would be nice to do this: impl Trait1, Trait2 for Item {...} 
[removed]
&gt; I will try to review and make any comment if pertinent. It will be great. I need feedback on real usage of this stuff :) And any suggestions about development is highly desirable too. Looking forward to your comments. Thanks a lot.
Error chain's a bit different--it's making a causal trace of the errors in your program. Each error has its own backtrace; together, you can watch how individual issues cascade across threads, modules, etc--error_chain complements backtraces by giving you a developer-defined trace on what happened.
It could, but that would involve more looking ahead, which the rust developers usually try to avoid, since it carrys a heavy price in compiler complexity and compile time.
How is that any different than in an Exception-based language, where a dependency catch an exception from one of its own dependencies and not preserve the deeper details when re-`throw`ing?
Slim is my favorite when using Ruby, there is an impl in JavaScript, https://github.com/slm-lang/slm
&gt;Yeah, but panics should hopefully be the rare case. So if you have a test environment, introducing a panic if a certain assertion doesn't hold seems to be a sensible solution. Apart from that, Rust's panic facilities build on libbacktrace, which does a bit of plumbing to set up the stack traces. I don't know whether one can directly use it to get a stacktrace from running code, but it seems worth a try.
This is really cool -- I wonder if rustup/cargo should gain the capability to route calls into docker containers... would be a pretty elegant approach to support all those "weird" combinations.
Similar project I've been using for a while now: https://github.com/clux/muslrust
One neat pattern to implement this is shown here: https://doc.rust-lang.org/std/thread/fn.panicking.html Basically, you use a stub struct with a drop implementation. When the thread panics, you can notify a global supervisor which takes appropriate actions (restart the thread, notifiy other actors, ....)
Is it not possible to pass information to LLVM via attributes or something that says "don't even attempt to optimise this code block", or something? So as to achieve constant runtime?
Try to document it properly, and explain what it does, with enough detail that an interested stranger can pick it up and use it easily. If you find it hard to explain what it does, it's probably too early ;) I randomly browse doc.rs and it's shocking how many crates don't have crate-level docs. People seem a little scared of `//!`.
This project looks great! But it only provides x86_64-unknown-linux-musl target, rust-musl-cross project supports more targets, currently: * x86_64-unknown-linux-musl * i686-unknown-linux-musl * arm-unknown-linux-musleabi * arm-unknown-linux-musleabihf * armv7-unknown-linux-musleabihf * mips-unknown-linux-musl * mipsel-unknown-linux-musl aarch64-unknown-linux-musl isn't supported for now because stable Rust don't support it right now, but it should be supported soon as nightly Rust already supports it for some time now. 
Yes, I tried it for my android app (sound generator, mix about 20 streams in realtime) and the tooling drove me mad (android studio). On top of that, the fact that I have to drop to unsafe very often, haven't found a rust queue that behaves like moodycamel's queues (no allocations on the audio thread) and that I still have to do awkward FFI to my AAC decoder led me to go pure C++ for the audio layers. I will gladly rewrite the whole thing in rust once I have found pure rust crates with: * hardware-accelerated / SIMD DSP operations * lock-free, allocation-free spsc queue * AAC decoder * Better ARM SIMD support If I have to do interop between Obj-C / Java =&gt; Rust, I really don't want to do C / C++ bindings in rust. 
 buf = 'y\n' * 1024 while True: print buf python yes.py| pv -r &gt; /dev/nul [1.38GiB/s]
This is on my 5 year old HP ProBook, with a Intel(R) Core(TM) i5-3210M CPU @ 2.50GHz processor. 
Ah, I see I ws pipped to the post. You're taking the same basic approach as me (using a big, pre-filled blob of 'y\n') but you're also using python 3 (mine was on py2.7) and writing directly to the underlying file. The original is basically torture-testing the write() system call with tiny amounts of output. By outputting larger blobs, you simply reduce the round-trip cost of the handling and calls. Either way, it shows you don't really need to go to all that extra trouble - even *interpreted* languages can beat C/Rust when its making too many system calls.
&gt; Compiles with stable rust That this was called out as a feature makes me nervous.
If not this, then something official. rustup being able to add libstd for targets is not enough when you also need platform specific cross tools like a linker. Ultimately not even depending on libc would be ideal, but that'll take a long time.
In an exception-based language, there must be code to do the bad thing: Catching and not re-throwing correctly. With `error-chain`, there must be code to do the right thing. If omitted, all you have is "file not found" _somewhere_ in your code.
Sentry uses Rust along with Python, maybe they supprt Rust as well‚Ä¶
Very nice! I wonder if this will help with canvas performance too.
&gt;Apart from that, Rust's panic facilities build on libbacktrace, which does a bit of plumbing to set up the stack traces. I don't know whether one can directly use it to get a stacktrace from running code, but it seems worth a try. It looks like that is what error-chain does here: https://github.com/rust-lang-nursery/error-chain/blob/master/src/backtrace.rs
In an Exception-based language, if you fail to handle it, it blows up at runtime. In Rust, if you fail to handle it, your program either fails to compile (forgetting to `unwrap` or what have you) or you get a compile-time warning because `Result` is marked `#[must_use]`. I understand what you're saying, but I'll take "As long as I refuse to use things like `unwrap`, I'm not introducing any unexpected control flow paths" over "If I miss an Exception or forget to catch the root of the type hierarchy, I get a surprise program-ending backtrace" any day. (Though I will admit that I'm still looking forward to the day someone with more free time than me decides to write a tool which will scan a codebase and its dependencies and report all un-whitelisted lines in the top-level project which can panic.)
I already submitted the crate by the time you posted but I opted to document most things on crate-level and not on item level. I've made a few errors in docs I realized post-release but I hope [documentation](https://docs.rs/typeparam) is clear enough.
Sorry that was not my point. I wasn't talking about panic at all. I meant: If you do the minimal error handling, which is to use some minimal error type (for some crates, std::io::Result is enough) and just use `?` everywhere, you won't get a backtrace into that crate. Still, such code is idiomatic rust, I would say. My point was just: In exception-based languages, stack traces are default unless you do somehting stupid (and tooling will most probably warn you). In rust, stack traces are opt-in and might not be deep enough.
They do: https://crates.io/crates/sentry I've used it. It works pretty well and was straightforward to integrate.
Not really, anything working with futures could easily be utilizing something from nightly
Add support for the target to ldd and make rust use that :). That's much better than messing around with virtual machines and stuff.
Very clear - it's an elegant way to use clap!
Rust traits might appear similar to for example Java interfaces, but they are fundamentally different. Traits are more a way to describe properties of a type, rather than describing a subtype hierarchy. See [this discussion](https://users.rust-lang.org/t/extending-traits/1802) for example.
Here's an assembly implementation in 376 bytes: https://github.com/faissaloo/yes
I already reinstalled Budgie.
Yeah, [Ubuntu Budgie](https://ubuntubudgie.org/).
Former Java coder here, worked many years on backend REST services. You know the old joke? The JVM transforms bytecode into stack traces ;) Honestly, good strack traces are great and in most exception-based languages, you get them for free. Java really shines there. (As do C#, Python, etc. I guess but I have much less experience with these). The only thing that is really annoying is with HJav Rust stack traces are much more cumbersome. You have to use error-chain, you have to supply "RUST_BACKTRACE" in the environment, and if I understood this correctly, the backtrace can technically only start when the error is turned into a an error-chain error. So unless you call `.chain_err()` all the time, you might only have function-level accuracy for the "code that first encountered the error". This can or can not be an issue. Having said that, and having rust code in production in the backend for about half a year now, there are really few cases where I encountered issues in production. I haven't seen a single `panic!` coming from other crates and the tools that rust provide you with really help avoiding a lot of error classes that you might have in other languages. The only errors I encountered in production were of the "multiple os threads interacting on files in ways we should have thought through more thoroughly", which I don't think any language that I know of can prevent. Also, _always_ knowning what error a function can return in pure gold! With rust, I have so much more trust in what the code is doing. No null pointers, no RuntimeExceptions/Errors, you know exactly what you're dealing with. That makes it so much easier to trust in your code. So bottom line: stack traces are not as good as in other languages, but errors are much less frequent. And apart from strack traces, the error handling concept in rust is just so much better (but a bit more cumbersome sometimes).
For this reason I, e.g., always enable `RUST_BACKTRACE=full` e.g. on travis-ci. On production, I only enable it during debugging.
Why aren't the types in `ResponseType` just tied to the `Handler` trait?
Well then just implement it in a trait method, and call this method in both trait implementations. The compiler can't know whether you want both implementations to be the same or not.
One impl per trait.
Kudos for working on non-Linux targets. Perhaps my main gripe with Rust right now is the limited list of tier 1 platforms. Would love to see a service like Travis CI that allowed projects to easily test on the BSDs.
I disagree with this. Don't claim names before you have something to put there.
Did you ever consider not reusing actor terminology? This is way different from Hewitt‚Äôs actor model and more like Erlang, which doesn‚Äôt describe things with this terminology. Hewitt himself doesn‚Äôt believe Erlang implements the actor model.
&gt; should have thought through more thoroughly The joys of the English language ‚Ä¶
Hello, we make heavy use of Rust in production at Faraday.io, and we find that error reporting is actually _great_, for the most part. Our rules of thumb: 1. _Always_ turn on `RUST_BACKTRACE=1` for all our programs. 2. _Always_ use `error-chain` in our code. Sure, this doesn't tell us exactly where errors occurred in other people's code, but we don't really care‚Äîif we can track the error from the line where it reaches one of _our_ crates, then we're totally happy. Also, `error-chain` automatically includes backtraces. 3. _Always_ use `env_logger` and log extensively. The `log` ecosystem is really amazing, and a huge number of Rust crates have extensive internal logging that we can enable. If we do this, then we get full backtraces for all errors, and fully-configurable logging. Seriously, I don't know how anybody does production Rust without `error-chain`. As for production debugging, it's nuisance, but that has nothing to do with Rust. We run everything distributed, inside Docker containers, and so the best we can hope for is to `docker exec` somewhere and attach a debugger if something goes seriously weird. But we've never needed to do that for Rust‚Äîwe've always been able to find the problem by looking at the logs, maybe adding a bit more logging, and then thinking hard. (Rust has surprisingly good `gdb` and `rr` support, though.)
I've been happy with it as long as I'm on Linux. But my primary machine is on OS X, and [this bug](https://github.com/rust-lang/rust/issues/24346) is the worst thing in the world if I have to debug a panic.
It sais *unsecured* though. My guess is that this is not intended to be used in production as is? Does anyone know what measures are needed to run this safely in production? I searched around on the github page and on www.imageflow.io which didn't bring me much further. I'm not sure if I'm missing something? The documentation seems pretty slim for the overall complexity of this application. Maybe that's the intention to sell *Platinum Plus Integration Contract's*?
I would like some middle-ground of debugging symbols. `cargo build --debug` programs run *way* too slow and `cargo build --release` programs much too little debug information. It should be possible to have a middleground where there's only 5-10% performance hit and much better (maybe function-level) backtrace.
Thanks, that makes it clear. 
Yeah, that solved my problem. Thanks!
I hope not! This opens the door for unreadable code, imagine that the handover to the struct only happens 10's of lines further. Then it becomes very difficult for a human to understand it while going back and forth. 
Have you seen [cross](https://github.com/japaric/cross)? It handles pulling docker images and orchestrating your build inside them. There's default images it will use, but you can easily override them in your `Cross.toml` file.
Ok(Ok(Ok(Ok(Ok(Ok(Ok(Ok("?"))))))))
Osx does something weird making devs life hard. News at 11.
:)
How can I call from Python my Rust function with an array (with known size) as argument and an array (with another known size) as a return ? The solutions I have read always consider taking a pointer to a value and a length to make it work but it failed for me because I could not succeed to pass two arrays to my Rust function (although I succeeded with one). &amp;nbsp; Here is my current code that throws a "Aborted (core dumped)" : #[no_mangle] pub extern "C" fn print_x(n2: *const f32, len2: size_t,n: *const f32, len: size_t) { let x = unsafe { assert!(!n.is_null()); slice::from_raw_parts(n, len as usize) }; println!("{}",x[1]) } With the Python code : input_arg=ffi.cast("float *", list_x[0].ctypes.data) output_arg=ffi.cast("float *", np.ones(12).astype(np.float32).ctypes.data) C.print_x(input_arg,output_arg) So it should normally output 1 but instead it panics because I am giving the two arguments. Thank you.
It is an important criteria for production systems.
&gt; Seriously, I don't know how anybody does production Rust without `error-chain`. I strongly recommand against `error-chain`. I explained my reasons [there](https://www.reddit.com/r/rust/comments/70t099/whats_everyone_working_on_this_week_382017/dn64em6/) and my opinion about `error-chain` changed indeed because I used it in production.
I was thinking of initially rewriting some of the core audio processing in Rust as a crate, building as static library and calling it from C++. Although that adds more than one extra step to my build process... There are a few audio processing crates around but nothing near JUCE which represents over 10 years worth of cross platform C++ code. 
Stable vs nightly is a huge and relevant distinction in every Rust project at the moment -- lots and lots of really popular libraries rely on nightly.
How would you use channels for asynchronous programming? I can see how you'd use it for parallel processing. Sending data / work between threads, but that's not the same as asynchronous programming, where you could end up doing everything on one thread.
Maybe it's just a cue that you need to give that function its own Trait?
Have you looked into what's required to build a C ABI library with cargo? It's almost effortless. 
&gt;If I have to do interop between Obj-C / Java =&gt; Rust, I really don't want to do C / C++ bindings in rust. I don't see how that follows, can you explain? 
You already get function-level backtraces even in release builds. They're hard to read because functions are inlined into each other, which merges otherwise distinct stack frames in the backtrace. Disabling inlining will probably cost you far more than 5-10% performance.
I think they're saying that the libraries they want aren't available in Rust yet, and thus if they were to use Rust they'd be binding to C / C++ libraries, which is too much inter-language bindings to write for their liking.
Asynchronous means independent timing or "don't wait for it". Async can certainly be done on the same thread. It is not the same as being concurrent (" parallel "). You can have all four possible combinations. Futures is about async model, tokio is about the threading model (shared threadpool). Channels are also about async, how you process the receiver's message is up to you. For a lot of cases, spawning a new thread for every message loop is perfectly fine. Depending on the OS, context switching overhead only becomes significant past 1000 threads. Once you reach that, tokio's complexity starts making sense.
I don't have much of a strong opinion on `error-chain` myself, but [from the looks of it](https://github.com/rust-lang-nursery/error-chain/issues/129), it was determined that the performance penalty you mentioned was MacOS-specific, and not due to `error-chain` per se, but `coresymbolication`.
I have not yet found the time to implement RFC #1909 ‚Äì I want to set up a proof of concept so we can iterate on the design.
I don't know why you necessarily need an array when Matrix4 gives you access to whatever you could need, but ok. You could make a macro for this: macro_rules! to_array { ($matix:expr) =&gt; to_array!($matrix, x, y, z, w) ($matrix:expr, $($params:ident),+) =&gt; { [ $([matrix.$params.x, matrix.$params.y, matrix.$params.z, matrix.$params.w]),+ ] } } and you just call `to_array!(matrix)`. Or you face the hard fact that there isn't much you can do to make it prettier: [ [matrix.x.x, matrix.x.y, matrix.x.z, matrix.x.w], [matrix.y.x, matrix.y.y, matrix.y.z, matrix.y.w], [matrix.z.x, matrix.z.y, matrix.z.z, matrix.z.w], [matrix.w.x, matrix.w.y, matrix.w.z, matrix.w.w] ]
If you lock standard out will this still be useful as a yes program, ie will it prevent the program you're piping to from outputting it's questions? I get it's kind of moved beyond the actual purpose to just a speed competition but seems like a significant departure from the whole purpose of yes.
I dont really have good answer. initially it was part of Handler, but I'd like to simplify `ResponseType`, in most cases it just `Item=(), Error=()`, maybe it would be possible in future to use specialization or proc_macro. this question is still open.
I used stable to constrain myself in design decisions. but good point, I'll remove this point :)
Aren't you missing the `len` values? You're calling `print_x` with only two arguments but it needs four.
this should be possible already. in actix you can wait for response for each message and if actor is gone, you get `Err(Cancelled)` 
In a way, that's a feature, similar to `@Override` in Java, except requiring to specify what trait is implemented near its implementation. Also has a bonus of allowing to implement your traits for types that you doesn't own (builtin, external crates, and so on).
Cgmath has helper functions exactly for your situation in the [conv module](https://docs.rs/cgmath/0.14.0/cgmath/conv/index.html)
In linked docs, there is a impl&lt;S&gt; Into&lt;[[S; 4]; 4]&gt; for Matrix4&lt;S&gt; fn into(self) -&gt; [[S; 4]; 4] so if your Matrix4 is a f32 matrix, it should work !? 
&gt; I don't know why you necessarily need an array when Matrix4 gives you access to whatever you could need, but ok. For sending the matrices to the GPU to actually be used, they must be in array format.
Oh, sorry. Missed that part :P
Yes, that's quite neat! Thanks!
initially I used slightly different terms. naming is hard, what other term could be used that has same meaning?
Yeah, Rust is really cool, but Go‚Äôs builtin crosscompile and shallower learning curve mean I‚Äôm using Go for most things. The performance difference is so small that I‚Äôm not likely to rewrite my stuff in Rust until non-Linux build bots become far easier to setup.
I really like the message handler design - nice work.
That is true. A restrict keyword was added in a recent C standard as well, afaik most C++ compilers also support something similar with an extension. 
/u/exobrain has a new paper!
Agreed. Generally errors fall into 3 categories * try again: in which case you just log the original error, and return an Option::None from what ever _thing_ is abstracting the crate. * total failure of operation: the libs error should be interrupted preemptively to a better more verbose error (and logged), then your higher level error returned. Better to dig in the libs source code BEFORE you break prod. * panic: now the Dev needs to prove a good panic message + original error. Error chain is not an abstraction. Abstraction denotes hiding the complexity of an operation. If you‚Äôre just bubbling up errors you aren‚Äôt hiding those abstractions and libs. 
This will depend on whether `fill_bytes` is inlined or not. If it is inlined, then it should not write the bytes twice, if it is not, then you'll probably end up writing the memory twice. That said: `memset`-ing something to zero is veeeeery fast. Especially when compared to cryptographically secure generating random numbers, I'd be very surprised if the memset there would show up in any profile. Anyway, the easiest way of doing this is probably Key(self.rng.get_iter().take(size).collect::&lt;Vec&lt;u8&gt;&gt;().into_boxed_slice()) I think that optimizes well, but who knows? The definitely fast solution is to use `unsafe` (but please don't if you don't have to!): let mut buf = Vec::new(); unsafe { buf.set_len(size); } self.rng.fill_bytes(&amp;mut buf) Key(bytes.into_boxed_slice()) 
Use `Vec::with_capacity(size)` first -- `set_len(size)` doesn't affect the raw allocation!
I didn't do that because the documentation for `rand` states that `fill_bytes()` has a default implementation based on the usual method but should be overridden with something more efficient, which kind of made me guess that it would be faster than most of the alternatives, including the iterator. "Memset" is the name of the cheat I was imagining? Like, it's doing what you just described--the unsafe thing--behind the scenes and then calling memset on a region of memory to zero it out? I was worried it might be doing what I'd do--like, `iter::repeat(0).take(size).collect()` or something along those lines. &gt;.&gt; I had no idea you could set the length of a vector. [Insert wide-eyed look of wonder here.] Thanks! That was informative. 
What happens when there is a connection failure? Does it try to reestablish the connection? I've been looking for a messaging/RPC/actor implementation lately and I had to reject most of them because they give no attention to network failure.
Why would the actor framework itself have to care about network failure? I've been using my actor library for network communication and it's handled just fine. I assume in actix one of your message types would simply be a result, or, you could handle it via a supervisor.
I tried it both ways. Neither one crashed. That may be random. Do you have any idea why?
&gt; We now run LLVM in parallel while generating code, which should reduce peak memory usage. Huh? Is this different from ThinLTO that just started testing? Will it help compile times? Is it default in both release and debug profiles?
It is different, if you click through to the PR, there's more details: https://github.com/rust-lang/rust/pull/43506
Yes, it's UB. So it might work, it might not, it might delete all your files. 
If only Rust provided some means of marking `set_len` as memory unsafe!
Most actor frameworks include a networking layer or two. If these layers disregard the network failures then you're going to see situations like waiting 20/30 seconds for a failed actor response (30 seconds is the usual TCP socket timeout). And if you have complex actor relationships set up, if you're in a middle of some conplex actor-based state machine, then tearing all that down because of some error might be harder than if the actor library could simply reconnect to an actor. I think the question why is rather naive, actually. When you're building and testing your actor-based software, most of the time you do that without thinking about the network failures. And if the failure comes, most of the time you'll be completely unprepared. The actor framework should care about reliability in order for you to keep flying in the clouds rather than crashing down to the earth of real-world issues *after* your project has been designed and deployed.
`rustfmt`, as published on crates.io, does not need nightly. `rustfmt-nightly` needs nightly. But, yes, the RLS preview hitting stable is a first-step towards bringing the new `rustfmt` to stable.
Argh, I'm an idiot, sorry, fixed!
Regarding the `&amp;5` change: since the constant is now made `'static'`, does it mean that (ab)using this feature too many times, especially with larger types, can lead to inflating your executable size? I'd imagine the previous version being easier to optimise away by LLVM, because the scope of underlying variable is more limited. On a semantic, I'm also curious to see if it helps with [cases like this](https://github.com/Xion/gisht/blob/master/src/args.rs#L286). If I understand the feature correctly, it should!
Yes, `rustfmt-nightly` is the latest release of "rustfmt" the tool, so that's what I meant here.
10 years ago -- *"‚Ä¶ the factors considered back in 2004 as the old Doug Bagley programs were replaced - we figured out that simply looping over old programs wouldn't increase work load in a graph reduction languages like Clean, so we came up with benchmarks that didn't do that. Like doh!"* 10 years ago -- *"What happened is that I [igouy] wrote a Clean program which didn't actually follow the problem description, and the Haskell programs (not unreasonably) took the same approach. Then someone noticed and we fixed the programs."*
I'm not sure we have any numbers on that, but given that it's not public, I'd imagine that the compiler can see all uses and should be able to optimize accordingly.
`memset` refers to a C function (and set of LLVM intrinsics) which sets a region to a particular byte. It generally has a very highly optimized implementation for a given architecture and is therefore very very fast. I'm not certain it does in this particular case, but in general, rust (via LLVM) should be pretty good at optimizing even things like the expression you wrote into a fast operation. 
As the person who initially fought to have `rustfmt-nightly` published instead of just updating `rustfmt` to a codebase that wouldn't compile and function on stable Rust, the distinction is important to me, People can and should still use `rustfmt` on stable Rust, that way they will still be in the habit of using it by the time the rewrite is ready for everyone to use on stable. I just didn't want anyone who might reach for `rustfmt` to be confused into believing that they can't install it yet by your statement there.
It should definitely use `with_capacity`, I just wrote that absent mindedly. It will NOT allocate space, `fill_bytes` will just trample over unallocated memory which may or may not cause a crash, corrupt data etc. But as I said in my originally, you shouldn't resort to unsafe, unless it's definitely a performance bottleneck.
This is, to my mind, the only useful comment in this thread. In Rust, if two traits happen to have methods with the same name, that's just a funny coincidence. They're completely unrelated methods. If you, as the author of the traits, want two traits to define the same method, then do what this example does: define the method in one trait, and use inheritance to include it in the other. Or, if that doesn't work for you, extract it into its own trait, and use inheritance to include that in both of the traits. 
I _think_ for canvas improvements we'll need [pathfinder](https://github.com/pcwalton/pathfinder) which is also going to be used (hopefully) for GPU accelerated text rendering and parts of SVG rendering.
Are you talking specifically about the Actor HTTP library presented or about actix itself? Just so I'm clear before responding.
I wonder if rust or LLVM can detect when a `&amp;5` (or 'an `&amp;5`'?) is only used inside it's scope, and optimize the `'static` away.
I think that's exactly their point - that we feel the need to note a library compiling on stable is an indicator that we still have too many libraries on nightly.
I'm not a compiler hacker, but I believe it works in the opposite way: it may be *promoted* to `'static` if used that way, otherwise, it is not.
Can you check `~/.cargo/config`? My token is stored here instead of `~/.cargo/credentials`.
Doesn't change anything :/
Am i right in thinking that this combination: let mut buf: Vec&lt;u8&gt; = Vec::with_capacity(size); unsafe { buf.set_len(size); } Is safe? This combination of with_capacity and set_len will result in a validly-structured Vec filled with garbage elements. For u8, garbage elements are valid, because every possible bit pattern that you could fit in a u8 is a valid value (is there a name for this property?). Thus, the resulting Vec is valid overall. This wouldn't work for a Vec&lt;&amp;u8&gt;, or for many other things, because garbage values of pointers aren't necessarily valid. Does that count as safe? How, and where, is "safe" actually defined in Rust? If it is safe, would it be useful to have some helper functions that will create valid-but-filled-with-garbage Vecs (or slices, or arrays), to avoid having to clear them before filling them? As others have pointed out, memset is fast, so this probably isn't worth it most of the time. 
unauthorized API access makes it sound like you're blocking Cargo from within your GitHub settings, but I really don't know anything about this.
&gt; Seriously, I don't know how anybody does production Rust without error-chain. Reasonably successfully, thank you very much. Collecting the backtrace for an error-chain error takes tens of milliseconds. If you can afford that great, error-chain is potentially very useful. But we're using Rust rather than a managed language precisely because we can't afford that kind of cost, so no error-chain for us. 
Your question makes no sense. And please don't waste time responding, I've asked a simple yes-or-no question about Actix, let's leave it at that.
We've seen error-chain errors be slow to create on Linux as well. They can certainly never be as fast as simple non-allocating enum errors, which are just a few extra values, and fully susceptible to optimisation. 
You don't need to disable inlining. If the compiler keeps track of which ranges of instructions come from inlined functions, and includes that in the debug info in the binary, then it's possible to resolve the program counter for a single stack frame to multiple layers of calls. This rapidly gets hard in the presence of code motion and other transformations during optimisation, but even a conservative analysis is useful. Go [does this](https://docs.google.com/presentation/d/1Wcblp3jpfeKwA0Y4FOmj63PW52M_qmNqlQkNaLj0P5o/edit#slide=id.g1b2157b5d1_3_84), although perhaps without the aggressive optimisations that make it difficult. I believe the JVM does this too, although i can't find any documentation about it, and i don't have the energy to start reading source code!
Im still very involved in python, do some elisp and getting more and more acquainted with haskell. One thin i love is good introspection capability ( python actually does shine here imo). Repl aids much here too. How good are rust's introspection capabilities? 
Probably better for the `data` member *not* to be public, seeing as it's a bit of a hack.
Every process has its own stdout, and the locks guarding it are a rust thing, not an OS thing, regardless.
Oh, I'm sure it is slower. I never disputed that. I was talking about the specific performance issue /u/antoyo mentioned, which led to being an order of magnitude slower on Macs, rather than a linear factor on Linux. That's an important distinction to make when evaluating whether one should use a crate.
Honestly, I was just trying to understand what you were talking about, but that's fine.
I still don't get what this TakeCell thing is. The paper says: &gt; TakeCell is for larger or more complex data objects. Rather than copy values out of a TakeCell, a program passes code in, through a closure. For example, a system call interface which keeps caller/process state through a structure named App can access it this way: &gt; &gt; struct App { /* many variables */ } &gt; app: TakeCell&lt;App&gt; &gt; &gt; self.app.map(|app| { &gt; // code can read/write app's variables &gt; }); &gt; &gt; Like Rust‚Äôs Cell, a TakeCell internally owns the shared data, e.g. a mutable reference, and a TakeCell can be shared by multiple callers. However, rather than copy values out of a TakeCell, its API consists of a single method, map(f). Normally, map(f) will invoke the closure f with a reference to the TakeCell‚Äôs internal data. However, in cases where there already exists a reference to the internal data‚Äîsuch as a recursive call‚Äîmap(f) is a no-op and does not execute the closure. &gt; &gt; As a result, TakeCell is a form of mutual exclusion. Like a mutex, TakeCell ensures that there is only one mutable reference to the internal value. However, unlike a mutex, it skips the operation instead of blocking. Once compiled, TakeCell is just as fast as unchecked C code. The [source code](https://github.com/helena-project/tock/blob/master/kernel/src/common/take_cell.rs) is short and well-commented. I think it's one of those things i'd have to actually use before i really understood it. 
For the curious, this is a self-rebuttal to the previous "Ownership is Theft" paper. It's related of Tock, in the sense that this is still based on our experience building Tock, but it's a more general argument. It's still a workshop paper, and a result, "lightly-reviewed" (meaning the peer-review just decides whether "this is interesting" _not_ "this is good science"). We also just published a paper that will appear at SOSP about Tock (and in large part using the Rust type system to enforce system security properties). That one is full paper that had real actual peer review: https://www.tockos.org/assets/papers/tock-sosp2017.pdf
&gt; For u8, garbage elements are valid My understanding is that within LLVM, 'garbage values' are represented by a special value `undef` which can cause undefined whenever it is used.
If you like this kind of work, you should seriously consider joining [Brad's lab](http://www.cs.virginia.edu/~bjc8c/) at University of Virginia. He just started a position there and is looking for PhD students! Applications are around the corner, so there's still time to apply for next fall!
&gt;I've been a big fan of Rust for a while, but unfortunately I haven't had the opportunity to use it at work in production. But I've always been curious to know how error reporting and debugging tends to work out for people in practice. I tend to go _overboard_ with logging. Anything that returns an error tends to get logged when it its the application/extern crate level, with an overly verbose statement. Overall the issues _from_ Rust are small. Mostly slow compile times, and OpenSSL weirdness. For the most part I spend most my time writing Scala/Java to help other teams as the Rust service doesn't break.
Does `RefCell` make sense to you? If not, then yeah, using them might help a little (or if you find me in person some time, it's basically my job to help people understand it). If yes, TakeCell is very similar to that except it forces you to use the shared value inside a closure as opposed to relying on you to release it back manually (or by dropping the reference).
what exactly is `needs_drop` and what is it good for?
I disagree. Sounded more to me like "nervous" about the library (why did you have to mention it compiles on stable? Etc), which implies some unfamiliarity with the current ecosystem
Check out its docs, and if it's still not clear, maybe we can improve them! https://doc.rust-lang.org/stable/std/mem/fn.needs_drop.html
Maybe so.
```Actor -&gt; Delegate Cell -&gt; Schedule Supervisor -&gt; Rep (representative)``` ?
No, that is not safe. With that code, you could read a value at some index *x* which is still uninitialized, do some other stuff, then read the value from index *x* again and you could get a different value. That should not happen in safe code. As an example, let's say *x* is an address in memory on a particular uninitialized memory page. 1. You try to read data at address *x*. Since the page is uninitialized, when you read data at address *x*, the kernel might see that the page is uninitialized, and reading from uninitialized memory is undefined behaviour, which means the kernel might not actually access the memory and just give you a zero. 2. Then, you can write some data to some location with another address, but in the same page. The kernel now has to load the memory page, and the page is not an uninitialized page any more, it contains some data. 3. Then you read from address *x* again. Now the page is not an uninitialized page, so the kernel now gives you the actual uninitialized byte from memory, which can be non-zero. For an explanation of something like this: https://youtu.be/kPR8h4-qZdk?t=1334
My C function declaration in Python is : ffi.cdef("void print_x(float *x,float *y);") And when I try to add the len arguments it says : TypeError: 'void(*)(float *, float *)' expects 2 arguments, got 4 So if I need to indicate the length I do not know how...
There's release+debuginfo mode, which I don't know how to get in Cargo, which gives you full linenumbers for backtraces and inlining.
What‚Äôs the difference between `TakeCell` and storing a non-`Copy` type in a `Cell`. It seems like map is just a wrapper on `Cell::take` in this case. Can `TakeCell` be leveraged to build an STM Cell type?
&gt; What‚Äôs the difference between TakeCell and storing a non-Copy type in a Cell Semantically, almost nothing. The fact that `map` is the main primitive lets us make a performance optimization for large types (though I think that's actually implemented in `MapCell` which has a nearly identical interface, but is implemented differently). It mostly exists because `Cell` allowing non-`Copy` types is relatively new (and I still think it makes the exposition a little bit simpler in the paper to just have types with different names, but that's just my opinion). Not sure exactly what you mean by an STM Cell type exactly, but forcing the user to work with in a closure _does_ seems like it would help with aborting changes. Could be pretty interesting and cool.
I don't think people have mentioned it in this thread, but you can actually write custom code to log any panics that happen. They key is using [`std::panic::set_hook`](https://doc.rust-lang.org/std/panic/fn.set_hook.html) to set a function that gets called when a panic occurs. From inside that function you can then freely print your stack trace and log it to a file. Note that to get the stacktrace, you currently have to use the [`backtrace` crate](https://crates.io/crates/backtrace). Quick example: https://play.rust-lang.org/?gist=9996590a7a7e81d70fa294a63abfa16a&amp;version=stable
I look forward to using it when implementing collecations. Basically a way to be able to optimize the no drop case.
Sure, that should just work now.
I've been wanting for_each forever. I didn't think it would happen because of the for loop. Wonderful little addition.
`npm isntall emoji-cat` `npm isntall emoji-bird` `npm isntall emoji-dog` 
I asked myself the same question when I posted my message. I figured out that the OS would not let a single process prevent all the other ones from using stdout. Well, I hope so ‚ò∫
The C function signature should match the Rust function signature. Try this: ffi.cdef("void print_x(float *x, size_t x_len, float *y, size_t y_len);") Then, pass in the lengths of the arrays to `C.print_x` and you shouldn't get a type error. Note: I've never done Python C FFI like this, so there may be other problems, but I'm fairly certain about the type signature
Me too; I actually missed that it was coming back! I was an advocate pre-1.0, but it was decided against; glad that it's in now!
Why can't `Cell` provide `map` as well?
Is there any official say on how to verbalize borrows? I'd say "a borrowed 5" there, but I've definitely heard "amp 5" instead. 
Nice release! `Iterator::for_each` is great to have and cargo patches look useful. Couple nags: - striaghtforward ‚Üí straightforward - operating-system ‚Üí operating system (maybe? I've never seen it with a hyphen) 
So you can have `Vec&lt;i32&gt;` not loop over every element when you drop it.
The documentation says: &gt; This is purely an optimization hint, and may be implemented conservatively. For instance, always returning true would be a valid implementation of this function. If it is purely an optimization hint, then I would expect that always returning `false` would also be valid. I suspect, though, it isn't purely an optimization hint, and that returning `false` isn't a valid implementation in every case.
AFAIK there's nothing like Python's introspection capabilities. You might emulate some of its uses in Rust using some kind of compile time meta-programming, but it won't be the same as Python's ability to iterate over an object's data members (for example)
&gt; But we're using Rust rather than a managed language precisely because we can't afford that kind of cost, so no error-chain for us. Ah, thank you! That's a good point. In our case, we use Rust to process large amounts of data, but if a Rust-level error occurs, it's probably going to require human intervention. So we've never even noticed the 10s of milliseconds, because at that point, the job is broken. On the other hand, tracing errors back to the originating site _without_ `error-chain` has been pretty ugly for us. Basically, when all else fails, the only way is `rr` or adding tons of logging.
Sure. Vec in particular is already fine without this.
As an experienced user of exception-based languages, I'll have to disagree about the "and tooling will most probably warn you", but fair enough for the rest of it.
I believe panic safety is a bit challenging to get right, if your contents is t an Option
guh, thanks!
What other data structures would have a huge gain from this? Vec can turn a million drop calls into a single deallocation. Node-based containers would still need to free each node, so skipping an empty drop call wouldn't do much.
I was wondering that too. I guess if it returned `false` values would still get dropped eventually. They just wouldn't get dropped in the most optimal fashion? But I agree that returning `false` should be perfectly valid.
Honestly? i don't think we've thought of it. Looking a tad deeper, `TakeCell` &amp; `MapCell` are more or less equivalent to `Cell&lt;Option&lt;T&gt;&gt; where T: Default`, and `TakeCell`'s `map` would translate to something like (I don't think this totally valid Rust, but give me some rope here): ```rust impl&lt;T: Default&gt; Cell&lt;Option&lt;T&gt;&gt; { fn map&lt;F: Fn(T) -&gt; R&gt;(&amp;self, f: F) -&gt; Option&lt;R&gt; { ... } } ``` there's probably a nicer way to do this.
Thanks for the hint!!!!
with you there; but my experience so far was that getting all the toolchains to play nice with each other on my system is tough... If rustup included the linker nicely contained that would help of course.
To keep up with changes as they come, would you please also use issues with links upstream for tracking, planning, and testing upcoming changes? Things like [async/await/generators/coroutines in nightly](https://internals.rust-lang.org/t/help-test-async-await-generators-coroutines/5835) make it important for any current futures/tokio project.
When implementing your own unsafe generic collection you might allocate some memory and, in the collection‚Äôs `Drop` impl, call `ptr::drop_in_place` at some parts of that memory to trigger the destructor of your items. You need to do this manually because you‚Äôre managing the memory and the lifetimes of the items yourself. To make things more efficient you might want to skip that if the item type doesn‚Äôt have a destructor. If `needs_drop` conservatively returns true for a type that doesn‚Äôt actually have a destructor, at worst you‚Äôll spend a little time making `drop_in_place` calls that don‚Äôt do anything. If however `needs_drop` incorrectly returns false, you‚Äôll leak some items and the resources they might own.
`Vec::drop` doesn‚Äôt loop itself, it calls `drop_in_place` on the entire slice of valid items. And that call returns early without looping if `needs_drop` would return false. So `Vec::drop` doesn‚Äôt need to call `needs_drop` itself. `HashMap::drop` however might have a large number of items that are not stored in a contiguous slice. `needs_drop` helps there.
HashMap already uses it and had a verified benefit from doing so. I'm going to use it in ndarray. Right now some algorithms are only implemented for T: Copy element types, since we don't want to pay the overhead for supporting elements that need proper drop. This function allows us to expand support to more element types without losing performance for the common case.
HashMap uses contiguous storage with open addressing, right? Yeah, that fits the same "one allocation, many elements" pattern that makes this change useful.
Possibly useful: The xi-editor project contains a text CRDT in Rust that could be converted to an ordered list CRDT reasonably easily, as well as a big document on the design of the CRDT.
Any insight on why it was removed? Having `for_each` in stable will make it super easy to switch back and forth between `.par_iter_mut()` and `.iter_mut()`, which is something I've been hoping for.
Hey /u/mre__, I found a better version of the `main` function from the optimized Rust version, would you consider updating it? I think the closure and the Cow in the current version are totally unnecessary. :P fn main() { match env::args_os().nth(1) { None =&gt; write(b"y\n"), Some(arg) =&gt; { let mut arg = to_bytes(arg); arg.push(b'\n'); write(&amp;arg); } } process::exit(1); }
Sure. I didn't say Vec doesn't need it because it doesn't need this optimization, but because it already has this optimization without using the `needs_drop` function.