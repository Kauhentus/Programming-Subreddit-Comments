Open up an issue! The maintainers would love that.
I don't remember seeing any official recommendations, but a common pattern when there are different "versions" of the same setter methods is something like this: set_x(&amp;mut self, x: T) -&gt; &amp;mut Self with_x(self, x: T) -&gt; Self x(&amp;self) -&gt; T Extrapolating that, using my not so fantastic naming skills, to a removal method, I would probably pick `without(self, index: usize) -&gt; Self` until I find a better name. It does also depend a bit on the semantics of the collection. Edit: variants of the above pattern include: x(&amp;mut self, x: T) -&gt; &amp;Self get_x(&amp;self) -&gt; &amp;T
I honestly dislike programming; but i wish every single thing in linux and linux itself was written in rust. As a user, memory errors and core dumps are extremely annoying (expecially in games, omg). Unfortunately, the users don't know what they're missing ;O 
I believe the Scala standard library is huge (in Android terms), although that's based on third hand reports. The Kotlin runtime is quite lean by comparison (note: the raw library sizes don't tell you much, proguard culls them down quite a bit depending on what you use). I belive its something like 8MB vs 800kb, although the apk size impact for typical apps using Kotlin is closer to 50kb from what I've seen. Huge dependencies suck. Both int terms of apk size, and also method count usage. The bigger issue is support and build time integration. Kotlin is now a first-party supported language, Scala is not used by many people (although that's largely a result of its size). Your more likely to waste time dealing with weird build issues. And Scala builds are purportedly slower, although that's not a huge factor if you have a fast development machine. (All the "cool kids" are using Kotlin now too, so I have a feeling it's going to become the de-facto standard over Java. Funnily enough, all the Googlers I've spoken to are sticking to Java for their existing apps though.)
Does it kill any other folding method previously set in a file, or does it cooperate somehow?
Wow that's a very information packed post. I'll have to read this in bits and pieces, as I tried in one sitting and it was a bit much. I think if you restructured and split up this post it would be easier to digest. All the "I'll go into the later" added more cognitive burden because I assume it'll be revisited later in the post. If you meant in another blog post, it would be better to be more explicit about that. Also,the theory section is long enough (and thorough enough) that it could have been its own blog post that was simply linked to from this one. You could describe at a high level how it works (eg recursive refcell, similar to how most of our mental models of mutability in Rust), then launch right into the examples with a link to the more in depth article. It's very interesting, but some of want to see it in action before diving into the meat of how it was achieved. Apart from those structuring problems I had with the blog post, it looks great and I look forward to reading it later :-) EDIT: also I'm interested to see the performance impact on debug builds from this. It would be nice to be able to turn this off with a flag if it impacts performance too much, as already my applications are painfully slow in debug builds (but release takes 20+ seconds, even on my 16 core machine) 
https://doc.rust-lang.org/book/second-edition/ch09-02-recoverable-errors-with-result.html#a-shortcut-for-propagating-errors-
No love for Bittrex? This might come in handy if I decide to automate some of my day trading.
I do not (nor the other contributors) use Bittrex, so this is not planed for now, but this should be very straighforward to implement :)
It's an underrated exchange it seems, very fast and with tons of altcoins but ranked like #12 (compared to other exchanges) for the most volume.
Wow this is really helpful thanks!
This was the initial ballpark estimate given by Mozilla to justify the development of Rust/Servo... 4 years ago now? Also, note that in a language with bounds-checking and defined overflow semantics, integer overflows are not as catastrophic as they are in C/C++.
The problem is that you would need dependency tracking to know what the test case depends on, but the very act of changing the software has changed what the dependencies were and without type-checking the new software it's unclear what they now are (because type inference). And of course, if you use a single path of a function which has two, you still need to ensure the second compile, and down the rabbit hole you go.
From the RFC: &gt; It is important not to use 1, even though EXIT_FAILURE is usually 1, because some existing programs (notably grep) give 1 a specific meaning; as far as I know, 2 has no specific meaning anywhere. This doesn't make much sense to me. That "specific meaning" **is** failure. Using a different code for abort/panic makes sense, but not here.
Oh good lord this is one of the ugliest things I have ever seen in Vim. I love it.
Ok, silly question: I want to make a small app that will take data (HTTP GET) from one service (Mastodon) and post it back (HTTP POST) to another service (Evernote). Since my background is Python, I'd go with something with [Requests](http://docs.python-requests.org/en/master/). Is there such library in Rust (something that makes damn easy to GET from one point and POST to another).
The link to the scholarships page is wrong, it points to https://blog.rust-lang.org/2017/07/18/(https://tilde.wufoo.com/forms/rustconf-scholarships/)
Yes, we have [reqwest](https://docs.rs/reqwest/0.7.1/reqwest/).
And don't forget the over-reliance of examples on `unwrap` for the sake of brevity!
I might suggest not writing functions with so much nesting. It's usually a code smell.
Thanks! https://github.com/rust-lang/blog.rust-lang.org/commit/1bfacf10fc5a2344f61d24454afbc2d8f8b30614
Rust doesn't have a type called 'int', although you can alias it. If you want to share constants like this, how about ... my_rust_decls.c: #define DEF_CONST(TYPE,NAME,VALUE) const NAME: TYPE = VALUE; Then in your build.rs, have a step which runs the preprocessor (gcc -E) on this, thus creating a valid Rust file which you can include in your build. Including the same file in two languages is nuts. The Rust toolchain has built-in support for code generation. This seems like a straightforward move to me. 
Ah sorry, I didn't realize I could have fixed that myself on github. Next time!
Looking forward to the talk on elliptic curve crypto! We need more crypto in Rust =)
Link that you are looking for: https://github.com/exonum/exonum Yet another cryptofin project in Rust. IMO. This is going to be the first industry to be dominated by Rust. It needs performance, and it costs way too much to have bugs and security holes. 
It's all good! :)
Do you mean Python-like list comprehensions? Then your best option is probably `some_vec.into_iter().map(...).filter(...).collect()`. It's a bit more verbose, but also more explicit with memory allocations and such.
what's the best way to cleanly and clearly unwrap a nested option? specifically i have an `Option&lt;usize&gt;` which maybe contains a key, and a `fn get_data(key: usize) -&gt; Option&lt;Data&gt;` which gets the matching data if it's available. in the surrounding function that accesses both of these, i want to return a `Data`, falling back on `Data::new()` if either option is none. na√Øvely i wrote an if statement but i feel like it could be a nice one-liner. fn try_get_data(key: Option&lt;usize&gt;) -&gt; Data { if let &amp;Some(key) = &amp;key { get_data(key).unwrap_or(Data::new()) } else { Data::new() } } i tried to instead use `key.map(|key| get_data(key))` but it creates an `Option&lt;Option&lt;Data&gt;&gt;`. i can then `.unwrap_or(None).unwrap_or(Data::new())` but it looks awkward. is there a better way to handle this scenario?
&gt; I program for fun and I don't have any particular projects in mind. Rust is for you! In learning Rust, you will go on an arduous, but carefully guided, tour of the frontiers of modern programming, taking in a powerful type system, many cleverly designed libraries, and the legendary compile-time automatic memory management. You'll sweat, you'll curse, you'll learn a lot, and you'll add a powerful arrow to your quiver. In learning Go, you will spend a little time learning everything there is to learn, and the rest of the time trying to build cathedrals out of mud bricks (although that's [actually pretty amazing](https://www.flickr.com/search/?text=Great+Mosque+of+Djenn%C3%A9) when [you manage it](https://en.wikipedia.org/wiki/Great_Mosque_of_Djenn%C3%A9)). You won't have to work hard, but you won't learn much. If your goal was to build something nontrivial as quickly as possible, Go might well be the right choice. Its power is that it's fast to learn, and hard to get wrong - that genuinely makes it the best choice for some situations. But that isn't your goal. 
All iterable types in Rust implement Iterator trait, so that's the way to go. See https://doc.rust-lang.org/std/iter/trait.Iterator.html for details, or https://docs.rs/itertools/0.6.1/itertools/ for additional useful methods.
Iterators are the answer you're probably looking for, but you can always use a crate like [cute](https://crates.io/crates/cute) that provides Python list comprehensions as a Rust macro. Example code is [here](https://docs.rs/cute/0.3.0/cute/). It looks like this: let even_squares = c![x*x, for x in 0..10, if x % 2 == 0]; Very Pythonic. To use it, it looks like you would add cute = "0.3.0" to your Cargo.toml file, then in your `main.rs` file you would add #[macro_use] extern crate cute; then you could use the `c!` macro.
Beyond zero (0) = success, non-zero = failure, Unix doesn't really have a standard for exit codes. However, I agree that Rust going out of its way to avoid one (1) is silly - the POINT of exit code 1 is to be a general failure indicator. You are supposed to use it.
The error is telling you that `hasher.input` expects a `&amp;[u8]`, and `&amp;T` (the type of `&amp;transaction`) is of course not `&amp;[u8]`. The `Hash` trait isn't involved at all; you're just passing an argument of the wrong type to a function.
Just to be clear, UB checking is a dynamic technique and thus needs a test suite that actually hits the bug to detect anything. So assume we also have fn main() { let _x = split_at_mut(&amp;mut [1,2,3,4], 0); } Now what would happen is that when the return value of `split_at_mut` is validated by `main`, the code acquires write locks for both references in the pair. The two write locked regions overlap, leading to UB. Does that answer your question? And thanks btw, I'm going to add this to the test suite. :)
I assume that by 'DOM' you mean something like 'tree, where the nodes are of different kinds, and where some kinds of nodes can have children of one or more specific kinds; each kind having specific fields and operations, but there also being some operations common to many or all kinds'. Maybe: * A struct per node type * Traits for the sets of methods shared by different node types (with trait inheritance as appropriate) * Expose fields through getters and setters where you want to expose them in a trait * Parent-to-child relationship handled by enums over the different kinds of node, where the only field in each variant is a box holding the appropriate kind of node * Judicious use of deref and trait implementations on the enums to ease polymorphism (eg an implementation of a trait for an enum could match on the variants, and delegate to the trait implementation for each one) You can't easily have child-to-parent references, so you'll need to track those on the stack as you traverse the tree. I've never built a DOM in Rust, and lots of other people have, so this may be a terrible suggestion, though.
Personally, I find the approach of Book 2 (first teaching about the heap, then moving to the stack) makes a lot of sense. Many confusing noob errors (such as lifetimes) go away as a result.
( Õ°¬∞ Õú ñ Õ°¬∞)
&gt; The first call to validate does not seem to acquire a write lock to *z. That's just a typo, right? Right, thanks. Fixed :) &gt; The suspension of the lock of *z seems to reference a lifetime 'z. Should that be 'inner? Yes. Fixed. Thank you :D &gt; After acquiring the lock to *z_for_fun, there's a release for *x, *z_for_fun. What's it doing there? As far as I understand it's not mentioned in the text either (the text mentions some release &amp; acquire that's not redundant, but I assume that refers to the suspense &amp; acquire, right?). That's releasing function arguments passed to `fun`, as discussed in the previous example. In the text, the corresponding point is "we release our write lock on `*z_for_fun` again". &gt; Thanks for any pointers (pun disregarded) :) 0x400. You're welcome. ;)
Me, too, already running around in circles because of this.
Yes, the type of the access doesn't matter (this is not TBAA!). You can however not pass two &amp;mut references to the same object to another function, no matter whether these references have the same type or not. Do you have a concrete small piece of code you are wondering about? That would simplify the discussion.
&gt; All the "I'll go into the later" added more cognitive burden because I assume it'll be revisited later in the post. And you are right. :) Did I mistype somewhere and use this to refer to things that are not covered in this post? I appreciate this is a big post. However, the things I present are all interconnected. I already split out the previous post (about unsafe code guidelines and UB) to make it slightly shorter. I would not know how to split up this one while still making every individual post useful on its own. &gt; You could describe at a high level how it works (eg recursive refcell, similar to how most of our mental models of mutability in Rust), then launch right into the examples with a link to the more in depth article. It's very interesting, but some of want to see it in action before diving into the meat of how it was achieved. That is valuable feedback, thanks. I considered swapping things around, to start with the example and then do the abstract theory. In hindsight that's probably what I should have done. But I was not sure how much I have to say in advance to make the examples understandable -- basically I have very little idea of how much background on the topic my readers have. Now I am wondering whether I should reorder the post now. Not sure if it's still worth it. Hm. &gt; EDIT: also I'm interested to see the performance impact on debug builds from this. It would be nice to be able to turn this off with a flag if it impacts performance too much, as already my applications are painfully slow in debug builds (but release takes 20+ seconds, even on my 16 core machine) Just to be clear, the final build is not affected. These extra instructions are all thrown away during LLVM codegen. Only miri interprets them currently. Or do you mean debug build compile time? Good question, I don't know. I didn't make concrete plans for upstreaming my branch yet, but indeed my thoughts were along the line of adding a flag that controls whether these extra instructions are added -- at least for now. Eventually they will be necessary because they will guide the noalias annotations emitted for LLVM.
The best answer to your question is "Avoid the nested Option in the first place." The function you need to do that is called `and_then` and works [like this](http://play.integer32.com/?gist=dc935bd9a5fe066e63aee59bfc1146e0&amp;version=stable). 
Looks like you want and_then for the first part and unwrap_or or unwrap_or_else afterwards
Why has to main return ()? Why not forall a instead? 
thanks, both you and /u/Destruct1! i wasn't aware of this fn for options, this improves the clarity of the code.
Yep, that's my correct reddit name (hint: in the future, [rustaceans.org](http://www.rustaceans.org/LukasKalbertodt) might help you). Thanks for pinging me! My course was focussed on Rust and was *not* a standard course in which I replaced C with Rust. So I guess my experience can't really answer /u/eric-douglas question here. However, Michael M√§chtel, a lecturer at Hochschule Konstanz, gave a operating systems course using Rust ([Link](https://htwg-syslab-bsys.github.io/)). His slides on Rust are largely based on mine (he asked to use them, that's the reason I know of him). The last time I talked to him (end of april), his students were actually really satisfied with the usage of Rust. But now that the semester is over, I will ask him about updates on that. I think he doesn't have a reddit account, otherwise I'd ping him. If I get more information, I'll comment on this comment and will ping you.
Arf, two third are in the USA. :-/ Not convenient!
Funny little tidbit: &gt; On 29/04/2011 12:16 AM, Andreas Gal wrote: &gt; &gt; Congrats! What are the build times these days, out of curiosity? Still pretty bad. It's not an hour anymore, but it's still ~15 minutes on a fast opteron for stage1 -&gt; stage2. But: (a) We know that 6.8mb of the 12mb binary coming out the other side is mostly-redundant structural typing metadata, which I have a wip patch in my stash to eliminate all the redundancy of. Just need to fix a little indexing bug in it. (b) We have *just* got to the point where we can interact with the llvm optimizer and runtime interface without risk of breaking rustboot compatibility. We've been largely holding still on all fronts while completing this step. So .. I expect that number to fall substantially in the next little while. -Graydon
Any possibility that [`ion`](https://github.com/redox-os/ion) could be useful to you wrt dropping the `sh` dependency?
Just curious, what are you lacking in terms of Rust crypto? Personally my issue is with the prevalence of assembly and/or C bindings in rust libraries today.
Hm? Half of the mentioned Confs are in Europe, only one has passed already. RustFest has a 6 month beat.
More puns, mostly. Sorry.
[This](https://github.com/andrejbauer/plzoo) might be a good place to look at examples. However, the examples are toys and they use lexer and parser generators.
This is awesome. Thanks for pointing cute out.
This is awesome. Thanks for pointing cute out.
* RustConf: Portland, OR, USA * RustFest: Z√ºrich, Switzerland * Rust Belt Rust: Columbus, OH, USA One is in Europe and the two others are in the USA. Did I miss any?
yeah, I've never seen it before, but I searched for "comprehension" on crates.io and it popped up immediately. Apparently it was only published in the last two or three days, which is fortuitous timing, and based on the examples and a quick test, it should work well.
RustFest Kyiv happened in April.
oh lol I get it now =P
RustFest happens twice a year now. One of them already happened. But I presume there will be another one early next year.
Is there an easier way to get the latest stable rust install package? Currently I use: lynx -dump https://www.rust-lang.org/en-US/other-installers.html |grep "dist/rust" | grep x86_64-unknown-linux-gnu |cut -b 7- |head --lines 1 
I've definitely thought about it! `just` recipes tend to be simple, so the featurefulness of `ion` might not be put to good use. Also, I think that familiar syntax is really important. And, I want a shell which has a library API, so that I can do a really deep integration. If there was a `sh` or `rc` clone written in rust with a library API, I would be all over it! I think that `rc` would be ideal; it's only slightly less familiar and fixes the biggest footguns in `sh`. I wish I had time at the moment to write an `rc` in rust!
I am tasked to evaluate Rust to re-implement a small data crunching module - for speeeeeeed. The scenario is crunching through a Postgres database. A single SELECT query basically returns millions of rows. The result of the query should be sucked in by a Rust module and processed row by row. Depending on a value in a field in a row the further processing of one row is forwarded to different submodules. So I imagine one CPU for postgres itself and streaming in the data, and then moving ownership of the row to the specific submodule -- each submodule runs on its own CPU core. So with Rust I should get close to processor speed and be able to safely use multiple cores in parallel for the raw calculations. Question: I'm a complete beginner in Rust and currently work my way through the book (second edition). Can you recommend examples for 1) streaming in the data from Postgres and 2) handing the rows off to subcores/threads as they arrive? Thank you! 
Sorry everyone wrong thread lol
Rust runs well even on tiny microprocessors!
Ah, whatever. I did some refactoring of the post, reordering sections 2.2 and 2.3 to hopefully make it all flow better.
What's your vim theme/plugins/vimrc?
Wow, it also supports dictionary comprehensions. That's great!
I'm curious how this differs to storing state in the (de)serialised, given one of those objects is passed to all (de)serializations?
This looks promising: https://github.com/rust-lang/rust/pull/42913
I'd like to run my rust tests with sanitizers, iterating over each sanitizer. https://github.com/japaric/rust-san#addresssanitizer Is there a way to export the proper flags/ run 'cargo test' iteratively with each one? I would also want to do things like set the opt level, incremental compilation, and other workarounds for current issues with sanitizers.
Why not rustup?
Hello everybody, I wanted to learn some Rust and how webserver were working so I decided to implement a Webserver in Rust. The goal here is purely learning and not replacing Iron or tiny_http. I'm trying to serve my custom home page that I've built here https://github.com/achntrl/Frontpage. It's an HTML page that loads .js, .css and images files (.svg, .png, ...) The thing that bugs me is [here](https://github.com/achntrl/rust-webserver/blob/919e7a42caea244162eed85ebdbe9b4a98ed6b97/src/main.rs#L49-L59) I have to read some files as String and others as slice for my page to load properly and I can't figure out why. Any explaination on this ? Any comment on the code will also be greatly appreciated ! Thanks a lot ! Edit : Changed link to refer to specific commit
A String must contain only valid Unicode. A picture like png or jpg does not contain valid Unicode, so a String can't handle it. I believe what you really want to do is use a Vec of u8 instead of a String and just do read_to_end and that will cover both cases.
&gt; Content-type:image/png;charset=UTF-8 I also don't think a png file can have a UTF-8 charset. using relative paths (../../whatever) you can access any file on the computer that your webserver process is running on, I believe.
Very nice! My friend wanted to implement similar thing in Ruby for his trading bot. (He later left that project.)
I use solarized dark all across my terminal apps. My vimrc is long, untidy result of years of tuning, so YMMV: http://sprunge.us/IeCj 
I assume you mean the (de)serializer, ie the object that describes the format? Although these objects store state, they have no way of acting in any special way given a particular type. Even if they did, each individiual format would need to implement it on their own whereas, at least in my case it is a property of the serialized data. On the other hand, types implementing (De)Serialize can customize the serialized data but they do not have a way of storing state as the those traits are generic over the (De)Serializer. Does that make sense? https://docs.serde.rs/serde/trait.Serialize.html https://docs.rs/serde_state/0.2.0/serde_state/ser/trait.SerializeState.html 
I have been working on one too! I can't share the code right now for Reasons, but I'll check yours out for sure. Can't right now, but by tomorrow!
Yes it was that PR. I'm really happy about this change!
Oh right, thanks for the reminder!
Let's hope! ;)
Ah, lovely! I'm sure that will be very helpful, thank you :-) I'm coming specifically predominantly from the (reactive / functional) JavaScript world so most of what I'm doing in Rust right now is new to me. Types I get just fine. The ownership model I need to review a bit. Local modules are confusing the heck out of me, though only really in terms of syntax. Once I have them working once I'll be set. I'm also coming at this as someone who's never made a proper game of any description before. I think the only game-y convention I'm consciously using thus far is delta time. What is the Entity-Component-System pattern? Anything else useful I should know? Cheers!
I've been thinking of submitting a talk to RustFest, but I need some encouragement. Would it be weird for an American to do so? Though it's possible I'll already be in Germany around that time...
The whole conference is in English, I and a few other American's were definitely in Kyiv for the last one so I don't see why not. Submit a talk! Worse case it's not chosen. Get some feedback on it then improve it or work on another one then! Good luck! :D
Does anybody know if Rust Belt Rust allows talks w/ multiple speakers? Their [CFP form](http://cfp.rust-belt-rust.com/rbr2017/new) is very geared for a single person. 
And the issue: https://github.com/rust-lang/rust/issues/24189 Good sleuthing!
I use rust to teach ambitious beginners who want in depth understanding programming(private lessons, for others I use python or .net languages)...I start them of with C, let them do some exercises on godbolt to get the understanding of machine code and then follow up with exercises which tempt null pointers and range errors. I have some templates I reuse by now that makes this happen in a couple of hours if it goes well, since the point isn't to actually teach C. If I feel mean I let them struggle with headers and writing a makefile by hand to really make them appreciate the next part, were I introduce rust. Then with the stick of the borrow checker and copy/clone restrictions they really start to understand how memory works, while the carrot of type inference and all the nice syntactic sugars rust and cargo offer keeps it fun. 
Kills with fire. 
hah I know, makes me not want to change it, it's too rare :P
Not much this week, I'll probably merge and publish a new version of validator based on https://github.com/Keats/validator/pull/27 Too bad it didn't get any feedback, we'll see how it works once it's published I guess
Yes, we certainly do! Just pick one speaker to fill out the main stuff and stuff a mention about the other speaker in the box at the bottom "anything else you'd like us to know".
&gt; let even_squares = c![x*x, for x in 0..10, if x % 2 == 0]; let even_squares = (0..10).filter(|x| x % 2 == 0).map(|x| x * x).collect::&lt;Vec&lt;_&gt;&gt;(); If anyone else will ever touch your code, I‚Äôd strongly recommend doing it with the standard library rather than something cute that isn‚Äôt the normal way to do it in Rust. Plus, you can often avoid unnecessary collecting into vectors.
&gt; something cute I see what you did there
&gt; C is a better language for demonstrating how a computer works, how to manage memory Absolutely. I think C is the best beginner language for any CS student. Rust would the next step. 
I've been working on the new rustdoc this week as well as my conference talk for RustConf.
&gt; Including the same file in two languages is nuts. .. even when they share a very similar execution model, FFI, etc?
This is great! It complained about "$type" (is that reserved?) but works with "$t" and then using it seems to give me the right validation (let me know if the syntax of the macro invocation can be easier - still new to this): fn main() { let foo :i32 = 6; println!("Count to {}", foo); for_type!(i :i32 = 0..foo; { println!("Number: {}", i); }); println!("Done to {}", foo); } It gives me an error if the i is ":f32", for example, as expected. Thanks!
what about asia :(
sorry, but where is rust code?
Will all the talks at each conference be recorded?
Thanks to everyone for the posts so far. This is very helpful, and generally consistent with what I was thinking, but I felt like I might have been missing something. I wasn't necessarily thinking of Python when I posted, but that kind of syntax is what I had in mind. Reading through this all, one remaining question on my mind is multiple generators, kinda like you can have in [Julia](https://rosettacode.org/wiki/List_comprehensions#Julia), [Haskell](https://rosettacode.org/wiki/List_comprehensions#Haskell), or [Scala](https://rosettacode.org/wiki/List_comprehensions#Scala). I'll have to look into things.
You can use [channels](https://doc.rust-lang.org/std/sync/mpsc/) to send data between threads. https://crates.io/crates/bounded-spsc-queue looks like very fast spsc implementation. EDIT: as for the db, [this](https://github.com/sfackler/rust-postgres) looks good, it has a [lazy_query](https://docs.rs/postgres/0.14.2/postgres/stmt/struct.Statement.html#method.lazy_query) function.
I can't speak for all of them, but so far, every conf has recorded every talk.
That looks like a very interesting resource. Thank you!
The standalone large package looks to be the simplest/robust way for reproducible integration into our local dev image/container that everyone uses. Download package once per release, cache locally, rerun as often as needed (depack, install, patch PATH) However I must admit that this covers only the executables side, I have not yet figured out how to share locally the index/crates cached in .crates for multiple users so that everyone gets the identical (without depending on repeated network downloads).
Cool thanks!
Thanks for the pointers. I knew about rust-postgres already, but some days ago I read diesel is faster for some things? - so it is not the only option? And streaming in at the same time while handing ownership piecewise to different threads.... I'm still not that far into my Rust knowledge and this still scares me :-)
Actually I'm baffled when quite a lot of people refer to RFCs as if they are parts of the language reference... It's like, a bat is a mammal happening to look like a bird.
browsers ensure that you can't go below the root of the server `http://localhost/../../../foo` is equal to `http://localhost/foo` but with manually crafted requests it should work
Thanks.
This is going to be a controversial statement but this is exactly the reason why I prefer 1) 2 spaces for indentation and 2) avoiding nesting if statements (and preferring early returns) whenever possible.
In general, and that goes for all confs: if you've got a clever, out of the ordinary thing to suggest, or just a thing that doesn't quite fit the form, submit it and write it the comments. We read all submissions in full and are happy to see ideas!
I can neither confirm nor deny anything except "RustFest has a 6 month beat" ;).
Conferences are community-run, so we'd need an asian organiser to step up. I know there are talks. If anyone wants to run something, I am sure we can muster some support. The same issue goes for Oceania, South-America, Africa...
Once you have a binary, simply follow the debian packaging guide. The first thing to do is to check if you depend on any C libraries or try to run it on a clean install. Your directory tree should look like this: helloworld ‚îú‚îÄ‚îÄ usr ‚îÇ ‚îî‚îÄ‚îÄ share ‚îÇ ‚îî‚îÄ‚îÄ your_binary_folder ‚îÇ ‚îî‚îÄ‚îÄ your_binary ‚îî‚îÄ‚îÄ DEBIAN ‚îî‚îÄ‚îÄ control The first thing is the control file, which is used for managing dependencies: Package: helloworld Version: 0.0.1-0 Section: misc Priority: extra Architecture: all Depends: [LIBRARIES HERE] Installed-Size: 8 Maintainer: Your Name &lt;yourname@gmail.com&gt; Homepage: www.yourhomepage.com Description: [DESCRIPTION] That's it. Then you can use `dpkg -b ./helloworld helloworld.deb` to package it. If you want a desktop entry, you need a logo in PNG format and a `.desktop` file. Your tree should now look like this: helloworld ‚îú‚îÄ‚îÄ usr ‚îÇ ‚îî‚îÄ‚îÄ share ‚îÇ ‚îî‚îÄ‚îÄ your_binary_folder ‚îÇ ‚îî‚îÄ‚îÄ your_binary ‚îÇ ‚îî‚îÄ‚îÄ pixmaps ‚îÇ ‚îî‚îÄ‚îÄ my_icon.png ‚îÇ ‚îî‚îÄ‚îÄ applications ‚îÇ ‚îî‚îÄ‚îÄ your_binary.desktop ‚îî‚îÄ‚îÄ DEBIAN ‚îî‚îÄ‚îÄ control In the desktop file, you need to reference the icon. [Desktop Entry] Name=Hello World Name[¬≠de¬≠]=Hallo Welt Comment=My user-visible description Comment[¬≠de¬≠]=Localized string Exec=/usr/share/your_binary_folder/your_binary Icon=my_icon Terminal=false Type=Application Categories=Development; StartupNotify=false Watch out for spaces. I took this tutorial from [here](http://www.tomprogs.at/tutorials/linux/debian-paket-tutorials-01-ein-erstes-paket.xhtml), however the site is in German. You can google the individual strings to find out what they do. Then you can give your friend the .deb file. Usually Ubuntu has a graphical installer where you can just hit "install". You can sign it and publish it, but that is beyond my knowledge of how to do. The application will be installed on his computer under `/usr/share/your_binary_folder`. Make sure the name doesn't conflict with anything and that the `Exec` entry is correct.
I think this is short-selling Rust. It also appeals to developers that want a modern trait-based language, a good alternative to Go with similar ease-of-deployment properties and modern concurrency support without having to opt for a model.
I'm a little confused by all these discussions. A very regular point is that _if_ you learn C or Rust, you should learn C, because you will make all the errors Rust fixes. This sounds like Rust is "just C with memory management", or "a patch to C" which it isn't. I never found it worthwhile to learn how to produce an illegal memory access and it's certainly not a beginner subject on how to work with them. On the other hand, people suggest getting started with higher-level languages. Now what, should I learn about these issues, or should I not? Also, these languages have _huge_ systems to provide the guarantees they have, to the point where properly explaining the actual _execution_ of the program becomes a 2 lectures subject. From the perspective of explaining _computation and memory on a low level_, in a certain completeness, I find Rust a very fitting language. Borrowing and Ownership aren't hard concepts. You can teach them very quickly, with rather natural terms. You can even avoid Borrowing for a while. Most of the lifetime modeling directly leads to a very good discussion about the actual execution model (what does the stack do, how does the heap behave?). Rust has a concept of Sizedness, which is important for many things when talking about memory interactions, calling conventions, etc. I can just as well explain what happens behind the covers when a function is called using Rust as I can do using C. Memory representations of arrays and vectors, I can just as well explain in Rust and in C. Lots of the discussion about "what do we teach beginners" suffer from people projecting how they learned as beginners.
It's definitely planned for RustFest
Wouldn't it make more sense to put the binary in /usr/bin/ so it is in $PATH?
Hm, the crates.io link still lists erickt and David as authors and just you as an owner, the "homepage" link goes to serde.rs.
This is good advice for building Debian binary packages that could be hosted elsewhere, but PPAs specifically require that you upload a source package, and the Launchpad servers build the binary package. Source packages are a little more complicated, because you have to set up build dependencies in `debian/control` and write a `debian/rules` makefile that essentially automates the above steps, with a lot of Debian-specific tooling. Haven't done it (yet) with Rust, though.
alright, sorry. Just wanted to give a general advice. He said "I just want to give my friend this binary", so I thought a simple .deb would be enough.
You could use a std::io::Cursor over the slice of the memmap.
Most of this work can be automated with [cargo-deb](https://github.com/mmstick/cargo-deb). It solved all my debian packaging woes. 
M√§chtel's course was the one I gave a lecture and exercise in and feedback was really good.
Yikes. Building a binary package like that is like carving stone tablets. Don't do it. Build a proper source package with [debcargo](https://anonscm.debian.org/git/pkg-rust/debcargo.git).
The crates are in a bit of a weird position since they are just the normal serde crates with some patches applied to them, only they can't be merged to serde proper as they have yet to prove themselves stable or useful enough yet. So the docmentation found on serde.rs still largely applies and I am not really an author anymore than if I submitted a PR that got merged.
But aren't runtime errors just as frustrating? Deriving an approach for newcomers based on your personal starting point, where - on top - the language you compare with didn't even exist, seems problematic to me. People teach haskell to beginners, with great success! At the _very beginning_, every compiler error points to unknown territory. That's fine! People are _good_ at learning the unknown!
Yes, I should have been more clear in my question. I'm particularly thinking about whether I need to upload each of the my dependencies separately. I guess I'll just take a crack at it and see what happens.
&gt; error: the linked panic runtime panic_unwind is not compiled with this crate's panic strategy abort The fuzzer relies on the error condition being an abort, not a panic, and [tells the compiler](https://github.com/rust-fuzz/cargo-fuzz/blob/307f91d4dbd712ae8e52272ada0647c95daa7934/src/main.rs#L314) to emit aborts instead of unwinding code. However, it sounds like some crate is still linking in `panic_unwind`, but it's impossible to say without seeing any code.
Fast and underrated in volume might be correlated^^
If you just want to give your colleague a binary, you could also try http://appimage.org/.
2 also very much has an unofficial meaning of "wrongly provided command line arguments". 2 for wrong command line arguments is very common.
The point is to push for having something like list comps a normal way to do it in Rust. The problem regarding list comps not being lazy is quickly solved improving those macros (adding a lazy version, or having only the lazy version). A problem with Rust iterator chains is that once you have more than one iterable to iterate over, and you want some nesting, the iterator syntax becomes harder to read and to write, you need "flat_map" and "move". This is an example with three nested: fn main() { let n: u32 = 100; let t = (1 .. n + 1).flat_map(|a| (a .. n + 1).flat_map(move |b| (b .. n + 1) .filter(move |&amp;c| a.pow(2) + b.pow(2) == c.pow(2)) .map(move |c| (a, b, c)))); println!("{:?}", t.collect::&lt;Vec&lt;_&gt;&gt;()); } With Python3 syntax: n = 100 t = [(a, b, c) for a in range(1, n + 1) for b in range(a, n + 1) for c in range(b, n + 1) if a ** 2 + b ** 2 == c ** 2] print(t)
You are definitely an author after getting stuff merged into the main project.
&gt; Does that answer your question? Yes this answers my question, thanks. I guess it probably cannot be done, but would it be possible to move the error a bit closer to the source? For example, inside `split_at_mut` there is an `unsafe` block, from which the two `&amp;mut [T]` escape. They don't necessarily need to be used in main, they could have been used inside `split_at_mut`, e.g., like this: fn split_at_mut(&amp;mut self, mid: usize) -&gt; (&amp;mut [T], &amp;mut [T]) { let len = self.len(); let ptr = self.as_mut_ptr(); let (a, b) = unsafe { assert!(mid &lt;= len); (from_raw_parts_mut(ptr, len - mid), // BUG: should be "mid" instead of "len - mid" from_raw_parts_mut(ptr.offset(mid as isize), len - mid)) }; // use a and b here (a, b) } Would it be possible for the check to tell me, in the original case (not this modified one), that the UB originates when those two references escape the `unsafe` block before hitting the return value of `split_at_mut`? I think this would be super awesome, because if the error is reported in main, then I know that something down the `split_at_mut` function call stack did something wrong, and then I'd have to investigate. But if the check can pin-point me to where UB was created, then fixing the issue is a no-brainer.
What would be a good way to find neighbours in vector of vectors, as in the game of life? I'm having trouble with handling negative offsets -- `field[x-1][y]`, etc. I mean, like this: use std::cmp::max; fn count_neighbours_at(field: &amp;Vec&lt;Vec&lt;bool&gt;&gt;, x: usize, y: usize) -&gt; u32 { /// find the number of (3x3) neighbouring `true` s, including the specified cell itself. /// Doesn't wrap; count_neighbours_at(f, 0,0) returns the number of true s in /// [(0,0), (1,0), (0,1), (1,1)]. let mut count = 0; for nx in ((max(x, 1) as isize - 1) as usize)..x + 2 { for ny in ((max(y, 1) as isize - 1) as usize)..y + 2 { match field.get(nx).and_then(|r| r.get(ny)) { Some(&amp;true) =&gt; count = count + 1, _ =&gt; (), } } } count } I'm feeling that this is too tedious, especially, * You can't subtract 1 from `usize`, so I have to convert it to `isize` and convert it back to `usize` to access. * `Vec`'s `get` doesn't allow negative indices, and converting `-1` as `usize` doesn't work either. So I have to manually check for cases where the index is negative (even though for cases where index is too large, `Vec::get` does the job). The use of `max` here mildly irritates me. If I limit the definition of neighbour to 2x2 ( [x][y], [x+1][y], [x][y+1], [x+1][y+1]), the code gets pretty simpler thanks to `get` for dx in &amp;[0, 1] { for dy in &amp;[0,1] { match field.get(x+dx).and_then(|r| r.get(y+dy)) { Some(&amp;true) =&gt; count = count + 1, _ =&gt; (), }}} and I'm thinking I'm missing something. Any help is appreciated, thank you!
It is incorrect to put a platform-dependent code inside `/usr/share` because that directory could have been mounted by different machines over NFS. Platform-dependent code should be in `/usr/bin`. `/usr/share` is meant only for resources (images, fonts...), which are platform-independent.
Does Launchpad even support building Rust applications?
Thank you! I may just do this if getting the PPA to work is too tedious.
Which is even on crates.io
Yeah, that's also true. My point is that I don't think it appeals to beginners, at all.
How does cargo-deb compares with [debcargo](https://anonscm.debian.org/git/pkg-rust/debcargo.git), that /u/Tobu posted?
I disagree with that, very much. That can be said about almost any language that people started programming with, especially C.
Also I could use the build.rs script to do this apparently, but I don't know how to do this from cargo when it downloads and builds the package from crates.io, here is the build.rs https://github.com/FractalGlobal/ntru-rs/blob/master/build.rs Any idea how I can set feature = "no-sse" and feature = "no-avx2" when I run cargo build after setting ntru = "0.5.6" in my cargo.toml? 
I think Rust already has a bit too much syntax, so i really, really don't want list comprehensions added. However, your example does illustrate nicely that the alternative is not very nice! What could we do within the framework of existing Rust, without resorting to macros, to improve this?
Now that sounds mysterious in a good way! Can you say anything more about it, or is it still super-secret?
&gt; See also the fact that we can actually make a named function equal to the type name (rusts syntax handles it fine). wait what? that does indeed work: [playground](https://play.rust-lang.org/?gist=739624b35dbea5ebf850fbe52a7c5093&amp;version=stable) Do have more information on this? Eg. documentation where this behavior is described?
I got it working, in my cargo.toml I had to do it as such: [dependencies.ntru] version = "0.5.6" features = ["no-sse", "no-avx2"]
I can't immediately tell from the readme: does this make source packages?
i think it's because functions and types are clearly in separate namespaces; &lt;ident&gt;{...} is selecting 'ident' from the 'type' namespace, whilst &lt;ident&gt;(..) is selecting from the 'function' namespace; other times types are referenced after a ":" (e.g. arguments) which clearly selects the type namespace. Now what I mention it I suppose *tupple structs* are crossover; I wonder if this is an unintentional loophole? but they can't close it now :) is the tuple struct creating an initialiser function, or is it searching for the tuple-struct when it doesn't find a function? the fact that it's a separate function we can define means we can avoid the hazard of re-ordering fields. Let me restate how strongly I despise ```Foo::new()``` ... if that was compulsory , it would be enough for me to reject Rust altogether. I'm so glad this workaround exists :) 
&gt; [`cargo test` now fails if no tests found](https://github.com/rust-lang/rust/pull/43145) I think that PR is about the compiler's own build system (`x.py`), not Cargo!
That's actually really cool! While I don't necessarily despise `Foo::new()` I've tried to work around it by defining an impl eg. `From&lt;(T, T)&gt;` for `Point2&lt;T&gt;`. Using it by being generic over `T, Into&lt;Point2&lt;T&gt;&gt;` doesn't work very well with type inference. Implementing this workaround right now as we speek, thanks! :) Extra +1 it doesn't require an extra import, importing the type also imports the function. Doesn't work after you type alias it but you can always create another function with the same name as the type alias.
I don't think the similarity of the execution model, FFI, etc, is relevant to sharing source files. The similarity of syntax and macro system could be. But mostly, i think it's just an unnecessary contortion to try to share source files between languages. Data files, fine. But source is inherently specific to a language, and you're going to trip yourself up eventually trying to share them. Thinking about this a bit more, i think i'd be tempted to put the constants in C, and then use bindgen to export them to Rust during the build. Then a maintainer of the C code is free to manipulate them in whatever way they like, without having to worry about whether it's going to work in Rust. 
it would be nice, IMO, to be able to declare FFI-friendly interfaces that are language agnostic, which would really mean 'being able to define them separately to any specific language' (i.e. you could see the interface and not care if the underlying implementation was C,or,C++ or Rust, whilst having those functions equally available in C,C++,Rust)
I too have struggled with coming to terms with Rust's module system as it's so different from how I'm used to think in C++. But over time I've come to accept its differences and just use the conventions Rust wants to you do ("When in Rome, do as the Roman's do."). One of the suggestions coming from the Rust developers is to allow more items in `impl` scope like consts and statics. Perhaps this can later be made to include additional structs, types and others (like how you can specify associated types in traits). I'd link you to the relevant discussion but I don't know how to find it again.
These TWiR posts are amazing to keep track of Rust development! For me specifically I mostly follow this subreddit. The TWiR sections most interesting to me are "Updates from Rust Core", "Approved RFCs" and "Final Comment Period". Thank you nasa42, llogiq, and brson!
Coroutines and yield can be another way to represent this. I don't know what's the current status of coroutines in Rust, but it could look like this: t = (for a in 1..n+1 { for b in a..n+1 { for c in b..n+1 { if a.pow(2) + b.pow(2) == c.pow(2) { yield (a, b, c); } } }}).collect::&lt;Vec&lt;_&gt;&gt;();
I can't recommend [the book](https://doc.rust-lang.org/book/second-edition/) highly enough for getting your head around ownership, etc. ECS is a pattern for structuring your game state and the logic that drives it. Instead of having a bunch of classes inheriting from each other (Animal inherits from Physical), it encourages splitting out all the different "natures" that your entities might have into different components (Velocity, Position, Renderable, Collidable, HoldsOtherStuff, Grue -- where Grue would contain _only_ the state that is unique to a Grue, etc.), and then each individual entity may or may not have instances of each kind of component. E.g. a cloud might have Position, Velocity, and Renderable, but not Collidable. From there you register a bunch of systems, and Specs will dispatch them automatically for you, running systems in parallel if they're not fighting over the same set of components. A system in Specs can have some state bundled in with it for convenience, but at its heart it's just a function that operates on some set of component types. So your physics system will iterate over all entities in the world that have Velocity and Position, and do something special with them if they also happen to have Collidable. My understanding is that it's generally best to keep systems doing as little as possible each, because it makes it really easy to switch in and out logic as the design of the game evolves. Have a look at the Specs examples; seeing the example code will communicate far better than I can how easy this pattern makes it to keep your game from turning into a giant mess of inheritance and confusing ownership graphs. I am by no means an expert in this stuff (my day job is very web-heavy, too) but I have found Specs really nice to work with, and I'm using it in [my toy game project](https://github.com/jeffparsons/planetkit).
&gt; These TWiR posts are amazing to keep track of Rust development! Thanks! :-)
This is the first time I heard about Rust On L4re, and it looks interesting. Thanks!
In all cases that I've actively noticed the final implementation differing from the original approved form of the RFC the RFC had been updated to match the implementation. So RFCs are usually pretty good documentation, if not necessarily what people usually mean by a "language reference."
What's the best way to give feedback about rustfmt, just open issues in the repo? Just tried it again after a while, and the default style generates some weird and/or inconsistent formatting, to the point that it sort of drives me to avoid rustfmt in the future. I was wondering if there's a point to commenting if the default style is still evolving.
What? I thought Rust doesn't crash?
Or you could just replace `yield (a, b, c);` with `t.push((a, b, c));` and prepend the whole thing with `let mut t = Vec::new();`. Then you no longer assign the whole block to `t` and remove the `collect`.
I can take writing named constructors for more advanced cases (e.g. a 'window' constructor could indeed have so many variations you might want to name some..) - I do also like embracing the idea that 'actually, constructors are just functions that happen to return something..' ; but I think '::new()' triggers me for the overlap of several reasons: it's got a fugly '::' in it; it's repurposing the word 'new' for initialisation instead of allocation; (whilst at the same time they're off debating how to do 'boxing/emplace' , surely they could be off making an 'operator-new-on-steroids'); it's bloating the name of the trivial case. I'm happy to break conventions where they suck :) conversely I find myself drawn to tuple-structs quite a bit, (when they got the ```'.0 .1 ..'```` accessors, they became awesome); I definitely like being able to group values and just refer to them by place, and rely on the types (rather than the field name) to work. vector maths... 'struct Vec3&lt;F&gt;{x:F,y:F,z:F} .. Vec3::new(...)' seems awful, but Vec3(F,F,F) with .0 .1 .2 accessors is great. but with the fn Vec3() i can have my cake and eat it.
That's what I do - upon noticing any suboptimal formatting I just report it. The rustfmt developers have been pretty quick to acknowledge and fix such issues. Reporting issues that come up in the real world also [uncovers](https://github.com/rust-lang-nursery/rustfmt/issues/1713) cases that are easy to miss in the fmt-rfcs discussions.
This is really, well, cute :) I'm also wondering if it has the one advantage that vanilla loops have over fluent iterators, i.e. the ability to use `?`/`try!` to exit from them cleanly. (Chained iterator adapters don't allow it since the implicit return from `?` would be from the inner closure, not outer function). The documentation for `cute` didn't show any examples for this, but also didn't rule it out, and the code is a little hard to follow to figure it out.
It must be a linked crate. Is there a way to override this? edit: code is here btw https://github.com/insanitybit/sqs-service-helper
Yes, but sometimes you might want this t as an iterator.
When I last looked at libsyntax, it panicked whenever there was a syntax error, and there was a `catch_panic` somewhere in the compiler driver that caught it and reported an error. Fun times!
macros are not evil in Rust, nor an option of last resort, they're a tool. overusing them isn't good, but if cute is the main macro you use from outside the stdlib, I personally don't think it has any negative effects on readability. now if you use a macro for everything, that will have a negative impact on code readability.
Uh no. It guarantees it memory Safety from things like double frees or things like that. It can still crash if something like panic!() I'd called or unwrap() on a None value but in each of these cases that's up to the programmer to decide rather than having UB cause a segfault
Currently working on [rust-3d](https://github.com/I3ck/rust-3d) . It's coming along nicely. I'm quite pleased with my traits and types. It's currently lacking in tests and algorithms, but that'll come.
Is there an open repository where we can test this out?
Rust Belt Rust plans to record the talks as well, but not the workshops. Workshops are hands-on so they don't fit will with recording. It also gives more incentive for people to come on out and meet their fellow Rustaceans!
I just vaguely remember that the rust homepage had something like "free from crashes". It now says "Rust [...] prevents segfaults" (just as you explained). What's the difference for someone who is using a Rust library between a segfault and a panic? Both result in a crash.
Panics can be caught. Additionally, there's often lots of arbitrary memory corruption leading up to a segfault.... (assuming it's not just a single NULL deref, which can happen too, but is pretty rare IME in prod software)
&gt; I'm writing what I intend to be a commercial game in ggez. I want to know more! &gt; mobile ... will be hard(er) if we leave SDL2 behind Actually, I find cargo-apk to be more convenient (but not nearly as reliable or tested, of course). You just install android-ndk and then run `cargo apk` - done, here's your `%projectname%.apk`, sir.
I've noticed you're using hyper 0.10.*. Any plans to move to hyper 0.11.* ? I really wonder how clients like this are implemented with futures and reactor stuff.
&gt; Plus, you can often avoid unnecessary collecting into vectors. Python also has "generator comprehensions" It has built in comprehesions for lists, sets and dicts; then there are generator comprehesions which just create an ad-hoc iterator basically which can thus be used to form any collection that can be formed from an iterator. If you ask me list comprehensions are useless and needless syntax and so is the for loop in Rust. I don't get this fetish with special syntax. Really a `Iterator::for` method you could use as so: (0..100).for(|i| { /* code */ }) Is good enough; is there really need for special syntax? Do we need some special syntax for `map` or `nth` on Iterators as well? I mean for i in 0..100 { Is a whole one character shorter than: (0..100).for(|i| { Is that really needed?
I don't remember the Rust homepage ever saying it prevented crashes. I've seen other people say that it's easier to write stable software in Rust, so maybe you were confused? https://kb.iu.edu/d/aqsj &gt; Segfaults are caused by a program trying to read or write an illegal memory location. [...] A segfault occurs when a reference to a variable falls outside the segment where that variable resides, or when a write is attempted to a location that is in a read-only segment. In practice, segfaults are almost always due to trying to read or write a non-existent array element, not properly defining a pointer before using it, or (in C programs) accidentally using a variable's value as an address That is not allowed to happen because of how Rust handles memory. A panic is more controlled and can output an error message, unlike a segfault. For the person that needs to do the debugging, a panic is much better.
Good stuff. Some comparisons with other pdf crates will be useful.
Panics are intentional. It's the difference between a segfault and a exit(-1), I guess.
A segfault usually results from undefined behavior, so it is likely to be exploitable. A panic is shutting down the program in a controlled way (destructors still run).
This is it: https://cgit.kde.org/kdev-rust.git/
Also, I will prefer builder pattern for constructors like this one: let line1 = Line::new(points1, true, true, true); Same goes for: image2.add_to_layer(current_layer.clone(), None, None, None, None, None, None);
Part of the reason is that it's already available in the compiler making it easier to implement the RFC. I think the other reason is that you would have to have that trait marker everywhere it's needed. Pretty sure it would clutter up function signatures. I don't see why you couldn't do it this way, I just don't see a clear reason as to why we should you know?
There might be better reasons, but the two that jump out to me are that any kind of type error would now include `MustUse` as part of it, even though that doesn't really add information. Furthermore, it seems like an antipattern to introduce an empty trait, although I suppose I can see value in it.
What's the reason for `lopdf_bugfix` and `rusttype_bugfix`? Are you maintaining forks of both?
People use C in computer science courses? I would use neither C nor Rust. You either teach rather abstract computation and algorithmic complexity or you teach the inner working of the machine and use assembly. I'm not sure what computer-science related stuff can be taught with C as an example language.
I'm making an SNES-style JRPG thing, somewhat inspired by the Dragon Warrior series. I'm keeping the source under my hat for now; once I figure out what the heck I'm doing with it I might open source the engine and just keep the assets proprietary. Not sure yet. ggez has hardly been hard to work with though. The goal is to have it publish-able or close to it by the end of the year, which... eeeh, might happen. I thought glutin didn't properly support android/ios? If I'm wrong, then I would be overjoyed. I confess I don't actually know much about those ecosystems and how one gets Rust code working on them; I should learn more.
`must_use` is intended to be a lint, not a hard type error. What you're looking for is true linear types, which are unfortunately very viral and is an open research problem whether they can integrate well with Rust's affine (has a destructor) types. If we had say a `Linear` auto-trait how would you write generics? For backwards comparability all existing generic functions couldn't except it, so now you'd need to take a `?Linear` bound on your function. But what if you store it in a Vector? Well Vectors move things around when resizing and can implicitly drop elements, can you use it with `?Linear`? I believe /u/Gankro has much longer and better explanation floating around on his blog.
&gt; Furthermore, it seems like an antipattern to introduce an empty trait, although I suppose I can see value in it. There are already empty marker traits like `Send`, `Sync`, `Eq` AFAIK.
&gt; Panics can be caught. Segfaults too, see https://github.com/Plaristote/segvcatch for example.
&gt; I don't remember the Rust homepage ever saying it prevented crashes "Rust [...] prevents almost all crashes*" https://web.archive.org/web/20150202171131/http://www.rust-lang.org/
If I do my_weak_ptr.lock()-&gt;foo() the SEGFAULT is also intentional and I see little difference to unwrap() in Rust.
I wrote C bindings for parts of `libapt-pkg`, which is what `apt` on Debian and Ubuntu uses for managing packages, then wrote [Rust bindings](https://github.com/FauxFaux/apt-pkg-native-rs) for the C bindings. I tried other things (e.g. `bindgen`); but the library is [too insane](https://github.com/FauxFaux/apt-pkg-native-rs/blob/3314733ebb4990d447cfb1132150fbfcb7db78c6/src/lib.rs#L11) to not jump through this intermediate. The intention is that it can be used by [`debcargo`](https://anonscm.debian.org/git/pkg-rust/debcargo.git) to build better Debian packages from Rust crates.
Wait what? How do we _not_ have a hall of fame for crashing rustc?
&gt; A panic is shutting down the program in a controlled way (destructors still run). Problem is, if I use a Rust library in my C++ program (e.g. KDevelop), my C++ destructors won't run on a panic, will they?
Wow look at that. That's some poor wording indeed. Though I like the disclaimer: "Rust prevents most crashes! Lol, jk, it might actually eat your laundry".
https://gankro.github.io/blah/linear-rust/
We have https://github.com/steveklabnik/glacier.
I can't say why it's secret at the moment, but I can give you the basic gist: what if it was just as easy to create a basic web service as it is with Node? That is, with Node you can var http = require("http"); var server = http.createServer(function(request, response) { response.writeHead(200, {"Content-Type": "text/plain"}); response.write("Hello, world!"); response.end(); }); server.listen(8080); console.log("Server is listening"); and you're done; that's it. What would something like this look like in Rust? It's not copying this exact interface, and for real projects, you'd want a web framework. But a lot of beginners just need a simple UX, and some extremely small basics. I'm building it on top of synchronous IO because &gt; I wanted to learn some Rust and how webserver were working so I decided to implement a Webserver in Rust. this is a great goal. So I want to make it simple for people to peer under the hood, see how stuff works. And finally, it's kind of an extension of the project at the end of TRPL.
A tuple struct generates an initializer function - see https://play.rust-lang.org/?gist=afdee958b42f44696d3739ee6b1a9b46&amp;version=stable for an example.
&gt; it's already available in the compiler Sure, in the context of RFC 1940 it makes sense. I was wondering why it was done like this initially, though. &gt; it would clutter up function signatures Why, though? `Result&lt;T, E&gt;` currently implements `Sized` and `Copy` and `PartialEq` and `Eq` and `Carrier` (on nightly), and `Sum` and `Debug` and ... but you don't see those cluttering function signatures either. You'd just return a `Result&lt;_, _&gt;` or a `MutexGuard&lt;_&gt;`, etc., and the lint would figure out that it's `MustUse`. The only places where you'd have to add things to the function signature, are the return types of functions you want to make 'must use'. I.e., once per function you're annotating. With RFC 1940, you _also_ have to add something once per function you're annotating.
&gt; I don't see why you couldn't do it this way For one, the proposed syntax of `fn foo() -&gt; u8 + MustUse { 0 }` doesn't make sense. That's not syntax that exists in any part of Rust, so it's just a completely different way of writing a `#[must_use]` annotation, but more complicated (because it requires changes to the actual language). So more broadly the answer is "because annotations are more powerful for less effort, given what we want to actually do with this". (marking that certain values or types cannot be silently ignored when returned)
It's been pretty hectic but if I can by some miracle find some free time this week I'm going to start working on a linear algebra library. I've heard a lot of people complaining there isn't a good one in rust. I doubt that mine will necessarily solve all of their problems, but it should be fun to see how much linear algebra I actually remember.
interesting to confirm that; does the compiler do something special to bring those in when the type is 'used', perhaps
Any compare with https://github.com/servo/skia ? I mean take skia, create canvas and attach canvas to pdf, and then draw text and stuff with it.
No, I have done a pull requests on `stb_truetype` which contains a serious bugfix, but the author seems to have left the repository. Same with `rusttype_bugfix` (same author). I don't know if they will ever be merged or if the author comes back. Otherwise, I have to migrate to `freetype`, but there are issues on Windows (see issue #1). `lopdf_bugfix` is different, the author is rather responsive, however he didn't release a patch version after my last merged PR. I've created an issue for this, but I wanted to release the crate today so other people can start using it. Once the new version is released, I simply swap the versions in the Cargo.toml, etc. `lopdf_bugfix` is the same as `{ git = "https://github.com/J-F-Liu/lopdf" }`, however cargo does not allow you to leave these in if you release a crate. 
yes, I know. I have toyed around with many libraries, etc. The most important thing was that I needed layers, which almost no library has because they are usually not very important. Not sure if skia supports this. Plus the build process for skia is very painful. I know that there are libraries that do almost or all of what this library can do. It's just that I need security that certain features are supported. Not sure if Skia does ICC color management or overprint or blending modes or or or ... I needed absolute control over the PDF and what I can do, because at the end of the day, that's what I'm selling to people. There should be no excuse "oh I can't do this because the library I am using doesn't support it, blah blah". Customers don't care about excuses.
Have you tried a release build?
it was the original post link, so it's over here: https://github.com/achntrl/rust-webserver 
Nitpick: the provided code calls `Data::new()` independently of whether the option is `Some` or `None`. Might not be important, but I'd consider it cleaner to use `.unwrap_or_else(|| Data::new())`. Or, if you want to get fancy, add impl std::default::Default for Data { fn default() -&gt; Data { Data::new() } } and use `.unwrap_or_default()`. EDIT: I should probably link to the docs for clarity. * [Default](https://doc.rust-lang.org/std/default/trait.Default.html) * [Option::unwrap_or_default](https://doc.rust-lang.org/std/option/enum.Option.html#method.unwrap_or_default)
I've added a few simple benchmarks [here](https://github.com/jkarns275/BufFile-rs/blob/master/src/lib.rs#L11), results are in README.md. I also have tested BufFile in a BTree I wrote and it sees performance gains in excess of 2000% (the original implementation used File).
Unwinding across language boundaries is undefined behavior (this is the reason for the existence of [`catch_unwind`](https://doc.rust-lang.org/std/panic/fn.catch_unwind.html)). Are you talking about Rust panicking or C++ panicking?
Yes and indeed the the slowdown disappears, but I was interested to understand what the program is really doing (approaches and best practices) to justify such behavior. AFAIK the release build may actually revert my change and logically move `buffer` outside the loop again.
yeah, this is a good idea. It's OK for now, I guess. The line thing is because if you end a path in a PDF, you have to specify an operator (stroke, fill, close, clip). I wanted to abstract this away, so this is why it has several operators. Clipping will be done later, but with a different API. The image thing is because usually if you add an image, you want a certain position and rotation and size. Images have a default size.
For the comparison, there are currently only three PDF libraries for rust: - rust-pdf - can only do the 14 default fonts + simple graphics - libharu-rs - does not compile on the latest version of libharu (threading issue with bindgen). Also can't do layers - pdf-canvas - can only do the 14 default fonts + simple graphics Non-Rust libraries: - skia - not sure. seems to be a good library, but I didn't want to take risks. Probably can't do layers. - Adobe library - $$$$ - poppler, podofo, jagpdf, etc. - I was simply unsure about what they support and how good the PDF output is. Some didn't compile, some are unmaintained, etc. I didn't want to take risks.
I think that there is a key difference in teaching a CS course versus teaching a programming course. A lot of people in this thread are saying that high level garbage collection languages are a better starting place. For a programming course I would completely agree, however CS is not strictly programming. My school started out of C++ at the beginning for the main track of classes and then had a class on the side for C and assembly with hardware development. I felt like this was a good way to do things because for the first semester we did almost everything (aside from one project I think) without worrying about memory management because everything was small enough to just stay on the stack. Then by the time we'd gotten that down we could move on to pointers and such. I liked this approach because it's important to get the very basics of programming down but it is also important to understand how computers work. We have all of these great new languages nowadays but at the end of the day our computers still have registers and memory addresses. For CS it's important to understand those details early on to have a solid foundation.
Well one difference is that it would be zeroing out the buffer on every iteration. Edit:. I'm also curious if rustc is calling drop on every integer in the buffer. Which does nothing here and will get optimized out, but in a debug build that might be a loop of calling drop 1000x
But in which context you use it? Actually I saw two usage examples of pdf generation. The first one, you have already something rendered on the screen, and you want print it to pdf. That's why `skia` have pdf backend, and as I remember `Qt` have similar feature. The second one, you want to print some document, but because of pdf is too low level, you allow user to work with html/xml, and then again you have some big fat library, like `wkhtmltopdf` that render it to pdf for you. But what to do with such low level pdf control?
Right, "doesn't make sense" is perhaps a bit harsh, but it's indeed "not syntax that exists in any part of Rust". So my solution wouldn't work for RFC 1940. In case it wasn't clear what my definition of `foo` was supposed to be, here is different way of writing it (mind you, equally invalid syntax) : fn foo&lt;T&gt;() -&gt; T where T: u8 + MustUse { 0 } This indeed doesn't work, since `u8` is not a trait. In fact, written like this, it doesn't make much sense: `foo` is parametric in a type that is fully constrained and hence not generic at all. The original version (`fn foo() -&gt; u8 + MustUse { 0 }`) wasn't type parametric, though. It just 'added' a marker trait `MustUse` to a type. Ostensively the ability to do so isn't useful anywhere else than in this RFC, so Rust doesn't support adding trait bounds to anything that is not a trait itself. I must admit I'm not entirely clear on what sort of traits you can `+ T` onto an existing _trait_ bound either...
Rust panicking. C++ "panicking" would be segfault / exception, right?
`&amp;utfstring` is a pointer to a string that will be dropped when `init` ends. Look at how [lazy_static](https://github.com/rust-lang-nursery/lazy-static.rs) is implemented, or even better - just use it directly.
So cool! Actually makes me want to try out KDevelop...
I don't see why it wouldn't if you `Build-Depends` on rustc / cargo.
Thanks for that. Clearly adding relevant types would be a hassle, and indeed mess with everything else (echoing the concerns w.r.t. "clutter up function signatures", or "that any kind of type error would now include MustUse as part of it" voiced elsewhere in the comments in this thread). I do feel that there is some difference though; I was not suggesting having a 'real' `Linear` trait, but merely a `MustUse` trait that has exactly the same reach (and hence limitation) as the existing `#[must_use]` annotation, and could hence leverage the same linting infrastructure.
TIL panics are like exceptions and should be used for regular error handling
`+` in trait bounds is basically just a comma in a list (which we can't use, because commas are already taken for generics). Foo: T + U + V is just "Foo conforms to T, U, and V". Edit: also yeah sorry if the previous comment came off as a bit mean. Just woke up, need caffeine and breakfast. :)
No, they shouldn't be. I don't know the precise history of rustc's parser, but that Rust code has had to evolve as the language has changed. IIRC, Rust didn't always have the `Result` style of error handling that we have now.
nice catch! i'll switch to impl Default, that seems like a better solution for this situation
The difference is that segfaults are caused by an error caught in the kernel when invalid memory is accessed. If the page your process is trying to access is not in memory and cannot be loaded from somewhere (e.g. swap), then the only reasonable thing for the OS to do is to kill your process (well, roughly, Linux sends a SIGSEGV signal). The point though is that segfaults are caused by trying to access unmapped memory. A segfault is not guaranteed to happen if you try to dereference any invalid pointer, as it may point to memory which is invalid, but mapped. In that case the dereference will succeed, but you'll likely have invalid data at that memory location, or worse yet, data that belongs to another object. This can lead to all sorts of weird bugs. That's why accessing a dangling pointer is undefined behavior. Panicking (Rust's name for throwing an exception) on the other hand is very well defined: an exception is thrown, the stack is unwound, destructors are called in each stack frame, etc. Your example would cause a segfault, but in a much more controlled manner: when you call lock() on a std::weak_ptr in C++, if the corresponding shared_ptr has expired, this will return a shared_ptr containing nullptr. The page at address 0 is intentionally kept unmapped by the OS so as to trigger a segfault on a null pointer dereference. (at least on the most widely used operating systems today)
&gt; Well one difference is that it would be zeroing out the buffer on every iteration. Yeah but it still does not justify the slowdown IMHO, try with a simple C program: #include &lt;stdio.h&gt; int main() { unsigned long count = 0; /* char buffer[1024] = {0}; */ while (1) { char buffer[1024] = {0}; size_t size = fread(buffer, 1, 1024, stdin); if (size == 0) { break; } else { count += size; } } printf("%lu\n", count); } Rust must be doing something more. &gt; I'm also curious if rustc is calling drop on every integer in the buffer. This makes sense to me but I don't know the Rust internals well enough to dig deeper.
Ok, I'm currently trying to get lazy_static working. I'm not sure how to put: -&gt; std::io::Result&lt;()&gt; into the lazy_static macro. I currently have this: lazy_static! { static ref GLOBALSTRING: &amp;'static str = { let mut utfstring = String::new(); let mut file = File::open("config").expect("No config file found."); file.read_to_string(&amp;mut utfstring)?; &amp;utfstring }; } I get the same compilation error when I remove the std io result part in the init function. 
Why do you need to put it there? Just `unwrap` the `Result` instead - your original version will also crash if reading the file fails, and an explicit `unwrap` will only make it crash with a clearer error message.
It was just a joke. But wow, that's an interesting concept: code that doesn't follow current best practices because they were created before those best practices.
There is a discussion about making it asynchronous here : https://github.com/hugues31/coinnect/issues/9
I'd start by feeding both versions into http://rust.godbolt.org/ and seeing how they differ. It'll show you the assembly language generated from your Rust code , use colors to show how each line of Rust maps to each span of assembly, and supports diffing two assembly outputs. From what I remember, unoptimized is default. (I'm not sure it even supports comparing optimized output yet. I know it didn't at one point.) Here's a diff view I set up for you: https://godbolt.org/g/r1T6Vu (It's typically intended for diffing different compilers against each other, so you'll need to click the "rustc 1.18.0" drop-downs to see the more detailed versions which say which side corresponds to which source tab.
As long as debug is "fast enough," I don't think folks pay much attention to what's actually going on because it doesn't really matter. I think your best bet here is to look at the generated code. In general, there's no real reason to expect Rust's debug code to be the same as debug code emitted by a C program. Rust tends rely much more heavily on the optimizer to peel away abstractions, which usually means lots of function inlining. I don't think anybody is going to be able to give you anything better than "here's a guess" without looking at the generated code.
How? &amp;utfstring.unwrap() doesn't work.
This is so cool, I didn't know godbolt.org supported Rust, thanks!
heh, looks like there's [a whole bunch of them](https://doc.rust-lang.org/std/marker/index.html) in `std`. I guess that's not a very good reason
Got it, thanks for the clarification.
I was thinking about the limitation on trait _object_ bounds that triggers [compiler error E0225](https://doc.rust-lang.org/error-index.html#E0225). It looks like those are owing to the way the V-tables are built, so I guess all marker traits could be supported as additional bounds. Oh well, that wouldn't be sufficient to make my solution for `must_use` work anyway. Silver lining: I learned me a Rust today (for Great Good).
He's saying you can use the function you already wrote. Just call it and unwrap it: `init().unwrap()`
Doesn't change anything. utfstring still doesn't live long enough. http://imgur.com/VXNt8oU
Maybe you could go with a [Snap](https://snapcraft.io/docs/build-snaps/). Snap-packages are the successor of .deb packages for **applications** on Ubuntu and are much easier to create. For distribution you could do: * Send them the snap and let them install it like this: `sudo snap install [snap_name] --dangerous` * The better way would be to create a repository on GitHub, containing your code and a snapcraft.yaml, than use [this](https://build.snapcraft.io/) service to automatically build snaps out of your master branch (it's super easy to setup). After some testing, propagate the auto-build snap from the edge release channel to the stable channel ‚Üí all users of your app get the newer version automatically.
Your function needs to return the string for `lazy_static` to work its magic. 
Exciting to see this fixed! I ran into the same parsing issue and ended up with a [work-around](https://github.com/iliekturtles/uom/commit/6b80140f46ff83822d9391d21875193ad75f8a39) in my macro.
IMHO, skia is huge. It's overkill to use for a simple stuff. Also, afaik, it's very GPU oriented, which is useless for tasks like this.
"crash" is an overloaded term in computing. For many systems programmers, "crash" means a segmentation fault. An explicit panic doesn't count; since an explicit panic is a deliberate thing that brings down the program when something has gone wrong before stuff can get worse. For end users, crash usually just means that the program stopped without being asked to. The reason this distinction is important is that segfaults are usually indicative of larger issues. If you had a segfault, you likely triggered UB, and the compiler is actually free to do scary optimizations that break things further and can potentially be exploited. An explicit panic, on the other hand, has well defined semantics, and while it brings the program down, it isn't exploitable and doesn't enable scary optimizations.
The first thing that came to mind is `rst2pdf`. It's currently abandoned and it will be great to have an alternative.
How do i make a static a String? static mut GLOBALSTRING = String::new(); doesn't work. static mut GLOBALSTRING: String::new() = ""; doesn't work either. How do I do the other things you named? 
I'm sorry, but if your C code is relying on segfaults for error handling it is terrible C code. There are two issues here. Firstly, segfaults may not always occur. If you try to read from null, yes, you will have a segfault, but a read from an invalid pointer may point to allocated -- but wrong -- memory. This means that your call may end up using garbage information. Often this garbage information can be turned into an exploit by tricking the program to overwrite executable memory. Now, even if you've ensured that you have no use after free or invalid pointers and the _only_ case you are worrying about is the null pointer, this is _still_ bad. Because there is no guarantee that even reading from null will segfault. At _runtime_ if there is an actual memory read instruction it will segfault. However, at compile time, you do not have this guarantee. It is _undefined behavior_ to read from null; which means that the compiler gets to assume that the pointer is not null and run optimizations based on that. That can cause pretty much arbitrary behavior if violated. Never use segfaults for error handling in C or C++. Explicit calls to abort are fine, but if you're dereferencing a pointer, null-check it (and explicitly abort in the null case if you wish). If you don't it is undefined behavior and anything can happen.
C++'s version of panicking is an exception. However in C++ exceptions are often used for general error handling (e.g. throwing an error the API consumer must catch), whereas in Rust panics are mostly used for cases where the programmer knows that there is a program bug if it panics. In the case of a program bug of that kind, you basically have no choice but to abort the application. Rust lets you run panics as aborts or unwinds (it's a compile time option). Unwinding panics behave like exceptions, and can be caught. They also get caught by default at thread boundaries. The intention is not for this to be a general purpose error handling thing, but it helps when you're doing FFI and stuff. Aborts will immediately terminate the application with no hope of being caught by regular means. Many C++ codebases do this too, they turn off exceptions and use aborts for "there is a bug in my program if this happens" errors.
Good point. Are there Rust deb packages already?
Unfortunately signal handlers aren't compositional :)
&gt; Well Vectors move things around when resizing and can implicitly drop elements ...Can they? When? 
&gt; It sounds to me like you're expecting corrode to do more than it was designed for. It sounds to me like est31 wants to do more than corrode was designed for, and so has quite legitimately chosen not to use it.
It adds clutter to an extremely common task that's handled cleanly in numerous other languages. To me that's a negative impact on readability.
Some would say this macro removes clutter.
What does that mean?
Types are free theorems really.
&gt; Furthermore, it seems like an antipattern to introduce an empty trait Empty traits are actually very valuable pattern.
Sadly enough CI has been failing, which means some of these are fixed, but i haven't had time to fix them yet!
Glad that wording has been changed.
&gt; You can't subtract 1 from usize Of course you can. You just shouldn't subtract 1 from 0usize (which you avoid by the `max`). In general, I would just add one row and column of empty cells along the edges of the matrix, which gets rid of all the conditionals in the indexing, and since they are always false, they don't change the result. (Plus, typically higher-dimensional arrays shouldn't use the `Vec&lt;Vec&lt;...&gt;&gt;` construction, as that introduces unneeded indirection - use a 1D Vec or a dedicated matrix data structure from a crate.)
Reminds me of the outline for Ivan Godard's latest Mill talk: &gt; Ivan Godard is CTO and a founder of Mill Computing, Inc., developer of the Mill family of general-purpose CPUs. He has written or led the development team for a dozen compilers, an OS, an OODBMS, and much other software. **He has no degrees and has never taken a computing course; such things didn't exist when he started.** *(For context, he participated to the development of the ALGOL60 compiler, in the 60s)*
The more I play with this type of stuff (eg, [[nodiscard]] in C++) the more I start to believe that it's far more correct to opt-in to ignoring a return value than to opt-out of it. I actually thought this was a standard warning in Rust until ignored a return value yesterday on purpose and it didn't warn. I was a bit disappointed.
Oh nice, this would be very handy in a project I'm working on where I need to de/serialize Box&lt;Trait&gt; and currently have to rely on a static global to dispatch the deserializer to the right struct based on a flag in the serialized data. This would allow me to inject that collection at runtime instead and allow it to be properly extensible.
The Rudy 0.1.0 was release on July 17th. The major feature in the next release is iterator support.
Vectors can implicitly drop elements whenever they discard an item instead of returning it. For instance on [`fn truncate(&amp;mut self, len: usize)`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.truncate) or on [`fn retain&lt;F&gt;(&amp;mut self, f: F)`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.retain).
This allows, for example, to deserialize Box&lt;MyTrait&gt; by dynamically injecting a lookup table that reads a type flag in the serialized data and dispatches deserialization to the appropriate struct then boxing it. Presently this lookup table has to be static, which isn't cleanly extensible.
It should be slower because the syntax `[0; 1024]` means initialize a 1024-element array to 0. Each loop iteration it has to zero it, whereas C would leave it uninitialized. &gt; To my surprise, because I was assuming arrays to be allocated onto the stack and even if a zeroing of the array happens at every iteration it just doesn't really explain a ~7x slowdown (and actually a similar C code does not express the same behavior). Why not? The C code isn't re-initializing the array every loop, which turns out to be useless and the release-mode optimization passes realize this and elide it. If re-initializing the array is more expensive than the actual work done in the loop, it could cause a massive slowdown because it happens on each loop. Re-init could cause some poor branch predictions on your processor, or something else, which would slow it down disproportionately from just what a memcpy would do, since debug mode is very inefficient for most things. The arrays are definitely stack-allocated in this discussion. To prove that re-init is the performance difference, consider this code: use std::io::Read; fn main() { let input = std::io::stdin(); let mut input = input.lock(); let mut count: u64 = 0; loop { let mut buffer: [u8; 1024] = unsafe { ::std::mem::uninitialized() }; match input.read(&amp;mut buffer) { Ok(size) =&gt; { if size == 0 { break; } else { count += size as u64; } }, Err(err) =&gt; { println!("{}", err); }, } } println!("{}", count); } It chooses not to initialize the stack array prior to use, and on my machine at least, it performs equally well as the version did with it outside the loop.
Sorry I was unclear, vectors can implicitly move elements when resizing, this may violate a `?Linear` requirement depending on the definition. `Vec` also has methods that can remove elements without transferring ownership such as `truncate`. It also has methods like drain that transfer ownership that are effected by the viralness of `?Linear` types, many current uses of `drain` would violate linearity requirements such as `vec![1, 2, 3].drain().take(2).collect()` by implicitly dropping elements. Gankro's post covers this better.
Rust does not call Drop on types that do not implement Drop, even in debug mode.
 use std::fs::File; use std::io::prelude::*; #[macro_use] extern crate lazy_static; lazy_static! { static ref GLOBALSTRING: String = init().unwrap(); } fn main() { println!("Global String: {}", *GLOBALSTRING); } fn init() -&gt; std::io::Result&lt;String&gt; { let mut file = File::open("config").expect("No config file found."); let mut utfstring = String::new(); file.read_to_string(&amp;mut utfstring)?; Ok(utfstring) } This is a fully working example using `lazy_static`.
Rust jobs section, pls stop being empty. ;_;
/r/playrust
A (possibly) more Rustic way to approach this would probably be to just read the file into a `String` and pass it by reference to each function that needs it, rather than having a global.
Alright, thanks. FWIW, I reported https://github.com/rust-lang-nursery/rustfmt/issues/1797 and https://github.com/rust-lang-nursery/rustfmt/issues/1798.
(shameless self promotion) Sounds like a nice and easy to implement addition to my [easyfibers](https://github.com/SergejJurecko/easyfibers) library.
There are a lot of ways to solve this. For really easy stuff, you could just spin an OS-thread off using a closure that waits until the right time, then does whatever it needs to. If you need to scale to a lighter-weight solution, you could implement a scheduled job queue using Rayon. Just have a global `Arc&lt;Mutex&lt;VecDeque&lt;_&gt;&gt;&gt;` with two helper functions: one to push jobs onto the queue and return an ID, and another to cancel/remove jobs with a given ID. On a separate thread, you would have a function looking for tasks that need to be executed. When a task is ready, spawn it using `rayon::spawn` and remove it from the queue. This will put it into Rayon's queue for immediate execution on the ThreadPool, and it will get executed as soon as there is a free thread on the system. You can probably do something similar with Futures. If you need a "serious" enterprise-grade solution that can handled hundreds of thousands of pending/active tasks, I guess you'll just have to try really hard.
&gt; There are a lot of ways to solve this. For really easy stuff, you could just spin an OS-thread off using a closure that waits until the right time, then does whatever it needs to. This is my current implementation but it means burning a thread for every timeout, and there are many, many timeouts (within a second you can expect hundreds/ thousands). &gt; On a separate OS-thread, you would have a function looking for tasks that need to be executed. When a task is ready, spawn it using rayon::spawn and remove it from the queue. How would it know when a task is ready? And how would it account for the time spent on the queue? &gt; If you need a "serious" enterprise-grade solution that can handled tens of thousands of pending/active tasks, I guess you'll just have to try really hard. This is for fun so trying really hard is the idea :P
Looks interesting, thank you.
&gt; How would it know when a task is ready? The queue monitoring thread would read through the list of tasks and find ones that have a scheduled execution time in the past. Ones that are still in the future would be left alone. You could have the queue monitor continuously spinning through the list of tasks, or it could check at a certain tick rate. &gt; And how would it account for the time spent on the queue? I'm not sure what the question is. You could store the `Instant` that the task was put onto the queue with the task and make that available to the task? If it needs to know how long it was in the queue. &gt; This is for fun so trying really hard is the idea :P This does look like a really fun thing to play with. I'm tempted to play with this myself, but I don't know that I have the time.
To make a more "enterprise-grade" solution, you would need to keep the task queue sorted by order of execution, which would speed things up a lot, of course, but then you have to balance the insertion cost and the sorting costs. A linked-list *might* actually make sense for this situation, since all you care about is the front item, except when inserting a new task or canceling one.
&gt;The first thing that came to mind is rst2pdf. It's currently abandoned and it will be great to have an alternative. Why not in case of `rst2pdf` use `rst` -&gt; `html` -&gt; `pdf`? I suppose it is more simple for coding, plus it gives more features, like for example usage of `css` to customize pdf generation.
&gt; The queue monitoring thread would read through the list of tasks and find ones that have a scheduled execution time in the past. Ones that are still in the future would be left alone. You could have the queue monitor continuously spinning through the list of tasks, or it could check at a certain tick rate. Well if the scheduled execution time is in the past, it's already too late. I could "buffer" and say like "if anything is 2-3 seconds away" and build that latency in. &gt; I'm not sure what the question is. You could store the Instant that the task was put onto the queue with the task and make that available to the task? If it needs to know how long it was in the queue. Yes, that would work. &gt; This does look like a really fun thing to play with. I'm tempted to play with this myself, but I don't know that I have the time. Agreed, I'm having fun already. I'll probably try a few approaches. I'm attempting to do a futures based approach currently but the rayon approach sounds good too. How would you solve the constant scanning of the list? I guess you could sleep until either a new message is enqueued or the last lowest time is about to expire.
Wouldn't an ordered SkipList work well here? Perhaps combined with a mutex/ wait condition for enqueues.
&gt; IMHO, skia is huge. My android app with hyper and bunch of other big crates plus skia, plus GUI on java is near 20M, I think not big. &gt; Also, afaik, it's very GPU oriented, which is useless for tasks like this. May be inside, but main API as simple as expected from 2D library - draw line, draw rect and so on. But my point is not specific for `skia`, `Qt`, `cairo`, any 2D drawing library, why use something specific only for drawing on pdf, why not take something that can draw on screen and also on pdf? I can think only for some specific for publisher software.
&gt; Well if the scheduled execution time is in the past, it's already too late. The past could be 1 microsecond ago or 10 milliseconds ago. It doesn't have to mean "30 minutes ago." Even if you execute the task at exactly the right instant in time, it will still take a few clock cycles for the branch into the function as well as to `push` whatever registers are going to get smashed inside the function, etc. So, unless you execute the task *ahead of time* (which would likely end up being the wrong time as well), it won't be precisely at some instant in time. So, you need to define (for yourself) how much variability is acceptable in actual execution time versus scheduled execution time. Zero is not an acceptable answer, since it isn't possible.
- no need for web browser to render it - cli only - faster - manual page-breaks (I'm not a webdev, but afaik it's hard to manage "pages" in html)
Yeah, naturally. In my use case it's actually fine and built into the logic that there's latency.
A SkipList is a Linked List that has additional links (layers) to access the interior faster. It might allow for faster insertion/deletion of tasks that are not near the front of the list, but at the same time, you would have to update more links (pointers) throughout the front of the SkipList, which could cost more than just traversing the linked list, depending on how many tasks you have on the queue and how many layers your SkipList has. It's definitely an option that *could* improve performance.
Oh, I think I've misunderstood what Linear implies
Tokio-timer has an efficient timer wheel implementation. Look at wheel.rs in there. For easyfibers that's what I used. Someone should make a crate from that wheel.rs module...
Well, it's an endless debate on specialization vs generalization. I prefer a tool that only do one job. I don't know about skia, but pdf support in Qt and cairo is tiny. Yes, you can draw lines and text, but that's it.
Great ! it worked ! Thanks for the explanation 
For what its worth, truncate and resizes are semantically ok, though may have some backwards compatibility concerns depending on how Rust would implement `?Linear`. You just need something to *explicitly* call the destructor.
Are there any plans to support a better Lisp than Emacs Lisp in this, like Guile?
Thanks, didn't know how to make a static a Stringtype.
Also would've been an option. Thanks.
You can't have two in a single program. You can't have, eg, Lua using it to abort contexts where an FFI causes a segfault AND ALSO have Rust using it to abort threads where an FFI causes a segfault. (Lua contexts are more fine-grained than threads). Exception-based / unwinding systems don't have that particular problem (although you can't catch exceptions from other systems and interleaving stack frames from incompatible exception/unwinding systems generally causes hilarious issues)
Regarding Rust's direction, is the language still changing much; and if so, is it getting simplifications, or added features? Is the language getting more complicated? 
I see, thanks for explaining :)
Even outside of Rust, you need a really, really good reason to use a non-static global. Just give an owned string to a struct and implement methods on it, this will turn out a lot better.
I have a similar piece of code (in Python) that uses a [heap](https://doc.rust-lang.org/std/collections/binary_heap/) of tuples (unix_t_wake, fn()) when it pulls something off that isn't ready it inserts it back into the heap and sleeps. Send the closure to a thread pool so the dispatch thread only dispatches.
are you planning to make a publishable crate out of this fun experiment? it's always nice to see what other people are up to in their code.
Would making it possible to explicitly call the destructer of all types in general help? (With it doing nothing for non-`Drop` types)
You can do that today with `std::mem::drop` the issue is really that the `Drop` trait has a great deal of special meaning to the compiler. A `?Linear` drop would have different meaning, and `?Linear` may even imply `!Drop` and you'd have to call some other consuming dtor function (or not! some types may chose to not be destructable).
I've tried a few times in the past to learn Rust and failed each time, largely getting stuck on ownership and lifetimes. Planning on making another attempt, and I see that the book now has a 2nd edition, and the new material on ownership looks *great* so far! Can anyone recommend good additional reading material on understanding ownership/borrowing, and lifetimes? 
Sorry, I should have read more of your code. :)
so the difference here is that `&amp;'static str` is the type for a string literal: it will pretty much be baked right into the binary, which means it's lifetime is the special `'static` lifetime, and it will "live" as long as the binary is in memory. `String` however, is a heap-allocated string that is created at runtime. Since `init()` runs after compile-time, the string it produces can't be `'static`.
I thought I'd write this down in case others might like to use the same approach or can improve it. The source is originally from [this thread](https://users.rust-lang.org/t/howto-generating-a-branch-coverage-report/8524/2).
Of course, I shouldn't forget that use case. In gluon I need to serialize trait objects as well so I will need to do something like that as well!
Rust is getting new stuff, but sometimes, those features can make the language simpler to use. Everything has to remain backwards compatible, so we can't remove stuff in order to simplify.
The language is guaranteed back compatible since 1.0, so it cannot be getting simpler in the sense of removing stuff. That said, the things being added often make the programming simpler overall. For example, in the latest This Week In Rust, they talk about accepting an RFC for allowing `?` in main and tests. Basically, instead of the return type needing to be `()`, you can also use `Result&lt;T,E&gt;` or any other type that implements the `Termination` trait. Sure, that means a new trait, some new impls, and similar. But it simplifies the amount of crufty wrapper code needed in test cases and such in an intuitive way. Its up to you if thats worthwhile.
This was changed quite a long time, libsyntax should only panic on bugs now
It's a curse. Anything tangentially associated with me (I filed the issue that led to the PR being written) will have something wrong about it in TWiR. Last week is was the FCP status of an RFC...
Thanks, Steve. I'm trying to gauge which way the Rust team leans when deciding between allowing addition of a useful feature, and keeping the language lean. Past experiences with C++ make me very wary of seeing features added, but I realize they're necessary sometimes.
"must use" doesn't have a meaning at runtime, only at compile time. It's not really a contract on what you can and cannot do with the type.
Thanks, kazagistar. Makes sense, and seems like the kind of thing that most languages would see added after enough use in the field has revealed some sharp edges that need to be smoothed down. 
i recently used apache pdfbox in a project, might want to compare to that 
One of the problems with this is to decide which data to validate. For example, notice that `self` also overlaps with both `a` and `b`! My "types as contracts" model is not based on whether you actually use some value to do something or not, it is based on the idea that the information is in the types, and in the types only. Such an approach cannot distinguish `a` overlapping with `b` here from `a` overlapping with `self`. Maybe there is a way to refine the rules for where to emit validation statement such that it would catch this inside split_at_mut, but so far, I wouldn't know how to do that.
http://rustjobs.rs/ also has a bunch of stuff
I know, right? If I had a good enough Rust opportunity, I'd definitely jump for it at this point. :)
I think that your solution need deeper integration into `rustc`/`cargo`, ideally it should be like `cargo build --with-coverage` and `cargo`/`rustc` should take care what include/exclude to calc coverage of this crate. May be already exists issue about coverage? 
Yes I can confirm, apparently that was the reason of the slowdown in Rust. Thanks for proving this using `unsafe`. &gt; Why not? The C code isn't re-initializing the array every loop How can you say that? I didn't mention it in the post. But of course I tried to match the same conditions in the C code, and indeed I made sure to actually re-initialize the array every loop. If you take a look at the C code I posted in a [comment](https://www.reddit.com/r/rust/comments/6o8ok9/understanding_rust_performances_a_newbie_question/dkfhfs7/), I'm using `char buffer[1024] = {0};` inside the loop. Yet the performances aren't so terrible, thus I find it hard to believe that a mere zero initialization in Rust is the culprit, rather I think that it triggers some other actions, but it's just a guessing. I'm going to try to figure it out by looking at the generated assembler as others have suggested.
You use `rls`/`racer` or your home-made parser?
If it compiles, you haven't messed up the pass off to threads. Its pretty neat.
 char buffer[1024] = {0}; Im 90% sure that code just writes one byte to the end of the array, outside its bounds. The Rust code zeroes out the whole array
How advanced typesetting are you planning to support? It would be cool to have a PDF generation library where it is not too hard to generate nice looking documents. I have looked around some for this and only really going through something like TeX produces good enough documents.
in my experience `wkhtmltopdf` generates huge PDFs which look quite crappy. When working in Ruby I mostly used [Prawn](http://prawnpdf.org/) which is lower level and requires more work but offers more flexibility and results in nicer looking documents.
In the Ruby world there is [Prawn](http://prawnpdf.org/) which is quite nice to work with since it provides some quite high level helpers and it has basic support for kerning. There is also a new library called HexaPDF which I have not tested. 
I'm aware of that site, already applied for the jobs I found open. No responses. ;_;
I haven't used Tex. Right now you have to do manual line breaks (I know it sucks, I'll shift to something like having `\n` insert a line break) and there is no alignment (because PDF does not know what alignment is). I needed this library for GIS / mapping (creating nice-looking maps). The typography is quite good, it supports everything that PDF has, except text knockout. This means: character spacing, word spacing (does not work with Unicode), horizontal scaling, leading, choosing font and font size (of course), text rendering modes (filled, outlined, etc.), text rise (sub and superscript), text rotation. It does not yet do vertical writing or gradients (couldn't find the time and they are not that important for maps). I wouldn't say that typography for large text layouts is the strongest point of this library. Kerning is currently not done, I'll do it some time later. But with these things, you can already do a lot of layout capabilities (for maps, where you only have to write a few words / characters). For example, you can do curved text is achieved by writing character by character and changing the text matrix inside a text section. I wouldn't recommend it for large texts, though, because of the manual layout / aligning right now. I am working on a framework for automatically laying out a page, but it's too alpha to show it.
Total nitpick: if the offset of foo() is big enough, you might be outside of the unmapped null pointer page and get "regular" undefined behavior. (If I remember correctly) Google for "null pointer exploit" and you'll find some examples that turned into exploitable vulnerabilities (e.g. in Linux kernel, windows kernel and opera, as well as some archivers it seems) Rust's panic does not have this issue. 
&gt; Im 90% sure that code just writes one byte to the end of the array, outside its bounds. I'm pretty sure of the contrary, take a look at the [standard](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1124.pdf) or try yourself. 6.7.8/21: &gt; If there are fewer initializers in a brace-enclosed list than there are elements or members of an aggregate, or fewer characters in a string literal used to initialize an array of known size than there are elements in the array, the remainder of the aggregate shall be initialized implicitly the same as objects that have static storage duration. 6.7.8/10: &gt; If an object that has static storage duration is not initialized explicitly, then: if it has arithmetic type, it is initialized to (positive or unsigned) zero;
Thanks for the answer. I mostly do things which require typesetting larger amounts of text (forms, invoices, etc) so this library is not something for me (at least not yet), but it is nice to see something for generating more graphics heavy PDF:s.
*scuttles out from under rock* You called? I uh .. don't really see a concrete question here aside from a request to write a long blog-post series concerning my memories of that time? Or maybe more like an evaluation of writing compilers in ML dialects vs. writing them in C++ or whatever?
Because GCC's `__attribute__ ((warn_unused_result))` is an attribute, not a traits or part of type system. 
:(
Interestingly... unsigned char buffer[1024] = {4}; printf("buffer: %u %u %u\n", buffer[0], buffer[1], buffer[2]); yields buffer: 4 0 0 not buffer: 4 4 4 It *appears* to ensure that the rest of the array *is* initialized to 0, though. [More detail here](https://stackoverflow.com/a/201116). I feel like since 0 is the only supported initialization value in C, it must be implemented in a special (always optimized) way no matter what.
A plain `for` loop can use the `break`, `continue` and `return` control flow instructions; an internal iteration form using just a closure can‚Äôt. It requires quite a bit of design work in a different direction before you can really handle that sort of thing at all elegantly. (You need something like coroutines, for starters.) 
&gt; It appears to ensure that the rest of the array is initialized to 0, though. The quote does say "it is initialized to (positive or unsigned) zero"; not sure what else you would expect. ;-]
This is in line with the draft quotation of my previous comment: the second and third `char` are not explicitly initialized so are set to 0. I don't think that, from an assembler perspective, there would be so much difference between initializing *a bunch of bytes* to 0 and to any other value. And anyway I didn't enable compiler optimizations. If you're curious here's what happens for `char buffer[1024] = {0};`: mov eax,0x0 ; QWORD to write (0) mov ecx,0x80 ; repeat 128 times mov rdi,rdx ; array address rep stos QWORD PTR es:[rdi],rax ; write RAX in *RDI++ 128 times (128 * 8 = 1024)
PPA's are a horrible hack and abuse of APT which is why Debian has nothing like it. APT is very much designed around the idea of central management that all packages you install are managed by a single centralized source because it has no way to handle name and file conflicts; it was never the idea that people distribute .debs that aren't your distribution and horrible things can go wrong. APT dependencies are just on a package name and version; they do not specify a particular source when it comes. Say your application is called `nmusoke` and so is the package as a consequence and you package it in a certain way and whatever and it eventually actually lands in Ubuntu and something else depends on it. The system of the users who installed it via your PPA will report they already have it but Ubuntu might have packaged it differently and put other stuff in it so the dependency breaks at that point. There's a similar problem with most packaging systems which is why things like Snappy and Flatpak exist; really do not provide your own deb or rpm or whatever packages‚Äîit was always meant to come from a single centralized source and fit together like a jigsaw. Binary distribution in general is a bit of a problem since the ABI of a system like Ubuntu can and will change between major versions.
Having looked at this in some detail: The key here really is how the array is initialized to zero. gcc generates a `rep stos` instruction (with quadword granularity), clang calls `memset()`. Both are pretty fast for zeroing a 1kB array. rustc generates a pretty weird loop with 5 stack accesses (apart from actually zeroing the array) and two jumps **per iteration**, writing data at byte granularity (so 1024 iterations). I'm definitely surprised by that, as I would've thought the IR emitted for zeroing `u8` arrays was just `llvm.memset()`, but apparently that is not the case.
yeah, I've been staring at the assembly for the past 30 minutes, and that's definitely what I'm seeing too. Maybe this is a chance for someone to improve the codegen? Debug performance isn't terribly important, but it's important that it is "good enough". This also backs up my assertion that GCC is emitting always-optimized array initialization code. I'm not sure you can get much more optimized than `rep stosq`. `rustc` with optimizations just emits a call to `memset`, so that performs much better than the oddly terrible unoptimized version.
Might be worth taking a look at [tarpaulin](https://github.com/xd009642/tarpaulin) as well.
This finally explains, thanks! Follows some more relevant excerpts from `perf stat`. Buffer outside `loop` (fast): 107,526,184 cycles # 2.686 GHz (79.88%) 91,292,669 instructions # 0.85 insn per cycle 18,441,195 branches # 460.693 M/sec (84.94%) 104,152 branch-misses # 0.56% of all branches (80.10%) Buffer inside `loop` (slow): 1,209,531,657 cycles # 3.018 GHz (83.08%) 1,176,135,105 instructions # 0.97 insn per cycle 214,006,274 branches # 534.055 M/sec (84.04%) 201,455 branch-misses # 0.09% of all branches (82.94%) 
Could be interesting to see these ideas contributed into the overly broadly named [cargo-travis](https://github.com/roblabla/cargo-travis).
oh, i saw the second link :) i would put the code in main function into another function so you can use try and result instead of unwrapping :) also let request_path = request_args[1]; would change it to let request_path = request_args.get(1) cause you can get result/try instead of unwrapping cool little project :)
Back near the time of Rust's stable release, i remember somebody describing `rustc` as having been quickly ported through a dozen languages, each Rust-like to varying degrees, on its way to 1.0.
Yep for sure. This is all part of an open source framework for sqs I'm writing.
Will do thank you
Well you can just have a return value from the closure for that like: Enum LoopEnd&lt;A&gt; { Continue ( A ), Abort ( A ), } You can even make it a trait so that `()` is interpreted as `LoopEnd::Continue(())` Also instantly gives you loops that return values.
If it can do it perfectly, why would you need probability? 
&gt; In this scenario, you've taken ten minutes to process everyone's orders: that's five whole minutes faster, even doing the same amount of work! Not really, though. You had to ask customers to stand aside, plus you had to pay attention for when multiple customers were ready to make their order. Async I/O will almost always involve extra "work" (that is, more CPU cycles) than synchronous I/O on a single thread. It's just that the decrease in latency and increase in throughput is worth the extra cycles (not to mention the extra complexity). And of course when you add multiple threads (like one-thread-per-connection) you spend more work context switching and possibly locking.
&gt; feedback You seem like a dangerous bot to keep around.
Ah, seems I was confused. I was hoping for some safe accessing for negative index (just because `if let` syntax and such are so fancy :)) but adding empty cells sounds fair enough. Using a 1D vector is something I have to keep in mind too... Thank you very much!
You're talking about CPU time, I'm talking about wallclock time.
Rust was designed with optimizing compilers in mind. Many of the design decisions of the language would be unacceptable for a language in the same ballpark as C if it weren't for a compiler that could do heavy inlining and constant propagation.
I'm having a hard time implementing something that I think should be simple, with Tokio. I want to create a trait called "StreamAcceptor" that accepts streams. Essentially, it will abstract over a TcpListener and a TlsListener, so users of my library can use TLS if they want without any real changes. The idea is that StreamAcceptor has a single method, "incoming(self)", that returns a futures::stream::Stream where the Item implements both AsyncRead and AsyncWrite and the Error implements Into&lt;Error&gt; (where that "Error" is my own thing). Here's a simplified version of what I want, and an implementation that should work (it throws the socket addr away, but I'll add that back later after I get this to work): // this will be implemented eventually pub enum Error {} pub trait StreamAcceptor { type UndStream: AsyncRead + AsyncWrite; type Stream: Stream&lt;Item=Self::UndStream, Error=Error&gt;; fn incoming(self) -&gt; Self::Stream; } impl StreamAcceptor for TcpListener { type UndStream = TcpStream; type Stream = stream::MapErr&lt;stream::Map&lt;Incoming, FnMut((TcpStream, SocketAddr)) -&gt; TcpStream&gt;, FnMut(IoError) -&gt; Error&gt;; fn incoming(self) -&gt; Self::Stream { self.incoming().map(|(tcp_stream, addr)| tcp_stream).map_err(|e| e.into()) } } I *think* this is how I'm supposed to do this, with associated types. Unfortunately, the FnMuts are unsized, and I don't know how to squeeze in a promise that, yes, they are actually sized in the implementation. Even better, is there a way around this tangled mess of types? I can't return a Stream since it's a trait and unsized. Having to write out every step of the stream is, frankly, a total mess. Edit: Added the implementation of incoming() in this case. Note how the implementation is way simpler and smaller than the type signature. Edit: I did manage to get it to work by throwing the futures::stream::Stream into a Box but I'm still interested in a way to avoid that.
I don't really follow everything that's being said, but I believe he's saying that the algorithm perfectly solves for the probabilistic relationships (between the input and the output?).
edit: oh the next three responses all suggested this exact thing. &gt; How would you solve the constant scanning of the list? I guess you could sleep until either a new message is enqueued or the last lowest time is about to expire. Yeah that second thing should work: I'm not certain that rayon has a priority queue (... was it you who asked about a parallel priority queue a little while ago?) but a priority queue with the timeout as the priority would allow always sleeping exactly the right amount, with O(1) lookups and log(N) to keep the queue ordered, which is I think as good as you can get?
A lot of interesting ideas here. I was thinking as a toy project to learn rust I would re-write a python bot that I have that reads and writes from a few different rest apis at a particular set of intervals. That application leans heavily on [apscheduler](https://github.com/agronholm/apscheduler), which is an amazing library. This thread has definitely been helpful in terms of thinking about how to mimic some of the functionality. Thanks! 
Sure, except the word you used was "work", which I think would mean CPU time. After all, you're not "doing the same amount of (wall clock time)" since you're finishing taking the orders in less real time.
Quite fair!
I apologize but it is difficult for me to take you seriously. A lot of your content is light on details in my opinion and is all self-published (i.e. your ideas have not been vetted by peer review). You say your ideas are related to dependent types, so I would think a deeper explanation relating how they are not subsumed by dependent types would be useful. There is also the unfortunate name collision of "path" with the kinds of things type theorists are currently doing in the Homotopy Type Theory (or Higher Type Theory or Cubical Type Theory if you prefer and given appropriate context). A path for you seems to be an approximation of a particular functional relationship. Your examples though so far (like the even / odd example in the wiki) are underwhelming. Something else that may be helpful (for those of us attempting to understand) is a complete and rigorous encoding of some simple theory (say Presburger Arithmetic) in your path semantics.
It's awe-inspiring how much work you've done exploring this space! I'd love to be as motivated for pushing some of my own projects along as you seem to be on this project. --- What's the algorithm? There's so much new (or, at least, unconventional uses of) notation and new terminology in the blog post you link to, and the paper(s) that it links to that I can't work out what is going on in isolation, or what exactly is "executable". Furthermore, using terms like "probability theory" and being "a new field of computer science" I'd expect a little more precision, because a lot of the work seems to play fast and loose with randomness, random variables and probability. For instance, in [Probablistic Existential Paths](https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/probabilistic-existential-paths.pdf), "P(if(A, B)(x)) = P(x)¬∑P(A) + (1-P(x))¬∑P(B)" doesn't seem to make sense: given random variables `A`, `B` and the event `x`, the random variable `C = if(A, B)(x)` isn't an event, so it doesn't have a probability. I guess you trying to talk about the distribution of `C`, and were using the notation to mean something like: for any `y`, `P(if(A,B)(x) == y) = P(x)¬∑P(A == y) + (1 - P(x))¬∑P(B == y)`. But, this also isn't true in general, as it is implicitly relying on `x` and `A` being independent (and similarly `x` and `B`): the missing step is something like P(if(A,B)(x) == y) = P(x &amp;&amp; A == y) + P(!x &amp;&amp; B == y) Splitting those two right hand probabilities into plain multiplications needs independence. (Also, note that this is a special case of a true probability theoretic phrasing, which would handle more than just `==` and would work for continuous probability distributions, i.e. arbitrary distributions over the real numbers.) From [Probablistic Paths](https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/probabilistic-paths.pdf) &gt; This means how likely the `even` function is to return `ek = true` (an even number) or `ek = false` (an odd number). Half of the numbers are even, and the other half is odd, so it returns 0.5 on all inputs This is also playing fast and loose with terminology: - "numbers" probably means "integers" (or maybe "naturals"?) - it is meaningless to compare the sizes of infinite sets (odd integers and even integers) other than "smaller", "same", "larger": the set integers divisible by 3 and set of those not also have the same size, but it would be unintiutive and not very useful to say that half of all integers were divisible by 3. I guess you're meaning something along the lines of [asymptotic density](https://en.wikipedia.org/wiki/Natural_density#Upper_and_lower_asymptotic_density), but that isn't useful for this purpose, because... - finally, and most significantly, it doesn't make sense to talk about the probability of function taking a certain value, without knowing the probabilities of the inputs (as in, having a probability distribution for it). And, there's no "default" I can guess at here: there's no probability distribution on the integers (or any infinite set) that gives an equal probability to each one: it doesn't make sense to say "pick an integer at random, all equally likely". One can keep equal probabilities by [restricting to a subset](https://en.wikipedia.org/wiki/Discrete_uniform_distribution) or have a non-zero chance for every value [if they each have a different probability](https://en.wikipedia.org/wiki/Geometric_distribution), but one can't keep both of those properties at the same time. --- I'm sorry that this is a bit rude, but research, and especially the communication of it (and double-especially for the communication of work-in-progress research and ideas), requires humility. Bold claims like a "new field of computer science" and throwing around buzzwords like "probability theory" while making a lot of mistakes in its application are... not humility. That said, through all of the streamlining of the ideas, the fixing of mistakes, and practice explaining them, I'm sure it will get easier for others to understand what is going on.
Closure types are unsized, but function values are sized and can be typed. Then you can use a type alias as a shorthand. The guts aren't all that elegant, but it avoids `Box`. fn map_tcplistener_incoming((stream, addr): (TcpStream, SocketAddr)) -&gt; TcpStream { stream } fn map_err_tcplistener_incoming(e: IoError) -&gt; Error { e.into() } pub type TcpListenerStream = stream::MapErr&lt;stream::Map&lt;Incoming, fn((TcpStream, SocketAddr)) -&gt; TcpStream&gt;, fn(IoError) -&gt; Error&gt;; impl StreamAcceptor for TcpListener { type UndStream = TcpStream; type Stream = TcpListenerStream; fn incoming(self) -&gt; Self::Stream { self.incoming().map(map_tcplistener_incoming).map_err(map_err_tcplistener_incoming) } }
Thanks!
/usr/lib and /usr/libexec for other platform dependent files. (libexec if for executables that aren't run directly by humans, (eg systemd spawns helper programs from here))
Thanks for the link. I can almost get it to compile but it stops with errors so I can't test it :(
I tried to compile it, but it looks like you need a very recent version of KDevPlatform. From the error message, I think you need [this commit](https://phabricator.kde.org/R33:97a5312772c215b247280c5e8005df9d5bbdf0e9) from April this year, which is not included in the latest released version (v5.1.1). 
Wow, never thought of that. Very interesting, thanks :)
It looks like you would benefit from futures and tokio (though, until await!() and #[async] lands, futures aren't as usable as they ought to be)
I found this https://github.com/rust-lang/rust/issues/34701
Yes, that's correct.
It's home-made (or rather just calling `libsyntax` directly IINM). See also her earlier blog posts.
You need probability because input are variables of certain type. You don't know which value they have. Therefore, you also don't know the values of the output, and any property they might have is uncertain.
I've now removed the misleading line.
Probabilistic paths are defined here: https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/probabilistic-paths.pdf. It is the seemingly gibberish beyond the line "A probabilistic path is defined as". It's not easy to derive algorithms without an understanding of path semantics, so I am working on practice problems to help building an intuition. Eventually this will be published as a book, where you start with the basics and work towards the more powerful ideas, building on previous steps. Currently all these ideas are spread across a lot of smaller papers. "P(if(A, B)(x)) = P(x)¬∑P(A) + (1-P(x))¬∑P(B)" Those are not events, they are Boolean functions. `if` is a higher order function that takes two Boolean functions of N arguments, and constructs a Boolean function of N+1 arguments. It is explained here: https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/assigning-probabilities-to-boolean-functions.pdf. Numbers are usually annotated with their type in the papers. I'll check them to see if anything is missing. Your comments about asymptotic density are spot on: What is required for full generalization is something like encoding probabilities in dual numbers, because for some functions you need to deal with distributions of sets of different sizes. I was curious if somebody was able to spot this, and you did it! This is very recent results, and I am working on streamlining the ideas. One spot that you are wrong: It makes sense to talk about the probability of a function taking a certain value, because if you write out all the atomic partial functions as tuples, e.g. `inc(x) = x + 1` becomes `(0, 1), (1, 2), (2, 3), ...` then a probability distribution is over these tuples. Therefore, there is a "default" distribution which is used by this algorithm, which is a uniform distribution. For other more complex distributions, one would need a more sophisticated algorithm, which would have their definition derived from the first algorithm that uses a uniform distribution. I guess you are right about the use of humility when communicating work-in-progress.
&gt; I thought glutin didn't properly support _android_...? It depends on what do you mean by "properly" :) [But it more or less works for me.](https://github.com/ozkriff/zemeroth/blob/master/do_android) &gt; ..ios As far as i know ios-backend is dead right now :(
I'm very out of the loop here. &gt; AdvancedResearch is an open source organization dedicated to make conceptual breakthroughs in system thinking. Cool! &gt; The AdvancedResearch community was previously a part of the research branch of the Piston project. Oh? &gt; Piston - a modular open source game engine But ... &gt; Piston was started in 2014 by Sven Nilsen to test back-end agnostic design of 2D graphics in Rust. What. How did a project to build a game engine wind up producing research on type theory, artificial intelligence, and geoengineering? All before the game engine itself went 1.0?
Yeah, of course. Good point.
The use of word "path" is inspired by Homotopy Type Theory. It carries over a similar intuition, and the name stuck because since everything in path semantics are about functions, it makes it easier to speak about than "functions relative to other functions by some functions". In dependently type theory, you put information inside the type e.g. `a : vec(4)` for a list of 4 elements. In path semantics you write `a : vec ‚àß [len] 4` where `len : vec -&gt; nat`. The advantage with path semantics over dependently types is that you can specify any sub-type, and you can translate a function into an equation. The advantage of dependently types over path semantics is that it is much easier to create machine checked proofs. Programs in path semantics are consistent only when the double-existential paths are logically equivalent to `id` or `true_1` functions of type `bool -&gt; bool`. The other two possible functions of type `bool -&gt; bool` are `false_1` and `not`, that corresponds to functions that fail type checking or do not halt. It is quite astonishing that this turns up without any pre-encoded knowledge about type systems or running programs, but that's the way it is! Since finding existential paths for all functions is undecidable, it makes it fundamentally hard to build theorem proving assistants for it, but it does not stop you from reasoning about functions using path semantics because you can check for consistency in principle if you solve each sub-problem. I believe a kind of interactive machine assisted theorem proving is possible, using a knowledge base. The examples of using even/odd are underwhelming indeed and you will find more useful ideas here: https://github.com/advancedresearch/path_semantics/tree/master/papers-wip. Path semantics points to "functions out there" all the time. E.g. existential paths are known to exist for all halting functions by using constructive proofs. Instead of defining rules for some sub-set of all functions, path semantics builds on the concepts of "functions out there" to create higher order functions where sub-type constraints are first-class citizens. These algorithms are not executable, but they are solvable given some background knowledge. It is completely orthogonal to existing theories of numbers, so you end up using those theories to solve sub-problems in path semantics. For example, I can use existing theorem proving tools, e.g. Idris, to solve a sub-problem, but I can't express path semantical ideas directly in Idris. Path semantics is powerful but also very abstract, and relies on general theorem proving abilities as "input". This is how you can construct a higher order algorithm in path semantics that predicts probabilities of sub-types perfectly for any function, while in practice you need to put in a lot of work to solve it for a specific problem.
I wonder why rustaceans generally prefer coveralls to codecov? The latter has nice [browser extension](https://github.com/codecov/browser-extension). Also see [comparison](https://www.slant.co/versus/7928/7929/~coveralls_vs_codecov).
I don't know either. The first step is having a run-time detection system that catches all UB before it happens. The second step is helping people fix it. My comment was about step 2 but maybe if we can just emit a nice backtrace of where the values come from we don't need to complicate the UB detection mechanism of step one.
When a piece of rust code uses syntax like `::A::B`, what does the double-colon do at the start? Having trouble finding an answer to this as it's hard to google.
Does this include coverage of doc tests?
Skia's most used backend is by far the CPU backend. The design of the library and its API is also very much CPU-oriented. the GPU backend came after and it's not nearly as mature as the CPU one. Edit: but yeah I totally agree with it not being the best tool if all you need is generate pdf (not sure how simple that is though). 
&gt; Give a man a game engine and he delivers a game. Teach a man to make a game engine and he never delivers anything. Source: https://twitter.com/sandbaydev/status/403219167236857856?lang=en
People will complain no matter what you are doing. Stop worrying about it and do what you want to do with your life instead.
A wise-sounding advice that doesn't fit in this context. Piston is mostly stable.
Also: Write games, not engines.
I don't think this has any relevance for Rust, neither the programming language (/r/rust, here) nor the game (/r/playrust)?!
https://doc.rust-lang.org/reference/paths.html &gt; Paths starting with :: are considered to be global paths where the components of the path start being resolved from the crate root. Each identifier in the path must resolve to an item. The link above contains examples
Thanks for sharing this. KDevelop is my IDE of choice right now and having Rust available in it would be great.
&gt; allow arbitrary sub-types defined by functions How does this compare to refinement logic as seen in, eg, NuPRL? This is a refinement type AIUI. &gt; Path semantics is an extension of type theory that grounds meaning (semantics) using functions But what's the meaning of a function? :) All in all this smells very much of computational type theory (in the style of Constable, Harper, ...) mixed with universal algebra. I don't have the time to actually ingest what you've written about all this stuff though. It's too different for me to do it quickly, and I'm too busy right now.
Here is an example of differentiating items depending on whether the identifier starts with `::`: https://play.rust-lang.org/?gist=d2031701a8c29fee7f87fe27ca3c7763&amp;version=stable
That's a good example, thank you!
Oh ok, thank you!
&gt; In dependently type theory, you put information inside the type e.g. a : vec(4) for a list of 4 elements. In path semantics you write a : vec ‚àß [len] 4 where len : vec -&gt; nat. This is _exactly_ quotients / subtypes as seen in the CiC. You take `vec` quotiented by the "predicate" (function) `\x. len x = 4`. In Lean, you'd write it as `def your_type (A : Type) : Type := { x : list A // x.length = 4 }`. Which is notation for a dependent record, which you can also just view as a sigma type. 
Maybe history and general knowledge? I didn't even know codecov existed - thanks for pointing that out!
You should start from: https://github.com/brson/rust-api-guidelines
If it is uncertain, how can you do it perfectly? 
Yeah, the approach I'm attempting right now involves using futures.
You might want to check out the rust libz blitz (https://internals.rust-lang.org/t/rust-libz-blitz/5184) to see what they are doing to improve the quality of crates. I see you're using error-chain. Afaik that has performance implications, and I'd be wary of that in numerical applications. But I know nothing really, so you'd need to check that yourself. Of course, you're not computing _the_ fixed point, but _a_ fixed point that derives from the starting values. This should be reflected in the documentation I think. Also, I'd say put a very simple usage example in the README.
Not if you re-evaluate the timeout on insert, right? Until you get into the hundreds of thousands or millions of jobs the slight jitter from a possible insert at the wrong time should be fine. Handling the "millions of jobs scheduled" send likely to be enough overhead that it would/ought to be a different mechanism entirely (or maybe adaptive on the size of the queue?)
Get yourself some sweet lifetime annotations! fn foo&lt;'a&gt;(v: &amp;'a Vec&lt;String&gt;) -&gt; Cow&lt;'a, str&gt; { Cow::Borrowed(&amp;v[0]) } By having `Cow&lt;'static, str&gt;`, the compiler is expecting something that has a `'static` lifetime, AKA the entire duration of the program. By declaring generic lifetime `'a`, you indicate that the lifetime of the returned value is tied to the lifetime of the Vec, as the value is borrowed from it. Actually, you don't even need to do that, just let the compiler elide everything. fn foo(v: &amp;Vec&lt;String&gt;) -&gt; Cow&lt;str&gt; { Cow::Borrowed(&amp;v[0]) } TIL lifetime parameters can be elided even if there are type parameters following them.
It's based on trigrams. The algorithm is relatively simple actually and supposed to be pretty fast. The only disadvantage it may not work correctly on small amount of text.
Thanks! That works perfectly for this small example, but not for the real code. I guess I'll have to rewrite the example and re-post so that it more closely matches the real code.
There are a couple issues. You generally shouldn't pass borrows of ```Vec```s. Instead modify your foo to take a slice instead: fn foo(v: &amp;[String]) -&gt; Cow&lt;'static, str&gt; Also, there's a needless borrow in foo. When you take index 0 of v, your reference is automatically dereferenced. So it should be: Cow::Borrowed(v[0]) But the real problem relates to the ```to_owned()``` call. That will make an owned copy of the ```str```. That copy cannot have a static lifetime. So remove the ```to_owned()``` call and modify the foo function to take a static ```str``` instead: fn foo(v: &amp;[&amp;'static str]) -&gt; Cow&lt;'static, str&gt; { This will now compile, but it's needlessly restrictive as it will only work with ```str```s that are pre-programmed in. You can change to definition of foo to make it more flexible: fn foo&lt;'a&gt;(v: &amp;[&amp;'a str]) -&gt; Cow&lt;'a, str&gt; All together: https://play.rust-lang.org/?gist=146d9c8ba429b2fad95fb142b318c70c&amp;version=stable
Yes, this is a common way of encoding sub-types, but you're just scratching the surface. In path semantics, you can factor out this sub-type and use it on the function, then build higher abstractions on top of this concept. For example: fn concat(a : vec ‚àß [len] na, b : vec ‚àß [len] bn) -&gt; vec ‚àß [len] na + nb { ... } Turn this into: fn concat{[len] na, [len] nb}(a : vec, b : vec) -&gt; vec ‚àß [len] na + nb { ... } Then "lift" the sub-type out of the constraints: concat[len](na, nb) = na + nb A shorthand version: concat[len] &lt;=&gt; add Which satisfies the equation: len(concat(a, b)) = add(len(a), len(b)) There is an asymmetric version that is written like this: f[g0 ‚®Ø g1 -&gt; g2] &lt;=&gt; h The field of path semantics now asks the following open problem: How do you find all solutions of this form? Which is equivalent to asking "what is perfectly predictable about functions?". Next, you go even further and the question "even when there is no way to make perfect predictions, what are the perfect probabilistic predictions I can make?". The answer to this question is the new algorithm. It tells you how to split the problem into smaller parts to derive the function for making perfect probabilistic predictions. The field of path semantics also poses problems such as "if you have a function `f`, and you know all existential paths to the function, written `‚àÉf{}`, what can you do?". It is hypothesized that if you can arrange the constraints in a way such that you get expected 50% of the derived functions returning true, recursively, you can construct approximations to computational oracles that are bounded by how fast the existential paths can be computed, and the number of input bits of `f`, or O(1) ignoring the time to compute existential paths. This has not been formalized yet. My point is that path semantics makes it possible to climb higher up on the abstraction ladder and derive definitions of concepts that only exists currently in human intuition. A lot of these ideas are overlapping with existing stuff, which is to be expected, but the reason I use it is because I can easier wrap my mind around it. Theorem proving becomes an activity that is split up into finding functions: The questions are posed in the form of functions, and the answers are functions. When I have a solid understanding of something, I can also read the work of others and figure out what they mean, because the definitions defined in terms of functions are solid. Functions are testable, and it's easy to see what kind of predictions they make in the path semantical notation. A lot of mathematical material depends on axiomization of a class of objects and then study the properties of these objects. However, it is rarely sufficient to show the proof, one must also describe the thinking in natural language, which tends to be abstract and complicated. I am more interested in a way of using mathematics to reason about programs, and how to make them faster. I want a notation that stays on the level where I can compute things and test it. Dependently types are limited, because I can't express everything I want to do. Path semantics is a very small core that allows this kind of reasoning, and the toolbox of rest of mathematics can be added, like plug-ins.
you've said you're using tokio, right? I would *imagine* (I haven't done anything with tokio) that it would look *vaguely* like: struct WaitingQueue&lt;T&gt; { current_monitor: TaskHandle, queue: SomeQueue&lt;T&gt;, } impl&lt;T&gt; WaitingQueue&lt;T&gt; { fn insert(&amp;mut self, core: &amp;mut tokio::Core, item: T) { self.queue.insert(item); core.cancel(self.current_monitor); // except you should correctly handle empty queue self.current_monitor = core.spawn(monitor_queue(&amp;self.queue)); } } As mentioned I haven't worked with tokio. Also I didn't think about race conditions or robustness. The general idea is just have a running monitor, and cancel it on insert. 
Do you have any formalization of path semantics that I can read?
If something is uncertain, you can assign a number between 0 and 1 that describes how likely is to happen. Given something you are uncertain about, you can calculate how uncertain you are ought to be about other stuff that is functionally dependent on the things you are uncertain about. Imagine that you living inside a box and receive 1 bit of information about the world at a time. If you are perfectly rational, you can create a model of the world as the bits arrive. Each bit brings you a little more knowledge about the world, which you construct by testing all possible functions predicting the next bit and rule out all those that are wrong. If you are making optimal predictions, to the extent it is theoretically possible, then you are doing it perfectly. This is not the same as being able to guess the next bit each time, you just get better and better at it. Path semantics allows one to formalize the shape of some uncertain knowledge, by encoding them as sub-types on a function. By assigning a probability to each sub-type, and some implementation of the function you want to predict, you can derive the correct probability to assign to any sub-type of the output of the function. A perfect rational agent can use this algorithm in part of an activity to construct a model of the world, by decomposition it into functions. If the bits arriving are deviating from the expected values over time, then the agent can assign less confidence in the functions that give less accurate predictions. This is how a perfect rational agent learns at the optimum possible rate, such that over time, it gets better and better at making predictions, much faster than any non-rational agent, measured in number of bits.
Path semantics is not fully formalized yet because the syntax leads to combinatoric explosion. I have figured out a way to measure progress on this, but I have not completed the formalization. I started working on a basic formulation here: https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/basic-path-semantics.odt (requires LibreOffice). I invented something called "slot lambda calculus" to reason about this: https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/slot-lambda-calculus.pdf On top of this there are higher concepts such as: - universal existential paths: https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/universal-existential-paths.pdf - inverse functions https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/asymmetric-inverse.pdf - domain constraints https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/domain-constraint-notation.pdf - inverse zero order existential paths https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/inverse-zero-order-existential-paths.pdf - path sets https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/path-sets.pdf The principle is to derive such ideas and express them in the same notation. I have read about the foundations of mathematics and history of mathematics to learn how to formalize stuff, but I find it very hard to do. My mind is not used to sequent calculcus... So, I define concepts in a way that I can understand, and hope that some of it can later be formalized and checked by machine. Initially I was very excited by dependently typed languages like Idris, but then figured out there are limitations. I also tried Coq but it could not express my ideas. Then, I started thinking about how to overcome these limitations. I build the intuition around how functions works, so if something is not working like functions should do, then it's wrong. I can't use set theory because it's too low level. I use path semantics for the same reason I don't program in assembly, even though I would like to know assembly, it is just not an efficient tool for doing what I want to do.
While dealing with lifetime issues, it's a good idea to only focus on a function where lifetime issue happens. Function calls just look at the function signature, not actual code - this allows for safely changing function code without risking that code calling it will stop working - as long its signature won't be changed. In this case, the function signature is lying. fn foo(v: &amp;Vec&lt;String&gt;) -&gt; Cow&lt;'static, str&gt; { Cow::Borrowed(&amp;v[0]) } This says that the function takes a reference to `Vec` of `String` and then returns Cow with `'static` lifetime (lifetime doesn't depend on lifetime of anything). This is a lie, as if you would destroy the `Vec` containing a that string, you would end with a reference to freed `str`, which would cause safety issues. What needs to be done is saying that `Cow` will live as long `Vec&lt;String&gt;` it was based on will live. fn foo&lt;'a&gt;(v: &amp;'a Vec&lt;String&gt;) -&gt; Cow&lt;'a, str&gt; { Cow::Borrowed(&amp;v[0]) } Or with [lifetime elision](https://doc.rust-lang.org/stable/book/first-edition/lifetimes.html#lifetime-elision), a lifetime could be removed, as there is only one input lifetime, so Rust can easily figure out that this input lifetime is used for output. fn foo(v: &amp;Vec&lt;String&gt;) -&gt; Cow&lt;str&gt; { Cow::Borrowed(&amp;v[0]) } Also, it's possible to remove the requirement for `Vec` reference when a reference to slice is sufficient. The only difference is that `&amp;Vec&lt;_&gt;` type allows getting its own capacity, which isn't really all that useful. There is an implicit coercion from `&amp;Vec&lt;T&gt;` to `&amp;[T]` (so called [implicit Deref coercion](https://doc.rust-lang.org/book/second-edition/ch15-02-deref.html#implicit-deref-coercions-with-functions-and-methods)), so there is no risk in doing so. fn foo(v: &amp;[String]) -&gt; Cow&lt;str&gt; { Cow::Borrowed(&amp;v[0]) } This change also allows you to not construct a vector in `main` - there was no reason to do heap allocation here. fn main() { let v = ["123".to_owned()]; foo(&amp;v); }
Why does `U` have a `Copy` bound [here](https://docs.rs/fixedpoint/0.1.2/fixedpoint/fn.fixedpoint.html)? The Rust API guidelines [state this](https://github.com/brson/rust-api-guidelines#caller-decides-where-to-copy-and-place-data-c-caller-control): &gt; The `Copy` trait should only be used as a bound when absolutely needed, not as a way of signaling that copies should be cheap to make. Consider if it's adequate to bound by `Clone` instead: calling `.clone()` makes your code more verbose, but [it will be inexpensive](https://doc.rust-lang.org/std/clone/trait.Clone.html#how-can-i-implement-clone) for the `Copy` case, while allowing more types to be used. However, if you are doing something that really requires a `Copy` (like using `Cell` with `.get()`), then relaxing the bound would require specialization.
&gt; Afaik that has performance implications What are those performance implications? Can someone do the same thing as error-chain, but in a "zero cost" way? (perhaps by hand-rolling the error types instead of generating them with the macro, but otherwise behaving like error-chain)
I have a lot of reading to do before I can explain that. Thanks for the pointers. :) What I do know is that these ideas seems to work, and I am not familiar enough with NuPRL to know whether all the ideas I have can be expressed in that language. I tried Idris and Coq but they weren't powerful enough. It is possible to write some syntax sugar in Idris that makes it easier, but it only works for sub-problems. Perhaps I'll figure out eventually.
Oh! Oops, I got confused who I was talking to. The same kind of thing works with threads, though: you would just have a threadId that allows killing, and synchronization would probably need to be a bit more explicit. The point is just that as long as you've got a handle to the current "shortest timeout" it's possible to cancel it from outside.
Some comments! 1. You're using env_logger, so no need for println!, i'd move them all to that. 2. running rustfmt over your code will give you some formatting suggestions; it's mostly a bit more whitespace to let your code breathe 3. others brought up the string issue, but you always want to be sending back bytes; don't try to decide based on filetype if something is utf-8 or not, that's not a web server's job 4. The comment above about the `../../..` issue is legit too 5. https://github.com/achntrl/rust-webserver/blob/master/src/main.rs#L70 you only ever use `request[0]`, so rather than collecting, `.nth(0)` might make more sense 6. same here https://github.com/achntrl/rust-webserver/blob/master/src/main.rs#L71 that's all i've got for now, looking good overall!
I'm not sure of the performance implications of using error-chain. Also, the performance hit should be very minimal and unless it can be shown that the difference is significant, especially when compared with the actual cost of computation within this crate, I'm not sure it makes a lot of sense to optimize there. error-chain provides some very nice abstractions for error handling at a very minimal performance cost. It's a trade-off that I'm currently willing to make. Atleast till a point arises where this is a performance bottleneck. Most function don't have _the_ fixed point, but a range of fixed points. This crate currently computes the first fixed point of the function. I should probably add that to the documentation. This was my gripe with Python's SciPy as well. It finds _a_ fixed point, when I wanted only the _first_ fixed point. I don't currently need it, but if someone were to want to use it, I wouldn't mind implementing a second function that uses acceleration techniques like SciPy does to compute any one fixed point of the function I have an example in the documentation, but you're right, I should add something to the README as well. Thanks for looking into it.
You could make a crate and try it? However it would be hard to support labeled breaks.
Thanks! I took a look at it. It's a fairly long and comprehensive document. Will try and ensure this and future crates meet the specs.
That's an interesting point you make. I don't absolutely require the `Copy` trait. However, without it, I need to call `.clone()` in about six different places in such a small function. Sure, this makes the code a little more relaxed in the types of objects it can handle, but is it really worth the verboseness that it adds? I've made the change locally, but I'm still arguing with myself on whether calling `.clone()` all over the place is better than the implicit `Copy`.
You can always check the [Syntax Index](https://doc.rust-lang.org/nightly/book/first-edition/syntax-index.html) for stuff like this.
When I think about it, verbosity in library code is less important than flexibility for library users. But only if the more general bound (here, `Clone`) could possibly be useful for someone. On the other hand, in an application, code brevity is more important than in a library. So I wouldn't over-generalize a signature if the code lives only in an application.
Can you explain the idea behind the generator? Why does it make the shapes that it does?
Just FYI, you can't assign *runtime* data to be `static` without using `unsafe`. The `static` lifetime can only safely be used by compile time constants.
Is there any simple way of reading a single character off from stdin without consuming the entire thing?
I can't believe this flew under the radar for me, and this is something that is going to help me out a lot in research! Thank you for this post, will donate. :)
Shouldn't unions have been released alongside some machinery to turn them into enums given a tag (possibly with overhead that amounts to tag size)?
Wow, unions ‚Äî I did not see this in nightly :D
&gt; In 1.19.0, rustc now knows how to find the 2017 tools, and so they work without a workaround. Yay
Regarding pre-published crates. Wouldn't it be possible to build a solution around *real* pre-releases instead of *fake* pre-published releases?
According to [this SO post](https://stackoverflow.com/questions/35547710/does-rustdoc-generate-runnable-binaries) at least a year ago, it doesn't seem like rustdoc is capable of exporting an executable for code coverage use. Since the current rustdoc isn't likely to be changed, I've opened [an issue](https://github.com/steveklabnik/rustdoc/issues/54) on the new rustdoc's issue tracker so that they can make sure to have this functionality in the future
Thanks! I think this is really important since in some libraries I've had to duplicate doc test into a normal test module just to get proper code coverage information. This duplication has always been obnoxious and one of the things I hate about Rust.
&gt; But only if the more general bound (here, Clone) could possibly be useful for someone. That is exactly why I'm hesitant. This is mostly used for numerical applications. All number types implement `Copy`. I can't think of an object type which would be useful here that does not implement `Copy`. 
&gt; It is the seemingly gibberish beyond the line "A probabilistic path is defined as". Yes, that's what I mean by "I can't work out what's going on". &gt; Those are not events, they are Boolean functions. Okay! That works too: they are events (they *have* to be, for there to be a probability), but all the events are implicitly `x == true`. &gt; if is a higher order function that takes two Boolean functions of N arguments, and constructs a Boolean function of N+1 arguments. It is explained here That paper also isn't correct: it suffers exactly the same independence problem I mentioned before. Suppose `P(x) = 1/2`, we have `if(x, false)(x) = x`, so `P(if(x, false)(x)) = P(x) = 1/2`, but the formula you give computes 1/4. &gt; Numbers are usually annotated with their type in the papers. I'll check them to see if anything is missing. Oh, yes, I see early in the paper that you constrain `even` and `add` to `nat`, so I guess all uses of the word "numbers" later are really "natural numbers". &gt; What is required for full generalization is something like encoding probabilities in dual numbers, because for some functions you need to deal with distributions of sets of different sizes Distributions of sets of different sizes? Do you mean that one argument to the function might be `nat` and another might be `bool`? This isn't anything particularly special: the distribution of the cartesian product `nat √ó bool` is the [joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution), which doesn't care about the sizes of the sets. &gt; It makes sense to talk about the probability of a function taking a certain value, because if you write out all the atomic partial functions as tuples, e.g. inc(x) = x + 1 becomes (0, 1), (1, 2), (2, 3), ... then a probability distribution is over these tuples (This encoding is typically called the [graph of the function](https://en.wikipedia.org/wiki/Graph_of_a_function), and by the nature of the construction, the inputs (aka first element, for this example) are unique, so it makes no difference to talk about the distribution of the tuples or of just the inputs.) There's still no probability distribution over those tuples: it is an infinite set, so what I said above still applies. One of the axioms of probability basically translates into the sum of the probabilities of all events is 1, meaning if `x` is a random choice `‚àë_v P(x == v) = 1` (where the `_v` "over each possible value `v`", apologies for the limitations of formatting on Reddit). In this case, there's an infinite number of `v`, so the sum is infinite. If all of these probabilities are equal and *non-zero*, then that sum is also infinite (diverges), therefore, the probabilities must be all zero... which isn't correct either because `‚àë_v P(x == v) = 0`. There is no probability distribution that gives equal probability to every member of an infinite set. Dual numbers don't fix this, the only relationship is that `Œµ^2 == 0`, not that adding a particular infinite number of them is 1. And, I don't think hyperreals nor surreals or any other form of infinitessimals help either. What if `v` was restricted to just the even numbers (or tuples with even first elements)? By the density argument, presumably the sum should be 1/2, and similarly for odd numbers, but all these sets are *exactly* the same size, so there's no particular reason for one to be 1 and others to be 1/2. It is a consequence of the axioms of probability theory is that there is no uniform distribution on an infinite set. It's perfectly possible that you can discard those axioms and have "P" and "probability" be completely your own symbol, but this is extremely confusing, probably means you shouldn't refer to it as "probability theory", and whatever definition you use is unlikely to correspond as closely to real-world things as the conventional probability.
Could this also be used with vectors and matrices, for a multi-dimensional problem? Those may be heap allocated and thus not `Copy`.
Was waiting for this
I get why it's unsafe, but how is union matching possible if there's no tag?
The union can match on a tag within the union, which is how C does it.
`MyUnion { f1: 10 }` means: "if interpreting the memory as if `f1` was stored and its value was `10` then". Note how in the second case you have `MyUnion { f2 }` which is an unconditional binding.
How fast it in compare with such approach https://jbp.io/2017/07/19/measuring-test-coverage-of-rust-programs ?
Does this account for trap presentations? Like, if union { bool, u8} that contains the bit pattern of 128_u8 is first matched against false? Is it going to be "UNDEFINED BEHAVIOUR HERE BE THE NASAL DEMONS" or is it just "nah, the bit pattern doesn't match a bool false, let's see what other things we've got"?
Instead of this: fn fixedpoint&lt;T, U&gt;( func: &amp;Fn(U, &amp;T) -&gt; U, x0: U, args: &amp;T, maxiter: Option&lt;usize&gt;, maxval: Option&lt;U&gt; ) -&gt; Result&lt;U&gt; where U: PartialEq + PartialOrd + Copy + Debug I would prefer something like this: fn fixedpoint&lt;F, T&gt;( func: F, x0: T, maxiter: Option&lt;usize&gt;, maxval: Option&lt;T&gt; ) -&gt; Result&lt;T&gt; where F: FnMut(T) -&gt; T, T: PartialEq + PartialOrd + Copy + Debug This allows you to pass a closure with an environment instead of passing the environment through `args`. As a minor thing, I also made it `FnMut` instead of `Fn`, since it does not need to share the environment of the closure. Sure, your function may have mutable state now, but so can `Fn` through stuff like mutexes.
Interesting.. I hadn't thought of that use case.. Though I have no idea why using a vector or matrix here would be useful, but I'm no expert in such areas of math.. The point is, it is possible for someone to want it, so I should probably keep the flexibility. 
C generally matches on a tag *outside* the union.
Same way it works on structs.
Would unions allow implementing SSO/SBO?
How would you do it safely? Seems like you'd have to roll your own 'Into'/ 'From' anyways.
I'm assuming you mean for `String`? SSO is not possible, and it's not clear that it's even advisable. To get SSO you'd have to use a different type.
Yeah, it's UB to access a union by a type other than the one it's supposed to contain. IIRC this doesn't apply for C `char` (Rust `u8`), I'm not sure how that translates to Rust (likely it is always safe to use any integer type to read from a union)
&gt; By having access to crates like syntex, tarpaulin aims to become the first open source code coverage tool to offer a solution for branch and condition coverage. This would, simply put, be AWESOME. I do admit though that I'm not quite sure how condition coverage depends from branch coverage, after all I model a `&amp;&amp;` as a `if` so it's just one more branch... am I missing something?
Sure, the tag can be anywhere. Assuming you have a tag and aren't just pulling hijinks like a union between u32 and [u8;4] or other goofy things.
&gt; Yeah, it's UB to access a union by a type other than the one it's supposed to contain. I hope not, because it would make `match` useless.
I like the idea that you propose. Being able to send a closure would indeed be nice. In fact, I might be able to simplify some parts of my existing project using this crate by passing closures.. I'm not sure of the implications of using `FnMut` though. Will have to read up the docs once to be able to discuss about it. 
I haven't ran any tests but I'd imagine tarpaulins performance is more akin to kcovs as they use a similar breakpoint method. However, I hope that actually accessing the variables during execution should enable me to push tarpaulin further (including into things like MCDC coverage)
"Here be nasal demons" just made my day.
Functions on many variables would normally have a fixed number of dimensions, and could receive a tuple or an array (something like `(f32, f32)` or `[f32; 3]`) which is `Copy` if the elements are `Copy`. But one could also implement such function on an input like `&amp;[f32]` or `Vec&lt;f32&gt;`. Would that be useful? I don't know, probably not.
Do you mean something like: every `union` automatically defines an equivalent `enum` that is identical except for having an extra tag (which probably needs a second `enum`, without a payload)? There could also be an automatic `unsafe` function to go from `union` to `enum` and a safe one for the opposite direction. I'm not entirely sure how useful this would be: whether it would be worth the pervasive binary size cost. Do you have particular applications in mind?
You can't `match` a union. `match` on unions _is_ useless.
So I'm going to slightly edit that as I found out when reading about the other rust coverage tool that lcov provides branch coverage. So with branch coverage on an if statement, it will check if the condition is evaluated to true and false. Condition coverage looks at boolean subconditions. So if I have the condition "x &amp;&amp; y" branch coverage hits 100% for that evaluating to true and false. Condition coverage is 100% when both x and y have been true and false so helps you realise when say one subcondition hasn't changed during any of the tests. There's also MCDC which is a bit harder (hence why aviation standards use it for guidelines). Essentially that specifies that every condition has to have an effect on the branch taken. so if I have "x || y" I need to test "0 || 1" "1 || 0" and "0 || 0". Condition coverage gets 100% by "0 || 0" and "1 || 1" which gets the same results with an AND. And branch coverage hits 100% with "1||0" and "0||0".
What is the reason that writing to an union member is deemed unsafe? As far as I can see it doesn‚Äôt matter what and where you write to, but when you read it you better make sure you are not reading garbage.
In the RFC there's examples of matching against a type that contains a union field ‚Äì that would be useful, since the type can contain a tag that makes the type of the union decidable. So even if a match against a single union wouldn't be useful, match against values that contain unions would.
I just checked the RFC text, and actually it seems to be more lenient than that interpretation: &gt; Rust code must not use unions to break the pointer aliasing rules with raw pointers, or access a field containing a primitive type with an invalid value. To me, that seems like a match against a value of match my_union { SignedOrUnsignedUnion { u: 10 } =&gt; { println!("u8 of value 10"); } SignedOrUnsignedUnion { i: -5 } =&gt; { println!("i8 of value -5"); } } wouldn't be UB since they don't contain trap representations?
AFAIK, it has something to do about destructors not being run
Yeah, because doing it with integers is usually fine. I forget what the exact rules are; IIRC it's never UB to use char, but it can be not-UB in other cases too.
&gt; Add bootstrap support for android [1] Does this mean one can _build_ the compiler on Android? I can rebuild rustc using my phone? When will bootstrap support land for WASM? I'd like to rebuild rustc on my browser. [1] https://github.com/rust-lang/rust/pull/41370
(Maybe the devil's in the details; before that, there is the phrase "In particular" which maybe tries to say that accessing a field containing a primitive type with an invalid value is just one example?)
Yes sure, but here we are talking about matching the union itself.
Is there any way to write Rust code for the Lego Mindstorms, or crosscompile it?
&gt; MyUnion { f1: 10 } means: "if interpreting the memory as if f1 was stored**,** its value is 10 then". &gt; `MyUnion { f1: 10 }` means: "if interpreting the memory as if `f1` **and its stored value** is `10` then". For want of one of these two phrasing corrections, I had to read your phrasing twice to make sense of it as "If `MyUnion`'s value is `10` when interpreted as `f1`..."
Oh, I am slightly disappointed in the definition of branch coverage then, since for my any short-circuit is by definition a branch (maybe I should stop thinking in terms of SSA/Basic-Blocks...). If one thinks in terms of basic blocks, then `if (x || y) { A } else { B }` translates into: 'x: if (x) { goto 'a } else { goto 'y } 'y: if (y) { goto 'a } else { goto 'b } 'a: A, stop 'b: B, stop In which case it's pretty obvious that one needs to test all 3 cases as specified by MCDC since it leads to a different execution path.
The release announcement seems to disagree with you and has a code example though?
I don't think that's right. It says right in the announcement that, unlike `enum`s, `union`s are untagged and I see no mention of a facility for plumbing in a tag. The "If the value is `x` when interpreted as `y`" reading of the `match` arms looks much more likely. (I don't have time to read the RFC right now to double-check.)
Uh... the announcement disagree thoroughly with you: &gt; Pattern matching works too: &gt; &gt; fn f(u: MyUnion) { &gt; unsafe { &gt; match u { &gt; MyUnion { f1: 10 } =&gt; { println!("ten"); } &gt; MyUnion { f2 } =&gt; { println!("{}", f2); } &gt; } &gt; } &gt; } And yes, the way it works is "special". I think it accounts for the C pattern of including the "tag" as the first field of each variant.
Writing to a union field *is* safe if the field is `Copy` (i.e. has no destructor). https://play.rust-lang.org/?gist=619a5cfd3a210f9a4d03108de62f15fc&amp;version=nightly
You know, I think it would be kinda neat if the unsafe rules would be defined so that pattern matching against an union using a valid value would be safe (however one must be very careful with mutable aliasing!). I even think it makes intuitively sense: you wouldn't be *constructing* an invalid value, but *matching against* some unknown value using a known-to-be-good value, and only if that unknown value matches, it can be thought as known-to-be-good too.
I'm guessing that `union` is meant for use by things like `bindgen` in `-sys` crates and you're supposed to do that yourself in the non`-sys` crate.
I'm considerably less worried about boilerplate in my definition as opposed to complication in usage of the API. By sticking with the builder pattern I'm telling users of my API "hey, use this pattern that everyone knows already" and I absorb the cost of implementation upfront for them. The macro solution is pretty nice and helps to solve that problem though. I like that. The familiarity issue is still there though. There's also: https://github.com/colin-kiegel/rust-derive-builder To help with the implementation boilerplate.
Yeah, that's a special case where both types are primitives of the same width that allow all bit representations. You should not do this for a general union.
My bad, I meant "in general". It's fine for the few cases where it is not UB.
String yes, possibly even Vec&lt;u8&gt; though that would require structural specialisation I guess. Why would SSO not be possible, let alone advisable?
A union is not _required_ to be tagged in any particular way, which is what separates them from enums. However, they are primarily for inter-op with C, and in C you will generally either tag your unions manually in a struct that wraps the union and tag as one, or you have unions where you know the correct usage of the data contextually. Here's an example from MSDN https://msdn.microsoft.com/en-us/library/windows/desktop/ms646270(v=vs.85).aspx My original wording was slightly off. You don't normally put the tag inside the union block, but in a struct that contains the union block. Though if you put the tag as the first field in every union option and left off the enclosing struct that'd do the same thing I guess.
Branch coverage is easier thought of executing each branch in the code, regardless of the condition. It's not the greatest metric but it is better than line coverage
&gt; I think it accounts for the C pattern of including the "tag" as the first field of each variant. It's actually just going down the list of match variants, and checking if the value of the union matches the value you wrote in the match variant. See [this example](https://play.rust-lang.org/?gist=90267712d0cf427b9429d713ae0e8935&amp;version=beta). Even though the variant is `a: u8 = 10`, it's detected as `b: u8 = 10` because `match` compared `u` and `U { b: 10 }` and found that they were equal.
Sure. Unfortunately it [doesn't appear that an error/warning is generated](https://play.rust-lang.org/?gist=e4de8455d5cdf6ebe9e1f141f1d2f733&amp;version=nightly) when the "common prefix subsequence" rule does not hold, or the structs' ABI is not defined.
Rust rarely produces warnings for UB and in general for unsafe footguns. This isn't new.
See [my response to the other comment](https://www.reddit.com/r/rust/comments/6oh6mv/announcing_rust_119/dkhg9b4/). It doesn't act like a "normal" match-- it's just looking for equality in the value of the union. it doesn't have any understanding of what variant is being matched.
https://internals.rust-lang.org/t/small-string-optimization-remove-as-mut-vec/1320 TL;DR: String::as_mut_vec
I think there was a misunderstanding. By: &gt; I think it accounts for the C pattern of including the "tag" as the first field of each variant. I just meant to say that it allowed matching so as to allow this practice, not that it gave any field a special meaning or anything.
Yeah... it's just that in my head a `&amp;&amp;` or `||` hide a branch because of their semantics, so I expect branch coverage to take this "hidden" branch into account :x
It should be simple to turn an enum into a union. The other way around would require the programmer to specify which variant to use and would be unsafe.
Ah so SSO would require SBO and structural specialisation. That's a bit of a bummer.
The RFC feels a bit too vague on this IMO, and the end of the pattern matching section: &gt; Note that a pattern match on a union field that has a smaller size than the entire union must not make any assumptions about the value of the union's memory outside that field. For example, if a union contains a `u8` and a `u32`, matching on the `u8` may not perform a `u32`-sized comparison over the entire union. Seems, to me, to imply by omission that it's fine to match against both a `u8` and a `u32` field as long as you only perform `u8` operations when you matched against the `u8` field.
There is a crate [inlinable_string](https://github.com/fitzgen/inlinable_string). It uses tagged unions, so 8 bytes overhead. However, it can't benefit from untagged unions now (String is not Copy and Vec has Drop impl): &gt; For now, unions can only include Copy types and may not implement Drop. We expect to lift these restrictions in the future. 
Perhaps. It may also be incorrect? We represent unions the was clang does iirc, so whatever is UB in C++ should be UB here too. It's also possible that due to borrowck strict aliasing doesn't exist so there are less reasons for it to be UB. Idk. cc /u/eddyb
You might have more luck with the KDE Neon development docker. It gives you very recent KDE libraries without hours of compiling. https://community.kde.org/Neon/Docker
there is also https://github.com/s3bk/istring
This can probably be done with a procedural macro on crates.io. Anyway, it would be nice if we could reify an enum variant somehow. For example, in Haskell a variant is a function that turns its parameters into the datatype.
I wish I knew all kinds of UB involved there, you'll have to find someone who actually deals with those details more often.
I disagree entirely. Builder pattern is elegant to use and it makes reading code completely straight forward and understandable at all levels. It's also easily debugable. &gt; It's usage is elegant, but defining it is cumbersome You are solving the wrong problem. The amount of time writing code is vastly smaller then the amount of time reading and debugging code. Clarity should be first priority. Macros and closures make things way harder to understand at first glance. Macros are a language within a language and it requires one to first figure out what the hell those macros are doing before understanding what is going on.
Does it cover integration tests as well? Does it produce Cobertura reports?
&gt; I'm not sure of the performance implications of using error-chain I'm not sure either, don't trust me please. If you'ver looked into it, all the better. I somewhat remember something about dynamic dispatch with error-chain, while a hand-rolled solution would use static dispatch, but I don't know 1) if that's true at all and 2) if it had any performance implications really.
and only `Copy` is supported for now, so... writing is safe.
You missed the point. TheDan64 was asking how it was possible to `match` on a `union` if there is no tag. I said I saw no facility for plumbing a tag into the `match` construct, which would be required for your interpretation to be a valid answer.
I'd like to see Spacemacs run on this.
Yes to integration tests, no to cobertura reports but I'll add it to the future feature list! 
Well, you can't *really* do it safely, because of how unions are, but the way unions are usually used is inside a struct with some additional field indicating the variant of the union. To be perfectly honest, I'm not saying I know the right way to do this (hence "some machinery"). Maybe an attribute for the union so you can associate an index with each variant? I was just thinking that since switching from unions to enums is going to be pretty mechanical and *normally* you want to avoid copying as much as possible, that if Rust had something built in to do it for you and correctly use the already occupied memory, that'd be good and reduce bugs. To be even more honest, this is really just my reaction to reading the announcement and thinking aloud. I don't have a practical need to touch unions right now.
Actually, thinking it a bit further, it's still good that it's unsafe even if it caused nothing of the likes of nasal pandemonium; the behaviour of whether the bitpatterns of two values of different types are the same or not is **implementation defined** behaviour. That means that the match could match or not at a specific variant depending on the compiler and architecture which is surprising and likely to cause bugs.
Sounds like they intend to lift that restriction in the future though, so adding it now may have been a backwards compatibility hazard for the future.
&gt; We expect to lift these restrictions in the future. Maybe the fact they're planning on changing that in the future is why?
I don't see how this is different from if-statements, just with the teeny tinesiest amount less syntax. If you started trying to match on inner values, especially, you would run into ownership/lifetime issues because you would be attempting to use the same value multiple times. also, a `match_all {} else {}` doesn't make sense to me. Why wouldn't it just be a normal `x if x &gt; 20 =&gt; whatever` branch? Using normal if statements let val = cmp.compare(&amp;array[left], &amp;array[right]); if val == Less || val == Equal { merged.push(array[left]); left += 1; } if val == Greater || val == Equal { merged.push(array[right]); right += 1; } let age: u8 = 18; if age &lt; 16 { println!("You cannot drive"), } else if age &lt; 18 { println!("You cannot vote"), } else if age &lt; 21 { println!("You cannot drink") } else { println!("You can drive, vote, and drink"); } I'm not a fan of adding syntax for this. I feel like a macro could do it, if needed.
The "tag" you match on would just be some field in the union if you're matching on the union directly. Or you can match on the outer struct's tag thats external to the union. Like how the example matches f1=10.
Correct me if I'm wrong - now with unions we could do proper 3D Vectors with arrays? I mean something like: struct Vec3XYZ { x: f32, y: f32, z: f32 } union Vec3 { arr: [f32;3], v: Vec3XYZ } I's very popular way to handle vectors in C/C++ for gamedev purposes.
This is a good question, and I think the example in the blog post is sort of confusing. [run-pass/union](https://github.com/rust-lang/rust/tree/0ade339411587887bf01bcfa2e9ae4414c8900d4/src/test/run-pass/union) has some nice examples of using unions, including a pattern matching example in [union-pat-refutability.rs](https://github.com/rust-lang/rust/blob/0ade339411587887bf01bcfa2e9ae4414c8900d4/src/test/run-pass/union/union-pat-refutability.rs).
i only skimmed the discussion but it seems that as_mut_vec is not a big problem if vec has sso itself. the concerns seem to come more from the additional branching required
[Small (or Short) String Optimization] (https://stackoverflow.com/questions/10315041/meaning-of-acronym-sso-in-the-context-of-stdstring) and [Small Buffer Optimization](https://akrzemi1.wordpress.com/2014/04/14/common-optimizations/#sbo)
The point of match statements of themselves is to avoid the long chains of if blocks testing on the same variable. Also your second part of code is not equivalent to mine. The equivalent block would be the following: if age &lt; 16 { println!("You cannot drive"), } if age &lt; 18 { println!("You cannot vote"), } if age &lt; 21 { println!("You cannot drink") } if age &gt; 20 { println!("You can drive, vote, and drink"); } Also about the `match_all {} else {}`, this was discussed and found to be clearer then having `_` only match if nothing else matched, which very arguably is not intuitive in a `match_all` scenario. It also couldn't be a normal `x if expr =&gt; expr` branch since that might have to change depending if the requirements change. With the `else` syntax you don't have to worry about it. Something similar would be arguing that `if {} else {}` is redundant and should instead written as a bunch of completely separate if statements. Instead of: if x &gt; 5 { // do something } else { // do something else } having to always write: if x &gt; 5 { // do something } if x &lt;= 5 { // do something else }
Yes, but the `match`ing example given has a union with a single, non-tag field per variant. That's what TheDan64 was almost certainly asking about when he said "how is union matching possible if there's no tag?"
I remember reading that you shouldn't rely on `struct` layout to be the same order in memory as it is defined. This was for optimal memory layout, so it may not apply here since all members are the same size.
Yep, I made a mistake, I was just quickly copying and modifying the code since I'm occupied with other tasks at the moment, and I didn't give it enough attention to notice the issue I had created. Regardless, I don't see this being beneficial enough to warrant new syntax. I'm a nobody, so my opinion doesn't matter much, but as a Rust enthusiast, that is my opinion. I'm not opposed to it if other people want it, though. Unless the implementation is restricted carefully, I think you could still run into some very confusing lifetime issues with this kind of control flow. I haven't actually thought about it long enough to come up with an example, so maybe not.
Sure, it's a recursive thing. String can't have it because it's built on Vec. Vec can't have it for other reasons.
Without having read the RFC, I can only guess the precise mechanics, but I'm assuming that it goes top to bottom trying each case until a match happens. With no tag in place, you might end up reading data using the wrong case and get nonsensical garbage. That's what makes it unsafe, and why you can only use Copy types for now.
I'm confused. What is the purpose of this post? Did you want to show off what you made (it looks like it -- congrats!)? There's not much info in the README about how this generator actually work (sounds like an interesting "how I did it" section to add), and this post only contains a link...so...
I believe it's UB to have an invalid enum discriminants or boolean value, so these unions are unsafe if you put certain values in `n` and try to read the other field: union Screwy { b: bool, n: u8 } union Screwier { o: Option&lt;u8&gt;, n: u16 }
This sounds a bit too final; from the discussions I've read about nailing down the rules of `unsafe`, it seems safe to assume that we *will* start warning about such things someday, so maybe it's not too soon to start now. :P
In practice that just means you'd add a `#[repr(C)]` to the top of the struct when you want to gaurentee the same order
Perfect implementation of unions, including treating as unsafe code! This language is seriously fixing programming with the type system, I lav eet!
Cool idea!
63 minutes seems like a long time, although I didn't really look at what the code is actually doing. Was that with a debug or release build? Also, consider using the `clap` crate for parsing command-line arguments. It's great.
Arbitrary precision numbers don't implement `Copy`, and they are a valid use case for a fixed point iteration.
For Unions/Enums where all variants are copy it would be safe right?
Just a quick question regarding the -Z flag. Are there any thoughts floating around to have a versioning for rustc itself? For what i see is, the -Z 'could' be a candidate for a Major Version Bump. Of course not for the language (it has nothing to do with the language itself) but for rustc! Today we have rustc just copy the version of the language it does compile. rustc 1.19 compiles Rust 1.19. I don't know if that makes any sense at all but what about having rustc using a different versioning? Like the -Z change would bump rustc to 2.0 but it would compile Rust 1.19 At the end rustc is "just" one of many different Rust compilers (lets dreaming for a bit here) But the question here is, does that makes any sense and is it not more confusing than it helps ... idk. But for such cases it could eventually. Just wanted to know if there is any discussion about it.
Get ready for a discussion so large that github itself hides many of the comments https://github.com/rust-lang/rfcs/pull/2052 (This isn't the proposal exactly, but it's a big part of the discussion)
I agree, your approach is definitely not a good fit with formalist systems like Coq and Idris. I think you might see a lot to like in something like NuPRL or (especially) RedPRL.
Looks like you suppose that: - Builder methods never do any computations - The builder is always constructed once and only in one place, there's no branching I believe these assumptions are not necessary always true. Beside that, the builder pattern is quite straigtforward, popular and provides a nice API.
Yes this makes sense ... i followed the epoch discussion a bit but i did not connected the two but it does makes perfect sense that these are tightly connected. 
\*nod\* That's basically what I was saying, but given more detail.
I don't understand what it is used for :(
Coercing non-closing closures to a function pointer is cool.
What are the pros/cons of using a union vs an enum? (How unsafe/more performant are unions?)
Complete n00b. What is the purpose of the Union over an enum (since enums can contain data, not just an enum-value)?
What's the motivation for that? If it's just for the `my_vec[n]` syntax, you could just `impl Index&lt;usize&gt; for Vec3XYZ`. If you need to pass around the vector's contents as a slice, a method `as_slice()` would be almost as convenient. I guess you could also use unsafe code to `impl Deref&lt;Target = [f32; 3]&gt; for Vec3XYZ`, though I'm not sure that's the intended use of `Deref`.
&gt; performant Unions use less space, because they do not need to store the tag. (Some enums don't need to store a tag, but that's relatively rare) &gt; unsafe Because there's no tag, you have to manually keep track of which part of the union is valid. If you mess it up, UB.
99.99% of the time in Rust, the purpose is "I have a C API that uses unions and I need to interface with it."
[Relevant entry on the jargon file](http://www.catb.org/jargon/html/N/nasal-demons.html)
It's not really about performance- it's mostly about FFI to C. Enums have a discriminant and its value and location is decided by the compiler. Unions in C may or may not have a discriminant, and if they do it's under the control of the program, so Rust needs a way to match that in order to interop with C.
Interop with C, which doesn't have Rust-style enums and thus frequently emulates them with unions. Also, memory layout optimizations for rare situations.
All the others are talking about unions and I think that `eprintln!` is the best of the new features.
This was to show off something I created - that's all. I'll go into the details more in the README.
Maybe the special-case-ness ought to be called out in the blog post, eh /u/steveklabnik?
IINM the `char` exception is only a C thing, because in Rust there's nothing for it to be an exception *to* (there's no type-based alias analysis in the first place).
Oh, no, I was only describing the current situation, not prescribing what it should be. My point was that it's not surprising that it doesn't warn (and you shouldn't infer safety from that), because we rarely warn on UB anyway.
Ahh, unions. Allowing this beautiful bit of code: union union&lt;'a&gt; { union: &amp;'a union&lt;'a&gt;, } Even better: union union&lt;'union&gt; { union: &amp;'union union&lt;'union&gt;, }
&gt; It is linux only and only designed with x86_64 support (so 64 bit AMD and Intel processors). Wider support is planned for future expansion, but this should work for the majority of users. What would it take to implement mac os support?
Isn't strict aliasing going to be a problem again with cell types?
Neat
Thanks for pointing out clap - I'll look into it. 63 minutes is very long, and that's the release build after a fair amount of optimization. The algorithm is unfortunately O(n^2) where n is the number of pixels, so for the 1 megapixel image shown, that's about a trillion steps. I'm looking into ways to speed up the algorithm without dramatically changing the output.
Oh! Think of it like an App Store but for things without a GUI. Think of it as somewhere to sell all the system tools released on github, as open source, simply because there isn't an easy way to monetize those tools.
This is just a pointer that has to point to a pointer that points to a pointer that points to a pointer... right?
Yep. That part isn't even new with Rust 1.19.0, though. My point was more around the amusement of the contextual keyword `union`, which allows `union` as both an identifier and a keyword.
Tbh, it would confuse me if I would start learning Rust. "rustc -- version says I have rust 2.0, however I cannot find any documentation for rust 2.0, just for 1.x"
I'm not sure without looking further into it but I'd have to find the equivalents of ptrace which Mac only does a subset of. I'd also need to find any os security features that could interfere with instrumentation and work around them. I'll have a look but I can't promise anything within the next two months. You could always submit a PR ;)
I got an answer from Michael: --- The students in his course on operating systems knew Java and C (at least the basics) before the course started. He asked his students about the usage of Rust: - All of them preferred Rust over C for systems programming - Students in the 3rd semester experienced a steeper learning curve than students in the 5th semester - Students in the 5th semester would have liked to program even more --- As a result of this positive feedback, he will offer a pure Rust course next semester. During the OS course, he invited a few external speakers: - Philipp Oppermann (he wrote a rather famous series of blog posts about OS dev in Rust) - Florian Gilcher (he commented here too!) - Dennis Schubert (works at Mozilla) All three talks were very well-attended. There was a lot of interest in Rust. Additionally, Micheal said that he is kind of disappointed that embedded and realtime systems are not at all a focus of the Rust core team. In his opinion there is a huge potential for Rust in those fields. --- ping /u/asmx85 and /u/eric-douglas
Thank you for understanding me :p
some sort of 'tag&lt;&gt;' trait to implement?
great way to confuse compilers r.e. their ability to keep things in vector registers..* ( * I haven't observed that lately though, I just remember that sort of thing being a hazard in the past. we had more to gain from the exact opposite, i.e. *preventing* field access, ensuring everything went through abstractions around intrinsics)
Builders can build multiple times (though they don't have to be). Builders can be cloned. The closure approach might be nice and better for some cases though.
The `-Z` flag changes are probably a good idea, but I'm going to miss being able to do incremental builds on stable.
This program seeds an image with some pixels of random colors in random locations, then repeatedly chooses a pixel of a random color and places it as close as possible to the pixel which has already been placed which it is most similar to in color. This leads to some beautiful images. After some algorithmic improvements, it runs in O(n^1.5) where n is the number of pixels in the image, not O(n^2), dramatically speeding up the program. (63 minutes -&gt; 3 minutes for the largest case)
&gt; [...] and it's not clear that [SSO]'s even advisable. Two or three years ago, I made a Ruby script that tracked all created `String` and `Array` objects, keeping track of their lengths and whether they have been mutated by pushing more data. I put this script in a shell where I imported all the Ruby modules and in a Ruby-scripted game of a friend, where, IIRC, I found that: - Most arrays had only 0 to 5 entries. - Most strings were shorter than 16 bytes. This roughly meant that assuming a 64-bit machine and assuming the traditional `(len, capa, data_ptr)` layout, about 80% of all strings and something between 60% and 80% of the arrays *(using 64-bit object pointers as entries)* would fit perfectly within the `(capa, data_ptr)` area. I expect the numbers to be quite less in average Rust programs, as most strings I see in the wild are either formatting-related or error messages and thus quite often longer than 16 bytes. And in fact, many of the caught Ruby strings were stringified symbols, i.e. function and class names. Therefore, keeping SSO isolated in a separate crate *might* be better than changing `alloc::String`. In addition to that, I didn't have the nerves back then to calculate the trade offs between doing less allocations with SSO and doing more range checks to correctly grab a data pointer, and didn't re-visit this experiment either. So... yeah, not clear. And I expect a tendency towards not worth it for the average Rust program.
I'm somewhat skeptical of the viability of such a model, given the following points, but to each their own: * Selling things which can be infinitely duplicated at no unit cost is more difficult. * Games get away with being sold because open-source competes poorly with them. * Open-source competes poorly with games because its strength lies in being the tortoise side of a tortoise-and-hare dynamic. (ie. You can iteratively perfect a free alternative which produces equivalent functionality over the course of a decade. You can't do the same with a creative product like a novel, movie, or game.) * There's a non-trivial portion of the open-source community (the subculture with roots in the free software movement) which take a lack of "gratis if you're willing to compile the source" alternatives as a personal affront. (Not to mention how people like [Eben Moglen](https://www.youtube.com/watch?v=NorfgQlEJv8) cast charging for software in terms of "paying license fees for various mathematical operations".) * There are even more novice coders who are affronted enough by "source is free, builds cost" models to offer their own builds. (eg. the builds of X-Chat for Windows) I suspect that any attempt to monetize copies of utilities for technical people rather than giving money proportionate to the actual scarce resource (developer time) will be undermined by said effects.
&gt; because they do not need to store the tag Or rather: Unions allow programmers to decide where the tag is to be stored, if one is needed, at the cost of having to manage the tags themselves. And quite often, the tag of unions is inlined within specific bits of the contained data to allow very tight packing, such as GC headers in Lua or Ruby. Something Rust's `enum`s can't do.
Now it only takes 3 minutes for the same size, after some algorithmic improvements. EDIT: And now it only takes 40 seconds.
What about writing u64 and reading signaling NaN or something like that?
If all your users are on 16.04 or newer this should be much easier than building a deb. No need to worry about dependencies and such; it should just run cargo to pull those.
I take and appreciate your points, but you're probably aware of counterexamples. I just left a company that is a counterexample - it very successfully sold proprietary, system level, networking software. Maybe you couldn't make a lot of money selling a grep clone, but how about a novel, niche-dwelling nosql database? As a systems programmer, I want to be able to sell my software and directly pay for system software that I use. When I look at how software marketplaces, whether for Apps or Games have enabled developers, I can't help but want the same for my kin :P.
Avoid using unions for type punning like this unless you can guarantee the layout. Makes more sense to implement Index on Vec3.
If anyone is curious... https://github.com/ruby/ruby/blob/trunk/include/ruby/ruby.h#L1004-L1013 and https://github.com/ruby/ruby/blob/trunk/include/ruby/ruby.h#L990 https://github.com/ruby/ruby/blob/trunk/include/ruby/ruby.h#L956-L965 and https://github.com/ruby/ruby/blob/trunk/include/ruby/ruby.h#L949 
I don't think so. It's just like gcc. It's currently on version 7, and it compiles C++17. Same thing with `rustc`: the compiler version is `x`, and it compiles up to version `y` of Rust. It's technically what happens today, only for the sake of simplicity we make `x` and `y` equal to each other. But the real one is `y`, the `x` is just a convenience.
After months of bikeshedding I'm glad that my preferred naming scheme was used :)
He's saying that you could write a uint64 in the pattern of the platforms signaling Nan, then try to read it as a float, and get a CPU trap. Basically, it's possible to break stuff by just writing bits if you aren't _absolutely_ sure those bits will never be interpreted as a float (or pointer, or so on).
Sounds like something you would solve with a macro if you really run into it often enough to not write out the conversion code manually. I don't see why this would need to be in the language / stdlib.
A feature like that would be fairly different than what this offers. What you seem to be asking for is some way to have some way of annotating the representation of an enum to indicate the size and layout of its discriminant and variants. That would be possible to add, but would be pretty much separate from this feature (since at that point, it wouldn't be an unsafe C style union, but just an enum with a specified representation). This feature allows you to do something that you can't do with separate tag and data, like using low-order bits of pointers for tagging, or certain forms of small-string optimization. It also gives the user the ability to manipulate C types using unsafe code, without having to add all of the complexity to the compiler to be able to specify enum representation. Basically, this is a small, incremental feature, that adds value now. There is likely a much bigger and more complex feature, that does what you would like, but this allows library authors to easily wrap C code now, at the cost of having to potentially copy data out to put it into safe types.
But then you would have to use `unsafe` to read from the float out of the union.
Judging from that benchmarksgame website, there is apparently a slight regression in performance when [compared](http://i.imgur.com/wCaQyhT.png) to 1.18. Can anyone observe the same behavior in own benchmarks? 
What are people's experiences with code coverage in Rust (and other statically compiled language)? From my understanding, it is especially useful in type-less languages like Python. In Rust, I don't have a need to have 100% CC as compiler and clippy did already quite a job to make sure silly mistakes are not there, and unit (integration, acceptance, etc.) tests should test functionality. Note: I am very happy that a tool like this exists, and I think this is awesome work. Just wondring how rigorously to apply it and when it makes sense. 
I haven't heard about that and I'm not sure what you're referring to, do you have a reference?
[Nope](https://www.reddit.com/r/rust/comments/6oh6mv/announcing_rust_119/dkhlls5/).
If you are targeting 1.19-nightly or above, you should use `-Zprofile` instead of the three flags `-Cpasses=‚Ä¶ -L‚Ä¶ -l‚Ä¶`.
I find coverage to be an excellent way to quickly identify places where I have yet to write *any* tests... for example, because (as is often the case) I had to put down a project for so long that I've forgotten where I left off. That said, the more fine-grained the coverage reporting can be, the better. Even clever uses of Rust's type system can only do so much to rule out logic bugs at compile time.
&gt; But... this pattern was invented before lambda expressions became popular. I seriously doubt that, Rust has an OCaml heritage and has had an affinity for closures forever.
Is it UB to have a function that takes arguments `&amp;Cell&lt;u32&gt;` and `&amp;Cell&lt;f32&gt;` where you pass the same pointer to both?
That's defnitely an improvement.
Would be surprised if it were. But cc /u/ralfj :)
I'm not sure why you're putting `x` inside the `if` function. Notice that probability notation, e.g. `P(A ‚àß B)` is transforming an operator `‚àß`. While you can interpret this as an event, you can also consider a program that traverses an AST and replaces functions with other functions. The randomized input is independent for each bit, so you don't get the problems you mentioned. The way of assigning probabilities to Boolean functions is just a way of formalizing existing concepts, but show that this holds in general for all functions in binary form. Probabilistic existential paths are probability distributions over the outputs of a function. When you enumerate the input type, there is an assigned output for each input for finite sets. You are right about things start to get weird when using infinite sets. You can have an algorithm mapping natural numbers to even numbers, so its existential path returns `true` for all even numbers and `false` for odd numbers. This is the `even` function. The probabilistic existential path `‚àÉ‚Ñôeven` tells how frequently `true` and `false` is returned. You don't assign a probability to each natural number, but to the finite type `bool`. The question is why it should make sense to assign 1/2 to `true` and `false`, when there are infinite ways of arranging the sets of even and odd numbers such that they are not of same size after a cutoff. Path semantics does not tell you why this makes sense, it only says "figure out this function". So, you use your naive intuition of what is a sensible thing to do and viola: You get the naive intuitively correct probability. Notice that in this case the algorithm predicts `even` of output from `even` of input, so it is symmetric. I tested another case using `even` on the partial function `inc{(&lt; 10)} : nat -&gt; nat`, which operates on finite sets, and it worked in that case too. Negative constraints and all just worked. Now, why the heck does this algorithm work in both these cases? Perhaps the answer is that a probabilistic path assumes a function of type `nat -&gt; nat` on the input, and it might work with any of them, as long you're consistent. This is something I should check closer. You're right that if this algorithm discards the probability axioms and instead predicts a broader class of probability-like predictions, then we should figure out what it does and call it something else. It is already suspicious how it seems happy with negative numbers in the terms, which is why I was looking for feedback.
The builder pattern is meant to separate required and optional inputs for the final object, and also encapsulates intermediate calculations nicely. Also, I don't see the benefit of your approach against a single method with optional parameters like: #[derive(Debug)] struct Foo { txt: String, } impl Foo { fn build&lt;Bar, Baz, Qux&gt;(bar: Bar, baz: Baz, qux: Qux) -&gt; Foo where Bar: Into&lt;Option&lt;i32&gt;&gt;, Baz: Into&lt;Option&lt;u32&gt;&gt;, Qux: Into&lt;Option&lt;f32&gt;&gt; { let mut txt = "Foo".to_string(); if let Some(ref bar) = bar.into() { txt = format!("{}, bar is {}", txt, bar); } if let Some(ref baz) = baz.into() { txt = format!("{}, baz is {}", txt, baz); } if let Some(ref qux) = qux.into() { txt = format!("{}, qux is {}", txt, qux); } Foo { txt: txt } } } fn main() { println!("{:?}", Foo::build(1, 3, 2.0)); println!("{:?}", Foo::build(1, None, 3.0)); }
The problem though is that by definition `union` users cannot know if their fields have been manipulated in a way that will cause a CPU trap. `unsafe` doesn't magically mean "everything you do here is A-Okay." Allowing anyone to write any bits would very much allow so-called "safe" code to break the application in some cases.
And the code would be broken. `unsafe` doesn't protect the developer against broken code; all it does is relax some strictness. Accessing that float will still lead to a CPU trap, and the bug in this case would have been the _safe_ code that wrote the bad bits.
Have you seen the [backtrace](https://docs.rs/backtrace) crate? With it, you can save the stack trace of the current point in the program, which also includes the calling function. It heavily depends on debug information being available, of course. IIRC this is what [error_chain](https://docs.rs/error-chain) is using under the hood for stack traces.
Yeah LLVM 4.0 caused some performance regressions.
Cool, was just curious. Awesome stuff anyway, it's very exciting!
With a language that compiles to native code (like Rust, C, or C++) inserting sufficient information to track that sort of thing all the way back up the stack basically means reinventing the kind of debugging symbols a debugger relies on. Luckily, Rust includes them in release builds unless you strip them out to produce a smaller binary (the size of debugging symbols is not something to be taken lightly) so that you can use the `RUST_BACKTRACE` environment variable to enable backtraces on `panic!`. To use them otherwise, you need code which can: 1. Parse the stack 2. Use the debugging info to translate raw binary offsets in the compiled code back into line numbers. The simplest way to do that would probably be the [backtrace](https://github.com/alexcrichton/backtrace-rs) crate. That's what [error-chain](https://github.com/brson/error-chain) uses unless you opt out of backtraces. (I opt out of backtraces because it's the only thing in my little utilities that would force me to install a full-blown `musl-libc` cross-compiler to make musl-targeting Rust binaries for full static linking.)
I was curious what symbols backtrace would give, so I played around with it a little. The selection might be wrong, and it might fail in some weird cases (I didn't even try it without debug symbols), but it might get you started: extern crate backtrace; use backtrace::{ Backtrace, BacktraceFrame, BacktraceSymbol }; fn previous_symbol(level: u32) -&gt; Option&lt;BacktraceSymbol&gt; { let (trace, curr_file, curr_line) = (Backtrace::new(), file!(), line!()); let frames = trace.frames(); frames.iter() .flat_map(BacktraceFrame::symbols) .skip_while(|s| s.filename().map(|p| !p.ends_with(curr_file)).unwrap_or(true) || s.lineno() != Some(curr_line)) .nth(1 + level as usize).cloned() } fn foo() { let sym = previous_symbol(1); println!("called from {:?}:{:?}", sym.as_ref().and_then(BacktraceSymbol::filename), sym.as_ref().and_then(BacktraceSymbol::lineno)); } fn bar() { foo(); } fn main() { bar(); } will print called from Some("/home/till/dev/foo/rust/stack/src/main.rs"):Some(22) 
&gt; You're literally describing how C code works, and then you seem to be complaining that Rust is letting you do the same things. In unsafe, everything is not suddenly A-Okay ... You're making my point. Rust does not allow one to do the things C does. That's, like, kinda sorta one of the main points of Rust. You've maybe heard. :) If you're using a Rust `union` to interface with C code, you're presumably trusting that whatever "legacy" code you're using is not filling in signalling NaNs in unions susceptible to such things. You're just _trusting_ it won't, because C is inherently `unsafe`. That's the point of `unsafe` - you letting the compiler know that you know you're not supposed to write mean bits into that field if they're later going to be interepreted* as a float. Pretty straight forward. (*) Which is all based off a possibly very-flawed assumption that Rust will even allow that. It's completely undefined behavior to write field `foo` and read field `bar` in a union in C, which most people forget... partly for these kinds of reasons. If Rust doesn't allow that, then we're right back where we started though: unsafe code being able to write to union fields would result in undefined behavior later on when reading the field if it doesn't also mark the right tag or whatever is used. Writing to a union is not safe; pretty clear. 
What I'm thinking of would be compile time only. It would basically be equivalent to having a `usize` argument to the function that would fill with `line!()` every time you use it, but instead the compiler does this for you automatically. No runtime symbols at all. Thus would also work with release builds, since the info is static from the source, and filled in at compile time.
Note that release builds and debug symbols are not exclusive, you can enable both. Looking at the way C# handles this, a macro or compiler-plugin might be your best option (or just do it at runtime...)
Another reason is that DMD literally never frees any memory, and as a consequence doesn't have to keep track of it.
&gt; Rust does not allow one to do the things C does. That's, like, kinda sorta one of the main points of Rust. You've maybe heard. :) Except in `unsafe`. This is kinda one of the main points of `unsafe`. You can trigger all kinds of UB in unsafe. This is not new. This is intentional. The compiler still tries to help you avoid it but ultimately the escape hatch exists. union is one such escape hatch. Edit: Oh, I see the point being made here. You're talking about general API contracts, /u/coder543 is talking about having the _ability_ to do type punning. You're right, in general writing to a union is not a safe operation because you may break the invariants of code reading from it. 
Are you suggesting that NaN is unsafe?
I think the point is that the unsafe code is incapable of checking for sNaN, because just reading its value can trap.
For the lazy: &gt; The largest new library feature is the eprint! and eprintln! macros. These work exactly the same as print! and println! but instead write to standard error, as opposed to standard output.
Can someone help me explain what the heck is going on in this section of code? https://github.com/iron/body-parser/blob/master/src/lib.rs#L118-L139 Specifically the pub struct where for syntax and the type + type syntax. Thanks so much, happy to read docs, just didn't know what to search for!
Started reading back in the comments, https://doc.rust-lang.org/nightly/book/first-edition/generics.html#resolving-ambiguities helps a _ton_, thanks all!
&gt; System programmers deserve to be able to monetize their software No. Software deserves to be free. 
This is pretty much exactly how I feel about this (except worded more eloquently than I would've)
I don't think a macro (at least a user-defined one) can handle this, because macros can't inspect the entire source of a project (to my knowledge), which they would need to to find every invocation of the function and add the line number argument. I can emulate this with an extra argument that I manually fill with `line!()` every time, but I hoped that there was a way to make the compiler do it for me.
&gt; Software deserves to be free. Free as in speech, or free as in beer? The GPL totally allows you to sell your code, or binaries. (Of course, the person you're selling to has the right to then give away whatever they were just sold for free)
Won't the cell copy the value out of the union though? So you're not referring to the same place in memory? So it's only an issue if you pass in two `Cell&lt;SomeUnion&gt;` and use the different variants (and even then you need to use unsafe to read it, so it's only possible in unsafe Rust) 
Not sure what advantage you're gaining with that example, since i'd rather write `None` for an argument than wrap it in `build!()`. More importantly, to use this method, you still have to know the usage details and fields of `FooOpts`. So why not just pass a `FooOpts` using struct notation? Foo::from_opts(FooOpts { bar: 1, qux: 3.0, ..FooOpts::default(), }); What (little) you lose in conciseness, you make up for in clarity and consistency. Plus you don't have to reason anything about closure capture rules. And I don't think there's any functional advantage to your method over this...
I don't understand why split borrows don't work in `frob_box` but do work in `frob`. Eh? https://play.rust-lang.org/?gist=10af40b9c5a725745f16f0f158f01bf7&amp;version=stable
If you're going to cross-post to both Reddit and StackOverflow, you should provide [a link to the StackOverflow question](https://stackoverflow.com/questions/45227519/iron-renders-html-as-text).
IMHO supporting matching for union is a bad idea. I don't see why it is useful at all. Setting one field could unintentionally match the other field leading to wrong code path. The examples show how to use them but not when to use them.
 #[derive(Eq, PartialEq)] pub enum IteratorControl&lt;A&gt; { Continue, Escape ( A ), } use IteratorControl::{Continue,Escape}; impl&lt;A&gt; Default for IteratorControl&lt;A&gt; { fn default () -&gt; Self { Continue }} impl From&lt;()&gt; for IteratorControl&lt;()&gt; { fn from ( () : () ) -&gt; IteratorControl&lt;()&gt; { Continue}} impl From&lt;bool&gt; for IteratorControl&lt;()&gt; { fn from ( b : bool ) -&gt; IteratorControl&lt;()&gt; { if b { Continue } else { Escape (()) }}} pub trait InternalIterator where Self : Iterator { fn iterate_or_else&lt;Ret, Control : Into&lt;IteratorControl&lt;Ret&gt;&gt;, Loop : FnMut(Self::Item) -&gt; Control, Else : FnOnce() -&gt; Ret &gt; ( &amp;mut self, mut l : Loop, e : Else) -&gt; Ret { let mut ret = None; for x in self { if let Escape(y) = l(x).into() { ret = Some(y); break}} return ret.unwrap_or_else(e)} fn iterate_or&lt;Ret, Control : Into&lt;IteratorControl&lt;Ret&gt;&gt;, Loop : FnMut(Self::Item) -&gt; Control, &gt; ( &amp;mut self, l : Loop, o : Ret) -&gt; Ret { self.iterate_or_else(l, ||o)} fn iterate&lt;Ret : Default, Control : Into&lt;IteratorControl&lt;Ret&gt;&gt;, Loop : FnMut(Self::Item) -&gt; Control, &gt; ( &amp;mut self, l : Loop) -&gt; Ret { self.iterate_or_else(l, Ret::default)} } impl&lt;I : Iterator&gt; InternalIterator for I {} // find implementation done this way // this shows the nice part of when not escapting iterate returns a default value, in this case None fn find&lt;A, I : IntoIterator&lt;Item=A&gt;, Pred : Fn(A) -&gt; bool&gt; ( iter : I, p : Pred ) -&gt; Option&lt;usize&gt; { iter.into_iter().enumerate().iterate(|(i, x)| if p(x) {Escape(Some(i))} else { Continue })} fn main () { let v = (0..10).collect::&lt;Vec&lt;_&gt;&gt;(); v.iter().iterate(|i| { println!("{:?}", i)}); (0..100).map(|i|i*i).iterate_or_else(|i| { if i &gt; 50 { println!("first number over 50 is: {:?}", i); return false} else {return true}}, || println!("there as no number over 50")); println!("First space found at: {:?}", find("Hello, World".chars(), |c|c==' ')) } It does have value-returns from loops though and uses the Python way of an "else" branch which executes when there was no break with a value. The nice thing about Rust's default values is that the else branch becomes optional on a defaultable value so it just return the default so it automatically works with `()` and doesn't need an else branch in that case. It uses the `Into&lt;IteratorCcontrol&lt;_&gt;&gt;` trait which is implemented for `()` which is always `Continue` and for `bool` where `true` is `Continue`and false is `Escape (())`
&gt; Allowing anyone to write any bits would very much allow so-called "safe" code to break the application in some cases. Creating a dangling pointer is not considered unsafe in Rust ‚Äì because the unsafety won't occur until you try to read it (which is unsafe). Likewise, writing to a POD union *oughtn't* be unsafe because unsafety doesn't occur until you try to read it (which is unsafe).
`unsafe` doesn't just taint the block, but the whole module. It is already possible to make safe code cause errors in unsafe code that *should* be fine.
Should let mut pointer = 0 as *const i32; be unsafe? If you try to dereference that (in an unsafe block) it's going to segfault. I'm not sure what the difference is between this and the union case.
So many layers.
A useful comparison is to the &amp;str type. It's unsafe to mess with its bytes because safe code assumes it's UTF8
I shouldn't.
You should hurry over to /r/playrust then.
they're html 
thx
Not posting a link means that people can end up wasting effort answering the question twice. It also means that if it only gets answered on SO, no one coming to the reddit post will find the answer.
no need
Why are you spawning an OS thread for each line of input? So, I believe what's happening is you're continuing to accept input, continuing to spawn threads. Each thread attempts to grab the mutex for `stdout`, but can't, because one thread is holding onto it, trying to finish printing its line. But, less isn't allowing more input right then. So, threads continue to pile up with more and more data to send. Each thread has a certain amount of overhead as well. You're also allowing lines to go to stdout in a different order than they arrive from stdin, which could have unintended consequences. Much more efficient would just be to use ['explicit synchronization'](https://doc.rust-lang.org/std/io/fn.stdout.html) to hold the mutex lock, and then just write bytes directly to stdout, rather than spawning tons of heavy OS threads. Or you might consider using a [BufWriter](https://doc.rust-lang.org/std/io/struct.BufWriter.html) instead of explicit synchronization, but definitely not an unbounded number of OS threads. If you think you are being performance limited by a single thread, the most you can hope for is to have one thread reading stdin and one thread writing to stdout. I don't think you can improve efficiency beyond that.
In fact it is not depend on programming language, with any programming language to get 99% insurance that all good you need 100% test coverage, because of with static typed languages it is place for logic bugs. But you can cheating with Rust, for example if you check hanlding of `Ok` and `Err` on caller side, you not have to cause all errors to check all lines in callee, you check only one error handling.
Are Unions useful if you're not interfacing with non-Rust code?
I suspect you may need to set the content-type, maybe something like: Ok(Response::with((iron::headers::ContentType::html().0, iron::status::Ok, ...)))
Have you seen my work on this topic? https://users.rust-lang.org/t/howto-generating-a-branch-coverage-report/8524 Potentially, tarpaulin is far more interesting because using syntex allows you to easily discard "irrelevant" CFG edges, like the ones to landing pads.
This isn't an answer to your question, but I'd recommend just writing ordinary code (maybe starting from existing, working code) in Rust. The compiler has gotten really good at explaining errors, so it can guide you well. In other words, it might help to internalize the rules by repetition and practice, rather than thinking hard about corner cases, which reading material tends to emphasize. 
They allow a zero-overhead way to avoid running destructors on a stack value. This is how [`std::mem::ManuallyDrop`](https://doc.rust-lang.org/std/mem/union.ManuallyDrop.html) is implemented.
% test coverage and reports is still important indication, mainly to see which parts of the code is not tested. I am not a strong believer of 100% CC though, specially in Java.
I think matching on unions is normally expected to happen as part of matching against a containing struct. The aforelinked union-pat-refutability.rs demonstrates this.
The code was just an example. I'm actually using threadpools, and the order of output doesn't matter to me as long as the entire write is atomic. Here's a better example of what I'm doing, using explicit synchronization from the link above: extern crate num_cpus; extern crate threadpool; use std::io::BufRead; use threadpool::ThreadPool; use std::io::Write; fn main() { let pool = ThreadPool::new(num_cpus::get()); let stdin = std::io::BufReader::new(std::io::stdin()); for line in stdin.lines() { pool.execute(move || { let stdout = std::io::stdout(); let mut handle = stdout.lock(); handle.write(&amp;line.unwrap().as_bytes()); }); } } However, I'm still getting the same issue. I think I've tried BufWriter before as well, with the same result: uses a ton of memory, as if it's ignoring the block on the pipe to less.
&gt; I'm not sure why you're putting x inside the if function. Why not? Nothing I've read seems to disallows it, and it's the encoding of the (slightly weird) function: f := \(x : bool) = if x { x } else { 0 } In your notation, what's `P(f)`? `f` is equal to `\(x : bool) = x`, so it should be that `P(f) = 1/2`, but doing the `if` statement breakdown incorrectly gives 1/4. The same thing applies to the (slightly) less silly function g := \(x : nat) = if (x % 2) == 0 { (x % 4) == 0 } else { 0 } This is the same as `\(x: nat) = (x % 4) == 0` and so `P(g) = 1/4`, but using the `if` encoding gives `P(g) = 1/8`. I guess you could try to define some canonical/minimal normal form, where the values inside an `if` are guaranteed to be independent of the condition. But you need to do this, for the definition of `P` on `if` to be consistent, or else, as my examples above demonstrate, two different ways to write down the same function (that is, both forms give the same value for each input) get different values of `P`. &gt; Notice that probability notation, e.g. P(A ‚àß B) is transforming an operator ‚àß. While you can interpret this as an event, you can also consider a program that traverses an AST and replaces functions with other functions. The randomized input is independent for each bit, so you don't get the problems you mentioned. The way of assigning probabilities to Boolean functions is just a way of formalizing existing concepts, but show that this holds in general for all functions in binary form. This is all irrelevant: my example had a single bit. In any case, treating each bit of a natural number in isolation doesn't work, e.g. the value of `x % 3` depends on all of the bits of `x`. Also, when I say "independent", I mean [the probability theory independence](https://en.wikipedia.org/wiki/Independence_(probability_theory\)), not "look at things separately". &gt; So, you use your naive intuition of what is a sensible thing to do and viola: You get the naive intuitively correct probability Mathematics, probability theory and "a new field of computer science" do not work with naive intuition: they definitely try to formalise and nail it down to get a coherent model with useful outputs, but just relying on intuition alone leads to incorrect conclusions, especially when trying to reason about infinite sets. &gt; The probabilistic existential path ‚àÉ‚Ñôeven tells how frequently true and false is returned. Aha, I think I understand a bit better now. You're viewing everything in terms of asymptotic density, so your `P` function could be defined as: P(f) = lim_(n ‚Üí ‚àû) #{ x ‚â§ n : f(x) }/n (As in, the limit of the number of natural numbers `x` up to `n` for which `f` is true, divided by `n`.) I think this makes more sense, and is at least coherent, but it is different to probability and *you* will need to prove that it has the properties you want it to have, from simple things like`P(f or g) = P(f) + P(g) - P(f and g)` to the `if` thing, and beyond. An explicit generalisation to functions with arbitrary inputs (including integers and reals) needs to be done too, if you're going to use it on them. Also, I'm not sure this definition is particularly useful: most interesting[0] functions have `P(f) = 0`; only variations on `\(x : nat) = (x % m) == k` do not. E.g. isSquare, isPrime, isPowerOfTwo all have an asymptotic density P of 0. [0]: An interesting fact is that for any real `d` in [0, 1], there is an `f_d` for which `P(f_d) = d`: in your notation, something like f_d := \(x: nat) = floor(x * d) &gt; floor((x - 1) * d) This translates to being true approximately at every `1/d`th natural, and truncations of the limit definition of `P` are just more and more accurate rational approximations of `d`. In fact, I suspect any `g` with `P(g) = d` can only differ from that canonical `f_d` by "finite" perturbations, but I'm not sure how to quantify the finiteness here: I'm pretty sure that any `h` such that `P(h) = 0` will have `P(f_d xor h) = r`, but there are more changes that are okay too, such as shifts `g := \(n: nat) = f_d(n + 100)`, and also applying local permutations (e.g. swapping adjacent bits `g(2k) = f_d(2k+1)` &amp; `g(2k+1) = f_d(2k)`).
**Independence (probability theory)** In probability theory, two events are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of occurrence of the other. Similarly, two random variables are independent if the realization of one does not affect the probability distribution of the other. The concept of independence extends to dealing with collections of more than two events or random variables, in which case the events are pairwise independent if each pair are independent of each other, and the events are mutually independent if each event is independent of each other combination of events. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
use .unwrap_or_else? let (conn, screen_idx) = Connection::connect(None).unwrap_or_else(|e| { error!("Couldn't connect to X Server"); process::exit(-2); }); 
Well that's super simple, thanks!
&gt; in the case of ambiguity, report an error. I'm not entirely sure what you mean but, from this, I assume you mean `rustc` looking up identifiers. &gt; (that would be a breaking change if ambiguity was retroactively introduced, .. but my question is 'would the probability be so low that people just wouldn't mind patching those up manually'. The probability doesn't really matter. This would not only violate the Rust stability promise, it is directly counter to the design philosophy which requires that you explicitly specify your types in function signatures. (Nothing technical prevents global type inference and languages like Haskell do it... Rust just values ABI stability and requiring explicit types in function signatures rules out one easy way to break an ABI without realizing what you've done.) Just the proposal for a mechanism to *opt into* breaking changes has resulted in [a ton](https://github.com/rust-lang/rfcs/pull/2052) of concern and discussion.
&gt; The GPL totally allows you to sell your code, or binaries. Yes, and when you're thirsty at sea you can drink as much water as you like. But it's not a long-term survival strategy. It may be the case that a few individuals and companies have figured out how to make money selling GPL software (for a time), but really you should be selling support instead.
Builder pattern is not from rust...
The typical way to do something like that is to pipe process-ending `Error`s up to `main` and exit there. That way, your control flow is more predictable and you're following the principle of least surprise. The [error-chain](https://github.com/brson/error-chain) crate makes this easy and includes an example `main` to implement it. **EDIT:** I've actually run afoul of a flaw caused by code like your example when working with PyGTK, where a dropped connection to the X server kills the process without letting me handle it to trigger any kind of last-ditch session saving.
Though note that it's currently still unstable to define a union with a non-Copy field.
Please, no. I *like* being able to tell where names are coming from. It doesn't take that long to write a few `use`s, and it helps reading tremendously.
No, but the Builder pattern usage in Rust came after closures were a thing in Rust, so the tradeoff was already considered.
If you check out their posting history you'll find a pattern of asking questions, being rude to those trying to answer, and ultimately deleting the question.
Your *build* function is very close to something called *tap* in ruby. I've once implemented it for all types: trait Tap : Sized { fn tap&lt;F: FnOnce(&amp;mut Self)&gt;(mut self, f: F) -&gt; Self { f(&amp;mut self); self } } impl&lt;T&gt; Tap for T {} It is fancy code that I'm proud of and... never used in my projects :) Mostly because it only obfuscate the code, I see no benefits of using it. Btw. the macro you presented is unneeded too. Instead you can use plain old Foo { bar: 1, baz: 2 qux: 3.0 }
I'm not sure whether I'm blind or not, but another thing to mention that I didn't see on the announcement was that static crt linking was stabilized I believe? 
Yes, but we're talking about Rust here. Given that Rust has a functional heritage it probably had closures since the early days, whereas the builder pattern is a much more complex thing that will have become popular in Rust much later (in fact I don't recall it being used much a year before 1.0).
So I was linked that yesterday and have given it a quick read and I'll probably come back again later. I already discard some irrelevant lines with a few more identified and on my "execution list" 
Given the nature of dynamic typing I'd say arguably you can't have 100% coverage especially while you have a public API people can misuse. I wouldn't apply it rigourously unless I was working on safety critical code, instead use it more like a heuristic to identify where there are holes in your tests that bugs can slip through
 union union&lt;'union&gt; { union: &amp;'union union&lt;'union&gt;, } fn union&lt;'union&gt;(union: &amp;'union union&lt;'union&gt;) -&gt; union&lt;'union&gt; { union { union: union } }
With the big difference that you have a 'gcc' or 'clang' compilers/toolchains for the 'c' programming language. In that world compiler selection is also a thing to be aware of. But with rust we have 'rustc' which implements the 'rust' programming language. That single character difference is a lot more subtle and (currently) you don't consciously choose a specific implementation.
I see where you'll be going to next. You'll ask a mean to specify which tag value will match to each specific variant, because that's what's needed to safely handle unions from FFI. That may be a bit too cumbersome to write.
Yup https://github.com/rust-lang/rust/blob/rust-1.19.0-relnotes/RELEASES.md#version-1190-2017-07-20
Unless `rustc --version` prints both... For example: rustc 2.0 for Rust 1.19
In think the problem is almost the same here, however instead of creating a thread for each line, the pool has a queue of all future lines to be printed. I think what you really want is a bounded multi producer single consumer queue as implemented in `std::sync::mpsc`, then you won't be able to schedule additional work if the queue size is reached.
heh global type inference is an orthogonal issue to this. I'm trying to change how i think about modules, traditionally I regard qualifiers as ugly noise , so did enough 'glob importing' to avoid them altogether (this is why I had friction, because in C++ I was used to *argument types* all contributing to the selection); but then I observed '.. but I'm usually still in the habit of making compound names, from C++'. If the filenames are entering the system *anyway*, perhaps I should try to use them *more deliberately *, i.e. really get into the habit of the source file name being part of the name of the contained identifiers. 'LMB' becomes 'Mouse::LeftButton', 'WinEvent' becomes 'window::Event', 'IdentityMatrix()' becomes 'matrix::identity()' etc etc etc, with source file names 'mouse.rs, window.rs, matrix.rs' being more significant. There's *always* a chance for breaking changes surely, e.g. someone can come along and remove features from a library, or refactor the calls entirely. 'deprecated'. My hypothesis is that after a certain level of name spacing, the probability of clash/disruption (especially with some sort of distance sort) might be so low as to ignore it. Also repeats might be less likely to clash in the way I specify: lets say there *is* another source file 'window' e.g. some *systems'* specific 'window' ('editor/window'), thats not going to repeat the generic work of 'window framework setup' (defining 'Event' etc.) Note at no point in this experiment am I suggesting any change to *raw* idents. I know that "Foo::new()" is an obvious counterexample , but thats a slightly silly convention in the first place IMO (which the language thankfully can already work around) where the 'new' isn't stating any useful information. Most of the time the names *do* actually say something of meaning. The 'other constructors' do start to say specific things' (e.g. 'Window::new()' is vague , 'Window::new_at_rect(..)', 'Window::new_with_name(...)' begin to narrow things down much more i.e. whilst many things in the system have rects and names, the set that are also called 'Window' is *much* smaller..) Note also I'm referring to the specification of types in the first place, e.g. writing "window::Event" instead of "WinEvent". (I also argue more type inference would help overall ... but thats a completely seperate discussion, if you don't have to manually write as much, you *can* be more specific in the times when you *do* actually refer to something. methods are an example of this in action i.e. you've specified a type, then you're relying on that to locate the specific functions without writing out the type again and again.. even in non-polymorphic cases)
right. and idiots like /u/thiez you always come to each post and then give unwanted advice and waist their time. those iditos are sure that if they've come here and posted something, I must always say "thank you" to them till the end of my life. they're also sure that minuses hurt me. aaaaaaaaahhhhhhhh minusssss . nooooooooooo.
I think they meant using a macro at the callsite, which kinda sucks unfortunately :(
how can I make the html content type default?
Why doesn't unity allow you to use a modern compiler? The Elvis operator doesn't need any new vm instructions.
That loop-returning-value feature is very cute, makes the expression-oriented nature of Rust more consistent. Understand why it cannot work for the other loops - and what would be the result of a `for` loop, anyway? (I know of at least one language that attempts to define that result, but it felt quite arbitrary and magical)
Change rust's versioning scheme to be more in line with other languages -- I guess 1.19 should be 'Rust2015.19' or something, year/patch level.
&gt; heh global type inference is an orthogonal issue to this. I was merely using it as a concrete example of the *design philosophy* behind Rust. &gt; traditionally I regard qualifiers as ugly noise , so did enough 'glob importing' to avoid them altogether I agree in principle. I generally use specific imports like `use foo::bar::{Baz, Quux}` to avoid said noise. That said, being able to figure out where an identifier came from without anything more complex than Ctrl+F is something I value, so I never use glob/wildcard imports. &gt; If the filenames are entering the system anyway, perhaps I should try to use them *more deliberately *, i.e. really get into the habit of the source file name being part of the name of the contained identifiers. 'LMB' becomes 'Mouse::LeftButton', 'WinEvent' becomes 'window::Event', 'IdentityMatrix()' becomes 'matrix::identity()' etc etc etc, with source file names 'mouse.rs, window.rs, matrix.rs' being more significant. That *is* the Rust way of things. The only real uses I've found for declaring modules inline are the `mod test`convention and the occasional situation where you want to constrain access to private members but putting something tiny in its own file would introduce more awkwardness than it would avoid. &gt; There's always a chance for breaking changes surely, e.g. someone can come along and remove features from a library, or refactor the calls entirely. 'deprecated'. Yes, but the Rust compiler and standard library are expected to be paragons of the example that a good library developer is supposed to follow. &gt; Note at no point in this experiment am I suggesting any change to raw idents. To be honest, you're only making me more confused about exactly what you're proposing... aside from my certainty that the principle itself is at odds with intentional Rust design decisions, regardless of what level you apply it to (with the possible exception of IDE autocomplete.) &gt; I know that "Foo::new()" is an obvious counterexample , but thats a slightly silly convention in the first place IMO (which the language thankfully can already work around) where the 'new' isn't stating any useful information. I fail to see how it's "silly". It serves a perfectly good purpose in that it removes the need for special syntax to implement a constructor... and method overloading to avoid having two completely separate ways to implement them. Just make a method which doesn't take any form of `self` as its first argument (thus making it a "class method" in more traditional parlance) and returns an instance of the struct, then prevent bypassing the constructor by making at least one field private so the struct can't be manually instantiated from outside. &gt; (I also argue more type inference would help overall ... but thats a completely seperate discussion, if you don't have to manually write as much, you can be more specific in the times when you do actually refer to something. methods are an example of this in action i.e. you've specified a type, then you're relying on that to locate the specific functions without writing out the type again and again.. even in non-polymorphic cases) I don't follow. Rust already uses type inference everywhere that isn't considered part of the API surface. The point of not running type inference on method arguments is so that you can be sure that changing one bit of code won't unexpectedly result in a change to the API that another bit of code exposes.
The NXT is rocking an ARM7TDMI microcontroller, so one of the [thumbv7*](https://forge.rust-lang.org/platform-support.html#tier-3) targets should work. The newer EV3 uses some ARM926EJ-S based cpu, for which the `armv5te-unknown-linux-gnueabi` target might work. I'm not really farmiliar with Lego Mindstorm nor ARM, so I recommend you just try it out and see what works. Hope this helps!
True, haven't thought about this possibility. 
It's possible, but that's an operating system thing, not a Rust thing. You have to explicitly tell the OS to feed your program raw characters, as soon as they become available, but that's not exactly the easiest thing to do. I recommend this stackoverflow post for Linux: [Link](https://stackoverflow.com/questions/358342/canonical-vs-non-canonical-terminal-input).
The builder pattern started rising in popularity like 2~3 years ago or so. Before that time i didn't really saw it that much in libraries. This is more a anecdotal observation working in Java and C#.
Why make Option and Result classes (nullable) instead of structs (like [this](https://github.com/nlkl/Optional/blob/master/src/Optional/Option_Maybe.cs))?
I'm afraid it would make code harder to read, and I spend a lot more time reading code than writing it. Sounds like a good job for an IDE. :)
&gt;&gt; "I fail to see how it's "silly". It serves a perfectly good purpose in that it removes the need for special syntax to implement a constructor..." fortunately thanks to the namespacing and unambiguous syntax, this works: ```struct Foo{ ..fields..}``` ```fn Foo(args)-&gt;Foo{ ..construct and init a Foo..}```. It's completely unambiguous thanks to the use of ( and { . Foo( and Foo{ are instantly recognisable as function / struct respectively. As such the tagging on of '::new()' which also disrupts existing expectation of 'new' being related to *allocation* (separately overloadable to *initialisation*) is a decision I argue against. I'm happy I can use the language in a way that rejects that :) named constructors with more purpose don't offend me though.. I do generally like this idea of 'saying constructors aren't so special'. tangentially I quite like a macro that rolls a ```struct Foo(constructor args){ ..fields with initialiser expressions.. }``` to save some repetition in declaring those things for a 'default constructor' , thats a clear 'win' over c++ &gt; I don't follow. Rust already uses type inference everywhere that isn't considered part of the API surface. I certainly see the sense in demanding full types for crate level exports, say; but I would like to see the option for the inference engine to be unleashed a bit further, e.g. certainly within modules. you do run into a situation with refactoring where you might have something working, then you pull out a lambda and want it available elsewhere and suddenly you have to figure out a load of complex types and constraints. I've also got another (hopefully less controversial suggestion) which i've done an RFC for in the case where the types are specified by the trait already. I have seen how in haskell full inference can get difficult, i.e. I know i need to write a few more types to get something working.. but that doesn't mean full inference is *useless*. it's still so great to be able to factor out with ease The impression I get is that losing 'general overloading' you could recover the syntactic efficiency sometimes by allowing the inference engine to work harder; (e.g. without overloading, the *function names* dictate more; whereas in C++ the inference is weak, but they let us leverage the *types* to select functions more readily. Thats why its taking me a while to get used to the rust way of naming things really... I'm used to the philosophy of 'saying it with types' and letting the compiler/IDE work for me rather than manually naming/namespcing.. because thats the tool I had at my disposal. Didn't matter if foo(x,y) foo(z,w) exist, because written in two places doing two different jobs with different arguments, the arguments sort it out.. and jump-to-def always figures out whats going on.
What's the other 0,01%?
It seems to me that there's always some kind of UB going on there, making matching a union relatively useless.
Nice wedding gift, thanks! :)
&gt;&gt; I agree in principle. the ```::``` symbol is particularly fugly, but i'm trying to hold my nose and remind myself *it is just syntax*: it's possible with 'emacs pretty mode' to draw a concatenated unicode symbol (although that might cause hell with alignment potentially?) .. it could be colour-coded too. I don't mind seeing the '.' at all when other languages use that for namespacing ( i gather rust doesn't use '.' there both for c++ familiarity, and being unambiguous vs field access)
In either case, it'd need #[repr(C)].
Is there ever the expectation that upgrading to a new version of rust and recompiling, without changing any code, that performance will increase or decrease? Or is each release mostly just new features, and performance of existing code shouldn't really change.
Macros definitely need good documentation. First, it's not always obvious *where* a macro is defined. Second, definitions are not especially easy to read.
&gt; Having many small segments instead of a few larger segments has a negative impact on search IO time, search CPU time, and index size. Quick note: On a SSD and with a parallel searcher many smaller segments can be faster than fewer larger ones. There is a trade-off here, but it's one of the reasons modern Lucene (which this seems to be inspired from? If yes: Great, Lucene rocks) doesn't recommend merging the whole index anymore.
Not to mention that macros can help a lot with the boilerplate of writing a builder. I'd rather have a macro helping me in the backend than a macro helping the user use my API.
As I remember, all of this stuff was already discussed in detail during the language's evolution, but we're reaching the limits of what I can personally convey, given that I don't have the URLs for said discussions on hand. Since I prefer the current state of things and I don't want to play unpaid researcher, tracking down said URLs, I'm going to have to bow out here and let someone else take over who is better at looking up these things. (I see no point in defending other people's decisions when, based on what's already been said, it seems very unlikely that you'll say anything which would cause me to re-evaluate my preferences.) That said, there *is* one other response I'd like to give so you have an idea of where I'm coming from: &gt; I'm used to the philosophy of 'saying it with types' and letting the compiler/IDE work for me rather than manually naming/namespcing.. because thats the tool I had at my disposal. Didn't matter if foo(x,y) foo(z,w) exist, because written in two places doing two different jobs with different arguments, the arguments sort it out.. and jump-to-def always figures out whats going on. I strongly believe that, if you need an IDE to comfortably write a language, then that's a flaw in the language.
To clarify, will the tools continue to be fully open source even after being published on Peddle?
Yes tantivy design is strongly inspired by lucene, and I certainly like it too :). 
I made a PR for another speedup and a bigger demo image 
As far as I heard,`nalgebra` is 'good enough' for general-purpose linear algebra and I'm pretty happy with `cgmath` for for CG.
didnt really think about it too much, what would be the pros of setting it as a struct?
because its crossplatform, but unity team works now towards modern compiler
I've finished the "Advanced OpenGL" chapter of [learn-opengl-rs](https://github.com/bwasty/learn-opengl-rs/) and am continuing to work on my [gltf-viewer](https://github.com/bwasty/gltf-viewer).
I mentioned it [here](https://www.reddit.com/r/rust/comments/6oh6mv/announcing_rust_119/dkhuzqg/). In case your unions need to be most compact and you want to inline the tag into specific bits of the variants' members. You'll find this a lot in VMs of dynamically typed scripting languages, where builtin types are allocated as unions and where the first few bytes are used as a GC header. Something like: *"Bits 0 to 4 are the builtin type ID (i.e. the tag), where 0 means 'not allocated'. Bit 5 is the grey bit of the Mark&amp;Sweep GC. Bit 6 is the immutable bit."* Besides the special case of non-zero types, Rust's enums allocate at least one byte for the tag. Thus, the only way to tightly pack type tags and other configuration bits would be doing this absolutely horrible and unusable thing: enum GcObject { NotAllocated, String(GcString), StringGrey(GcString), StringImmutable(GcString), StringGreyImmutable(GcString), HashMap(GcHashMap), ... }
Sure, it's possible for an upgrade to change performance. For example, just recently, the standard library `sort` implementation got some improvements, so that could make some code run faster.
There's an empty bullet point in the "other new features" section. Typo or is anything missing?
lcov does treat `&amp;&amp;`/`||` as branches.
Any particular reason to not just use a wrapper macro? fn wrapped_add(caller_line: u32, a: i64, b: i64) -&gt; i64 { println!("called from line {}", caller_line); a + b } macro_rules! add { ($( $arg:expr ),*) =&gt; (wrapped_add(line!(), $( $arg ),*)); } fn main() { let a = add!(4, 5); let b = add!(2, 6); println!("{}", add!(a, b)); } Syntax for caller is almost the same, and it's only a minor implementation inconvenience. Biggest issues with macros currently is importing and the trouble they create for tooling, but if you really want the line number, I think this is the simplest solution currently.
They're stack allocated rather than heap allocated (so they should be cheaper and lower your application's memory pressure) and they are not intrinsically nullable (though you can wrap them in `Nullable`).
you have a point. btw didnt know someone already did a project like this :). i see you are coming from native windows dev, my code was wrote to compile with unity3d and old c#.net
they licensed their compiler from xamarin, who would not give them the latest and greatest because they feared competition from them. now thst microsoft owns xamarin this is no longer an issue but much work has to he done to bring in the new stuff and make it work on all target platforms.
https://news.ycombinator.com/item?id=14816558
The blog posts are only a subset of the release notes, which are a subset of all changes.
On GitHub, it's not blank... https://github.com/rust-lang/blog.rust-lang.org/blob/gh-pages/_posts/2017-07-20-Rust-1.19.md#library-stabilizations hrm.
I've been trying to integrate it with my travis build, after running it successfully on my local computer. But, looks like travis is not happy :( note: cc: error: unrecognized command line option ‚Äò-no-pie‚Äô
Hi I've seen the issue and how to have a fix out over the weekend. I'll let you know when it's sorted
&gt; btw didnt know someone already did a project like this :) You may be confusing me with /u/Smaehtin, I'm a different person :D. Also [`Nullable` is a standard C# type used to make "value" types nullable](https://msdn.microsoft.com/en-us/library/b3h38hb0.aspx) (as they aren't by default). &gt; i see you are coming from native windows dev I don't come from windows dev and do very little of it, I just have surface familiarity with .net.
The funny part is, performance with [u8] is the most interesting part for me.
Not sure why you responded this way, but GP is trying to help you out. A flat refusal without reasoning seems to contradict your original intent to get your question answered, so why not hear him out? Not a rhetorical question, by the way. If you see a reason not to, then speak up!
If saying "thank you" helps you maintain a cycle of getting answers when you need them, it sounds like a fair trade to me. Be respectful when you come to this community. You are expected to at least be polite. You don't have to agree with everything members of this community say, but then again being rude isn't necessary to disagree.
Yeah I feel pretty stupid now, thanks!
I've played around with similar sorts of ideas in C# and here's a couple of thoughts: - Get rid of unwrap. If you can, you will, and it breaks the point of having it at all. I know, I know, I've had this argument with people about unwrap in rust too... but I maintain, it's more robust if you just don't have it. - Its really frustrating having to use `Result&lt;TRtn, TErr&gt;.Error(err)`. I know that it sucks, in general, in C# you you can't partially specify generic types, but I've found that syntax extremely clunky. As a result, option is far more practically useful than result is. - Use structs, so they can't be nullable. - Unity has stated publicly they will eventually support .Net standard 2.0, so you may as well make a .Net standard 2.0 library using the preview (and just copy the files in for now) for this. (then you can have stuff like unit tests, etc) - In C# 6, the null check operator (`.?Foo()`) makes stuff like the option type somewhat less obviously useful, although this is really just sugar for a null check. - I'm not a fan of the naming conventions, but https://github.com/nlkl/Optional is a good read as a background. For example, the extensions types are quite cute. :) 
(FWIW linking here with that title as-is may be a bit misleading, makes it sound like some official announcement rather than a brainstorming thread.)
&gt; From a code safety point of view, I find it really troubling that even if we work so hard to make the compiler happy to prove our code is secure, we are still depending on libc and the platform‚Äôs OpenSSL. I remember the first time I started digging down into Go and realized that it wasn‚Äôt even linking libc, and it gave me a fantastic feeling of finally cutting the cord to the old, dangerous way of programming. With Rust (and hyper) we‚Äôre right back to depending on C again. Yuck. If you've never studied crypto, then you wouldn't understand. Writing your own crypto libraries is a recipe for disaster that has nothing to do with anything you would expect. Side channel attacks can extract your private keys using nothing more than how long it takes you to respond to a request, for example. Writing your own crypto from scratch is an arrogant thing to do, which is kind of how Google operates. They have unlimited budget, so a little thing like crypto isn't going to stand in their way. There are native Rust crypto modules under development, but we do need to choose a "blessed" one that attention can be focused on. It takes time and patience and a lot of code review to develop crypto... it's not something to be done casually. &gt; With Rust (and hyper) we‚Äôre right back to depending on C again. Yuck. It's not "yucky", it's the *practical* thing to do. There have been [efforts](https://github.com/dikaiosune/rusl) to rewrite `libc` in Rust, but throwing out battle-hardened code before the alternatives are ready is irresponsible. I'll also remind you that large portions of your operating system are written in C -- disgusting, I know, right? There is a [pure-Rust alternative](https://www.redox-os.org/) under development. &gt; Pure Rust code is also self contained in the program‚Äôs ELF file. But the 5.1 meg release image does not count several quite large shared libraries that it depends on: It's possible to [statically link Rust executables](https://doc.rust-lang.org/1.12.1/book/advanced-linking.html#linux). Rust executables are larger than C ones ~~because it bundles in jemalloc, which is not small. There is an experimental feature in Nightly Rust to use the system allocator, which makes executables noticeably smaller, at the cost of not having the better allocator of jemalloc.~~ [see here](https://www.reddit.com/r/rust/comments/6onxx6/my_first_rust_program_comments_welcome/dkj0xue/?context=2) EDIT: I just realized that the documentation I linked to is using a custom version of musl. About a third of the way down [this page](https://blog.rust-lang.org/2016/05/13/rustup.html) they show how to link to musl using `rustup`, which should be a lot simpler.
&gt; I strongly believe that, if you need an IDE to comfortably write a language, then that's a flaw in the language. conversely, as these machines and languages get more powerful, they can work harder for us.. that's the point surely. the compiler/IDE between them (when its working right, the IDE is a gui for the compiler) always know more about a source base than any human. (it doesn't have to be a 'pretty gui', it can be text mode, but the tool is still there). I'm not one of these 'everything should be graphics' people, the most powerful tools and ways of reasoning combine *both* text &amp; graphics; given the amount of source code out there.. no one can keep it all in their heads 'overloading' is also how we communicate naturally.. natural language is full of extreme ambiguity - when analyzed at the low level (verb or noun? and so many uses of the same word..) - but when given holistic contextual information, this ambiguity disappears. I think there is a lot more room for our programming languages to advance in that direction.. &gt; if you need an IDE to comfortably write a language, then that's a flaw in the language. Some of what I'm talking about is purely the *compiler* working harder for you rather than the IDE as such; its just the time when you need to know where the definition is that you need the IDE to find it, otherwise just searching for the def get all of them. I know rust has a lot of welcome syntax cleanups , I definitely like how you can grep for 'fn foo' etc unlike in C++ ... but thats orthogonal to most of what I'm talking about (there's still going to be multiple potential 'trait impl's ' ) to sift through..)
I only do a little bit of C# but isn't it ill-advised to to make a value type Nullable?
Even C is a bit too far from the metal at times for crypto. Inline asm being stable will help for Rust crypto. There's too much importance in writing branchless code that keeps in mind NUMA. Not using `if` isn't sufficient to being branchless
I tried compiling with the --target option, but rustc says 'can't find crate std' and that i have to intall it for the target. I can't find any more info on where to get that. Could you maybe point me somewhere?
C# also just added pattern matching [https://docs.microsoft.com/en-us/dotnet/csharp/pattern-matching](https://docs.microsoft.com/en-us/dotnet/csharp/pattern-matching) would make it even nicer to use, dropping the MatchSome/MatchNone. I've tried a few Result/Option types in C#, haven't adapted anything yet as I have an entire C# team that I'd implement it for. But definitely on my todo list soon. 
&gt; I strongly believe that, if you need an IDE to comfortably write a language, then that's a flaw in the language. ... the lack of an IDE earlier (i'm trying intelllij) hurts the rust experience; working with mature 'dot-autocomplete' (also for fields) is just way more pleasant all round, overloading or no overloading. the real problem with C++ is legacy syntax (headers), but even then the availability of a mature IDE allows one to get things done quicker (thats to do with momentum rather than language design.. I realise that with cleaner syntax it should be possible to get a superior Rust IDE *eventually*. Right now however, Intellij IDEA (java? working on Rust) makes my laptop overheat, whilst QtCreator (C++ working on C++ ) does not. When everything is working right, it should be possible for the compiler to cache information between building and interactive search ... again this is just momentum.. how many people use the language -&gt; how many people can the world afford to have working on refining it's tools
&gt; I'd like to rebuild rustc on my browser. /r/madlads 
Onion union
&gt; if you need an IDE to comfortably write a language, then that's a flaw in the language. ... one more point, rusts design choices mean *more* work looking things up (hence more demand for an IDE). e.g. you needed to discover 'split_at_mut()' before you can modify 2 parts of a collection, you needed to discover 'cast::transmute() std::mem::offset()' etc etc to do the unsafe jobs, etc etc .. i'm finding it way more 'vocabulary-heavy'
Welcome! As others have already commented on the context of the blog, I will give some comments on the code itself. This pattern of error handling... let mut resp = match result { Ok(resp) =&gt; resp, Err(e) =&gt; panic!(e), }; ...Can be replaced with just this: `let mut resp = result.unwrap();`. Note that it is generally recommended to use `unwrap` to express "This should never happen", and normal errors should make use of the [`?` notation](https://doc.rust-lang.org/book/second-edition/ch09-02-recoverable-errors-with-result.html#a-shortcut-for-propagating-errors-) to return `Result&lt;T, E&gt;`s. This match expression: match fields.next() { Some(_) =&gt; {} _ =&gt; { println!("Bad SHA256 line: {}", line); continue; } }; Can be written like this: if let None = fields.next() { println!("Bad SHA256 line: {}", line); continue; } [`if let`](https://doc.rust-lang.org/book/second-edition/ch06-03-if-let.html) is used if you really only care about one of the variants in an enum, as is the case here. Edit: actually `if fields.next() == None` works too, and is arguably simpler. There is also no need to import `std::u8`, as it is already in scope. There are probably more ways to simplify the code. In case you don't know already, [clippy](https://github.com/rust-lang-nursery/rust-clippy) is an excellent tool for cleaning up these sort of things.
A builder in OCaml is a lot less code than any of that: type my_struct = { arg1 : bytes; arg2 : bytes; } let builder ~arg1 ~arg2 () = { arg1; arg2 }; let partially_constructed_struct = builder ~arg1:"some value" let full_struct = partially_constructed_struct ~arg2:"other value" () let partially_constructed_struct = builder ~arg2:"some value" in let full_struct = partially_constructed_struct ~arg1:"other value" () So, defining a builder can be a one-liner, depending on how much validation you need to do, thanks to labeled arguments and partial application.
This is really cool!
If I got it right, the gist is to split the compiler up and make it even more pull- instead of push-based, allowing for finer-grained parallelism and perhaps compiling while typing. I think we should go one step further and fully incorporate miri to allow for an interpreted mode √† la OCaML. This would probably also allow us to add a REPL for on-line debugging. That said, I'm not using REPLs much. What I'd love would be [infinitest](https://infinitest.github.io) for Rust, with good error mark integration.
It'd have to produce a statement rather than an expression, which would be inconsistent with most of Rust's syntactic constructs, including match.
The "Pythonic" construction also isn't really "Pythonic." It appears in many languages and is similar to mathematical set notation. I don't mean this antagonistically, but I genuinely don't understand the resistance to the construction as in cute. It's a pretty standard type of construction and is easy to read, especially when you get into multiple iterators. 
You *really* want them to be classes rather than structs, because C# structs are not as useful as you might like them to be.
Can miri interoperate with already-compiled code? (e.g. I believe GHCi's interpreter can do this)
I mean, sure? ...Except that you might ask, "How old is this person?" and that person never entered their age into the form, because that field was optional. You can't say `0`, because that's an answer--that's an affirmative expression of their age, which *you don't know.* So nullable value types in C# are used kind of similarly to the way options are in Rust.
It's ill-advised to make a value type Nullable when there's no reason for it to be, nothing more and nothing less. Just as it's ill-advised to wrap a Rust type in Option when there's no need for it to be. Nullable is just C#'s version of Option (with the caveat that it only applies to value types as reference types are always nullable).
I think the trend is that the code gets faster due to better optimizations. Either by the compiler or by changing the std/core lib. It's possible for performance to deteriorate, but AFAIK there are regression tests, and a performance regression is treated as a bug.
I'm finding it super helpful with writing my safe wrapper of a FFI library. There have been a bunch of times where tests exposed a "hey it's possible for this function to return null, so my return type should really be an Option!" And I expect my tests to also be helpful when trying to support additional versions of said FFI. On the other hand, a subset of the FFI is really inconsistent and I haven't yet figured out why. So I've had to comment out a couple tests because every other or so run they would fail..
Why would it have to produce a statement instead of an expression? Match can produce either (leading to assignment by match) and so can if statements (also leading to assignment). I don't see what that cannot be the same for `match_all` let x = match_all y { 0..5 =&gt; 10, 4..10 =&gt; 20 } else { 15 } This would produce the following table of values: y value | x value :--------|---------: 0, 1, 2, 3| 10 [4, 10) | 20 else | 15
I much prefer method receiver type inference over method overloading, and I'm not sure you can have both.
Alternatively, `if fields.next().is_none() {` makes a bit more sense to me. I reserve `if let` for enum variants where I want to access an inner value, generally.
Ah, now I get it. You are trying to entangle two arguments to trigger a bug in the probability function. Notice that the derived probability is a function of the probability of arguments, so when you're entangling two arguments like that with `x = true`, you are implicitly adding a constraint/sub-type: P(f){(= 1)} In path semantics this function is not identical to `P(f)`, because it doesn't have same path sets. In order for two objects to be identical they must have the same paths, which is translated roughly as "everything that can be said about that function". A partial function can have more than two paths. In normal function theory, you would easily miss this, because constraints on input doesn't change the overall behavior of the function. You have to track these things in path semantics because it obviously changes existential paths, and these are used all the time to check that what you're doing is consistent. The `g` function has the type `nat -&gt; bool` and shouldn't use this derivation of probability. There are only functions of type `bool -&gt; bool -&gt; ... `etc. that are Boolean functions. I believe you misunderstood the connection here with independent probabilities, because I do not assume that each bit in the encoding of a natural number is independent. This hasn't anything to do with probabilistic paths, only probabilistic existential paths of Boolean functions. OK, here are two insights that might help you understand path semantics: 1. Path semantics is not a theory about constructing objects 2. Path semantics is about deriving properties of functions, operating on objects This is why "generalizing path semantics to probability theory" does not mean "construct probability theory in path semantics", but rather "apply probability theory to the field of path semantics for more powerful reasoning about functions". Path semantics is only concerned about how functions work and are related to each other, nothing else. When I mentioned naive intuition, I'm talking about the mental process when you solve a probabilistic existential path. I was not talking about formalization. This requires some explanation. There is no intrinsic mechanism in path semantics that tells that a given set of axioms of objects is consistent according to its theory. You can't prove anything to be consistent without checking every case, which means you have to compute everything. On the other hand, if it ever violates the fundamental intuition about how functions works, then it's wrong. Path semantics can't formalize the intuition about functions, because you can only construct new concepts about functions using the concepts of functions you already trust. It starts with atomic partial functions and then builds up more and more powerful ideas. You can implement a specific theory about functions in path semantics, but this would have extra properties that the general concept doesn't have, because it's a specific implementation. Unlike a theory based on axioms, you can't write down an implementation without path semantics starting to self-reflecting on it, so there is a lot of hidden connotations. Path semantics is not less precise than other theories, in principle, it is just not closed over a domain of reasoning about objects. Instead, it is closed over the domain of reasoning about functions. If I use intuition when working with path semantics, then that's OK as long as it's never proven to be inconsistent (with functions). People are shocked by this because they are used to think inconsistency with a theory about objects, like sets. However, in principle there is nothing wrong with this line of thinking. The problem is that the more complex concepts you use, the more things are added on the side of checking for consistency. It gets the heck of difficult very quickly. This problem is not new to mathematics. Axioms are built from intuition. What people often wrongly assume, is that "true mathematics" defines how you should think about programming, and not the other way around. Like there is a one-way street from top to bottom of the ladder of abstraction. For example, I remember somebody said (don't remember who): "The way functions are modeled in set theory aren't functions the way we use them in programming.". Also, dependently types makes things much more complicated, e.g. because they are mixing function spaces of finite sets. I don't even dare thinking about inductive types yet. So, path semantics follows the line of thinking that "whatever, you'll never get stuff done if you're building things from objects". Then, it starts with functions, and then try to add ad-hoc rules for checking for consistency as we get more experience using it. For example, existential paths was made up after "normal" paths, and this means that you couldn't even check for consistency up to that point, except exhaustive checking. However, path semantics was used to solve real world problems because of it's ability to help reasoning about functions. I found it immediately useful, also as a way to understand logic better. Just look at what people working on Homotopy Type Theory are doing: They want to reason about spaces because they make mistakes, so they look at type theory and figure out the isomorphism to homotopy theory. Then, they start realizing that the foundations of mathematics could be formalized in dependently type theory. Then, they figure out that wouldn't it be pretty neat to reason more about directions, or extend type theory to cupical types, or color type theory etc. They invent new theories all over the place because there is some interesting aspects to it, and the existing theories aren't satisfactory efficient for reasoning about a specific domain. All this is driven by programming and intuition, with the desire for mathematical reasoning as background. Back to the problem of checking for consistency: The problem of checking for consistency applies to the algorithm for probabilistic paths, because it's so complicated. Since path semantics only reason about functions, and everything that is true about functions depends on what you have figured out in the whole theory, you don't get any theorems for free about the kind of objects that satisfies the theory initially because there is a bazillion things you have to check. When I define an algorithm for probabilistic paths, I don't know which theories about probability that satisfies the algorithm. In the examples, I should have made the natural numbers finite by adding a static constraint. Yesterday, I was pretty confused about this because I had been writing and thinking a lot. Today, my head is a bit clearer about it, and can see a lot of holes where I need to clarify path semantics. It is easy to restrict the usage to finite sets, but you can also ask the question "are there any other kind of sets where I can use this algorithm?". Currently, it is unknown whether asymptotic probability holds. There could be some function of type `nat -&gt; nat` that doesn't work. I also don't know whether infinitesimals works, so I need to check it. It might work for sub-types that are ranges of real numbers. I have a rough plan written down so I can get started working. I was actually surprised by the level of quality of the feedback I received in this thread. I didn't expect that, and it was very useful to see at which points people were getting confused about path semantics. Notice that this doesn't mean it's bullshit, like some people seem to believe. I trust my intuition to make the judgement that path semantics is valuable. I have been working on this for a long time now and nothing seems to indicate that I'm on the wrong track. Let's say I assign that belief a probability 0.95. :)
Good point! I have completely forgotten that there is such a method, and I agree on the usage of `if let` too.
Rust had an OCaml heritage, but Rust wasn't OCaml. I don't think this kind of behavior existed at any point (could be wrong, Rust has changed a _lot_) My point was to say that rust has had closures long before it has had the builder pattern as a style norm.
Ooh, really pretty! Thanks.
hi, i think you should be able to replace : let mut bytes = Vec::new(); for i in 0..(hex.len() / 2) { let res = u8::from_str_radix(&amp;hex[2 * i..2 * i + 2], 16); match res { Ok(v) =&gt; bytes.push(v), Err(e) =&gt; { println!("Problem with hex: {}", e); return bytes; } }; } with: let bytes: Vec&lt;_&gt; = hex.chunks(2).map(|h| u8::from_str_radix(h, 16)).collect();
Thanks. I'm familiar with the tradeoffs in crypto. I think the time has come for crypto in pure safe languages. I'm glad Google is investing in it for the community and I'm glad Rust will get there one day (soon?) too.
Yes; you can also use them to more easily build memory-efficient data structures in pure Rust.
- If you are doing `match` and then panic on an error, you can just do `.unwrap()`, it does exactly the same. I mean, there is no point in re-doing `.unwrap()` yourself. - `"".to_string()`, rather do: `String::new()`, more readable. - mut is not a bad thing. It's just for marking which variables you mutate. - There is no reason to do a `&amp;String` instead of a `&amp;str` - they mean the same thing (a `&amp;String` can be coerced to a `&amp;str` and `&amp;str` allows slices to be passed into it. Here is an implementation in completely idiomatic Rust, but it requires nightly features and it's pretty much code golf in terms of readability: https://play.rust-lang.org/?gist=7da86b17baeec136ee244d4080e2bd9c&amp;version=nightly You can decrease the size of the rust program if you use the system allocator, use `lto = true` (link-time optimization) and strip your binary. Put this in your `main.rs` file: #![feature(alloc_system)] extern crate alloc_system; And this in your Cargo.toml: [profile.release] lto = true panic = "unwind" debug = false Use `strip target/release/mybinary` to strip the debug symbols. This should at least help a bit with the size issue. The dependencies ... yeah, it is an issue. You need `musl` to do completely static linking, plus it depends if the crates you use specify that they want to link dynamically. [This](https://lifthrasiir.github.io/rustlog/why-is-a-rust-executable-large.html) is always my guide for large binaries.
&gt; hex.chunks(2).map(|h| u8::from_str_radix(h, 16)).collect(); This does not compile, &amp;str and &amp;String don't have `.chunks()` defined, it is only defined for slices. Plus, you need to map the `ParseIntErrors` somehow. See [here](https://play.rust-lang.org/?gist=7da86b17baeec136ee244d4080e2bd9c&amp;version=nightly). This is also unsound because of possible Unicode. Even if you'd convert the string to bytes first, you might not be able to forsee that `\u{0066} == 'f'`.
The builder pattern was in the original design patterns book written in 1994.
I really enjoyed everything about this post and thread. Community, keep this up!
In `frob`, the borrow is splitting on `Thing`, while in `from_box` the borrow is on the `Box` itself via `DerefMut` and not on the `Thing` it owns. You can split if you do an explicit reborrow first. fn frob_box(mut self: Box&lt;Self&gt;) { let slf = &amp;mut *self; bloop(&amp;mut slf.x, &amp;mut slf.y); }
Could this be done with a compiler plugin? I love the idea of a two sets of args for a function, or a separate metadata struct that gets passed in. It could even be elided if never referenced.
... you can't skip anything. This is the point of a linked list. You have to recursively iterate the list until you hit the last element. I'm not sure what you are trying to do, changing the pointer to `self`???
oh my bad. i thought it was all slices
Rather than recursively skipping the list, I want to change a pointer (doesn't have to be self) iteratively. I can't get it to work in rust no matter what I do.
Wow. /u/jeffrallen stripping the binary makes a huge difference. I didn't realize this had such a big impact. On my Linux system, the code in your post compiles to a 5.2M release binary. After running `strip` on it and doing nothing else, the size drops to 1.6M. Using LTO, the binary drops to 1.2M after stripping, which isn't a whole lot less, but it's still a 25% reduction from 1.6M. Using the system allocator doesn't seem to make a noticeable difference on my end in terms of binary size, so jemalloc doesn't make as much of a practical difference as I had thought it would. I apologize for being wrong.
I don't think so, at least right now. Currently compiler plug-ins (or proc macros) only receive the tokens of the item they are used on (function, struct, etc.) a plug-in that did this would need to examine the whole source, to find every invocation and add the line number argument. This is what happens in C#, but rust macros aren't set up for this as they are right now.
Could you expand? AFAIK, the only semantic difference was in the default visibility of fields, and that structs can't inherit. (Neither of which is useful here)
rust seems to have basically recovered overloading through traits, e.g. you can now do matrix * matrix, matrix*vector .. and the technique via which that is done would be applicable to named functions. I heard they did a Qt wrapper that basically tupled the arguments to 'fake overloading' both haskell and rust seem to restrict the inference to work forward to make this work before that you were still able to do the same thing with 'double-dispatch' anyway thats a subject for another discussion moving between all 3 (c++, rust, and then dabbling with haskell .. rust has helped demystified that a little for me) I'm sure there are ways in which rust can be loosened up a little to reduce the issues i have at the minute; I noted how some features in haskell mean it's type classes don't quite feel as oppressive to me as traits (they do allow whole-program inference - and the 'instances' don't need to repeat the type-signatures defined in the 'classes'. The latter should be a much less controversial suggestion. they also seem to allow omission; I have another feature-request which is 'compile-time unreachable' (stronger version of 'unimplemented!()' ) , which would allow you to write bigger traits then omit some, whilst giving a proper compile-time error. rusts philosophy tends to be too extreme, 'this hurt me.. so i'lll never allow it ever'.. no knives, you have to learn to eat just with a fork. Its' like the battle between OOP and FP .. neither is right, a mix of both is superior. The same with traits. Yes, having no traits makes C++ error messages horrible when using STL which is designed for maximum generality.. but that doesn't mean traits are *always* better. there's 1line functions where the trait bounds are 5x as big as the function body. Apologizing for that reminds me of the C++ people that delayed auto etc by about 10 years insisting that writing out std::vector&lt;T&gt;::iterator it=.. every blinking time is good for you. So Loosen it up a bit , and we can have the best of both worlds.. )
I change what self points to until it's at the end of the list, and then I add on the new node. 
check out `rustup toolchain add`, or you may need `xargo`.
This is not possible without recursion. If you change "the pointer" (in this case "self"), you are recursing. This is what recursion is, running the same function, but changing the element every time (in this case a pointer to self). You'd be reinventing recursion.
In the comments: &gt; Also, last time I was proposing more parallelism I was informed that rustc typecheck heavily relies on caches internally. So splitting up into multiple threads may make your compilation faster, but it will draw more battery and cause more heat on mobile devices. Why do we care about the power consumption of a compiler on a mobile device? If you're compiling a large Rust project on a phone, isn't that a problem in itself?
You should have a look at how the c# compiler Roslyn serve as compiler and language server, its syntax tree keep whitespace and punctuation in SyntaxTrivia structure contained in classic SyntaxNode with SyntaxToken : https://github.com/dotnet/roslyn/wiki/Getting-Started-C%23-Syntax-Analysis
I'm messing around with procedural macros, with `syn` and `quote`, in order to automatically generate code that manipulates tuples. However, I can't find a way to substitute in numbers without any suffix. The following produces an error in the generated code about `self.0usize` not being a field. if let &amp;VariantData::Tuple(ref fields) = data { let field_names = 0..fields.len(); write_fields = quote! { #(self.#field_names.write_to(v));* }; Does anyone know the right syntax to get just `0` on its own? EDIT: Turns out the answer is to build each number into an `Ident`
What? A function that calls itself is completely different from a function that iterates over something. A recursive function call pushes to the stack each time, last I heard rust doesn't have good tail end recursion optimizations, which is why I want to change it. Here is code in C++ that doesn't require compiler optimization for the loop: struct List { int a; List* next; } void append(List* s, int a) { while (s-&gt;next != nullptr) { s = s-&gt;next; } List* x = newList(); x-&gt;data = a; s-&gt;next = x; } 
So we need syntex-dom-rs to do whole program transformations?
I think they might have meant laptops.
Semantically I meant without a recursive function call, not necessarily recursion. Sorry about the confusion
I must admit I don't know enough about the rust macro system or future macro systems to tell you that.
The perils of contextual keywords
Will this work? The variable `s` is used to emulate the `self` in the recursive version. The `curr_s` variable is used to break the borrow chain. fn append_loop(&amp;mut self, val: u32){ let mut s = self; loop { let curr_s = s; s = match *curr_s { Nil =&gt; { *curr_s = Cons(val, Box::new(Nil)); break; }, Cons(_, ref mut y) =&gt; y }; } } 
No, or rather not much better than C FFI, if that ever gets implemented (instead of just emulating common system/libc functions). There's a much better solution, preserving MIR for all functions in the rlib metadata, and which you can already enable on your libraries. Check out miri's testing setup (e.g. for Travis), which involves using Xargo to build libstd in that mode.
Thank you! It works. Can you explain how curr_s breaks the borrow chain?
Don't forget `steed`, /u/japaric's project to rewrite Rust `std` so that it doesn't depend on libc to start with.
&gt; What are people's experiences with code coverage in Rust I've used it in xml5ever to figure out which part of the specification weren't tested. It helped me discover several bugs. Then kcov stopped working... 
Simple example: `VecOpt&lt;T&gt;`. A `Vec&lt;Option&lt;i32&gt;` will use 8 bytes per element: 4 for `i32`, and 4 for the tag (including padding). If instead you use *two* arrays internally: a bit array for tags and a raw array for the elements, then you are back to 1 bit of overhead per element.
How so? It is safe not to run destructors; that's what the Mutpocalypse was all about.
Reading is `unsafe` so it's fine. I'm not sure why writing is however.
Not if you coerce the union fields to Cells. I don't know that this is possible yet but if not I believe it's coming.
Oh :(
There is no difference in the default visibility of fields (it's still private), although you are correct regarding inheritance. The biggest difference is stack allocation. Coupled with the fact that C# doesn't have any of the niceties regarding structs that Rust has (e.g. `fn foo(bar: &amp;Bar)` &lt;-- the `&amp;` thing there), and it can be very expensive to move them from one place to another. (You can pass a value type by reference, but someone will assassinate you if you try that everywhere.) They are good for improving performance in specific conditions, but using them ubiquitously without thinking about it will more commonly be *detrimental* to performance. Forcing me to use a struct is a good way to ensure I just don't bother.
If you try the bad code below, `y` is a reference into `s`, so `s` is borrowed mutably and cannot be changed and the assignment fails. Basically, this is trying to assign to `s` from `y` which borrows `s`, there is a kind of loop. With the `curr_s` version, the original pointer in `s` is moved into `curr_s`, so then `y` is borrowing `curr_s` not `s`. This is trying to assign to `s` from `y` which borrows `curr_s`, and there is no problem there, no loops. `curr_s` only lives inside the loop block and is not carried through different iterations, it is a temporary for one iteration, unlike the `s` in the bad version. // bad code loop { s = match *s { Nil =&gt; { *s = Cons(val, Box::new(Nil)); break; }, Cons(_, ref mut y) =&gt; y }; } 
Would this work: https://play.rust-lang.org/?gist=63aaec96976aa6f5ab3ccb363cba5827&amp;version=stable
I also think it is too early to determine where the trade-off lies. The compiler won't be at all special in having parallelism that isn't perfectly linear, in fact, it fairly rare to get that for anything nontrivial, but it can still be beneficial for power draw. I believe there have been measurements that show that parallel servo draws less power, because when doing computation chips typically increase their frequency/voltage, but, AIUI they can rarely control it for a single core independently. That is, if a single core is running at 100%, there may be other cores that are also drawing more power even if they're not actually doing much computation. Parallel programs run all the silicon for a short period of time and then let's everything switch back to idle/low power sooner. I think it is pointless worrying about about this for now, especially given the compiler already has ways to limit the amount of parallelism, and there doesn't seem to be any intention to change that.
I may be wrong about this, but AFAIK the only circumstance you'd run into where you could wind up with an expensive-to-move struct is where you have a lot of fields (which is bad anyway) or possibly if you have a large array as a field. Including a class type as a field only costs the size of a pointer because virtually everything is actually living behind an `Rc` in .NET. Additionally, the compiler is free to put any object on the heap [if it wants](https://blogs.msdn.microsoft.com/ericlippert/2009/04/27/the-stack-is-an-implementation-detail-part-one/), even structs. I don't know what the current framework does if you present it with an unreasonably large struct though. 
Wait, are you saying we can have repl on compiled languages, i thought that was impossible, that would be amazing. It's my favourite feature of python when compared to java.
Ooh, that's exactly what I came here to learn about. Cool.
I modified your function a bit: fn append_loop(&amp;mut self, val: u32) { let mut current = self; loop { match *{current} { ref mut x @ Nil =&gt; { *x = Cons(val, Box::new(Nil)); return }, Cons(_, ref mut y) =&gt; current = y, } } } Since self is an *immutable* binding for a mutable reference, we can't make self point to something else. That's why I introduced `current`. The next trick are the curly braces in `match *{current}`. They force a move making `current` invalid but reassignable. Otherwise `current` would be considered mutably borrowed in the `Cons` match arm. If it were not invalid nor considered mutably borrowed, we'd have two ways of referring to the same thing mutably (via `current` and `y`). Rust does not allow this kind of aliasing for good reasons. But if `current` is considered borrowed the compiler won't let us reassign `current` inside the `Cons` match arm. So, curly braces it is. ;) I should note that the Rust team is working on different improvements including "non-lexical lifetimes" (NLL) and [improved match ergonomics](https://www.reddit.com/r/rust/comments/6h89kb/match_ergonomics_rfc_has_been_accepted/) which should make implementing this loop simpler. When these language/compiler improvements are available, the following code should work as far as I understand: fn append_loop(&amp;mut self, val: u32) { let mut current = self; loop { match current { x @ Nil =&gt; { *x = Cons(val, Box::new(Nil)); return }, Cons(_, y) =&gt; current = y, } } }
Actually I found that https://crates.io/crates/data-encoding was a good fit for this job. See its base16 implementation in 1.2.0.
Haskell has a repl. It works like ipython without the magic. Really great for beginners.
That's pretty cool, i found it really useful when it's been a while since i've used a part of a language, and then i can try out some syntax instead of running my whole program or going searching trough the documentation.
This week, being on vacation with very little internet connectivity I can't really work on mdBook, so I thought I would begin a new project in order to learn the basics of compiler design. So I am using the book [Writing an interpreter in Go](https://interpreterbook.com/) as a guide to write a compiler / transpiler for the [Stylus language](http://stylus-lang.com/), a CSS-preprocessor. I am currently working on the lexing phase, and am using [nom](https://github.com/Geal/nom) for this. You can check out the code on the [azerupi/stylus-rs](https://github.com/azerupi/stylus-rs) and the progress on the [roadmap issue](https://github.com/azerupi/stylus-rs/issues/1) 
There are cryptographers working on crypto in pure Rust, and they're having pretty good results so far. https://twitter.com/isislovecruft/status/887792374684020736
Yes. I just don't want a library making that choice for me, because the whole point of structs in C# is to have me do that.
Great article, I really liked it. I like that because Rust uses the system linger, that things I knew from hacking embedded systems with the Gnu toolchain still apply.
It's still a stupid reason to keep everyone's compile times slow.
FYI Java 9 has a REPL (and will be officially released in September). In addition, Eclipse, IntelliJ and NetBeans all have plugins with similar functionality.
It's up to the author of the tool. Closed source, open source, paid, and free tools would live side by side much like you see in other software marketplaces, Steam, App Store, etc.
Do you still need to make a class first, or will all repl statements be run as if they were placed in a premade main class, and do variables get remembered between lines like in python?
It's the latter. You can write like if you already were in an unnamed class' static block.
You may want to check out the authoritative source on the subject: [Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/)
Visual Studio lets you edit C++ code _while your program is running_. They tooling they have is crazy. (You can also step-through debug a program across the native-.NET boundary.)
Wait how does that work? Do they compile it again and replace it while it's running ?
Thanks for the explanation; that makes sense. All my functions end in a move of `self`, so something like `let me = *self;` worked for that.
Ahh. Probably a good idea to make that case on the front page then. As-is, it doesn't really give any examples to guide the reader's mind to what the target audience is, so what came to mind was: 1. Is it a simple utility? Then someone will write a free clone as soon as it's proven to be desirable. 2. Is it used for web development? Then, the only apparent niche where people still pay for parts of their toolbox in any consistent way is GUI-based utilities for MacOS developers. (MacOS has a long history of ecosystems based on shareware instead of freeware.) 3. Is it a server-side component? Open-source things with paid support and/or freemium models like MySQL/PostgreSQL/MongoDB/Redis/etc., Rails/Django/etc., and Apache/nginx/etc. have taken over the market. That said, that "a lot of work to write and very specialized appeal" niche is definitely exception to the rule. Look at how long it's taking the Linux desktop to get video-editing software that's both solid and featureful. Open-source 3D modelling is almost entirely dominated by Blender 3D, which came about because money was raised to buy and release the source to a bankrupt company's in-house tool. I wish you well, but I'm unsure whether an app store model will appeal in a context where the buying decisions are made by executives more than the people who actually work directly with the product. (That's why textbooks are dry and often front-loaded with marketing pages compared the kinds of books meant to be bought directly by the users. They're written by professors to appeal to professors, not students.)
I draw a distinction between learning the API for recurring dependencies and getting up to speed with the current codebase. Your argument could be generalized to absurdity to argue that anything invented after assembly language makes things worse because of safety feature X or Y that must be learned.
You're missing my point. I said "if you **need** an IDE". The whole point being to steer the planning to make a language that requires an IDE as little as possible, then anything an IDE brings to the table will improve on an already comfortable base rather than first having to make it comfortable. &gt; Right now however, Intellij IDEA (java? working on Rust) makes my laptop overheat, whilst QtCreator (C++ working on C++ ) does not. ...and having to do fancy lookup of identifiers would waste either more CPU time or more RAM for caching while just coming out and noting where they came from all along removes one source of needless IDE and compiler busywork. This sounds more like it favours my position.
One of my big concerns with this: The way you enforce invariants is not sound. As was discovered in another context (the DAO hack, in Ethereum), [it is _not_ sufficient to transform invariants into a precondition/postcondition pair](http://hackingdistributed.com/2016/07/13/reentrancy-woes/). In particular, your invariants may be invalidated by any call you make unless the language is pure. Furthermore, since in Rust operators dispatch to traits, the same problem applies even when it doesn't _look_ like a function call. You could possibly paper over this by adding invariant checks around _every_ operation, but... It gets even worse if you have concurrency - your invariants may be violated by code executing concurrently, even without any markers in your function. Solving it in the presence of concurrency pretty much _requires_ serious static analysis - either to prove that your function _is_ pure (and thus cannot be interfered with), or the (much less tractable) whole-program analysis that would ensure: 1. Nothing that can invalidate its invariants can execute concurrently, or 2. That such things _do_ not invalidate its invariants when executed concurrently.
&gt; conversely, as these machines and languages get more powerful, they can work harder for us.. that's the point surely. Yes, but the whole point of good design is to think hard once during in the design phase, so the machine and the user don't have to think as hard every time they use it. I was drawn to Rust from Python for two reasons: 1. It allows me to be more explicit in my code, so I rely less on my memory, alertness, and tooling to achieve a given level of quality. 2. It's less wasteful of resources. (When I buy a new CPU, I expect to do more work with it. That's why my Linux desktop relies heavily on software intended to give slow, ancient old machines a new lease on life.) &gt; 'overloading' is also how we communicate naturally.. natural language is full of extreme ambiguity - when analyzed at the low level (verb or noun? and so many uses of the same word..) - but when given holistic contextual information, this ambiguity disappears. You'll want to look up the relevant discussions on that. All I know is that it did get discussed heavily and you're not going to change anyone's mind unless you can address the points that have been already made and do so in the forum appropriate for discussing language evolution at a given stage. I admit that overloading can be convenient, but I know enough about how little I know that I won't be convinced until I have a clear understanding of why it wasn't there in the first place. &gt; Some of what I'm talking about is purely the compiler working harder for you rather than the IDE as such; its just the time when you need to know where the definition is that you need the IDE to find it, otherwise just searching for the def get all of them. The problem is that your proposal feels like tying one of the compiler's hands behind its back, then praising how well it overcomes that handicap.
Yep! The reason that Roslyn has a Concrete Syntax Tree that preserves whitespace and comments is so that people can write refactorings that simply take a tree and produce a tree and can keep the original formatting.
I don't know about C++, but this is what we do for the dotnet libraries. In Roslyn it's called Edit And Continue and you can read some of the sources here: https://github.com/dotnet/roslyn/tree/master/src/Features/Core/Portable/EditAndContinue
Uh...well, I'll admit: by itself, I had a hard time parsing this: &gt; In particular, your invariants may be invalidated by any call you make unless the language is pure. The article you linked has a bottom line statement that makes more sense to me: &gt; The immediate takeaway for contract programmers is as clear now as it was when The DAO got hacked: do not perform external calls in contracts. If you do, ensure that they are the very last thing you do. If that's not possible, use mutexes to guard against reentrant calls. And use the mutexes in all of your functions, not just the ones that perform an external call. So, in my understanding, it's important that impurity NOT be present in contract code -- i.e., making database calls, web requests, reading globally-accessible variables, etc. Is that it? If that's the case, how is that any different than normal code, where being mindful of reentrancy is important for security? Perhaps could you give other examples, Underhanded Rust style? EDIT: More importantly...if you see a problem, what are your suggestions for resolution?
Random guessing: Turn off optimizations, turn every function call into a virtual look up, swap out virtual table entries on the fly when code changes.
For what it's worth, I do 90% of my Rust development on the bus to/from work on my laptop.
Lets suppose it is really important to not slow down compilation on laptopts for a moment. I think it is easy enough to detect whether you are using laptop or desktop and choose best compilation strategy accordingly anyway. Therefore their argument makes no sense. 
If the [epochs RFC](https://github.com/rust-lang/rfcs/pull/2052) is finally merged, we can mark `union` as reserved keyword in the next epoch, right?
Still pretty cool.
.NET has a real garbage collector, there's no reference counting in there.
One problem is that everything is already so complex that often you can't do debug builda any more because the code would be too slow.
I don't see your point. When a library provides a struct, you can create a wrapper class to ensure the data lives on the heap. When a library provides a class, you can create a wrapper struct and accomplish absolutely nothing. So surely the library providing a struct gives you more choice than a library providing a class?
There's reference counting to work out what objects can be GC'd. The only differences is that it can actually spot cycles, and there's no guarentee that the finalizer will run immediately after the last reference goes out of scope. (Or at all, for that matter)
If I wanna make my own wrapper for this, I'll just skip this whole conversation and not use the library. ...I mean, hell, I already implemented this. (Did I not mention that in this thread? I might not have.)
Perhaps you should read about the [identity function](https://bluss.github.io/rust/fun/2015/10/11/stuff-the-identity-function-does/) :D. But /u/sellibitze got it right, you can force a move using curly braces (as-if using the identity function).
I'm sure there are all sorts of details and complications, but I think one of the first things they do is add an extra no-op instruction to the start of every function. Then when you change a function, they can compile the new version somewhere into memory, and then replace that no-op with a jump to the new implementation.
Can that lead to data races if the swap is happening during a read?
Same here. Well, or at home with a kid in my arm.
&gt; The only differences is that it can actually spot cycles, and there's no guarentee that the finalizer will run immediately after the last reference goes out of scope. (Or at all, for that matter) ... yes, so it's almost, but not *quite*, entirely unlike reference counting :p And the GC performs (as far as I'm aware) reachability, which doesn't require actually counting the number of references at all. Reference counting is a form of garbage collection, but not all garbage collection is reference counting (at least not when using the usual definition of reference counting).
So the "choice" thing is a bit of a red herring? I for one don't see the point of having a nullable option type. Might as well have used the `[CanBeNull]` [JetBrains](https://www.nuget.org/packages/JetBrains.Annotations) annotations at that point (which I do :p).
The post uses a different meaning of "contract" than you - the topic of discussion is "smart contracts" on the Ethereum blockchain, and thus "contract code" (in that context) means "code". As a result, your read of that paragraph is both stronger _and_ weaker than what it means. Code _using_ design-by-contract can be impure safely - preconditions and postconditions, for example, are unaffected by this issue. The problem is that by transforming invariants into a precondition/postcondition pair, you change their guarantees from "this _continues to be true_ across the whole function body" to "this is true at these two points in the function body". This is a much weaker guarantee, and as a result, code might violate the strong guarantee without the weaker guarantee catching it. Because of this, someone might write code that _really does_ rely on something being invariant, but makes calls that first invalidate and then restore the invariant. That code would pass all its contract checks, but would still explode. As an example: static X: Cell&lt;usize&gt; = Cell::new(0); const Y: [u32; 1] = [3]; fn evil() { X.set(!X.get()); } contract! { fn uhoh() -&gt; () { invariant { X == 0 } body { evil(); Y[X]; evil(); } } } This will access `Y` out-of-bounds, but will not be caught by Adhesion. (In a language that's less memory-safe than Rust, this would be really nasty; logic errors Rust does _not_ protect against can also happen this way). The problem is that, without compiler help, I'm not sure you _can_ fix this. "invariant" is a _very_ strong guarantee, and I think it might _not be possible_ to make that guarantee without compiler assistance. Personally, I'd favor _not offering_ the `invariant` contract clause, because it gives _no additional protection_ over a precondition/postcondition pair as implemented, but leads to a false sense of security. Another example, with no external calls: const X: [u32; 3] = [3, 1, 4]; contract! { fn busted(y: &amp;mut usize) -&gt; () { invariant { *y &lt; 3 } body { let z = *y; *y = 4; X[*y]; *y = z; } } }
Oh ok. I was confused because you said "all the system tools on Github" so I took that to mean it was open source only.
Not when you pause all threads, I imagine.
&gt; so it's almost, but not quite, entirely unlike reference counting :p +1 for H2G2. Also, IMO, calling reference counting a form of garbage collection is slightly misleading - IMO, a defining feature of GC is the non-RAIIness and indeterminancy, i.e. that you actually have data that has a status of "garbage", rather than going from being-used immediately to being deallocated. And FWIW, the original comparison wasn't to say that everything is literally hiding behind a RAII `Rc`, but that almost everything is living on the heap, it takes an indirect lookup to get to, and there is often no unique owner of a given object in the sense Rust handles that. 
I'm not saying this is how they do it. I'm just guessing at one way you might try. 
Oh good lord... Yeah, this entire discussion is a red herring. LOOK: using System; namespace cs_scratch { class Program { struct Wrapper&lt;T&gt; { public T Foo; public Wrapper(T item) { Foo = item; } } class Bar { public string Name { get; set; } } static void Main(string[] args) { Bar bar = null; Wrapper&lt;Bar&gt; wrapper = new Wrapper&lt;Bar&gt;(bar); if (wrapper.Foo == null) { Console.WriteLine("How is this helpful?"); } } } } Lookee there. A (functionally) null value type. :| Edit: Seriously, if you could depend on crap in C# not to be null, I wouldn't be using Rust. :p C# is turtles all the way down--right up until *someone* writes `if (foo == null) { throw YourComputerHatesYouException("I hate you"); }`, and at that point what difference does it make if this is a struct or a class?
So, the problem is semantics, then? The issue seems to be that `invariant` is connoting something that's simply not feasible -- which is ensuring at ALL points that the `invariant` holds true. Obviously, if we could find some way to apply the invariant as implied semantically, then that would be ideal (though maybe expensive). Since we can't (macros don't have that capability, of course), what if the block were renamed to something like `prepost`, which would be more indicative of what the purpose of the block actually is? Would that be better, since we're no longer "lying" to ourselves, as it were?
Also, side note: thanks for the awesome feedback! I'm learning more about this because you're taking the time to teach me. I appreciate that a whole lot! :)
But how much of that are you actively compiling? And there's no reason to not make it configurable.
Here, short implementation: using System; namespace cs_scratch { class Program { class YourComputerHatesYou : Exception { public YourComputerHatesYou(string message) : base(message) { } } class UnNullable&lt;T&gt; { private T _item; public T Item =&gt; _item; public UnNullable(T item) { if (item == null) { throw new YourComputerHatesYou("You can't do that."); } _item = item; } } class Bar { public string Name { get; set; } } static void Main(string[] args) { var bar = new Bar { Name = "Hello" }; var wrapper = new UnNullable&lt;Bar&gt;(bar); // wrapper.Item = null; &lt;-- impossible // var emptyWrapper = new UnNullable&lt;Bar&gt; { Item = null }; &lt;-- also impossible Console.WriteLine(wrapper.Item.Name); var otherEmptyWrapper = new UnNullable&lt;Bar&gt;(null); // runtime exception } } } In a more robust implementation, you would return one of two different types from the constructor (which would not be a constructor, but a static method), and one of those types would represent the null (or `None`) state, while the other would represent having an actual value. There would be no way to get the `IHaveAValue` variant without actually having a value (or, more realistically, a non-null reference). Even then, though, you could still end up screwed, because you can do this: var foo = new Foo(); var x = new NonNullable&lt;Foo&gt;(foo); foo = null; ...and because C# is evil and `YourComputerHatesYou`, the compiler will do NOTHING to stop you. This ends up being mostly just a neat way to get access to Rust's `map()` function. :|
Yeah, that's wacky! There is also ring, which I really like because the ASN.1 and TLS parsing are in pure Rust and only the crypto is in C.
&gt; Even then, though, you could still end up screwed, because you can do this: That doesn't screw you at all, because references are passed by value. The problem with a class `NonNullable&lt;T&gt;` is that the instance itself can be `null`, and not just the thing inside it. Then again you can also `default(some_struct)` so ultimately there isn't much you can do in C# :(
Based on my (limited, probably outdated) understanding of that RFC, yes, it could be addressed in a subsequent epoch.
Those blog posts are mostly for letting off steam and also for laughing at myself later when I come back to then. The amount of out of date stuff about Rust on the net is a massive problem, though. Lots and lots of wasted time from that, but it gets better one you have more context and can quickly recognize bad advice instead of wasting time on something that can never work.
Sorry, you're right about that reference by value thing. But, yeah. I think you get what I'm saying: the language does not (currently) support the kind of guarantees necessary to make this really useful, and it's hard to overestimate how far some people will go to give you a useless object. :| Edit: my #1 pet peeve is a null `IEnumerable&lt;T&gt;`. I get those all the time. Makes me want to beat someone with a keyboard.
I don't know, but ROOT is also a C++ framework and has a REPL (but I can't say how much it can do as never used any functions except loading a file).
Yeah I've had a few `maybe_enumerable_of_T() ?? Enumerable.Empty&lt;T&gt;()` instances :(
At the least shouldn't 4 and 5 produce (10, 20) or something? It seems weird that you would have them run the code for each match arm but not return the expression result for each match arm.
It might be easier to generate the binding code with [bindgen](https://crates.io/crates/bindgen) rather than writing them yourself. I'm not sure about bindgen support for virtual functions, though. You can try and file bug if it doesn't work as expected. Also note that, on Windows 32bit, methods use "thiscall" convention, which Rust doesn't support in stable ([added in nightly](https://github.com/rust-lang/rust/pull/42058) not long ago), so if you are testing on that platform, you may not be able to call any method no matter whether it is virtual.
I already tried using bindgen which gave me some weird results that got me confused tbh, cause it created very weird link names And I just tried the "thiscall" convention, but it doesnt seem to fix my problem in nightly either unfortunately :/
`cargo check` on every keystroke
That sounds wasteful. Perhaps turning down so that it only runs a `check` if the file hasn't changed in the past 5 seconds?
So it actually only runs on every document save, and I have my editor configured to auto-save if I stop typing for 500ms, so it's a *little* bit more sane.
Windows does have very weird link names, something starts with "?". That's expected.
ye it looked like that but the problem was that rust couldnt find the link names to link them with though. Also could it be that I have to pass the object itself as a parameter to the method itself maybe?
Wow yeah, nalgebra look super well put together. I'll probably still try to do it anyways, just for the experience if nothing else :). Thanks for pointing that out though, that'll be nice to have a resource I can turn to if I'm ever stuck. 
Just as an "Is it plugged in?" question, you have confirmed that either Rust doesn't need the `.lib` or that you have it, right? (Windows dynamic linking for C and C++ expects a `.lib` **and** a `.dll` at compile time where Linux would expect a `.so`, with the `.lib` containing link-time metadata omitted from the `.dll`.)
i know just wanted to make it work with old c#, but thx for feedback :)
I'm writing a simple game in rust (the game itself isn't the issue, though), and was trying to implement an entity-component system. The idea is that there is an `Entity` struct which does little on its own, but has a vector of `Box&lt;Component&gt;` so that each component can do things like draw to the screen, update details within themselves, etc. whenever the `Entity` is updated. `Component` is a trait with the methods `draw` and `update`. The issue is that the `add_component` method on `Entity` requires that Components be of the `'static` lifetime (e.g. `fn add_component&lt;C: Component + 'static&gt;(&amp;mut self, component: C)`) to fit the bound for `Box`. While I understand why boxed traits must be static, the issue is that one particular `Component` called `SpriteRenderer` needs to hold on to a non-static Texture (which, under the library I'm using [`sdl2`], lives only as long as its `TextureCreator`, which is also not static). How would I go about doing this? I'm considering just creating a global spritesheet and instead of holding a `Texture` within the `SpriteRenderer`, hold a `Rect` with the coordinates to grab from the master spritesheet. This obviously isn't optimal, for in the end, I'd like to be able to load a Texture from an image and then give that `Texture` (or a reference to it). Any help is appreciated! Thanks in advance!
yes i saw nlkl library! and was kind of inspired by it. i will bump up c# version when unity will bump up its version. considering changing classes to structs! but i think its good to leave unwrap. i use it in rust for prototyping, its easier to find unwrap than missing nullcheck. :)
also about the result... i also had a feeling that result was clunky so i implemented SimpleResult :)
If you want a general-purpose terminal IO library, I'd check out [Termion](https://github.com/ticki/termion). It supports raw mode for input/output, as well as colors, formatting, etc.
All good points, thanks. I will definitely be refining the landing page -- it's been unclear to a lot of people. Countering the niche tool point we both agree on: Looking at some genres of trivial apps in mobile app stores, I'd say free doesn't necessarily win out. A lot of free apps end up undershooting a large portion of the market that wants more features, better design, more external integrations, more commitment from the author, etc. This analogy might not hold in this historically FOSS space, but my intuition leads me to believe it could. As for execs being the large dollar buyers, point taken. I think we've seen a lot of software trickle up from devs internally pitching tools they like, but for very large deals there's likely an opposing current in that direction. Companies like Oracle I doubt will ever be a customer on the selling side. Thanks for the well wishes! Cheers! 
There are REPLs for LISP and Scheme, both compiled languages. There are REPLs for Clojure, a compiled language, and for several other compile-to-java languages. Hell, there are REPLs for C!
Not only is such a thing possible, it has been around for a long time for C++ in the form of [Root](https://root.cern.ch), which is heavily used by experimental particle physicists.
&gt; Countering the niche tool point we both agree on: Looking at some genres of trivial apps in mobile app stores, I'd say free doesn't necessarily win out. A lot of free apps end up undershooting a large portion of the market that wants more features, better design, more external integrations, more commitment from the author, etc. This analogy might not hold in this historically FOSS space, but my intuition leads me to believe it could. You have a point there. The mobile space does resemble MacOS more than Linux in that respect.
I need the output to be &amp;str
upvoting this. errors are really informative.
Going offtopic, but this is unhealthy in all possible senses, you people must be masochists. I won't work on anything without a proper table, large monitor and keyboard, unless it's an urgent fix for some super critical situation. 
Yup, `prepost` would resolve that concern for me - though I would suggest being very explicit about the _order_ that these contract validations execute in, to guard against people writing validations with side effects. If someone's `pre`, `post`, or `prepost` clauses have side effects, then the following orders can have different behavior: pre prepost_pre body prepost_post post --- prepost_pre pre body prepost_post post --- pre prepost_pre body post prepost_post --- prepost_pre pre body post prepost_post
No problem, glad to help!
The README and the docs for the `contract` macro have the ~~last~~ first order documented already. :) So all that needs to be changed is the name! I don't love the name, though; it was merely the first thing that came to mind. Any alternatives would be appreciated! ;)
you could have a python-style else block on for/while loops which would be the result if you didnt break.
&gt; Yes, but the whole point of good design is to think hard once during in the design phase, so the machine and the user don't have to think as hard every time they use it. The way you mentioned IDE made it sound like 'C++ needs an IDE to be comfortable' That isn't the case. The lack of an IDE made rust *UNcomfortable* Now I'm not blaming rusts design for that - its just community momentum - but the point is there's no need to restrict features just so you can navigate 'by hand' .. **the point is you can never navigate by hand as fast as with machine assist** The compiler has type resolved type information, it can always do more. &gt; It's less wasteful of resources. (When I buy a new CPU, I expect to do more work with it. Those resources are cheap compared to programmer time. You're going to compile your program *anyway!!!* . As such, you should be able to count on having a resolved program database. I would argue if you do it right , the caching should *save* resources (e.g. how much changes from build to build) &gt; "The problem is that your proposal feels like tying one of the compiler's hands behind its back" Its more like we have a superman compiler (with an amazing inference system) then we hobbled it a little. 
Circumcondition? In the same vein as prefix, postfix, and [circumfix](https://en.wikipedia.org/wiki/Circumfix).
&gt; ...and having to do fancy lookup of identifiers would waste either more CPU time or more RAM for caching it's going to do that anyway, every time you build. Caching will save work between builds. i.e. there's only so much a human can change between builds. The storage is there. As it stands I navigate Rust with Grep, which means scanning the raw source every time. **I assert that is MORE cpu intensive than caching some dedicated structure that knows about the task at hand.** you can't fully leverage rusts namespacing features if you *do* just rely on grep. the point of the namespaces was to use shorter names, with contextual information (position in the program and 'uses') to shorten. Relying on grep you'll run into a point where textual ambiguity re-surfaces 
&gt; Your argument could be generalized to absurdity That would be a straw man because the approaches I've used in C++ rely on the compiler for other forms of safety. One of the use cases is dimension-checking ,plugging in custom types and having them flow through the calculations. This is much harder to do in Rust, infact I gave up. It's like you've recovered the arcane-ness of other forms of TMP where you're basically repeating the program in an arcane angle-bracketed form. I've got the 3 situations side by side - C++, Rust, Haskell - to show me *what* is possible. rust and haskell *do* have the ability to recover C++'s 'forward flowing types' (and it's explained in haskell tutorials why, when doing multi-parameter use cases) but rust cripples that by demanding the type signatures be written *everywhere*. All it takes is 2-3 intermediate results and the types explode. In discussions rust people tel me "use a macro" but thats the kind of fugly workaround I can already do in C++. it means writing some functions in a completely different syntax, but you can recover the same 'hazard' (writing re-useable expressions without spelling out the types) By doing so you *lose* the ability to easily refactor back and forth, and for the compiler to decide where it should or shouldn't be inlined.
&gt; I admit that overloading can be convenient, but I know enough about how little I know that I won't be convinced until I have a clear understanding of why it wasn't there in the first place. it's been recovered for maths , rust can do 'matrix*matrix, matrix*vector' now .. it just requires a much more complex system of angle-bracket annotations to use. .. which makes it harder to go further ('matrix * velocity vector -&gt; velocity vector.. 'force dot displacement = work done ' etc) And it's not like our existing use of C++ is the be all and end all: we have array indexing which is stilll basically 'unsafe*' (the bounds check is an admission that you *don't know* it's logically sound. You'd need empirical testing *anyway* before you can rely on a Rust program for say aircraft engine control software. a bounds check means you admit your plane could still fall out of the sky. When you're confident it's logically sound, *you can eliminate that check*.). A lot of the people in the rust community don't admit this.*there's other forms of correctness, beyond the reach of the type system*. So you've got to write various other tests *anyway*. Further improvements to a type system or more complex attractions (which means more nesting and heavier trait bounds) should be able to reduce the need for 'on-the-fly bounds checks' (e.g. one thing i have in mind is to build an abstraction for 'an index buffer that caches it's bounds', so the bounds check can be done *once* when you pair the buffer, rather than for every element every time). I gather there are more advanced ideas r.e. "dependant types" that can go further there. (* I realise in Rust 'safe' really means 'web safe' but in my contexts 'safe' means 'the program will do the right thing', and a *good* program needs to do that *faster than rivals*. To me, a bounds check is for debug builds, we can do that in C++ easily , and slot in other types that do numerical verification)
How about profiling the app and figuring out where the hot spots are and *ONLY* optimize that portion of the binary. 
hahahahah they also make some decisions in rust admitting that you will use *google* to help. (e.g. they moved from ```~T``` to ```Box&lt;T&gt;``` saying "it's more google-able", instead of allowing ~ to be overloaded for generality) so instead of your 'whole program searches' happening locally using some cached database infront of you , they're relying on global infrastructure to connect you to some centralised server (which of course is doing the work of caching queries anyway..) ... keeping a browser window open beside your text editor... burning up more screen space..
..Name brainstorming? Don't mind if I do! :D - `preserve` - Emphasizes "was true at the start, and we don't change that" - `wrap` - Emphasizes that it wraps the body in checks - `jacket` - Slightly more fanciful "wrap" - it's a jacket "protecting the body" - `guard` - The body is guarded on both sides by the checks - `usher` - Because it escorts control flow into and out of the function
What's your dev setup? I'm a newbie, I just use Atom to edit (with just `language-rust` installed) and then compile/run in a terminal. Looking for options for something a little more integrated, and "cargo check on every keystroke" had me intrigued.
I also thought of: * `inout` -- a throwback to a const variant keyword in D, somewhat descriptive * `round[trip]` -- coming back the way you came * `double_check` -- you're checking twice! I like `wrap` and `double_check` myself... Let the votes begin!
&gt; The way you mentioned IDE made it sound like 'C++ needs an IDE to be comfortable' That isn't the case. The lack of an IDE made rust UNcomfortable That's a matter of perspective. I find Rust quite comfortable with only Vim's syntax highlighting and a terminal to run commands like `cargo check`. Your proposal would make it less comfortable in that context. Also, regardless of what an IDE may do, I recoil at your proposal for the same reason I prefer Rust's monadic error handling over C++'s exception-based error handling. Rust's current approach (and monadic error handling) makes it easy to understand what's going on using only local information. &gt; Those resources are cheap compared to programmer time. You're going to compile your program anyway!!! . As such, you should be able to count on having a resolved program database. &amp;nbsp; &gt; Its more like we have a superman compiler (with an amazing inference system) then we hobbled it a little. It's pretty clear from these two bits that we're not going to agree on this for the same reason that I some people who prefer dynamic type systems for their "get something running quickly" characteristics simply can't see things from my "get it as perfect as feasible the first time, even if it takes a little longer" perspective.
I'm not entirely sure what changed how I was looking at things, but you have a point about caching there. I have no problem with caching a representation of the code that's updated incrementally. Heck, I assume that's what RLS either does or will do in order to be efficient for large codebases. I'm just not onboard with the idea of using that as an excuse to be sloppier about use of identifiers when choosing to accept or reject source code. As others have said, put the clever identifier lookup in the IDE, not the compiler. (I'm in the "just because we *can* infer types more globally doesn't mean we *should*" camp.) Now, there *are* places where the current state of the type system *is* a major pain, but that's being worked on. For example, the plans for [impl Trait](https://github.com/rust-lang/rfcs/pull/1951) as a way to support returning anonymous, bounded generics.
That may be true, but talking about it here will never amount to anything unless you either write an RFC or convince someone else to do so... and that means that you need to convince people that you've considered all of the nitpicks and edge-cases that were revealed when people talked about ideas like this before. The shallow "upsides only" way you talk about this gives a strong impression that you haven't done that, which makes me feel that there must be some less obvious downsides that prevented the very smart people who built Rust from choosing this course of action. **TL;DR:** Nobody will take you seriously unless you come across as having "done the research" by talking about known downsides of your idea and why they're trumped by the upsides.
Mono has had a C# REPL for many years now. http://www.mono-project.com/docs/tools+libraries/tools/repl/
Genuinely curious: what companies besides redhat have been very successful with the support model? I personally think it is probably easier to sell the software for a profit than give the software away and run a support model.
That looks awesome! Excuse my ignorance / laziness, but is there a way to automatically turn these checks off in eg release mode? That would be super helpful for me, because there are a lot of generic tests I want to run after operating on a massive data structure (the layout tree in Way Cooler specifically) and it would be cool if I can add that as a contract instead of manually in the code. But I disable this for release build because it's rather expensive. 
&gt; they also make some decisions in rust admitting that you will use google to help. Even assuming that this comment was intended as a continuation of one of the other replies, I'm not sure how this argues against anything I said. &gt; they moved from ~T to Box&lt;T&gt; saying "it's more google-able", instead of allowing ~ to be overloaded for generality Whether or not that's true, it's a decision that was made back when I was just starting to hear about Rust and I lack the context to have a strong opinion on it. From what little I remember, it was because `Box`, `Rc`, and the `Gc` they were anticipating adding at the time all originally had special sigils reserved, but they decided it didn't make sense to give those three cases special syntax when, design-wise, they weren't inherently different from what a third-party library might provide. I'll admit that there are still a couple of spots where `Box` gets special treatment by the compiler, but they do want to resolve those once they've handled more pressing stuff. &gt; so instead of your 'whole program searches' happening locally using some cached database infront of you , they're relying on global infrastructure to connect you to some centralised server You can generate offline docs for the current project, plus all its dependencies, with built-in offline search, and open it in the browser, using `cargo doc --open`. Even if that weren't the case, I use the [Zeal](https://zealdocs.org/) offline doc viewer to perform my searches on many different kinds of HTML-based documentation (including Rust stdlib, if I haven't used `cargo doc` yet). &gt; keeping a browser window open beside your text editor... burning up more screen space I keep a browser window open 24/7/365 anyway, so that's not really a powerful argument.
&gt; it's been recovered for maths , rust can do 'matrixmatrix, matrixvector' now .. it just requires a much more complex system of angle-bracket annotations to use. .. which makes it harder to go further ('matrix * velocity vector -&gt; velocity vector.. 'force dot displacement = work done ' etc) What you're talking about is far enough outside the kind of Rust coding I normally do that I lack the expertise to identify what concrete examples would correspond to what you wrote. &gt; And it's not like our existing use of C++ is the be all and end all: we have array indexing which is stilll basically 'unsafe' (the bounds check is an admission that you *don't know it's logically sound. You'd need empirical testing anyway before you can rely on a Rust program for say aircraft engine control software. a bounds check means you admit your plane could still fall out of the sky. When you're confident it's logically sound, you can eliminate that check.). A lot of the people in the rust community don't admit this.there's other forms of correctness, beyond the reach of the type system. So you've got to write various other tests anyway. I'm not sure what point you're trying to make here. From my perspective, it feels as if you're stating something that was never in dispute and then pointing at it as if it were some kind of debate-ending bombshell. Yes, there's a lot that can't be proven at compile time. The Rust community is well aware of that and strongly desires to improve that situation. In the years I've been lurking in /r/rust, I've seen a **lot** of discussion and experimentation to try to find acceptably robust and general ways to do more of this stuff at compile time. See, for example, the [discussion](https://github.com/rust-lang/rfcs/issues/1930) that's taken place around adding Pi Types. (Currently at a [let's get const generics first, then revisit](https://github.com/rust-lang/rfcs/pull/2000) stage of things.)
For this reason I usually set the debug opt-level to 1 in Cargo.toml for my projects. It gives me, in my opinion, a better balance of compile times and runtime speed.
It would be good if you explain how this differs from [General Filesystem Caching](https://www.kernel.org/doc/Documentation/filesystems/caching/) already included in Linux kernel.
It's technically not a alpha, it's a public beta. https://en.wikipedia.org/wiki/Software_release_life_cycle#Stages_of_development
Check out https://atom.io/packages/ide-rust if you're on Linux or Mac (I haven't tested on windows yet). I still have a bit to do before it does everything I want so I haven't formally announced it yet but it works great right now.
fs-cache doesn't work with fuse filesystems (at least last I checked) and requires root to setup.
I am concerned about the complexity. It seems like something is wrong if we need this much complexity to offer reasonable build times.
Profile-Guided Optimization is a thing. Though my understanding is that it‚Äôs on the advanced/heavy-weight end of the spectrum, not typically something for quick edit-compile-test cycles.
VS Code works pretty well with the Rust Language Server using the Rust plugin. This gives you error/warning checking as you type as well as auto-completion for various things. I think the RLS is still only available in the nightly branch, though.
https://atom.io/packages/ide-rust does the same for Atom and will install and configure RLS for you.
I don't think anyone would be suggesting they'd nerf compilation for everyone, it's highly likely something like this would be configurable if it ever comes to fruition.
&gt; Ah, now I get it. You are trying to entangle two arguments to trigger a bug in the probability function. &gt; &gt; Notice that the derived probability is a function of the probability of arguments, so when you're entangling two arguments like that with x = true, you are implicitly adding a constraint/sub-type: Okay, this sounds like you're trying to say [conditional probability])https://en.wikipedia.org/wiki/Conditional_probability) (well, conditional asymptotic density), which is *exactly* the way to fix your `P(if(...)(...))` decomposition, but there's no indication that any of this is considered in any of your descriptions, and it has to be there, or else the decomposition is incorrect. &gt; OK, here are two insights that might help you understand path semantics: &gt; Path semantics is not a theory about constructing objects &gt; Path semantics is about deriving properties of functions, operating on objects It doesn't. "constructing objects", "deriving properties" and "operating on objects" are all too generic to tell me much (e.g. I don't see a notable difference between thinking about "constructing an object" or thinking about the properties of a function that returns an object). Buzzwords don't bring clarity, and, your comments here on reddit and the linked blog post are full of them. &gt; This is why "generalizing path semantics to probability theory" does not mean "construct probability theory in path semantics", but rather "apply probability theory to the field of path semantics for more powerful reasoning about functions". Path semantics is only concerned about how functions work and are related to each other, nothing else. Note that my point is that you've been applying probability theory incorrectly, at no point was I, or (I think) anyone else, under the impression you were trying to use path semantics to model probability theory. You've taken some of the conventional intuitions about probability in a form that is mostly correct (once I've worked out how your notation translates into the conventional notation, and done the rather major change of substituting "density" for "probability", hoping without proof that that has the intended result), but at the moment it is missing the formalism that actually makes "probability theory" a mathematical theory.
**Conditional probability** In probability theory, conditional probability is a measure of the probability of an event given that (by assumption, presumption, assertion or evidence) another event has occurred. If the event of interest is A and the event B is known or assumed to have occurred, "the conditional probability of A given B", or "the probability of A under the condition B", is usually written as P(A|B), or sometimes PB(A). For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person has a cold, then they are much more likely to be coughing. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
I'm on my phone so sorry if this is too short. Look up what the ? operator does. It expands to a `match` statement. You can replace each `match` statement with your own version, handling errors however you like. Hope this helps :)
The appropriate section of the book is [Recoverable Errors with `Result`](https://doc.rust-lang.org/book/second-edition/ch09-02-recoverable-errors-with-result.html#recoverable-errors-with-result), which explains how all this works. Is there some part of this in particular you don't understand; a general answer to your question will likely just re-state what's in the book unless you can be more specific.
&gt; but, AIUI they can rarely control it for a single core independently I believe this is less true in mobile. In fact some phones have a BIG.little architecture, where different cores have different speeds and power consumption.
Maybe naive, but can't the compilation process just make a call graph and bundle callee type signatures with each caller? That would give a bunch of independent compilation units (one function) and eliminate a lot of dead code, as well. For dependencies it would only compile functions reachable from the public functions actually called by the dependent module.
" The Rust community is well aware of that and strongly desires to improve that situation." For what I observe, i think there is 'reactionary' approach to c++ out there (and hence a lot of these people end up in a community like rust), i.e 'it does many bad things' -&gt; 'EVERYTHING it does is bad' etc the JAI language jonathan blow is developing is the closest to my preferences/thinking. even there I observe him doing the same thing: he's permanently associated the "dot call" idea with OOP which must be purged. so in his language he's got foo(a,b) and a 'foo' b (kind of like haskell) , which means he loses 'dot-autocomplete' for multi-parameter functions. He's missing the point that once you have extention methods .. the coupling hazard of OOP goes away. we all make fun of the java-esque ritual of getters and setters but conversely that doesn't mean the ability to swap a field into an accessor isn't useful (one real world case was swapping column/row major matrices... do you want an objects 'position', 'forward axis' etc to be stored in fields (= column major matrix) - or do you want to describe it with a 'row-major' matrix ready for dot-product instruction based multiply.. or even *neither*, and store the orientation as a quaternion. the getters let you try both approaches and pick which one is empirically measured to be faster
Eclipse (and maybe other Java IDEs) does this also. It hooks the VM and redefines the class, then magically does stack replacement to fix in-process calls. It's crazy voodoo stuff, but it works.
There are two key things to understand about Rust's error-handling approach: 1. Errors in Rust aren't actually special... they're just ordinary return values paired with what you asked for via Rust's "one of the following" `enum` construct. 1. `io::Result&lt;T&gt;` is an alias for `std::Result&lt;T, io::Error&gt;` where `T` is a type you choose. 2. A result is an `enum` which can be either the thing you asked for or something representing an error. 3. To get at something inside an `enum`, you have to tell Rust what to do in all possible cases. (In this case, "To get at what you asked for, you have to tell Rust what to do if it contains an error instead.") 2. The `?` operator began as a macro named `try!` and means "either evaluate to what I asked for or `return` the error." First, I'll explain the parts of that code not related to error-handling to make sure we're on the same page. They're basically equivalent to this valid, runnable Python 3.x code: import os, scandir # one possible implementation of walking a directory only visiting files def visit_dirs(dir, cb): if os.path.isdir(dir): try: for entry in scandir.scandir(dir): if entry.is_dir(): maybe_err = visit_dirs(entry.path, cb) if maybe_err: return maybe_err else: cb(entry) except OSError as err: return err def test_cb(entry): print(entry.path) if __name__ == '__main__': print(visit_dirs('.', test_cb)) As for the `?` operator, think of it like this: **Compact:** let entry = entry?; **Expanded (equivalent, not actual):** if entry.is_err() { return Err(entry.unwrap_err()); } let entry = entry.unwrap(); * The `unwrap_err()` and `Err` are needed because `entry` is a `std::Result&lt;DirEntry, io::Error&gt;` and your function signature says you're returning a `std::Result&lt;(), io::Error&gt;`, so you need to pull the error out of the old `Result` and build a new one. (Trust the compiler's optimizer here.) * `Ok` and `Err` are actually `Result::Ok` and `Result::Err`, but Rust `use`s them by default so you don't need to type their `Result::` part everywhere. * `unwrap()` and `unwrap_err()` mean "either get me the content/error, or quit the program" and are meant for situations like this where it's clear that the "or quit the program" case should be impossible.
its good but 'self driving' is a bad analogy. the extra safety is manually guided. it's more like 'a car with a built in breathalyzer' , 'controls unlocked by signalling first', etc
With "mobile devices " I've meant laptops. And I didn't say that parallelism was bad, in fact I'd quite like to have parallelism. But its not that its *that* easy, it won't give you a linear scale up for the same power consumption.
&gt; they decided it didn't make sense to give those three cases special syntax when, design-wise, they weren't inherently different from what a third-party library might provide. the fact you have the tools to make your own smart pointers is definitely awesome. But to me this was a a 'bait-and-switch' moment. We already had smart pointers in C++ and they were being upgraded. A big part of the draw to rust was 'having the common case syntactically lighter'. they kept saying 'you don't need box, it's allocations' but you can use it in enums (to get around the padding), you can pass bits of state between threads, the allocations might be pooled (e.g. to me 'a buffer holding particles, rapidly created and destroyed' is very different to 'system level allocations that can have side effects on the MMU'). there's the case of 'optional-box' ; I also really liked the way you read ~ as 'pointer to owned allocation;' , and it combined as ~T, ~[T], ~str. (again, a form of resistance to vector&lt;T&gt; is that sometimes passing a pointer to a buffer rather than &amp;vector&lt;T&gt; is syntactically lighter..). of course in C++ the syntactic overhead of writing a smart pointer means a temptation to revert to the multipurpose '*T'. (when you *know* what you're doing, it's more pleasant to write, because it melts away) (I wish they just made the sigils overloadable .. the fact they were hardcoded wasn't a problem IMO). They made their decisions from measurements of the *compiler* sourcebase, which is just one type of program.. It's a whole separate rant.. but related - if we had the option of whole-program inference , the syntactic cost of the smarpointers would be reduced.
i've done an RFC for the 'elide types in trait-impl' idea, presenting 'how it goes in haskell' as an example; I think it's the least controversial of my suggestions; there the types *have* been spelled out. With this tweak, I'd resent traits a lot less - it would become synergistic - "declare the pattern first: *now you don't have to repeat the details every time you use it*"
https://github.com/rust-lang/rfcs/pull/2063 
Well, I have read this session when I got through The Book. And I think I have figured it out what does error handling exactly do in this session. But I still get stuck in a real scene. I will check it again. 
Wow, what a detailed answer, thank you so much for your reply. But I am still confused what does `Ok(())` mean, and What does the function signature `io::Result&lt;()&gt;` mean. Is it something to do with `statement` or `expression`. By the way, it seems there is no such `statement` or `expression` thing in Python. Is something end with semicolon `;` is `expression`, otherwise, it is `statement`? Because Sometimes I get `()` as a return value, is it a tuple ? 
&gt;&gt;"As others have said, put the clever identifier lookup in the IDE, not the compiler." .. IDE plugin based on the compiler's library; Making the same information available to the IDE and compilation should avoid repeated work &gt;&gt; (I'm in the "just because we can infer types more globally doesn't mean we should" camp.) we should infer as much as possible.. make the tools available and let people use them optimally per situation. Sometimes the types make things clearer, sometimes they don't. Those who don't like overloading want more explicit function names... if thats the case doesn't the more elaborate function name make the details of the type less important to read *every time*. Conversely if you do want to be explicit with the types, why not leverage that more through overloading..
Writing to a union field will run the destructor on the old value. This assumes that the old value is of the correct type, which is unsafe. [From the RFC:](https://github.com/rust-lang/rfcs/blob/master/text/1444-union.md#writing-fields) &gt; Assigning to a field with a type that implements `Drop` will call `drop()` on the previous value of that field. So the unsafety *is* related to destructors, but not in the way that OP claimed.
&gt; I find Rust quite comfortable with only Vim's syntax highlighting and a terminal to run commands like one of the things giving rust a new lease of life for me is trying out the intellij integration. dot autocomplete and function parameters appearing on the fly is worth it's weight in gold. My foray into rust taught which parts of the C++ world I took for granted. if you bring up the structs you're dealing with (again , burning up more monitor screen space, if you're concerned about physical resources.. i could work around it a bit by keeping more windows on more monitors open..).. they've usually got components.. you have to dig through manually. With 'dot-autocomplete' this happens automatically , dot..dot..dot all they way and the IDE explores the graph for you. &gt; who prefer dynamic type systems for their "get something running quickly" Dynamic types are not the same as statically inferred types. Statically inferred types give you the 'best of both'. it's the synthesis between the extremes (just like multi paradigm vs pure OOP /pure FP) &gt; ""get it as perfect as feasible the first time, even if it takes a little longer" " I strongly agree with jonathan blows philosophy presented in his talks - code goes through a 'maturation cycle', the road to 'perfection' is **experiment.**, so the easier the experiments are , the better the result will be. As such his language syntax is designed to make it easy to move things around (e.g. 'some code starts out as lambdas, then gets promoted to functions..' .. ). Something else I agree with form elsewhere - 'working code that does *something* is better than any design document'. Pretending you can plan everything upfront is a fallacy. if we could forsee everything in our heads, we wouldn't need computers.. Getting something *onscreen* will inspire more useful thought than anything else.
The whole perspective is relevant, but I suspect it might just annoy a little more somebody who is already annoyed by Rust evangelists.
[Looks like a bug in the Markdown parser.](https://johnmacfarlane.net/babelmark2/?text=*+%5B%60%5Bu8%5D%3A%3Areverse%60%5D%5B123%5D%0A%0A%5B123%5D%3A+https%3A%2F%2Frust-lang.org) (The page takes a while to load, but when it does, you'll see that Kramdown outputs an empty list item.)
If you want to include the string into the binary itself, you might want to look into [include_str!](https://doc.rust-lang.org/1.4.0/std/macro.include_str!.html)
&gt; What does the function signature io::Result&lt;()&gt; mean I'll have to explain this in steps: 1. `io::Result&lt;()&gt;` is just an alias for `std::Result&lt;(), io::Error&gt;` 2. `std::Result` is declared something like this... enum Result&lt;T, E&gt; { Ok(T), Err(E), } ... and is a data type that can contain one of two things: `Ok(T)` or `Err(E)`. 3. The `T` and `E` bits indicate that `Result` is a generic, meaning that it's a blueprint for building types, rather than a complete type on its own. (Translation: `T` and `E` are placeholders that have to be filled in. For example, you might write a file-reading function that returns `Result&lt;String, io::Error&gt;`) 4. The `Ok` and `Err` bits are necessary because `enum`s lets you create something like this... enum Action { Up(u8), Down(u8), Left(u8), Right(u8), Shoot }; ...where not all variants contain data and, for the ones which do, `Up(2)` and `Down(2)` don't mean the same thing. &gt; what does Ok(()) mean Because it's the last line in the function, it's equivalent to `return Ok(())` and it's how you create a `Result` which means "we succeeded and the result was an empty tuple". If it helps, think of `Result` like `return success_or_failure, data_or_error` in Python, except that the compiler can catch more mistakes for you. &gt; By the way, it seems there is no such statement or expression thing in Python. Actually, there is. An expression is something you can put on the right side of `=` or use as an argument in a function call. In Python, `x &lt; y` is an expression because you can write `foo = x &lt; y`. In Python, `if thing is True:` is a statement because you can't write `foo = if thing is True:`. &gt; Is something end with semicolon ; is expression, otherwise, it is statement? Basically. It may help to think of it like this: 1. In Rust, almost everything is an expression. (eg. you can write `let a = if b { 1 } else { 2 }`.) 2. The semicolon works sort of like a comma in a list. 3. A `{ block }` evaluates to the last line inside it. 4. If you put a semicolon at the end of the last line in a block, Rust will insert an invisible `()` after it. &gt; Because Sometimes I get () as a return value, is it a tuple ? Yes. `()` is an empty tuple and Rust uses as the equivalent to Python's "If you don't `return` anything, the function will return `None`" because Rust only has `None` as part of `Option&lt;T&gt;` and `()` means "only one possible value... nothing" while `Option&lt;T&gt;` can be `Some(T)`.
Mobile is being defined here as laptops, not phones.
Well, I like Rust, but there is too much legacy software which you can't rewrite without putting much money and time into it. And the platform support is also bad and there is no professional GUI framework. Thus, I find this blog post a little bit overstating.
The std::Result&lt;T, E&gt; type is a regular enum, defined as follows: pub enum Result&lt;T, E&gt; { Ok(T), Err(E), } This means that you can have types of `Result&lt;T, E&gt;`, where types `T` and `E` can be anything you want, and data of type `Result&lt;T, E&gt;` will be either an `Ok(T)` or an `Err(E)`. For example, you can have a type of `Result&lt;i32, bool&gt;`, its values can be either be an `Ok(i32)`, or an `Err(bool)`. All of the following values have type `Result&lt;i32, bool&gt;`. let a: Result&lt;i32, bool&gt; = Ok(1234); let b: Result&lt;i32, bool&gt; = Ok(-42); let c: Result&lt;i32, bool&gt; = Err(false); Now if we have a look at the definition of `io::Result&lt;T&gt;`: type io::Result&lt;T&gt; = Result&lt;T, io::Error&gt;; We can see that in fact `io::Result&lt;T&gt;` is a type alias of `std::Result&lt;T, E&gt;`, where the `T` type can be anything, but the `E` type will always be an io::Error. In other words, a type of `io::Result&lt;i32&gt;` is equivalent to `std::Result&lt;i32, io::Error&gt;`. All of the following values have type `io::Result&lt;i32&gt;`: let a: io::Result&lt;i32&gt; = Ok(1234); let b: io::Result&lt;i32&gt; = Err(io::Error { /* fields ommited */ } ); You are correct saying that `()` is an empty tuple. Now given that the function in question returns an `io::Result&lt;()&gt;`, which is equivalent to `std::Result&lt;(), io::Error&gt;`, it must have two variants: either an `Ok(())`, or an `Err(io::Error)`. By having an `Ok(())` at the end of the function, we return this value, indicating that that function has executed successfully.
rustc and especially RLS are a _huge_ drain on my battery life and - as a travelling consultant - this is a major source of pain for me. I think the main problem here is that rustc cannot adapt at all, currently, not keeping things slow for everyone.
It sounds Rust is some kind of functional programming language, because I write some Emacs Lisp before, I find Rust sometimes reminds me of Lisp, a familiar feeling, but with a lot of difference. Because Rust's philosophy is different from other language derive from C, such as Java and python, so it is a little bit hard for me to get used to Rust, thanks again for your reply :)
Much as I love Rust this article will fail to convince anyone who isn't already convinced. Putting the phrase "C and C++ are incredibly bad languages for programming" at the top of your argument will lose you almost anyone who is not already converted. They're NOT incredibly bad languages and telling someone who uses them daily to do quality work will mean that they won't listen to a word you say. Enormous amounts of incredible code has been written in them, and they're enormously popular for a reason. At worst they're very dated languages, with the tradeoffs they make and the design they use belonging to a different decade. A decade where security holes weren't a problem and we assumed that good programmers and good practice could prevent memory mistakes. The Rust team in general seems to be very careful about the way they talk about Rust. How they are never negative about other languages, won't even put a Rust vs X comparison on the website. To me the Rust community is probably the all time example of how to build a large helpful programming community so they must be doing something right.
I'm not sure how this is unhealthy? I have a notebook with a large enough screen, a good angle and a good keyboard. I pay a hefty sum for that. I have a proper table setup at my office and I struggle using a large screen. I just don't know what to do with it. I agree that not having proper seating is shitty, but once you are sitting in the train for 6 hours anyways, being in a bad seating position for 6 hours because you read a book or because you are programming doesn't change much. 