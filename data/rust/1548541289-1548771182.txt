Or better yet, infix notation: \`'label: { ...; await 'label; ... }\` 
IIRC it used to be fat, but in recent versions it was changed to thin.
Perhaps you misunderstood my comment to be claiming that the Coroutines TS is better than Rust's approach, it certainly isn't. P1063[1], Core Coroutines, under the 'Allocation and performance' section talks about the fact that current implementations of the Coroutines TS doesn't adequately elide allocations and isn't truly 'zero allocation', as you note. The section also notes that extensions to the Coroutines TS that could statically guarantee zero allocations are possible, "*These all appear to be pure extensions, so they could be done post-C++20 if need be.*" I'm trying to clarify that I believe the Coroutines TS is incomplete but stands as a valid path forward, something /u/Rusky seems to disagree with. [1]: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1063r0.pdf
&gt; I intend to generate 4 random f64s for every pixel on the screen. If you want more speed after the improvements mentioned by other commenters, here's where I'd look. If you're using these f64s to generate pixel values, you're discarding most of the randomness- there's at least a factor of 8 less random bits in the image displayed than the input used to create it.
Adding `Clone` is cheating :-) as you may be working with some struct in an external crate you can't derive `Clone` for. The performance penalty may in any case be quite major, depending on how big the thing you're planning to clone is. See my ancient [`parseargs`](http://github.com/BartMassey/parseargs) for an example of how you can do a reasonable argument parser without builders. (Trigger warning: Haskell.) I personally think just filling in an array of structs that defines the argument parse is easier than figuring out all those builders; it's less error-prone as well in my humble opinion. I personally use `structopt` instead of raw `clap`: it has this same philosophy, but Rust's deriving makes it even easier to use.
Problem with docs.rs - they're using the nightly build of Rust, and that's causing my freshly minted documentation to fail building. Any thoughts?
I have a rather opinionated take, but here's what I think &gt; I don't want to return to the era where people need to run gtk on Mac or Windows to make an application work To me, this isn't much different from using Electron or any custom UI library which performs its own rendering + signal processing. Either way, you are sacrificing the use of pure native widgets and only approximating them in style and behaviour. The alternative is using a wrapper library like WxWidgets, but I feel like that leaves you stuck with the worst of all worlds. GTK+ applications can ship with custom themes in order to better match the style of the host OS. They also have control over what their keyboard shortcuts are, and the layout of their menus - but this is *not* something which is free or automatically. It requires extra blood, sweat and tears - and that is clearly not ideal. On the other hand, the porting boils down more to presenting a consistent user experience than it does to actually making the thing work in the first place. &gt; not to mention that gtk integrates with nothing on the OS This is actually exactly what I like about GTK+, and what I expect from a GUI library in general. I want the GUI library to do GUI stuff, and leave the rest of my program to its own devices. I'm not looking for an application development framework, just something which can throw some widgets on the screen, connect signals to the rest of my program, and let me choose the best tool for the job in whichever domain my application needs. This is more painful in languages like C and C++ which have rather limited standard libraries, but with a rich standard library and the whole cargo ecosystem, it isn't very difficult to make the backend of an application clean and portable. The biggest problem with GTK+ on Rust is the culture shock. GTK+ is an extremely object-oriented framework, and it makes extensive use of behind-the-scenes reference counting. These idioms feel very out of place within the greater Rust ecosystem, and the implementation can feel like pounding a square peg into a round hole. Rust deserves something that feels more natural. That said, the bindings are pretty good. Finally, let me just reiterate: I don't mean to say you are wrong in any way, or come across as confrontational. Everyone has different priorities and different needs which are defined by the project they are working on. These are just the opinions of some dude on the internet :)
areweareweXyetyet.com
It's totally a data race. I don't want to use atomic: I plan to only ever use it in single-threaded code and I don't want the overhead of the atomic add instruction. The worst that's likely to happen, I think, is that two threads get the same variate. In my application I can live with that.
For `&amp;mut` - Less verbose for complex expressions (conditionals, loops, etc) - Easier to deal with when your builder is a member of a struct For `self` - Less overhead if your built type can take advantage of complex objects being transferred from your builder (i.e. not having to `clone` in your `.build()`. - Since `.build()` should also take `self`, you error recovery is less ergonomic Since most of my builders are in private APIs, I oscillate based on my circumstances.
I use web/http for cross-platform UI work. Take (or write and then take) a crate that does HTTP/HTTPS work (depending on how much you care you really don't need HTTPS for localhost!), serve on a localhost port, and then force the user to crack a browser (or cave and use electron (via [neon](https://github.com/neon-bindings/neon)), The upside to that approach is I tend to either use or devise some protocol and it allows for interoperation work between Rust and whatever else. Most of the UIs you'll deal with will require some kind of horrific "translation" into their chosen format/approach anyway.
I'm having trouble expressing a certain lifetime relationship. [Here] is the code on the playground. It does compile, but I think it does not express the right intent, and it fails on me in a more complex context. The troubling part is impl&lt;'a&gt; Lines&lt;'a&gt; { fn from_slice(&amp;mut self, v: &amp;'a [u8]) { ... } } Note that `&amp;mut self` does not have an explicit lifetime parameter, but is a mutable reference. I could put, say `'b` on it, but I think I want to express that `'b` is shorter than `'a` (or at least, may be shorter). How could I do this? I can'a use `&lt;'a: 'b&gt;` because `'a` has been declared before (and that's indeed the lifetime I want to use here). I'd need something like `&lt;'b &lt;= 'a&gt;` or so...
Im using cargo to try and build a binary to run on an AWS EC2 instance (Amazon Linux 2 AMI). Anyone who could help me with figuring out what target I should use for the build?
Can you say which university that is?
Depends on if you are Yoda or not.
It's not obviously documented anywhere, but after groveling the compiler source for 15 minutes I can confidently report that lto=true and lto=yes are the same as lot=fat. `rust-lang/rust/src/librustc/session/mod.rs`: 573 config::LtoCli::Yes | config::LtoCli::Fat | config::LtoCli::NoParam =&gt; { // All of these mean fat LTO return config::Lto::Fat; 
&gt; The section also notes that extensions to the Coroutines TS that could statically guarantee zero allocations are possible, "These all appear to be pure extensions, so they could be done post-C++20 if need be." This seems like an extremely optimistic view to me. I tried to lay out why differences between the user-facing semantics of C++ and Rust coroutines strongly limit the degree of control users have over how allocations happen. The specification intrinsically *requires* type erasure. At best, the user can (in the future) supply an allocator that is used to allocate the stack frame, but that's much more restrictive than simply having the stack inside of an anonymous value type.
x86_64-unknown-linux-gnu or x86_64-unknown-linux-musl should both work, though musl is sometimes harder to get working (C dependencies and whatnot).
Thanks for the insight. 
I am trying to be a mitm between two [TcpStreams](https://doc.rust-lang.org/std/net/struct.TcpStream.html). But since they can both read and write, I get into all sorts of borrowing issues. The closest I've come is to have a `handle` function, that takes a `io::Read` and `io::Write` and then clone each of the two streams into two handle calls, but whoever gets it first ends up blocking the other.
At some point [TryFrom](https://doc.rust-lang.org/std/convert/trait.TryFrom.html) should help with this. For now, just make a little function: fn try_cast(x: u64) -&gt; Option&lt;usize&gt; { if x &lt;= usize::MAX { Some(x as usize) } else { None } } 
I'm working on a GUI library for Rust, however, it's very very new and very very basic. In its current form, it can't really be used for a full fledged UI, but it's got the skeletons in place to start creating some basic widgets. I'm currently in the process of working on a 0.2.0 release, which will contain more complicated widgets, a callback system with closures, and a few other features. Check out https://www.github.com/KenSuenobu/rust-pushrod/ I've spent quite a lot of time on it, and I'm still maintaining it pretty actively. This is my first Rust project, but I have pretty extensive experience with Atari GEM and Qt development, so, I'm taking inspiration from said libraries, and using philosophies from them in my lib. Feel free to comment about it - I'm trying to gain interest in it from the community.
Thanks for weighing in. I'm not sure I follow the last point—how does taking `self` in the methods and in `.build()` make error recovery less ergonomic? 
 impl&lt;'a&gt; Lines&lt;'a&gt; { fn from_slice&lt;'b&gt;(&amp;'a mut self, v: &amp;'b [u8]) where 'b: 'a { ... } }
That's an option alright. But that's a runtime check even in 64bit (by far the major usecase), and I'm sort of performance-wary (and simply interested in this stuff :)). Can this be a compile time thing? I imagine fn cast(x: u64) -&gt; usize { #[cfg(usize is &gt;= 64bit)] x as usize #[cfg(usize is &lt; 64bit)] if x &lt;= usize::MAX { x as usize } else { panic!() } or so...
Data races are technically [undefined behavior](https://doc.rust-lang.org/beta/reference/behavior-considered-undefined.html#behavior-considered-undefined) though.
Oh, and when u64 == usize the check is trivially optimized out: https://rust.godbolt.org/z/L3e5gn For future reference you can use `#[cfg(target_pointer_width = "32")]`, etc.
Here is a non-builder example: [`OsString::into_string`](https://doc.rust-lang.org/std/ffi/struct.OsString.html#method.into_string).
Ahh no, that's not exactly what I need. The lifetime of `v` needs to be the parameter given in `impl&lt;'a&gt;`.
The magic of optimization, nice, thanks a lot! 
Yeah. After reflecting on your comment and /u/burntsushi's I updated this so those functions are marked `unsafe`. 
&gt;Coincidentally, I just started writing some rust... I find this sentiment strange; people don’t look to other languages because you can’t do something in c++, but more often because you can. I'd rephrase this as follows: "people don't look to other languages because C++ is not an alternative, but more often because the alternative is C++.
Thanks for the quick response!
It's interesting. I tried to work with C++ and found it annoying, and I was bored. Since I started learning rust though, I have yet to find a dull moment and actually enjoy the language. Could be due to time passing (it's been years since I did anything useful with C++), but it just feels better. 
Wrong subreddit, you want r/playrust.
Apparently this also works, learned something new today: impl&lt;'a&gt; Lines&lt;'a&gt; { fn from_slice&lt;'b&gt;(&amp;'b mut self, v: &amp;'a [u8]) where 'a: 'b { ... } }
I think what he means is that languages like Rust *disallow* things that C++ allows, which ends up being useful for programming. For example, C++ lets you have two mutable references to a value or lets you mutate a value protected by a mutex even if you haven't locked the mutex.
I've written a simple console-based Yahtzee clone in Rust and I'm looking for someone to do a code review on it. Is there a better place to ask than here? (In particular I'm looking for a review of Rust 'best-practices' and bad form. I'm experienced enough that I am not asking for high-level code logic style reviews, but I'm open advice there as well.) https://github.com/JeremyJStarcher/rust_dice5
Perfect. 
Counter-intuitively, I liken Rust to Python, where the language helps you get something done, not become the project itself. [The huge grain of salt for this statement being the learning curve.]
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/watchpeoplecode] [Re-writing flamegraph in Rust \[x-post \/r\/rust\]](https://www.reddit.com/r/WatchPeopleCode/comments/ak6sjm/rewriting_flamegraph_in_rust_xpost_rrust/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I use arrays to store a given amount of data sharing their meaning (e.g. A list of scores, the ten last phonecalls, etc.). Whenever the data varies in its meaning, even if it's the same type (e.g. X and Y coordinates, name/firstname), I use a tuple, because the position of the data in the tuple carries meaning, while an array is more loose on the order of its items. 
I actually found C++ very expressive. const functions and const generics are great. I don't have problems with C++ in saying what I want to say. The problem is to avoid saying what I don't want to say.
I think I may have found my own solution. Posting it to get some feedback. Maybe I am doing something horrendous :) ``` fn main() -&gt; Result&lt;()&gt; { let pool = ThreadPool::new(4); let client = TcpListener::bind("127.0.0.1:9000")?; for request in client.incoming() { let server = TcpStream::connect("127.0.0.1:8080")?; let s_1 = server.try_clone().expect("Clone server"); let s_2 = server.try_clone().expect("Clone server"); let request = request?; let r_1 = request.try_clone().expect("Clone request"); let r_2 = request.try_clone().expect("Clone request"); pool.execute(|| { handle(r_1, s_1); }); pool.execute(|| { handle(s_2, r_2); }); } Ok(()) } fn handle&lt;R, W&gt;(mut reader: R, mut writer: W) where R: Read, W: Write, { loop { let mut buffer = [0; 512]; let n = reader.read(&amp;mut buffer).unwrap(); if n == 0 { return; } // Do thing with buffer let _ = writer.write(&amp;buffer[..n]); } } ```
I'm taking a look at it and will probably just send a pull request with a bunch of suggestions when I'm done! First thing I'm noticing though, is that it actually won't build. It looks like the issue is that you're using old-style paths instead of 2018 edition paths, even though it is set to build on 2018 edition?
What about this pattern: pub fn name(mut self, name: String) -&gt; Self { // ^^^^^^^^ self.name = name; self } Note that the only difference is `mut self` instead of `&amp;mut self`. This makes the function pure; there's mutation but only to a local variable and completely unobservable from the outside. 
Wrong subreddit. Might want to learn how to read and post in /r/playrust
10.0 feet ≈ 3.0 metres ^(1 foot ≈ 0.3m) ^(I'm a bot. Downvote to remove.) _____ ^| ^[Info](https://www.reddit.com/user/Bot_Metric/comments/8lt7af/i_am_a_bot/) ^| ^[PM](https://www.reddit.com/message/compose?to=Ttime5) ^| ^[Stats](http://botmetric.pythonanywhere.com) ^| ^[Opt-out](https://www.reddit.com/message/compose?to=Bot_Metric&amp;subject=Don't%20reply%20to%20me&amp;message=If%20you%20send%20this%20message,%20I%20will%20no%20longer%20reply%20to%20your%20comments%20and%20posts.) ^| ^[v.4.4.7](https://www.reddit.com/user/Bot_Metric/comments/8o9vgz/updates/) ^|
&amp;#x200B; sorry new to reddit
Aha, okay. Just wasn’t sure if this was an XY problem, but it sounds like you really do want the name of the terminal. 
I'm not a Rust expert, but I've been using it on and off for the past couple of years and have gone through the book a couple of times and made some contributions to the Cookbook. I started a project for a conference paper using Rust and encountered so many issues I ended up switching to C++ and had everything working in a day or so. The issues weren't fighting the borrow-checker, they were things like getting a grasp on structuring the project, satisfying various traits for Rayon, etc. Nothing was simple and in the end it ended up feeling like it was more trouble than it was worth. 
Update to nightly and use the --optimize-ak flag
Although there is always unsafe, which I think brings up related language philosophy. The language syntax is designed in favor of a default set of semantics that you'd want to use 95% of the time. C/C++ have similar sorts of best practices that you'd want to employ 95% of the time, but you find yourself opting-into it constantly. Which gets back to another philosophy regarding cognitive load.
I guess your mileage might vary. I started using Rust for work, and was fully productive after a couple of weeks. I wouldn't even have dared trying to write it in C++, because I didn't know what I didn't know, and the C++ wasn't going to tell me, whereas the Rust one wouldn't let me do anything stupid. I suppose this calculus would be pretty different if you already know C++ though.
worth trying lol.. 
Actually, I don't care about windows. I'm aiming for Linux, macos 
Is that native? How is the license?
I appreciate you taking a look at it. I'll have to look up the difference in how the paths work -- thanks for the head's up. I figured there might be some version differences, so here is the version I built and ran with: rustc --version rustc 1.32.0 (9fda7c223 2019-01-16) 
this sub reddit is dead m8 idgaf
We got nearly a [10x speedup](https://twitter.com/Jonhoo/status/1089314148708728838) over `stackcollapse-perf` in the end!
I'm glad you clarified this. 
I mean is the package written fully in Rust not any c++, secondly it’s not attached to any platform so I can have my own design for widgets such as buttons in different looks than the Os style. And of course be able to compile and run on linux and macOS platforms. I really don’t care about windows platform 
That’s grate, but I realised that your package is requires few more packages to be installed. Your package cannot be standalone ?
Example 1, support both or neither. The common convention is `(x, y, z)`, so you could follow the convention explicitly, assuming you never go more than three, or you can require them constructed by name (which is very easy for them if they share the variable name). Example 2, I really don't like sharing u32 for the type. Make a simple enum or wrapper for them, so you have the tuple parameters typed themselves, such that, they can't have the wrong order. `SomeStruct(u64)`
I think it probably depends on how you define productive. I was able to quickly write a working project of medium complexity, but as I started trying more non-trivial things I quickly ran into road blocks. The other part of the frustration was figuring out newer changes like properly structuring a project using modules - I found a bunch of cursory or outdated info but nothing in depth enough. I eventually got things working but it felt like a wasted hour or two. With some of the more recent changes it’s a bit of a struggle filtering out relevant posts and previously asked questions that are no longer applicable. Small things like that added up.
I've also had some trouble with finding non-trivial application guidance in Rust rather than library tutorials. I've been planning to make my own blog series on this for non-trivial examples, so hopefully for others you can share your knowledge too.
I think this is a great idea
It's important to keep in mind that even if Rust allows 'unsafe', thereby circumventing the borrow checker at compile time, the borrowing rules are still enforced at runtime, wherein any violation of the borrowing rules at runtime will result in panic. This is much better than what C++ allows at runtime. Therefore, Rust is still 'memory safe' even in the case where 'unsafe' code is allowed.
I'm trying to work on Servo and I'm having some trouble getting a text editor/IDE working. CLion with IntelliJ Rust plugin is giving me a lot of unresolved reference errors, while VSCode with the rls plugin just refuses to give me definitions for a lot of things. Any idea on how I can fix either editors? An example file would be: https://github.com/servo/servo/blob/master/components/script/dom/htmlanchorelement.rs
I like how quickly dude changed his sentiment once John Freaking Carmack came in with the slightly dissenting viewpoint.
It took me a solid two months to get acquainted with the language (I've had roughly 15 years of programming experience before that). But once I got going, it stayed out of my way. Python took me a couple of weeks to get productive in. Obviously, the two languages are quite different, but that's my experience.
Wait what? Like I have x : &amp;mut int32 and I do var y : &amp;mut int32 = unsafe { var z : *int32 = &amp;*x; &amp;*z } *x = 4; *y = 5; this will panic? (pardon my syntax errors) I can't see how it would be possible for the borrow checker to check whether that pointer was multiply borrowed mutably. Now, there are standard-library things (e.g. RefCell) that do runtime borrow checking for you, but those aren't unsafe, they just panic if you use them incorrectly.
Because those things play right into memory safety. Sure your c++ version was probably eaiser to write. It also likely has memory safety bugs 
I’ve made some fairly terrible mistakes resulting in silent memory corruption and undefined behavior via unsafe in rust. If you’re lucky you’ll segfault but that’s still not as nice as a panic. It’s just as likely the program will carry on until you try to do something useful with the corrupt memory.
Well he also said that C++ is strongly-typed so...
Oh wow, I missed that...
Fake news. What you said (moving checks to runtime) applies to [`RefCell`](https://doc.rust-lang.org/std/cell/struct.RefCell.html), not `unsafe` blocks.
I appreciate the sentiment, but I don’t think this is helpful to the discussion or to the Rust community. Of course we all know a big part of Rust is memory safety. Telling someone their code is full of memory safety bugs (without seeing it!) isn’t helpful to anyone and comes across poorly. Anyways, it’s not a big deal but something to think about in the future!
Is the builder pattern common in Rust due to lack of keyword/optional arguments?
I'm using `actix` and `actix-web` to make a simple web server, and I noticed that `HttpServer`'s `bind` method takes a full socket address instead of just a port. With other web servers I have worked with (such as `express` for node.js) I have not had to provide a specific IP. Since I am hosting this application on Heroku, how should I address this? I know Heroku provides a `$PORT` variable that you must use. Do they also provide one for IP address? Is there a way to find the correct IP address from rust itself? Any help is appreciated! Thanks!
Late here. The file global decorator is very Rusty :)
I don't think this is true. Arrays are bounds checked, and integers panic on overflow (at least in debug mode), but the special capabilities unlocked by `unsafe` don't panic if used incorrectly. Instead, they cause undefined behavior - that's what makes such code *unsafe*. The [nomicon](https://doc.rust-lang.org/nomicon/what-unsafe-does.html) has the following to say on the issue: &gt; The reason these operations are relegated to Unsafe is that misusing any of these things will cause the ever dreaded Undefined Behavior. Invoking Undefined Behavior gives the compiler full rights to do arbitrarily bad things to your program. Among the things listed as undefined behavior is: &gt; - Dereferencing null, dangling, or unaligned pointers &gt; - Breaking the pointer aliasing rules &gt; - [Producing] dangling/null references All of these are things that you would do by "circumventing the borrow checker at compile time." There's no way a language in which "any violation of the borrowing rules at runtime will result in panic" could work whilst achieving the aims Rust does. Trying to check whether a pointer is dangling any time it is dereferenced would require some sort of garbage collection and be grossly inefficient (at least for a systems language). It's Rust's *compile time* checks that help it accomplish safety. When you use `unsafe`, there are no safety rails anymore.
I think there's a decent chance WebAssembly could come close to this "single language", but of course only because so many different languages can or will compile to it.
&gt; A function `fn(&amp;mut T)` can essentially be thought of as an optimized version of `fn(T) -&gt; T`. Almost, except that you can turn the former into the latter but you can't go the other way around without [going through some hoops](https://docs.rs/take_mut/0.2.2/take_mut/). After all, what would happen if you were trying to use the latter on an `&amp;mut T` and it panicked?
borrow checker is compile time only. nll was a compiler feature. because borrow checking is like type checking, compile time only (in langauges without reflection). 
as a rust intermediate, rust does become the project. i totally understand what the borrow checker is doing. i just don't know enough about how to structure my program to get the borrow checker to accept it. maybe my approach is inherently wrong, but i don't want to sit down and plan ownership patterns for one off command line programs. "so just write in python", sure, but i *want* to write in rust. one of the things I've read that the steering committee or whatever is that there is a lack of docs for intermediates, like myself. i think this is actually pretty good self awareness on their part, and i hope it moves forward.
Is that wrong? 
Part of the design philosophy of c++ is to be 99% compatible with C, that is not all c code works in c++, but enough should work that it compiles with minor changes (this is how stroustrup puts it in 'the c++ programming language'). This decision is arguably the reason why c++ is such a popular language historically, because it has access to a wide list of decades of libraries out of the gate. But it also means it's impossible for c++ to enforce safety the way rust does, otherwise tried and tested libraries that are used everywhere would never compile. This is why almost all safety in c++ is opt in. Rust obviously doesn't have that requirement, so it can focus on default safety out off the gate, and provide tools for making unsafe actions when needed. 
Indeed, the panic safety is the "essentially".
You're right lol
Referring to WebAssembly as a programming language is comparable to referring to compiled x86 machine code as a programming language. It makes the term "programming language" less useful.
C++ will happily coerce all sorts of types. One of the most common ones is turning string literals into `std::string`s. It's actually nice a lot of the time (not having to write the explicit function calls like in Rust), but it can also be a source of headache in all the ways you could expect type coercion to be.
I suppose it depends on what you mean by strong typing. Personally, I'd want to hear a bit more from people who say C++ has strong typing, because the language has things like well-defined rules for casting between pointers and integers, and well-defined rules for casting away `const`. These things exist for good reason, but they are all about doing things contrary to what the type says.
In rust, safety guarantee is orthogonal to no-panic guarantee. Taking two mutable refs on a `RefCell` will cause a panic that will kill the thread before it does any real damage. This is *much* safer than a segfault or memory error that can occur in `unsafe` code, which on certain architectures can expose security vulnerabilities.
It wouldn't cause a panic or segfault, because pointer aliasing is not undefined behavior. You do this sort of thing in C all the time, and works, but can introduce bugs if you aren't careful. It is unsafe behavior because the compiler cannot guarantee that it isn't undefined.
That was a lovely read. Thanks for sharing!
Thanks :)
One person's right is another's responsibility. The more restrictions there are on the implementer, the more guarantees there are for the caller.
I like how you show the differences between how Rust handles closures in the type system compared to how other languages don't handle closures any differently from other functions. But I do have two gripes with this blog: &amp;#x200B; \&gt; Closures that is \`Copy + Clone\`: they are almost as powerful as \`Fn\`, because they can be called without restrictions. Combine with \`FnOnce\`it gives the best flexibility: the callee have no restrictions as well! &amp;#x200B; This is wrong, the callee still has the restriction that everything they borrow must be \`Copy\`. &amp;#x200B; \&gt; One thing I didn’t mentioned, is that \`FnOnce + Copy\` implies \`Fn\` &amp;#x200B; This is wrong, they may be used in similar ways, but one does not imply the other. Also, \`Fn\` doesn't just mean I can call this closure multiple times, it also means that I am free to share this closure and call it from more than one place at once. This is impossible with \`FnMut\` and \`FnOnce\`, because if you have \`Copy\` you are now not calling the same closure multiple times, but different instances of the same closure, which as you showed, has different behavior. &amp;#x200B; &lt;sub&gt; shameless plug: [Closures: Magic Functions](https://krishnasannasi.github.io/rust/syntactic/sugar/2019/01/17/Closures-Magic-Functions.html), here I walk through how closures are desugared, which demystifies closures and explains everything about \*why\* they function as they do &lt;/sub&gt;
There are a number of concepts that you need to be familiar with in advance, before you can be productive in Rust. It's not really the fault of Rust, though. Rust is a rather sophisticated tool that can accomplish some really complex feats, but you have to master it to take advantage of those capabilities. I write software in Rust full time at my job, and have been at it for a little over a year now. I haven't struggled with with any issues specific to the language since the day I was hired. Though I've been writing software in Rust since 2015. My concerns from day to day are more about handling corner cases and writing new features.
`express` does the standard thing and defaults the IP address to either `0.0.0.0` or `::`, depending on if you're using IPv4 or v6. So you're probably good if you just use `0.0.0.0` as the IP.
Wait seriously? Ah man I looked into that but I didn’t think it would work 😂 Guess that’s what I get for not trying it. Thank you so much!
No, it relies on side projects that translate OS events into an event loop that can be handled by an underlying library. It also requires a separate 2D drawing library to handle the blitting of data to the screen. Doing all of that myself would be a monumental task.
Yes it works. thx you guys.
For sake of comparison, I did a check with `htop` on my Mac, and I got 10% CPU usage from the draw loop alone. This indeed seems to be a problem!
Thanks for the time you took to explain. I'll carry on with staticbitset in the hopes that it fills a niche.
Random question: do you use the font IBM Plex Mono?
I don't remember how that specific aspect has changed in the 2018 edition but, in the 2015 edition, that's an absolute path and allows your project to reference the contents of crates that you've specified in your `Cargo.toml` but not `use`d.
On a different note, instead of .flat_map(|value| value) Why don't you use .flatten()
so can we say that this is obsolete? 
Aren't those just operator overloading?
Rust let's you have two mutable references too, either as refcells on one thread or in unsafe blocks across threads. Its just obvious if memory races then arise where they happened.
&amp;mut self doesn't actually work in example, it's a mistake.
It's due to lack of null for me. I'm strictly against having optional for something that isn't optional. I think it has nothing to do with optional keywords arguments, it's just allows code to be cleaner with very minimal overhead when optimizations applied. To me it makes code more flexible - public signature of function stays the same, sensible defaults don't break existing code, 1:1 mapping when you enable RPC access to your code. Easier to write C bindings to your code.
Idk, I think benchmarks should try to use same version of LLVM. Otherwise you benchmarking LLVM instead of rust. That's all.
Yes.
X11 actually has its own UI toolkit that works with vector graphics.
&gt; The disadvantages is the unfortunate interaction with ‘?‘ where ‘await thing()?‘ does not do what you expect. What do people expect it to do, and what does it actually do?
OP's bio includes the words "Parody account."
In the 2015 edition, a path starting with `::` is relative to the crate root. (i.e. any item defined in the root, any extern crates, core, std, etc.) In the 2018 edition, a path starting with `::` can only start with a name of a crate from the extern prelude (i.e. any crates passed in with `--extern` flag or `extern crate` items). You can read more about path qualifiers here: https://doc.rust-lang.org/nightly/reference/paths.html#path-qualifiers
Thanks for the feedback, all. I've recently release version 0.1.4 up on Crates.io, and I encourage you all to take a look, and run `cargo doc --no-deps` to read up on the docs. ;-) I'm working on some major optimizations for 0.1.5, which should increase the speed of the run loop significantly, as well as some fairly substantial documentation. I'm still on track to keeping this library simple, fast, and easy to use. New graphic functionality is coming (a unified graphics library), and support for signal masking is also due. I'll make another major announcement soon. 0.2.0 is on its way, and it's looking good so far!
On the otherhand, that kind of sentiment can discourage contribution. For example, I guess i support that sentiment, but as a result i rarely participate in discussions like this out of fear i'm not adding value, or enough value, or even others disagree. I subscribe to a few issues/RFCs and theres usually next to no discussion except among team members, so i suspect i'm not alone in this. "Does my opinion as a "mere user" rather than member/contributor add value here?" and all, i guess. Issues and RFCs from This Week In Rust, so they should get some decent visibility at least. But the only ones that seem to generate any discussion are controversial ones or super popular bikesheds, like non-ascii idents or async. Managing discussions sure isnt easy
&gt; the callee still has the restriction that everything they borrow must be `Copy`. Yes I didn't mention that. I just thought it is obvious. I assume people will think what their closure needed first, and then decide what to do with them. `FnOnce + Copy` gives all the freedom to the second, but restricts more from the first. &gt; Also, Fn doesn't just mean I can call this closure multiple times, it also means that I am free to share this closure and call it from more than one place at once. The fact is, if you implement the trait yourself, you will find that once you have implemented `FnOnce + Copy`, you can implement `Fn` as well, regardless however the action `call` being implemented. Also, unless you dig into `unsafe` code (and I doubt even it can), there is no way to observe the difference between a true `Fn` and a `Fn` on top of a `FnOnce + Copy`. Not only that, you can implement `FnMut` on top of `FnOnce + Copy` in the same way, this `FnMut` will behave the same as the `FnOnce` and the difference cannot be observed in the outside world. This is also partialy true for `FnMut + Copy`: You can still define `Fn`. but now the behavior of this `Fn` is different than the `FnMut`. Furthermore, the `FnMut` implied by the `Fn` behave like the `Fn` not the original `FnMut`. This means you have two `FnMut` that behave differently.
I'm responding to /u/topostheory who claimed that the borrowing rules (which absolutely do disallow pointer aliasing) are enforced at runtime. I'm pretty sure they aren't, because doing so in the presence of unsafe code would require an expensive abstraction around borrowed values to verify that they weren't aliased by other borrowed values.
&gt; Yes I didn't mention that. I just thought it is obvious. I assume people will think what their closure needed first, and then decide what to do with them. A lot of people view closures as magic, so I wouldn't assume that in what seems like an introductory post to closures. &gt; The fact is, if you implement the trait yourself, you will find that once you have implemented FnOnce + Copy, you can implement Fn as well, regardless however the action call being implemented. Also, unless you dig into unsafe code (and I doubt even it can), there is no way to observe the difference between a true Fn and a Fn on top of a FnOnce + Copy. Yes, that is true, I didn't notice that.
&gt; std::vector&lt;std::string&gt; f {{"a", "b"}}; &gt; Wait, what constructor is being called here on the inner `std::string`?
Thanks for you answer! Currently i'm switched from array to vector, but if i decied to switch back again - i defenitly check you crate.
The braces in your `use` clause are unnecessary. You can remove the `String::new` from `all_chars` to save an allocation and use `len()` to calculate the length, as you have no non-ascii chars. Finally, not all of us are guys. Please use more inclusive language here. I personally use "folks", but "people" or "y'all" seem to work well, too.
Off topic: I’m not a native speaker, but I’m sure I heard women adress other women as “Hey guys” before. Maybe it can be used for both. 
 template&lt; class InputIt &gt; basic_string( InputIt first, InputIt last, const Allocator&amp; alloc = Allocator() ); The `const char*` arguments are valid iterators, but attempting to construct a `std::string` from them does some great UB. The puchline to this situation (which I forgot of course) is that removing one pair of the braces makes this line do exactly what's expected.
That just serves to reinforce the notion of the male default. We strive to do better here.
Not that I can really do anything about it, but as a semi-prominent Rust and gamedev tweeter I'm pretty annoyed by that twitter thread. I try to push C++ as hard as I can to accomplish my correctness and stability goals, and in the end it's just not good enough. The specific reason isn't not good enough is because all good practices in C++ are explicitly opt-in. I'm exaggerating only a little to say that I don't think it has a single sane default behavior. Rust feels cohesive and modern, but one of the things that has and will continue to make it great is its "correctness by default" attitude. I hope that property of Rust is never compromised, and the areas where it's not quite true... like unwrap prevalence... can be improved as time goes by.
Which again begs the question of how you ended up here 
All right. Got it.
But it makes sense to do that because the extra pair of braces means that you’re value initialising the vector with a single std::string, rather than initialising it with two strings (“a” and “b”). Any C++ developer would see that.
I am sorry for my bad language and that I offended you, I was angry because I really misunderstood your idea and I am really sorry ! I was thinking that you had chosen Rust just because it was popular and you are ashame with the current name. Thank you for your kindness and please forgive my bad behavior.You will not hear bad things from me again, I was stressed and I made the mistake to act on stress and say bad things. I am really sorry ! 
&gt; A lot of people view closures as magic, so I wouldn't assume that in what seems like an introductory post to closures. I have already updated the text in response to this. Thanks for your advices anyway.
Anyone have an idea of what Rust features he might be referring to that are also “surprisingly” available in C++? ...obviously it’s not the borrow checker!
The programmer might like C, it allows you to hack anything at any point without any actual thought how to do it properly. As long as they are not held responsible for the problems this approach causes it is the best contract. Users on the other hand would like the promise of absence of several classes of errors, even if it means writing the code takes a bit more design up-front.
This page suggests that it is still unstable https://doc.rust-lang.org/std/primitive.never.html
Of course you always can **not return** from function, regardless of it return type.
Ha! This is *exactly* the same thing I’ve run into at work this week as well! I was trying to iterate through fields of an enum to generate a `switch` statement that would convert integer to an enum value. Sadly, Rust has its own, fairly similar, limitations. For example, you cannot yet implement a trait for arbitrary tuples or fixed length arrays. This is why you see [so many almost duplicate implementations](https://doc.rust-lang.org/std/primitive.tuple.html#implementations) of traits and all of them stop at [certain count of elements](https://doc.rust-lang.org/std/primitive.array.html#implementations).
I can't be to sure, but i often hear – from C++ programmers that took 5 minutes to look into Rust – that C++ has smart-pointers, linters, ASan/TSan/MSan, etc ... (a million other tools), so they don't need Rust. 
Well, he's got plenty to rewrite :-p
I second this. Rust emphasizes "composition over inheritance", but arguably the next logical step is "entities over composition". It's a very generic and common wish to structure related items and their interaction in meaningful ways. While I'm not sure if this is something that can or be a language primitive, or whether a single implementation should be part of stdlib, at the very least a curated approach to this would be helpful (just like to other important crates such as `rayon`), and see how things evolve from there.
&gt; It wouldn't cause a panic or segfault, because pointer aliasing is not undefined behavior. That's not true actually, Rust is not C, `&amp;mut`s [cannot alias other multable references or pointers](https://doc.rust-lang.org/reference/behavior-considered-undefined.html) as they are marked `noalias`. So while what they wrote probably won't segfault, what they wrote *does* invoke undefined behavior and the LLVM output will likely be nonsensical.
I may be mistaken but I believe they're implicit ctor calls: any single-argument ctor which is not marked `explicit` can be invoked implicitly to reconcile a source and a destination types (I don't know if they can be chained though). So `std::string f = "foo";` will implicitly call the `string (const char* s);` ctor.
&gt; Any C++ developer would see that. Sorry I got off on a tangent, the point of this isn't whether C++ is comprehensible. The point is how pervasive implicit conversion is.
This is being worked on under the name of ["const generics"](https://github.com/rust-lang/rust/issues/44580).
Which is why most C++ guidelines forbid implicit constructors. &amp;#x200B; I think forbidding them is too harsh, but I do think 'explicit' should have been the default. If only Microsoft Research had finished that time-travelling standard writer.
How i can display only one part of image? I use piston crates and piston image/graphics crate don't have such function(at least i think so). So i found imageprocess crate(that have some "Rect" and "Region" stuff, that, i think, it's what i need), but it's too difficult for me. Maybe anyone already make such thing and can give me an easy example? Or maybe you can advice another crate to make what i want? Thanks!
&gt; Which is why most C++ guidelines forbid implicit constructors. That doesn't help with the dozens of existing ones which are there either for convenience or backwards compatibility or support of other new features though (e.g. C++11's std::string gained a move ctor and an initialiser list ctor, both are implicit). &gt; I do think 'explicit' should have been the default. That would probably have been a good starting point. Or use an other easier to inspect/notice mechanism for the definition of implicit conversion hooks (I guess you wouldn't do without assuming you want to easily bridge C and C++ at least).
Your formatting is a bit wonky. I think reddit formatting prefers the 4 space prefix for code (aka `sed 's/^/ /'` your input): // map function names to functions in the sub module macro_rules! map_functions { ($($i:ident),*) =&gt; { { let mut map2: HashMap&lt;&amp;'static str, &amp;Fn()-&gt;&amp;'static str&gt; = HashMap::new(); $( map2.insert(stringify!($i), &amp;sub_module::$i); )* map2 } } }
By the way: Clippy reports it as an anti-idiom (https://rust-lang.github.io/rust-clippy/master/#option_map_unit_fn).
Why 'const generics' and not 'variadic generics', at least for tuples of unknown length?
is that a UI toolkit or just a vector graphics library (i'd heard of the latter)
Nice to see some major projects starting to use tower-grpc! In my so far limited [experience](https://github.com/input-output-hk/rust-cardano/tree/master/network-grpc), the Tower stack exposes a lot of complexity as generic abstractions to the application developers, perhaps to avoid forcing some significant choices on them. For a gRPC service implementation, you need to supply statically defined types for asynchronous responses, which can be a chore. In contrast, Actix heads this off by letting the server application return boxed dynamic trait objects. While this may be a performance tradeoff (questionable though &amp;ndash; surely the ongoing requests have to be managed as a collection of boxed dynamically dispatched objects at some level), this approach is definitely easier to use.
For user defined types only one implicit conversion is allowed, so you cannot chain A -&gt; B -&gt; C. But non-user defined types can have multiple implicit conversions. Source: https://stackoverflow.com/a/867647
Hrm, then my opinion differs from Clippy's. 
Ah, sorry. I believe const generics only help for fixed size arrays. Tuples would indeed need [variadic generics](https://github.com/rust-lang/rfcs/issues/376), but don't expect them anytime soon. As far as I know, there hasn't even been a formal RFC for them yet.
There are two concepts: strong vs weak typing, and static vs dynamic typing. Python is dynamically and strongly typed. C is statically but fairly weakly typed. Wikipedia will do a better job than me at explaining what the terms mean.
&gt; Yup, basically this. If you feel like your position has already been represented in the thread and is being heard, please don't spam "I also think this" comments. This is a big reason I don't think Github is the best place to discuss design because a threaded display is a much more intuitive approach; it's so much easier to catch up if you can start at the top-level comments and work inward. It also has the added benefits of at least burying most of aforementioned kind of comments within those threads. At the same time, I don't really see the issue with allowing Github reactions to represent popular opinion; if nothing else, it keeps the noise in the thread down. I get that the reactions can seem sometimes quite petty and getting a lot of thumbs-down can be discouraging to commentors who actually had something to contribute whether people agree or not, but getting dissenting replies would be no less discouraging--it just requires more effort on the dissenter's part. I can understand some people not wanting to use Reddit, though; the voting system would be even worse for this. However, I can't really think of another discussion platform that treats threads as a first-class element of the UI.
Mm, but is this really a data race rather than just a logic race? I don't see how this can result in any sort of memory unsafety as a data race might. So I think it should not be marked as unsafe, as there is no way to cause memory safety issues through its use.
Not suprising; the perceived "danger" to their own comfort zone is much greater for C++ programmers. It's much easier to sell Rust to e.g. Python programmers, since they're likely to see it as another tool in the toolbox which doesn't want to replace Python. (Even if it does, eventually...)
It does, but his tweets indicate he is serious in his opinions. It seems to me it's an escape hatch when people start showing him how wrong he is.
Damn. I was quite excited to see the 'nearly done' comments on that first thread. There are a few things I'm working on that would benefit from not needing to macro generate dozens of impls for different size tuples.
If you want a full blown IDE, I'd try CLion with the IntelliJ-Rust plugin. If you're OK with something lighter, look into Visual Studio Code with the Rust plugin - it also supports debugging and linting via Clippy.
Not going to lie I don't remember. I'll try to find it tomorrow. Technically speaking Xt is a toolkit, but doesn't have any widgets.
A good place to start looking at what is being built could be [awesome-rust](https://github.com/rust-unofficial/awesome-rust).
Perhaps you would like to check [https://areweideyet.com/](https://areweideyet.com/) for additional information.
I think Clion is a very good IDE, I have used it with C++, I will try it
Thank you
Well, you have two options really: anything that speaks LSP or anything runs intellij idea plugins. Both have up and downs. VS Code, that you don't like, is the only one that supports debugger out of the box. I guess define what is IDE to you?
I do agree that a `SecureBuffer` would be useful. `zeroize` also mentions that `mlock`/`mprotect` is out-of-scope but I believe that is because it should be more portable. There are other crates, such as `memsec` that AFAIK also protect against copying memory to swap space, but then again this has dependencies and is not as easy to use across different platforms. I don't believe that it is only `Vec` but also Rust in general, as the 'nomicon [mentions](https://doc.rust-lang.org/nomicon/what-unsafe-does.html) that it is deemed "safe" to not call destructors.
IDE to me is a software that brings as much as it can features like building and so on for working with specific languages. I know a text editor can have IDE features, but they don't make it an IDE because it's based on packages/extensions that are not stable. I hate Visual Studio Code because of it's way of complicating things, creates useless .vscode folders. I want something stable, not something that might break with big projects, not something that has a lot of depencies, having to download all of the depencies when moving on a new machine. VSC will never be in my "box of good text editors". I didn't find one that hasn't stressed me with the bugs that I find or depencies hard to configure. I like to work with IDE-s. 
&gt; but they don't make it an IDE because it's based on packages/extensions that are not stable. That's nonsense. &gt; I want something stable, not something that might break with big projects, not something that has a lot of depencies, having to download all of the depencies when moving on a new machine. What is an IDE if not a pile of dependencies?
Try and understand my point of view, with an IDE those depencies are tested a lot by a specific team and developed regulary. With an editor you depend on packages that might be abandoned at one time and I don't want to work on extensions for an editor. It's not an nonsense, just think about it. I am not arguing with you, I love sharing my thoughts with people that respect me. 
&gt; with an IDE those depencies are tested a lot by a specific team and developed regulary. This is true for text editor extensions as well. Not for all of them, of course, but if it's an extension people rely on to get their work done, you'll usually find excellent code quality. &gt; With an editor you depend on packages that might be abandoned at one time This is true for an IDE as well. You seem to confuse things here. An IDE is not inherently more stable than any other piece of software. Nor is a text editor less stable. Dependencies aren't unique to either style either. I'm not saying that there aren't good reasons to pick an IDE. What I'm saying is that the reason you list here are not the reasons you'd want to pick an IDE over a text editor.
An API for streaming parsing of PCAP files would seem useful. Currently the API seems to be built around a `Reader`, but this does not allow to incrementally ready PCAP data from a stream. You can't simply parse all data you currently have, collect all parsed packets that are in that chunk of data and then proceed to wait/request further data and repeat. At least not without involving threads and implementing a blocking `Reader` around some data queue that is filled from another thread as needed.
Hmm, Intellij Ultimate is an ultimate IDE for Java, it heavenly dependent on extensions and create stupid .idea folders... There is no purpose built IDE for rust and won't be anytime soon of ever. You probably want CLion + intellij-rust. Majority of crusterians use VS Code. I use CLion and NeoVim. You probably want to learn on how to manage your editor settings. I can move my NeoVim setup to any machine within 5 minutes and it will work doesn't matter what OS it is (I use two different with the same configuration)
I am not going to argue with you, you are too right about some things. I don't like text editors and that's it.
He just wants something like Android studio for Android, VS for C#, Idea for Java, PySide for Python. Which rust will probably won't have anytime soon.
I will give NeoVim and VSC a chance
Eventually I believe the plan is to allow macros in the same crate to be `use`d just like any other item (similar to how it works currently in the 2018 edition across crates) at which point order will hopefully no longer matter and we'll be able to do away completely with the old macro importing mechanism...
Yeah, I don't argue that people shouldn't want an IDE -- whatever works for you. However, the arguments that OP listed for not wanting to use a text editor just aren't founded in reality. And I think it's important to keep things straight in your head. You want an IDE and don't like to use text editors for your work? Nothing wrong with that. But trying to rationalize this preference with arguments that simply aren't founded in reality is how you breed a toxic 'us vs them' mentality.
The program will panic when the stdout is closed, but it can be a hard to make a small reproducible. Here is one test case that panics. Using -q, grep will close the stdin as soon as it finds the first "Hello" string. I ran into this problem when I did \`rustup show | grep -q nightly\`. &amp;#x200B; fn main() { for i in 0..10000 { println!("Hello, World! {}", i); } } &amp;#x200B; $ cargo run | grep -q Hello Compiling println-panic v0.1.0 (/home/niklas/projects/rust-test/println-panic) Finished dev \[unoptimized + debuginfo\] target(s) in 0.38s Running \`target/debug/println-panic\` thread 'main' panicked at 'failed printing to stdout: Broken pipe (os error 32)', libstd/io/stdio.rs:700:9 note: Run with \`RUST\_BACKTRACE=1\` for a backtrace.
Oh yeah, that seems to express what I want, thanks! But it's not the same as above... I kinda missed the where clause, that seems to give the flexibility needed ;)
Then the 2 implementations are equivalent, that's just a style preference.
StructOpt author here. StructOpt can't exists without clap's builder pattern.
Did you read https://doc.rust-lang.org/stable/book/ch09-00-error-handling.html? It explains the basics where well. 
What are your thoughts on what is limiting the performance of this method? I am aware of the rather large overhead using three u64's on top of the kv-pair, my use case is actually for large-ish (40 bytes) kv-pairs. Any implementation starts to stutter when the data does not fit into memory, thing about using memory mapped files is that you hand over the caching to the OS, which might not be ideal for every use-case? Would love to hear your thougts!
So [Inferno](http://www.vitanuova.com/inferno/) has changed from being an OS to flamegraph port? 
A pile of dependencies tested by the same team and developed to actually work together and not in their own little silos.
`::` is equivalent to `crate::` in the 2018 edition. 
Interesting! Thanks for checking in. Could you expand on this?
&gt; For example, I guess i support that sentiment, but as a result i rarely participate in discussions like this out of fear i'm not adding value, or enough value, or even others disagree. Same here. Most often by the time I am aware of a discussion, there's already been somebody voicing my point of view (or several persons voicing part of it), so I don't have anything to add. They're not always worded exactly as I would have, but as long as they are articulate enough that I think the point is getting through, it's good enough for me. It's comforting, in a sense, to know that my "vision" for Rust is shared by other members of the community. 
As a C++ convertee myself, imo a lot of it's some sunk cost, a hint of masochism, and simply not knowing what its like to have a sane language in the first place. After all, "*I* *know* what i'm doing, and theres plenty of tools for checking that kinda stuff, and cmake exists, `std::move`, C++20, doesn't sound like Rust has anything new", so one may think. The culture of being "clever" doesn't help either.
Yeah, pretty much exactly the same for me. I'm likely to thumbs up things like that though, but also usually only if someone else already has since "standing out" is scary.
The common case is that a function foo returns an `impl Future&lt;Item=Result&lt;T, E&gt;&gt;`, thus you would like to await first and then propagate the error. However, `await foo()? ` is interpreted as `await (foo()?) `. To get `(await foo())? `, the common case, you will always need parentheses. You would expect `await foo()? ` to just work as await-and-propagate, but it actually works as propagate-and-await.
&gt; but i don't want to sit down and plan ownership patterns for one off command line programs. "so just write in python", sure, but i want to write in rust. You could just `clone` or `box` things then. It'd be a big ugly and certainly not idiomatic, but it'd work fine, especially for a one off programs, and not be any worse than what Python is doing anyway. It'd probably still be better than Python too, less to keep track of, smaller objects, proper types instead of runtime reflection. A few copies or clones here or there, while not ideal for a more serious project, should be fine for one offs.Of course, theres always the desire to write code the "right way", to be "idiomatic" and "look good".
&gt; but i don't want to sit down and plan ownership patterns for one off command line programs. "so just write in python", sure, but i want to write in rust. You could just `clone` or `box` things then. It'd be a big ugly and certainly not idiomatic, but it'd work fine, especially for a one off programs, and not be any worse than what Python is doing anyway. It'd probably still be better than Python too, less to keep track of, smaller objects, proper types instead of runtime reflection. A few copies or clones here or there, while not ideal for a more serious project, should be fine for one offs. Of course, theres always the desire to write code the "right way", to be "idiomatic" and "look good". But sometimes it's ok for it to just "work", even if it's "bad". Bonus points if errors are handled.
You are inspiring! Are you going to port perf next?
Because a data race is UB. The compiler is legally allowed to do literally anything.
Thank you for making this! I love the amount of solutions to "The JavaScript Problem" that keep popping up. I'm coming from PureScript/Haskell, and one difference I noticed from the frameworks I've used there is that the HTML and CSS attributes are stringly typed in Seed. Have you thought about implementing them as enums, to get some additional compile time validation? 
There is actually a weekly thread inviting everyone to share what they are working on: https://www.reddit.com/r/rust/comments/ai7640/whats_everyone_working_on_this_week_42019/ I invite you to have a look.
There doesn't seem to be any basic example shown there with it explaining custom error handling (as far as I can see).
&gt; Any C++ developer would see that. The [No true Scotsman](https://en.wikipedia.org/wiki/No_true_Scotsman) fallacy is a very poor argument.
I would note that the latter is, unfortunately, *also* Undefined Behavior, in case it's not clear. That is, `0` is converted to a null `char const*` since there's a `std::string` argument taking `char const*`... but unfortunately it's Undefined Behavior to call this constructor with a null pointer. And there's not even an `assert` about it.
Actually, `RefCell` will panic at run-time if you attempt that. `unsafe` will not, for now, however Ralf Jung (Rust Belt project) has been working on enhancing the MIRI interpreter to correctly detect such violations in `unsafe` code, though not across threads AFAIK. Of course, executing the whole program "interpret" may not be practical, but executing unit tests should be.
I looked at it a bit and it's quite interesting. I have never tried to implement an equation parser, so seeing how you did it is interesting \^\^ Let's come to the requested feedback. * First of all, I formatted everything with rustfmt. Normally I don't like that for my own code, but it splited up many one-liners and that made the code more readable to me. * In lib.rs I would `use crate::equation::Equation;` so one can omit `equation::` in `equation::Equation::new()` below. * `fn solve_component( ...` and `fn solve( ...` are really long. Maybe you can split them up into several functions and reuse some parts, like simplifying? I really like the use of enums and matches. * The recursive `Component` enum perfectly models algebraic expressions. * Matching on the operator is clearly readable. * Rust's enum deconstruction syntax really shines here. Just be careful to not get lost in the tenth ident level. * Also there is no overengeneering with advanced features of Rust. Simplicity is good. I didn't dive deep into the details, but I hope my feedback is useful to you anyway. Btw, this was my first review here on reddit \^\^
&gt; btw, the solution is to use write!() which does give you the underlying os error. `write!` does give the underlying error, but rust apparently never bubbles up EBADF (ENOSPC may also be an issue, but there's no /dev/full on OSX and I can't be arsed to create an emulation via a ramdisk) so `write!` always returns `Ok(_)`. Even `write!`-ing kilobytes of data in a loop with a closed stdout never ends. Also FWIW in the description of the issue you're talking about stdin, it should probably be stdout.
Thanks yeah you are right, I meant stdout instead of stdin. It's a shame that all the errors are not bubbled up.
The nice thing about Rust is that's it's very pedantic about memory usage: for day to day usage, on an existing project, it's just *great*. The annoying thing about Rust is that's it's very pedantic about memory usage: it's a big road block when starting a project, because the compiler refuses to let you play until the architecture is just right... and for an exploratory project, that's non trivial. --- If I may, my advice is to forget object-orientation, module-structure, etc... and focus on **data**. The Rust programming language, due its pedantic borrow-checking, is all about data-flow. Forget about everything else until you have figured out the pipeline: that's your high-level blueprint.
One step at a time :) In the coming year(s), the generics of Rust will see (a) const generics and (b) specialization. It'll take time to explore the implications of those features, and therefore better understand the potential interactions with variadics.
I like this idea. It already validates Tags and Events using enums internally, but this could allow for . The downside is lost parity with established naming conventions (Eg AcceptCharset instead of accept-charset), which for example, caused controversy in React. I think the best approach for now is to offer both options. (And set up internal enum validation for Attrs and Style, when used with strings)
The problem seems to be that this RFC once again fell into the trap that different people find different things readable. And everytime someone comes out and says what they find how unreadable, someone tells them that they're wrong and it's perfectly readable. If repeated putting forward of arguments is considered noise, repeated putdowns of them should be as well. But I doubt that will happen. And once your subjective argument seems negated/ignored people fill an itch to reestablish it.
Ahh yeah, I missed that 'custom' part. I suggest reading https://blog.burntsushi.net/rust-error-handling/, it's not short, but very thorough. After half of it, theres "Defining your own error type", which is what you want.
The attribute that you add with StructOpt are transformed directly into method call to clap's App or Arg builders. Thus, you can almost use all of the builders within StructOpt. Thus, StructOpt can't exists without these builders.
Complementing /u/sdroege's remark: one usecase I have at work is reading PCAP files that are *being written to*. That is, we have continuous PCAP streaming infrastructure, and sometimes you just want to peek and see where it's at; for example, check the timestamp of the last (complete) packet. This involves reading files without error if the last packet is not yet complete, and being able to resume reading once sufficient data is available.
(First of all, it's `struct Coord` in Rust, not `class Coord`) I'd prefer to write one of // a bit shorter player.pos = (2.5, 3.2).into(); // avoids trait magic player.pos = Coord { x: 2.5, y: 3.2 } To me, an array is a collection of data that has the same meaning, with arbitrary length. Coordinate components don't fall into this category.
Did I really? I'm still under the impression that many Rust features are praised as if they are new inventions. I never claimed Rust is equivalent. Rusts borrow checker alone is a considerable advantage, with the C++ equivalent only slowly coming to life. 
I always appreciate advice and will keep this in mind during my next project! I do sometimes get a little caught up in in doing things correctly from the get-go.
What's the best book (actual paper one) on Rust? Heard good reviews of Programming Rust but isn't it outdated? Also not a question but the officical Rust FAQ link in the sidebar is not working.
Nope, it's Noto Sans Mono :) 
Sadly I think porting perf would be quite a pain, since it's a part of the kernel tree. Could be that it's worth porting perf script though... That's a good idea! 
&gt; If repeated putting forward of arguments is considered noise, repeated putdowns of them should be as well. But I doubt that will happen. And once your subjective argument seems negated/ignored people fill an itch to reestablish it. Yes :/ Which is why I think that GitHub isn't the right medium for such discussions. I think you would need something more formal. A curated tree-shaped discussion could help, for example. Or maybe something where comments should either raise *one point* or reply to an existing *point* (with a hierarchy of points). In the latter example, you would have: - a top-level point for each syntax proposed, - a sub-point for advantage/disadvantage, - a sub-sub-point to discuss whether the advantage/disadvantage is actually perceived as such. And redundant advantage/disadvantage would be more immediately noticed, and more easily curated.
As for hacker tools, the lecture content so far is up on the website, so feel free to browse it at https://hacker-tools.github.io. We actually mention most of those tools in the command-line environment lecture! 
Huh, three u64s. That would explain the massive overhead. Personally, I'm using this for 11-byte KV pairs, so it makes sense my dataset isn't fitting in memory with &gt;200% overhead. I looked at the code, and I don't understand why: 1) `next` is a u64, when a u32 should suffice 2) `next_checksum` exists. If `next` really does need to be protected, why not hash K+V+next all at once to preserve space? Is it really that much slower? 3) the checksum needs to be u64, when u32 (and possibly even u16) would likely suffice. Re: 3) - I think it would be nice to be able to choose the checksum's size depending on your needs. For my small 11-byte KV pairs I might want to choose u32 or even u16.
There are fun things you can do with this stuff for efficiency, if you are going to solve an equation for many different values of a the variables: &amp;#x200B; * constant folding - recursively traverse the tree and compute any constants in the equation (or other mathematical facts like 0\*x = 0) * stack machine - turn the tree into a stack machine. you make a list of instructions and push and pop results onto a stack. (though you will want to just use an array that you treat as a stack by hand for best performance probably). However if you are trending to evaluate a function just once, the set up time will . &amp;#x200B; Something you can do to improve performance generally is put left and right in an array, so they are contiguous in memory, rather then in separate boxes on the heap. Then as you traverse the tree you get more cache hits. 
Looking at the code I don't see where the error would be swallowed up though: * `Stdout::write` proxies to `FileDesc::write` https://github.com/rust-lang/rust/blob/master/src/libstd/sys/unix/stdio.rs#L25 * `FileDesc::write` wraps `libc::write` in `cvt`: https://github.com/rust-lang/rust/blob/master/src/libstd/sys/unix/fd.rs#L100 * `cvt` simply converts a `-1` to an `io::Error`: https://github.com/rust-lang/rust/blob/master/src/libstd/sys/unix/mod.rs#L129 yet according to the strace in your bug report, the write(2) call does fail with a -1.
&gt; I think you would need something more formal. Yes, I absolutely agree! It will be the only way to get a good and neutral mapping of the problem space. As long as everyone needs to contrast their preference with all other presented ones, you'll get an avalanche of comments. And due to the uneven power dynamics involved, a normal back and forth will have a hard time leaving people satisfied. I also feel that the current version marginalizes some groups. For example, I'm unsure how many proposals are seen by people who don't speak english as a native language, or only partially. I also never see accessibility discussions when things like "syntax highlighting can fix that" comes up, like interactions with screen readers. But before everythings over, it's even hard to know if "syntax highlighting will fix it" counts as an argument, and by how much. I believe a more formal format is also the only way we'll be able to establish a baseline like "should syntax design be able to rely on the availability of good syntax highlighting", which then also includes "should general syntax and grammar be kept simple to always allow good syntax highlighting without a fully semantic aware IDE" and such. A formal process will also make it easier to avoid backlash for disagreeing parties. If a solution space has a gap where problems can arise, it's easier to accept that existance and try to figure out ways to alleviate the problems, like optional lints. But then again, this has been in talks for *years*. &gt; And redundant advantage/disadvantage would be more immediately noticed, and more easily curated. This is another good point. It would mean minority disagreements wouldn't disappear, but instead be well recorded and findable afterwards. Readability discussions in RFCs currently feel like an acceptance of the RFC means your readability concern is considered not real.
Thank you for your comment. I greatly appreciate it.
Hey, you’re not a bad person - you were angry and people often say unkind things when angry. I get it, that’s why I asked why you commented that way. I like Rust a lot and believe it will become more and more influential. Thank you for your time to respond and apologize. It’s a sign of bravery when people do that. I appreciate you. 
&gt;Fake news. Ouch, that hurt! &amp;#x200B; &gt;What you said (moving borrow checks to runtime) applies to [RefCell](https://doc.rust-lang.org/std/cell/struct.RefCell.html), not \`unsafe\` blocks. Thank you for the clarification. I appreciate it.
Right here: https://github.com/rust-lang/rust/blob/01af12008d63a64446a86d995e772f8a539a4202/src/libstd/io/stdio.rs#L88-L118
&gt; Can't capture dynamic environment in a fn item about iron lib But now there is another problem： ```bash error[E0373]: closure may outlive the current function, but it borrows `cass`, which is owned by the current function --&gt; src/main.rs:360:19 | 360 | let handler = |req: &amp;mut Request| { | ^^^^^^^^^^^^^^^^^^^ may outlive borrowed value `cass` 361 | // convert the response struct to JSON 362 | let out = serde_json::to_string(&amp;cass).unwrap(); | ---- `cass` is borrowed here | Note: function requires argument type to outlive `'static` --&gt; src/main.rs:367:5 | 367 | router.get("/", handler, "index"); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Help: to force the closure to take ownership of `cass` (and any other referenced variables), use the `move` keyword | 360 | let handler = move |req: &amp;mut Request| { | ^^^^^^^^^^^^^^^^^^^^^^^^^ ``` why need use the 'static'? about lifetime? P.S.(Because i change return type, so just use "cass", remove "cass_it" already)
Oh yeah, thanks, I'd completely missed the indirection through `Maybe` and its relevance. That's a bit brutal, when the original purpose was just to not panic println!, print! and friends but it turns out to make entire classes of IO errors impossible to track.
I've been using the pcap library, it'd be nice to not need to have lib-pcap though. I'll try comparing yours.
Should be able to fix that by moving it into the closure: ``` let handler = move |req: &amp;mut Request| { // ... let out = serde_json::to_string(&amp;cass).unwrap(); // ... }; ```
I'm confused, you tweeted that C++ had Rust features for years, but now you're saying the burrow checker equivalent is only beginning to appear. It sounds like a contradiction to me.
1. Try use `rusftmt` because your code may look much niceer 2. Try to express your calculations as declarative pipeline, instead of imperative directives. 3. nth is a very bad method, you should use it if required only --- I tried to fix these issues and here what I've got: ``` use std::time::SystemTime; use std::io; const ALPHABET: &amp;'static str = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ~!@#$%^&amp;*()_+`-={}|:\"&lt;&gt;?[\\;\',./]"; fn get_permutations(n: usize) -&gt; Vec&lt;String&gt; { match n { 0 =&gt; Vec::new(), 1 =&gt; ALPHABET.chars().map(|c| c.to_string()).collect(), n @ _ =&gt; { let others= get_permutations(n - 1); others.iter().map(|x| x.clone()).chain(others.iter().flat_map(move |rest| ALPHABET.chars().map(move |c| format!("{}{}", rest, c)))).collect() } } } fn main() { println!("Please input string length (recommended between 0 and 5):"); let mut line = String::new(); io::stdin().read_line(&amp;mut line) .expect("Failed to read line"); let max_length: usize = line.trim().parse() .expect("Not a number"); let now = SystemTime::now(); let permuts = get_permutations(max_length); for s in permuts { println!("{}", s); } println!("Completed in {:?}", now.elapsed().unwrap()); } ```
I need to set up communication between 2 threads in a request-answer style. That is, at some point threat A get a message from external sources, passes that as a request to thread B. Thread B does some computation, and sends the result to thread A. Thread A then passes this result to the external source. The communication Thread A &lt;-&gt; external source is set up, but I'd need a hint for the communication A&lt;-&gt;B. I can imagine doing this with 2 channels, one for direction A-&gt;B, and one for B-&gt;A (actually, I've already got A-&gt;B set up), but I'm wondering, is this the basic idea? Or is there something like a 2-ways-channel I should use? Thanks for any pointers :)
I'm using the `iui` crate to make a simple GUI and I'd like to draw some 2D stuff to the screen, for which it appears I need a `DrawContext` and then call `stroke` on it. However, I don't see any way of creating or obtaining a `DrawContext`, apart from making one from a `uiDrawContext` pointer (which doesn't really help because I'm not sure how I'd get hold of one of those).
Tweet was spawned by this thread. https://twitter.com/sebaaltonen/status/1088866668187709444?s=21 Most of the things mentioned as great in Rust are possible in C++, and have been criticized heavily (for example strong types , or binary literals, or tuples, or enumerate(), etc). 
What a great walkthrough of configuring the clocks--thanks for posting! And I'm glad you didn't stop at wrapping the C code exported by the (very interesting) STM graphical tool--it was nice to see your 100% Rust version.
As a rule I don't like to give people a hard time for changing their minds. It's not like we want them to be embarrassed about learning something.
Maybe you can add unary operators, too :)
&gt; I'm still under the impression that many Rust features are praised as if they are new inventions People really do this? I'm fairly sure the rust team pays homage to many languages and is not shy about sharing their inspiration. Whenever I have to describe the language to someone else I'm like "imagine C++ without all the bagage"
Oh nice! I've used it just a few days ago in a yet-unreleased project (hopefully will be announced in a week or so). I would appreciate a bit more info on what the worst case complexity for the algorithm is. [Wikipedia article](https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm) says it's worse than boyer-moore, but does not really elaborate. Speaking of docs, "Consider denying 0/n factorizations" is in the readme but not in the docs. And since I'm not intimately familiar with the algorithm I'm not sure what that means exactly. I'd appreciate more info on this as well. Also, the fuzzing targets appear to be out of date since they reference crate "twoway" and the current name seems to be "subslice". Kudos for fuzzing in the first place though! I'm asking all this because I'm going to expose subslice search to untrusted data (although the needle fixed) and I need the program to be both memory-safe and DoS-resilient. I guess I am a bit spoiled by Rust's DoS-resilient `HashMap` and `slice.sort_unstable()`.
All paper books are outdated, all the time: rust releases every six weeks, and printing books takes longer than that. Programming Rust is based on 1.17, IIRC, and TRPL on rust 1.21. That said, the fundamentals are still the same, even though they may be missing some of the latest fancy stuff. TRPL online has had significant updates, and we’ll be getting the print edition updated soon.
I agree with your categorisation, but it doesn't help that these concepts (as many others in CS) are not rigorously defined.
From the project side of things, we're pretty upfront that Rust is to a huge parts "good ideas, integrated into a whole" and that C++ is a huge inspiration.
The OP is a frequent offender in playing the boundary between discussion and just terrible spite.
You can be exclusive by being pedantic about speech too.
I went and read the tracking issue. It's almost like never-type has glitched the stabilization procedure slightly. (Which seems oddly appropriate.) The thing is that a bug was discovered shortly after final comment period which prevented type inference from figuring out [this code](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=88d419b84523a7b054c1836b608d7810). (It does now.) [Decision to revert stabilization](https://github.com/rust-lang/rust/issues/49593#issuecomment-377974905) Currently the worst blocker seems to be [breaking unsafe code](https://github.com/rust-lang/rust/issues/48950#issuecomment-380603530). The example given in relation to [winit](https://github.com/tomaka/winit) - fn noop() {} // This function is perfectly safe if `T` is `()` unsafe fn hogehoge&lt;T&gt;() -&gt; Result&lt;T, ()&gt; { use std::mem::transmute; let fp: fn() -&gt; T = transmute(noop as fn()); Ok(fp()) } \- looks unsound as hell to me. But this is not an indictment of winit - the example is a simplification of far more non-obvious unsoundness in `objc`, [which I've just commented on](https://github.com/SSheldon/rust-objc/issues/62#issuecomment-457927454).
you know people say rust evangelism force is bad... well c++ has one too and it's just the same 
C++ is strongly typed, what's your point?
Can you give a code example? Is it available in any other languages?
Yes, C is weakly typed. But C++ is not.
Please inform yourself about the [paradox of tolerance](https://en.wikipedia.org/wiki/Paradox_of_tolerance). Being inclusive sometimes requires excluding those whose behavior leads to the exclusion of others.
Looks fine on new Reddit ¯\\\_(ツ)\_/¯
Are you familiar with what sockets are and how they work? And for HTTP there are plenty of good crates, such as Hyper.
## Arvid Gerstmann: "Game dev people discovering Rust is pure comedy. Tweeting out all the benefits, and I'm like "Yup, all the things you mentioned are also available in C++, and you've been ignoring and criticizing them for years". ¯\_(ツ)_/¯" ## John Carmack: Coincidentally, I just started writing some rust... I find this sentiment strange; people don’t look to other languages because you can’t do something in c++, but more often because you can. ## Arvid Gerstmann: Good point, yes, C++ is a behemoth of a language, it can do many things (it's debatable how good it does them, though). Everybody has their own sort of dialect. Maybe we should've looked into formalizing that, instead of inventing a bunch of new languages. ¯\_(ツ)_/¯ ¯\_(ツ)_/¯ ¯\_(ツ)_/¯ 
This crate implements a stack-allocated fixed-capacity string and also links to other crates that do similar things but in slightly different way: https://github.com/paulocsanz/arraystring I have not audited any of them, just pointing out that they exist.
For HTTP requests (eg get/post), try [Reqwest](https://github.com/seanmonstar/reqwest), which builds on the lower-level Hyper lib lzah mentioned.
Thanks, what esle should I read about ?
Thanks, what esle should I read about ?
First tell me how familiar you are with the web.
Not particulary, the best web project I did was making a [chat bot](https://github.com/sn99/sn99_alfred_bot) for telegram and hosting it on heroku, I had no idea what happened behind the curtains
I've found it depends a huge deal on what exactly you're doing. For trivial projects, the Rust code is trivial (lol), but for most non-trivial things the borrow checker really gets in the way. Big generalization, but usually true from my experience.
&gt;I'm a little disappointed that I'll never get to be a real programmer since I probably won't learn any low level programming language other than Rust. You're writing code that does stuff. You \*are\* a "real" programmer.
I see. Well if you’re interested in the web, first I’d say learn HTTP fairly well as it is (in my opinion) easier to do than sockets. You can find many good articles explaining the different methods (too lazy to find one now as I’m on my phone, sorry). After that you can start looking at other protocols. It’s definitely possible to build a chat system over http though sockets would be more suited for the job.
It'd be really helpful to have your crate docs either describe what pcap files are and what they're used for or link out to somewhere else that explains them. You may think it's not necessary because people looking for this Crete will already know what they are. However it ignores the people who are curious about things and find this and don't know they care. The docs could help recruit those people as well as users of the crate. 
Thanks for the help, so http and sockets are my go to, thanks again.
It works fine on new Reddit and some Reddit apps, but old Reddit and mobile Reddit breaks with triple ticks. (I swear half my comment history is about this. Reddit dun fukd up with this design choice)
If you find any more people who are mistaken about this, you can just send them this link: https://doc.rust-lang.org/reference/influences.html
Is the "?" really a clean/idiomatic way to write code? I'm just thinking about readability and I understand different people have different tastes. Take the following A: let result = ...?; Ok(result) B: match(result) { Ok(x) =&gt; x, Error(e) =&gt; return Err(e.into()), } It seems to me that A hides lots of stuff going on there. It looks fine at first sight but the "?" is just ambiguous on what it is exactly doing. So it takes more time to write B but it is worth it. What is your take guys?
I'm by no means criticizing the language or the team behind it. Rust is doing fantastic things, and I'm hoping it'll continue to do so. I'm criticizing the "unfair" treatment of C++. For years the game dev community criticized "modern" C++ features, only to suddenly embrace them in Rust and preach them as amazing. Sure, a bunch of modern C++ is questionable, but there are so many gems which have also been adapted in Rust (not necessarily from C++). 
That is how I interpreted it as well. Rust seems nice. I just have been more enthralled by other languages to really sit down with it and learn it. It seems unfortunate that beginners get frustrated enough with Rust to resort to “clone” to make things work at all, but then we’ve probably been making expensive implicit copies of things in C++ rather than expensive explicit ones for quite some time heh.
Upvote for having Esperanto in the prefill. Oni nur devas enmeti vorton enhavantan ĉapelitan literon por esti certa, ke oni uzas Esperanton. 
What are `prelude` modules?
Wouldn't that need an async parser ? Or do you want something like /u/matthieum: a way to stop and resume a parsing ?
Thank you, it was very useful and elaborated. I will try to apply the principles and ideas you told me about. You should pursue giving feedback!
This is not a pcap library, it can't be used for capturing packet. It's only purpose is to read and write pcap formated data.
You can't use `crate::crate_name` to reference extern crates, which is (was) a common use of `::crate_name`. 
What's the difference between a reference and a pointer?
Unfortunately in Rust, there's a fair bit of boilerplate to get this off the ground (there are crates like [custom_error](https://crates.io/crates/custom_error) which effectively cut this in half), but it's good to see how it actually works, first. In practice, once you're set up, it's very simple to add additional error types. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a4ad10abdf72c953b2343c5fd04b3a2b
You're missing the forest for the trees. Language design isn't just about cherry picking features. It's about forming them into a cohesive unit where each feature works well together. Just because [C++ technically has the ability to do pattern matching](https://github.com/solodon4/Mach7), for example, doesn't mean it's anywhere remotely as pleasant to use as it is in Rust. So when you "Well, ACTUALLY, ..." someone who says, "oh yes, finally, Rust has pattern matching which I've missed from C++," then you've completely missed the point of what they're saying.
I know, I'm just using it to read already captured data. I'm making a pcap visualizer with amethyst.
Yeah, I think it's unfortunate that the change was made to Stdin/Stderr and not just `println!()`, etc. I think it would make sense for `write!(stdin, ...)` to return this kind of error since that's the whole point of using `write!` over `println!` when writing to stdout. I think it would be technically legal to change the behavior to this given that the API docs don't mention this behavior, and the implementation of RFCs often change before they are stabilized. I don't think such a change would be likely to break existing programs, though if it did it would be hard to move forward. In any case, it's easy enough to make your own stdout that behaves however you want: let mut stdout = unsafe { File::from_raw_fd(io::stdout().as_raw_fd()) }; // from/as raw handle on windows write!(stdout, "hello world!"); and this is already necessary to override other default behaviors of stdout like line buffering or locking.
Rayon has both map and reduce, enabling an easy parallel map reduce implementation: [https://github.com/rust-lang-nursery/rust-cookbook/blob/master/src/concurrency/parallel/rayon-map-reduce.md](https://github.com/rust-lang-nursery/rust-cookbook/blob/master/src/concurrency/parallel/rayon-map-reduce.md) &amp;#x200B; Note though, that many people associate map/reduce with distributed computing on a cluster, such as the Hadoop project, while Rayon is for a single computer.
I tried to write a crate of regexen over arbitrary element types, like [regex-applicative](http://hackage.haskell.org/package/regex-applicative) in Haskell, but was frustrated by the lack of GADTs and higher-kinded quantification.
I see. Thanks!
This had me confused too. Usually you'll see `Result&lt;()&gt;` in documentation to signify that a result alias is being used. If you click on it (rust doc) it will show you the alias declaration. The standard library `std::io module` declares the following: `pub type Result&lt;T&gt; = result::Result&lt;T, Error&gt;;` This avoids having to write `Result&lt;T, Error&gt;` everywhere in your lib if it always return the same type of error. Now you can just write: `fn some_method(name: String) -&gt; Result&lt;String&gt;{...}` &amp;#x200B; The other commenters are correct in their definition of () but what got me was looking at the docs and just seeing `Result&lt;()&gt;`. &amp;#x200B;
This is cool. I have one of these inkyphats too, although I think my current level of rust knowledge/low level knowledge is a bit limited to be able to switch away from the python API for this display. Good read though! 
&gt; But sometimes it's ok for it to just "work", even if it's "bad". It is almost always better to get something out the door with significant optimization issues than never get it out the door in the first place. Optimization/refactoring can come later and you may find you don't really need to do them former anyway.
&gt; Note though, that many people associate map/reduce with distributed computing on a cluster, such as the Hadoop project, while Rayon is for a single computer. while Rayon is for a single computer. right. I definitely mean on a single computer, just as a regular programming tool. I can imagine how to do this in C++ aswell, it's just a nice example that would show how the better signatures of trait-objects (for compile-time resolvable lambdas) &amp; error-checking of generics would be more helpful to the user. What makes it seem to me like just bolting together map &amp; reduce together won't cut it is the need for a parallel sort in the middle, plus perhaps some variation on the question: what's the best way to get a variable number of key-value pairs out?. Could you , for example, save on allocations by reducing them to an accumulator per key straight away? (maybe if you just compose map and reduce, a specialization can turn it into something dedicated as I describe? I haven't followed in detail recently so I don't know if that exists in Rust yet or if it's planned.)
&gt; Note though, that many people associate map/reduce with distributed computing on a cluster, such as the Hadoop project, while Rayon is for a single computer. while Rayon is for a single computer. right. I definitely mean on a single computer, just as a regular programming tool. What makes it seem to me like just bolting together map &amp; reduce together won't cut it is the need for a parallel sort in the middle, plus perhaps some variation on: what's the best way to get a variable number of key-value pairs out. Could you , for example, save on allocations by reducing them to their keys straight away. (maybe if you just compose map and reduce, a specialization can turn it into something dedicated as I describe? I haven't followed in detail recently so I don't know if that exists in Rust yet or if it's planned.)
On the other hand, if, as a user, I was told to use something like `regex-applicative`, I'd be immediately frustrated by its intense abstraction and would be constantly wondering whether it was actually going to be fast in practice. There are some things I miss about HKP personally (and GATs will help those cases), but there are some pretty intense upsides IMO to not having it.
Pcapng support
Boyer-Moore-Horspool is `O(m * n)` worst case time, where `m ~ len(needle)` and `n ~ len(haystack)`, where as a correctly implemented Tuned Boyer-Moore is `O(n)`. See: https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm#Performance This crate also seems to use twoway, though, it also has a BMH implementation.
Now, n-body has caught up, 'Rust #7' being #1 :) :) [https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/nbody.html](https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/nbody.html) &amp;#x200B;
Completely forgot about that, to be honest. I got into rust when the 2018 edition was on nightly, and used that almost exclusively :/
I've been too busy to get properly familiar with the changes, so I couldn't say.
Yeah, it seems like it would make sense if it was only `println!()` that ignored errors. I suppose this could be implemented a library if it's impossible to change std.
I wish this was easier. The world would be such a better place.
I think you'll find this chapter illuminating: https://doc.rust-lang.org/book/ch04-02-references-and-borrowing.html tl;dr: references use the borrow checker to prove correctness and they cannot be null or otherwise be invalid, pointers do not use the borrow checker and they can be null or point to invalid memory. Therefore pointers require unsafe{} to dereference.
I think this page has a good explanation: https://doc.rust-lang.org/std/prelude/index.html Essentially they're modules that re-export commonly used things so that users can just `use some_crate::prelude::*` rather than having to `use some_crate::module::{Thing, Foo, Bar, baz, baz_bar_something}; use some_crate::module_2::{More, THINGS_ARE, in_here};`
There are multiple approaches to this, for example * Having some kind of parsing state that has a `push(&amp;mut self, input)` &amp; `pull(&amp;mut self) -&gt; Option&lt;Output&gt;` API * Doing it like `nom` and requiring the caller to implement buffering logic (i.e. return some kind of `NeedMoreData` error from your parsing function if no complete item can be returned) In both cases you would require the caller to pass data to your parser, instead of the parser pulling data out of something (the `Reader` in your case). With a `Reader` you could possibly work by handling `WouldBlock` as "no more data available, will be called again later" but that seems less elegant.
Thanks for the fantastic writeup! The STM clocks (like most MCU clocks) are the bane of STM development: hard to get right, but necessary if you want to do anything. If you have access to a high-precision frequency counter you can get a ridiculously accurate measurement of the system clock offset and jitter. This will allow you to calibrate the RTC in software. GPS-clocked (!) high-precision frequency counters are available a number of ways these days. The off-the shelf ones are pretty expensive, but there are a number of hobbyist/kit solutions. You may have access to a university or industry one, also.
I was confused about how a compiler might make use of the fact there's a data race. But of course compilers assume that a program is data race *free*, which is how problems may arise. I still don't see how that can mess things up here, but that's likely a lack of imagination on my part. Found this nice article about why even simple data races could cause UB problems: https://software.intel.com/en-us/blogs/2013/01/06/benign-data-races-what-could-possibly-go-wrong
Got it! I'll say "dudes" next time. &amp;#x200B; Just kidding. I'll just say "frosting" next time, that way I'm not abusing anyone's demographic or excluding other genders. Also, it's okay for me to use it because I made a few cakes sometimes. &amp;#x200B; You think I'm joking but I'm not.
The map reduce you're talking about seems to be slightly different than Rust's use of the names. The map part should just be a flat_map() in rayon. Rayon doesn't seem to have a good way to represent the group/sort step in the middle. Although you might be able to do it with a couple recursive partitions and then have your final reduce step allocate a map or vec per thread to do the final deduplication. Dunno how performant this would be though Alternately could just use differential-dataflow
I'm not quite sure what you are asking for. The primitives of map and reduce, by definition, make up MapReduce, and the cookbook link I pasted above shows a nice example of how to use it. I don't really think trait objects and generics in Rust really help make MapReduce any more ergonomic; what does help is the safety offered by the borrow checker: the borrow checker helps prevent data races by enforcing thread safety. &amp;#x200B; Rayon also has a parallel sort, though if you want it as part of the MapReduce operation, you typically implement this by having the map function map in to bucketed queues to be merged at the reduce stage (essentially implementing MergeSort with mapreduce). &amp;#x200B; If you want an easy plug-and-play parallel sort, you can always collect into a vector and then use rayons par\_sort: [https://docs.rs/rayon/1.0.3/rayon/slice/trait.ParallelSliceMut.html#method.par\_sort](https://docs.rs/rayon/1.0.3/rayon/slice/trait.ParallelSliceMut.html#method.par_sort) &amp;#x200B; &amp;#x200B;
Okay this is gonna sound like a bit of a surprise but I don't know what most of those methods are called and why do I feel dirty calling them methods? &amp;#x200B; So does it compile a list of all the permutations and then print them one by one? That seems slow.. I'll test it. &amp;#x200B; Edit: NOT SLOW NOT SLOW hahaha
It was facetious. Note how it interjects the paragraph I wrote without any intro. SEE?! SEEE?!?
Oh thanks, I've missed that! I thought `find` was only in the `bmh` module, but turns out there's a trait that uses the two-way algorithm. According to the docs, the two-way algorithm is only implemented for `str` and `[A]`. Is the latter an implementation on a slice? Why is it not `&amp;[A]`?
Thanks :)
Let's define the problem a little better. Rust the core language doesn't know what the Internet is. It knows how to do stuff with the CPU - and within the limits of being user-mode program. Rust the standard library does know the bare basics of using the UDP and TCP protocols. It can send and receive datagrams (UDP) and use reliable modem-like connections (TCP). It also knows a little bit about how to lookup host names ("for `rust-lang.org` contact `13.249.87.53` via ip4"). And that's about it. For the other pieces you'll need to write them yourself or find a crate to use. Halfway-ish between those in difficulty you could interface with a library that has a C-compatible API. So, are you trying to contact a server via an existing protocol or to define a new one? Either approach could teach you new things. I would actually advise against http unless you know that you need to work around a somewhat hostile network environment (schools with network filtering, etc). This way you can focus on the core problems like "what is a message anyway?" and feel like you've created something you fully understand. IRC is sorta simple - it started simple but has accumulated a ton of elaboration over the decades. You can download and run a server locally, and that would be the polite thing to do while developing. XMPP looks significantly more complex at first, but if you want to dig into XML parsing and serialization, it would teach you that. I'm not impressed at first glance by the IRC crate. Oh, it's a lot of work, but saying "you must use the async paradigm I've laid out for you" is a bit of a warning sign. There are *tons* of things a protocol library can do that aren't tightly bound to how the user will manage the control and data flow - parsing and generating messages, say - and if those parts aren't well expressed then it doesn't bode well for more common use-cases. But, come to your own opinion.
I agree with you.
Per tio mi iom provas disvastigi la lingvon :) Jes, gxi ne subtenas "cx", "sx", "gx" kaj tiel plu.. Oni povas vidi trigramojn cxi tie: [https://github.com/greyblake/whatlang-rs/blob/master/misc/data.json#L91](https://github.com/greyblake/whatlang-rs/blob/master/misc/data.json#L91) Versxajne, mi devos sxangxi tion. 
Just a note, http is poorly suited for persistent bidirectional communication that a chat app would require. Generally http is used for short lived connections that retrieve content from a server. While WebSockets are an option with http, you might as well just use plain old TCP sockets. Also, when dealing with sockets, you have a choice of what networking protocol to use. For starters, I'd recommend TCP. It abstracts your connection to a computer as a stream of bytes. Another protocol available is UDP, in which you send small amounts of data as packets. The packets you send, however, aren't guarenteed to arrive in order or even at all. You would have to make a system that can detect lost and out of order packets and deal with them. TCP in fact is built on top of sending packets. It has ways of checking for lost data and resending it so that the programmer doesn't have to worry about it.
I think you're missing the point of the criticism. A bad implementation of something doesn't mean a good implementation isn't possible. C++ suffers from implementing these features in overly convoluted ways using template magic instead of actual language features. Nobody is complaining about the core concepts (that I've seen anyway), but about the specifics of how they work in C++.
It was completely obvious /s
Actually, there is a Rust rewrite of `jq` that supports many more formats: https://github.com/dflemstr/rq
The idea is that on bigger projects you're capable of making seemingly small errors that become impossible to track down bugs because you're passing around mutable state to different areas of the program and potentially changing it in unordered ways. So you will still be thinking about all that stuff in Python if you're doing it right, but in rust, the borrow checker helps you make sure you're doing it right and catches you making mistakes at compile time that you would have otherwise found out about in runtime with Python.
&gt; Is the latter an implementation on a slice? Yes. &gt; Why is it not `&amp;[A]`? The methods take `&amp;self`, so the actual argument type is `&amp;[A]`. 
&gt; if, as a user, I was told to use something like `regex-applicative`, I'd be immediately frustrated by its intense abstraction Fair enough, i find it very convenient. If you or someone else have a better idea for defining regular expressions over arbitrary slices, i'd like to see it. &gt; would be constantly wondering whether it was actually going to be fast in practice This is also a concern for me. I think such a library written in a language like Rust would make me worry less than one written in Haskell about guaranties of performance, but it remains a concern. Nonetheless, it would be a useful tool for me to have at all. 
Thank you very much!!!
I wouldn't call the behavior of the `?` operator *ambiguous*. The desugaring is precise and fairly easy to understand. e.g. this let x = x?; becomes this and only this let x = match std::ops::Try::into_result(x) { Ok(x) =&gt; x, Err(e) =&gt; return std::ops::Try::from_error(std::convert::From::from(e)), }; *"But what if the type of x has a custom implementation of `Try`? It could do anything!"* Well, I suppose, but you can always look at the implementation and see what it does. `x` will always have a single static type since the `Try` trait is not object-safe and thus cannot be made into a trait object; `x?` will only ever do one thing based on the type of `x`. Also, this situation isn't any different from other operator overloadings or traits like `Add`, `Clone`, or `Display`. Each trait has a contract, and you are relying on the author of the type to uphold that contract when they implement those traits. These traits are hard to implement wrong, so unless you're dealing with a malicious or clueless author I can't forsee it being a problem. But that was a tangent, I think the main thrust of your question is "isn't that a lot of stuff that happens due to one single character?". Yes, I suppose it is. When `?` was being introduced there were a number of people who felt the same way, and who vowed to keep using `try!()` forever. I'm sure there are some who still stick to that, but from the code I've read and the sentiment I've heard that feeling is basically gone. I was also convinced eventually due to a number of reasons: - When I'm reading Rust code and see a function which returns `Result&lt;&gt;` or `Option` I intuitively expect that it's likely to return from anywhere, which helps when reading the code to understand its behavior. - I usually don't care which lines can cause an error return unless I'm specifically debugging questions like "why did this function return that error", in which case it's pretty easy to look specifically for the question marks. This is an improvement over my experience writing C# where I have to hover over every function call in the body and hope that they document what exceptions they may throw (if I don't have a stacktrace). - Having subtle early return points could be an issue if you have code which needs to run before a function returns. In Rust, however, it's idiomatic for code like this to use a `Drop` implementation, like `std::thread::JoinHandle`, or to pass a closure to a function which handles the cleanup, like `rayon::scope`. Using either pattern is also necessary to run some code before a function exits due to a panic. - When writing Rust code it's easy to forget the `?` (just like it's easy to forget `&amp;` or `*`), but the compiler will tell you with a type error or an unused_must_use warning. I can't say that it's never annoying, but its convenience outweights the risks to me.
If you told a C++ developer “what’s wrong with this line of code”, then sure. If you’re trying to track down a crash elsewhere in your code, it’s a lot less obvious. 
I'd say that having two channels is the idiomatic way, yes.
I wanted to find out how much overhead using enums does vs. a true generic implementation of the actual code I am using. But it does not to seem that much of a difference. Size: 421.1892 MiB Enum: 10.1922 | 10.0313 | 10.3449 Generic: 10.0290 | 9.81870 | 10.1392 Size: 771.2079 MiB Enum: 19.8759 | 18.9715 | 17.9426 Generic: 19.7387 | 18.5588 | 17.5299 I uploaded the code to [GitHub](https://github.com/ucyo/benchmark-dyn-traits). I thought that the generics would monomorphe into a faster codebase, but as the result shows, that this is really not the case. Is there a chance for you to take a look at the codebase and send a PR if there is a mistake in the code?
I was a bit lazy but it's possible to do it w/o allocation with pure iterators. You can also get rid of recursion if you get a pencil and piece of paper. &gt; That seems slow.. I'll test it. In my tests it was several times faster than original code, but your results may differ. Beside that, you can use `rayon` and `par_iter()` since I have rewritten everything on iterators. Finally, don't mind the performance before you're done and you benchmarked the profit. [Premature optimization is the root of all evil (c)](http://wiki.c2.com/?PrematureOptimization) so don't do it. Write as clean code as you could and then optimize it if you really need. Clean code &gt;&gt; fast code.
Thanks for the write-up. I really like seeing common components from projects like these broken into their own crates, as you did here with the display driver and font crates. 
Unwanted sjw spoted itt. --- Nobody really cared about readers gender before you appeared. Looks kinda double-faced.
And for any kind of system that needs to analyze the resulting code.
Huh. If you close stdout and later open a file, is it guaranteed to not reuse the magic file descriptor? I gotta look at Posix, but it wouldn't surprise me. If so than I guess there's a reasonable way to talk about "closed stdout" in the sense of specifying behavior for stdlib. This is a pretty cool issue. 
&gt; the fuzzing targets appear to be out of date since they reference crate "twoway" and the current name seems to be "subslice". Thanks, good catch! I pushed a commit which unbreaks the fuzz targets. &gt; I need the program to be both memory-safe and DoS-resilient If subslice is not both memory-safe and DoS-proof i consider it a flaw, so your comments are appreciated. The algorithm should be *O*(`self.len() + other.len()`) worst case. It's forked from the twoway crate (i hope the commit log and noting the earlier author in the crate metadata is attribution enough) so i'll need to check to make sure, as the semi-naïve algorithm would be *O*(`self.len() * other.len()`). 
&gt; if, as a user, I was told to use something like `regex-applicative`, I'd be immediately frustrated by its intense abstraction Fair enough, i find it very convenient. If you or someone else have a better idea for defining regular expressions over arbitrary slices, i'd like to see it. &gt; would be constantly wondering whether it was actually going to be fast in practice This is also a concern for me. I think such a library written in a language like Rust would make me worry less than one written in Haskell about guaranties of performance, but it remains a concern. Nonetheless, it would be a useful tool for me to have at all. 
Happy to see that my font builder in embedded graphics is useful! I might use your profont crate in https://github.com/TeXitoi/rusty-clock
Great suggestion, thanks!
If you are looking for a stack only container, check crate seq [https://crates.io/crates/seq](https://crates.io/crates/seq) Lifetime of elements depend on the function -scope, Sequences can fork into multiple sequences, so that elements of sub-sequences are shared. Let me know what you are missing
&gt; I did not know that enums are actually preventing monomorphisation and that they generate overhead. I like to think of it as not preventing monomorphization, they just aren't a cause of it. A function accepting a generic parameter and an enum will still be monomorphized over the generic parameter, it just _won't_ be monomorphized over the enum. Enums store their variant in a byte-sized flag next to the data, where generic parameters store which thing is which by what function is being run. Enums aren't 100% bad, they're quite useful, and all variants will always be the same size/shape so you don't need generics. But for your case, I think generic parameters make sense. &gt; But your suggestion would be to not use the function `fn calculate&lt;T: Trait A, U: Trait B, V: Trait C&gt;(val: u8) -&gt; u32` but to split it up into different "subfunctions" like `fn calculate_tmp&lt;A: TraitA&gt;(val: u8) -&gt; u8` and then `fn calculate_result&lt;B: TraitB&gt;(val: u8) -&gt; u32` assuming that `val` of `calculate_result` is the result of `calculate_tmp` Yes - this sounds right. It's a bit roundabout but it allows you to keep using generic parameters without needing to make it a `Box&lt;dyn Trait&gt;`. Then once you have found what each type parameter should be, you can just continue regularly. Returning stuff and using sub-functions regularly will work as long as you aren't trying to climb from a context with more generics out to a context with less generics while retaining the information of what that generic parameter is.
Very interesting stuff! I am working on an embedded project in rust, and I agree with you that `rtfm` has a bit too much macro magic (and tasks, unlike futures, don't compose well). I was thinking about implementing an executor myself, but I will definitely check yours at this point!
Another famous Rust sighting https://twitter.com/xychelsea/status/1089616375277146113
Here's a Wikipedia article that should start you on your journey to learn more about this: https://en.m.wikipedia.org/wiki/Executable 
You're misreading the error: look at the actual error not the "note:". It simply says that the method was not found. The "note:" points out the existence of the method on `Iterator` but it's just a hint. Saying that it's a "bad [error]" because it gives more helpful information suggests you'd be better off sticking with javascript or C or something which give precisely the level of helpfulness you're looking for ;)
The code looks all good to me! I can't find anything wrong or anything that would negate the effects. I'm surprised at the small difference too, but I guess that the overhead of branching in `to_u32`, `to_new_u32` and `to_u8` just wasn't that much compared to everything else it's doing? If the majority of time is spent in `compact_u32`, this could make more sense. It only has one branch for which variant is being used at the start of the function so when it's called only once there shouldn't be much of a difference. I'd definitely try using a benchmark framework like [criterion](https://github.com/bheisler/criterion.rs) to see if anything in the benchmarking process is off, but that probably won't change anything. I'm just not familiar enough with how the benchmarking process can bias results to know whether running them once each in sequence would result in any bias. It would be interesting to know what the results look like for NoLZC, though! 
A proper answer to this question is more than is going to fit into a reddit comment, but it's absolutely worth learning about. A basic answer is that a binary file is a set of instructions for the operating system about what to do. It usually consists of - Headers (that tell the OS what part of the file is about what, where to start execution, etc) - Parts of the file to copy into memory (blocks of constant values to use like strings, blocks of machine code to execute) - Parts of the file that say how the OS should edit what it copied into memory (because the executable doesn't know what address it will be loaded at, these are things like "this part you copied over here is a pointer into this block, add the offset of the block to it") - Parts of the file that just contain metadata like debugging symbols - References to other files that should also be loaded (called dynamic libraries). On linux binary files are called "elf files", on windows they at least used to be called PE files. I'd choose a format (the linux one is probably better documented on the internet) and google it!
Those are some excellent suggestions, I will definitely try to implement them. It seems like you have not finished your third paragraph though. By putting left and right in an array, do you mean I still keep the binary component, but instead of using box pointers for left and right I use an array to store both? Thank you very much!
Binary isn't human readable, don't expect to get much out of it from a text editor. Use a hex editor if you actually want to look at it. Very generally speaking, the binary is composed of two sections, text and program data. The program is all the compiled and assembled machine code. The text section is any static/global data referred to by your program, e.g. string literals. The program data however can be referred to using "symbols." For example in a library, the symbol data can be stored to allow whatever links against it to look up where a particular part of the program is by some symbol name, or a debugger can use them to step through code, or a profiler can use them to time a program. If you look at the binary in a text editor the only thing useful you'll probably find are some bits of the text section and symbol information. The instructions aren't human readable. For more detailed info you can read the ELF documentation, which is a common binary format. 
Now that `const fn` is stabilized, have you considered making preprocessing of the input a `const fn`? I believe this would allow for constant needles to be pre-processed at compile time, eliminating the runtime overhead.
Cool (not-yet) crate! I would have expected that a solver for simple expressions like this would be more complicated, I'm surprised to see a simple solution like this. Anyway, you asked for it, you'll get my 2 cents: 1. You are using `char` for operators. That's good for a small code base, however, I would probably use an enum for your operator type (like `enum BinaryOperatorType {Multiplication, Division, Addition...}`) to prevent enums and variables from getting mixed up. I know enums can add some boiler plate (adding a `use BinaryOperatorType::*;` can help), but you would also clean up those operator matches that you currently have: &amp;#8203; return match operator { '+' =&gt; Component::Number(f1 + f2), ... _ =&gt; Component::End, // Can this even happen? }; With an enum, the compiler statically knows that the "default case" cannot happen, reducing the number of possible code paths. This is a general rust idiom: Try to prevent illegal state (e.g. an unknown operator) at compile-time. 2. For variables, I would at least use a type alias instead of `char` for the moment - in this case, the variable type can be easily changed later, e.g. to `String`. 3. I see `&amp;**` occurring at some places to get a references to the content of a `Box`. I think using `.as_ref()` is more idiomatic in this case. As a general rule, code with fewer or more evenly spread &amp; and \* is preferred. 4. `solve_component` is really nested (about 8 indents deep). Do yourself a favour and split that up! I'm mentioning that point as, for my experience, this will also help to improve point 3. I believe that most usages of &amp; and \* can be prevented by using the right argument types in some smaller methods (`char` instead of `&amp;char`, `&amp;Component` instead of `&amp;Box&lt;Component&gt;`). 5. The `solve_with` method requires a `Vec` as input. In cases like this, using `IntoIterator` as type is preferred: pub fn solve_with(&amp;self, vars_raw: impl IntoIterator&lt;Item=(char, f32)&gt;) -&gt; Component { let vars: HashMap&lt;_, _&gt; = vars_raw.into_iter().collect(); Self::solve_component(&amp;vars, &amp;self.expression) } In this case, users can call `solve_with` with *any* iterator (including a `Vec`) they like, without the need of creating an intermediary `Vec`. The same logic applies to `solve_for`. 6. In parser.cs: You determine if a char is white space or not with the expression `x != &amp;' '`(line 114). Maybe [Char::is\_whitespace](https://doc.rust-lang.org/std/primitive.char.html#method.is_whitespace) is more what you need? `Char::is_digit` could also be used in line 48. Otherwise I think that the general design of the crate is in a good initial shape! The split into a parsing and an equation module is good, also the `Component` struct has the right rustic Layout. Anything else will just take time. I hope that the points were only mildly confusing, otherwise, feel free to ask!
This is super interesting. Just one more question; why are most of the characters in the file just question marks/ invalid characters?
Thanks!
Will read this!
You're not the only one wanting something for that :) Thankfully 1024-tuples basically don't happen, so the macro solutions are less bad for tuples. And while variadic generics is almost certainly not going to happen in 2019, it might make the list for Edition.Next (2021? Who knows).
Wow I'm on mobile so I can't dig into the source for each variant, but I'm a bit shocked how much faster Rust seems to perform here. It's considerably ahead of the #2 spot. Nice work OP!
Coming from C#, I tend to like builders _better_ than the overload soup you get in C#. I've started just making structs to pass all the extra options around there too. (That said, I do want FRU to work on things with private fields, so that I can just write `Options { a: 4, b: true, .. }` instead of needing to write all the builder methods most of the time.)
Because compiled binary files are encoded as a set of binary numbers (hence the name) in the range of 0 to 255 each. When you open them in a text editor or some other tool that expects text, it tries to interpret those binary numbers as text, but not all binary numbers actually map to valid text characters (such as letters, numbers or punctuation), and all the numbers that didn't map to valid characters get replaced with ? sign or other placeholder. The exact mapping of numbers to text is dependent on your system's [character encoding](https://en.wikipedia.org/wiki/Character_encoding). To view the numbers verbatim you will need a hex editor, but that will not help you interpreting them. If you want to view the actual CPU instructions in the compiled binary you will need a disassembler that will interpret them for you and display them in a more readable form.
So apparently you'd get it from`AreaDrawParams`, except you can't actually use that yet since it's not exposed (or even used for that matter). [Relevant issue](https://github.com/rust-native-ui/libui-rs/issues/24).
A file is really a list of 8 bit values, only a fraction of 8 bit values are defined as characters in the standard representation of strings (ascii or UTF-8) the question marks and such are just how your editor is choosing to print those bytes. If you run `xxd some_binary_file` (on linux or WSL) you'll be able to see the mapping of values (in hexadecimal on the right) to characters (on the left, '.' for "this isn't an as cii character") 
Is their a reason so many of these submissions hand define what PI is? Is it somehow more efficient than using the std version? Is this trying to cut down on the binary size?
My guess would be that it encourages the compiler to be more aggressive about evaluating the math as a constant where it can. The constant in the standard library also defines it out to many more digits. I don't know if that is related in any way though.
The points were not confusing at all, thank you for the many great points. I am still getting used to some things in Rust and this will really help me. I will definitely try to not make things as nested and split them up in functions, as some other people also suggested this. In point 6 you said that I can use Char::is_digit can be used, but that doesn't seem to work for floating point numbers (containing a dot). 
I love scrolling to the bottom -- I know some things are slow, but how do they not time these things out before an _hour_?
Shouldn't make a difference in performance or binary size. My guess is that this is a remnant of the ported code (even C has `M_PI`, but possibly it was just easier to copy the definition from the C header instead of importing `std::f64::consts::PI`).
It would be cool if some of the inline SIMD instructions would be achieved by a more generic SIMD library. Is there a way to use std::simd in nightly while compiling to the same instructions? (I know it can't be submitted yet, but it would be so much cooler)
New dev here. If I do not support SSE instruction set what the \`rustc\` do with vector intrinsics? They'll be unpacked?
It's also #1 in reverse-complement: https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/revcomp.html and spectral-norm: https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/spectralnorm.html just by smaller margins. It's also falster than C in pidigits but only #3 because Fortran and particularly Chapel edge it: https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/rust.html The vs C page shows there's some work to do in a few tests though: https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/rust.html The comparison is against GCC and rust is not using the latest LLVM so part of it may be there.
yeah if everything is always two children the array is always two long, with left at 0 and right at 1. you can of course extend this to any number of children
The comparison is against C compiled by GCC so if anything it would still be unfair to rust even with LLVM 8 since GCC tends to be faster than clang.
Potentially to keep the number of decimal places consistent with the submissions in other languages
You may want to give hex editors a try to examine those files. That way you can actually see what lies behind the missing characters a regular text editor is unable to render. Compilers such as Rust produce machine code which is a stream of binary data that the CPU knows how to interpret. For each instruction the processor can do there's a special magic number for it. If the instruction takes arguments the magic number is followed by that data. It is very much possible to read the binary in the hex editor and understand what the program is doing. But there's an easier way to do this. This is where assembly language and assemblers and disassemblers come into play. In essence assembly language is just a way to make writing those magic bytes tolerable for humans. All of the magic constant bytes that the compiler understands gets a human readable name (eg MOV, JMP, NOP), and the arguments passed to those instructions are correctly encoded by the assembler into binary format. Years ago, for the x86 architecture you could get the reference manuals from Intel website (probably still can), where each of the CPU instructions was described in extensive details (the magic numbers and binary format for the arguments). Disassemblers are another tool in your toolbox when examining binaries. If you're compiling your own code you can just pass a flag to the compiler to output assembly (every compiler has that option, incl Rust) and skip the need for a dedicated disassembled. An online option you can use to examine what kind of assembly certain code snippets produce is https://godbolt.org But there's more.. The executable file that the compiler outputs is actually another binary format that contains a lot more than just the machine code - machine code, embedded data, file icon, digital signature, debugging symbols, etc. Different platforms have different formats that they support. On Linux the ELF format is the predominant one, on Windows its the PE format (if memory serves). When you run an executable the OS knows how to deal with the file format and how to load the machine code from that file into memory and where to start executing it. This is very simplified overview of how it works. Everything described above has a lot more to it. But I hope it gives just enough information to make finding more information on each of those topics slightly easier. 
&gt; You are inspiring! Are you going to port perf next? Psst... [I already did it](https://github.com/nokia/nperf). (: Well, it's not 1-to-1 as far as features and usage goes, but it's close enough for casual profiling.
Is or will this be available as a crate? I'd love to use it in [nperf](https://github.com/nokia/nperf).
Ohh I love this project, so well done. The DOS font does look pretty good already.
I've glanced through some of the benchmarks and implementations, and one thing in particular stood out to me: the C# `regex-redux` implementation is using the `pcre` library to do all the heavy lifting. Is that really okay? It doesn't seem to say anything about the C# language itself, except maybe that it can stay out of the way of an optimized C library well enough so as not to be *that* much slower than the C implementation (which does basically the same, but I think it's more fair since `pcre` itself is written in C). I don't see why Rust's implementation couldn't do the same, if that particular regex library performs better for the task at hand...
Wow, that is not a small win either. Like, even over fortran, that's a very significant improvement. I'd love to hear what accounts for this.
All x86_64 supports up through SSE2. If you compile on x86 target that doesn't support through SSE2 (those must be ancient), then you will likely get a SIGILL at runtime. If you try to compile SSE instructions on a target that doesn't support them at all (e.g., arm), then you will get a compilation error.
How do the fortan entries use so little memory?
Thanks! Also I've found out that via cfg macro it can be clearly checked in compile-time.
Both regex-redux and pidigits allow use of third-party libraries — "Some language implementations have regex built-in; some provide a regex library; some use a third-party regex library." "Some language implementations have arbitrary precision arithmetic built-in; some provide an arbitrary precision arithmetic library; some use a third-party library (GMP); some provide built-in arbitrary precision arithmetic by wrapping a third-party library."
&gt; the C# regex-redux implementation is using the pcre library to do all the heavy lifting. Is that really okay? Yes. This has been covered over and over. And yes, someone could submit a Rust benchmark that uses PCRE. There are even PCRE bindings for Rust: https://docs.rs/pcre2 It is simply the case that nobody has done it. Just back up a bit and look at this from a grander perspective. What use is a benchmark for a regex library written in C# that nobody uses in practice? If you opened your own benchmark game and set it as a requirement, good luck getting submissions. A general purpose regex library is a lot of work for just a game.
I see; thanks for the answer.
That's true, but runtime dispatch should be preferred if possible since it doesn't require special compile flags and produces portable binaries if done correctly.
For example [regex-redux C# .NET Core #5](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/regexredux-csharpcore-5.html)
I see what you mean. It does make some sense. Altghough, as somebody who mainly deals with C# at `$dayjob`, in my experience, using `pcre` is absolutely *not* the normal thing to do when you need a regex in C#. There is a regex implementation in the standard library, and all the regex usage that I've seen uses that.
You sound like someone who wants to learn assembly language. Dig up some tutorials online. It's a lot of work, but also very fun, and sometimes even useful. ;-)
&gt; an hour? And then there's the old CERN [C interpreter](https://web.archive.org/web/20171022014657/http://benchmarksgame.alioth.debian.org/u32/nbody.html) :-)
Woo hoo! But-- who has time for casual profiling? We need industrial strength!
While I didn't watch the 5+ hour coding session, I skipped around to try to find interesting steps. The moments where you profiled the rust and then refactored to optimize are really valuable for me. Optimizing Rust using your new profiling tool and techniques can be an entire series in itself.
I'm confused about this as well. Is it a bug? how can it be a couple order of magnitudes less than the other entries?
I dunno. It does not seem to be the case. The runs include compact_u32 and it does not seem to be the bottleneck.
Will do. Always have been interested just never had the drive to do it (coming mainly from a Rust / C++ background)
Ty! I’ll try out all of this. Appreciate the info a lot.
Yeah, I uodated my comment to account for that.
It is unfair and wrong benchmark then.
Would using sphere coordinates just be more elegant since the real calculations would be the same? Also whats the reason in using SOLAR_MASS as 4 * pi * pi instead of like 1?
Maybe using they use the exact same constant to produce the exact same result?
It's fine for what it is. As a comparison between languages given the toolchains that are actually available and more common. C/C++ has the advantage of having GCC while rust doesn't have a GCC frontend, so from the point of view of users that's what counts. The numbers are close enough that no one really doubts that C, C++ and Rust are all in the same performance league as pure languages and that the differences are mostly micro-optimization and not inherent issues with the languages. It would be nice if they included C and C++ with clang though. It would add a nice extra data point. Maybe that data exists somewhere.
Try `par_bridge` instead of `par_iter`. `par_bridge` is for creating parallel iterators from iterators.
the compiler will downgrade intrinsics the target architecture doesnt have sometimes
To be far with deref coercions, infinite implicit dereferences on self method calls, and from/into Rust can have a lot of potentially confusing type gymnastics going on as well. The obvious difference is that it is a lot harder to shoot yourself in the foot with Rust's abstractions.
&gt; It’s definitely possible to build a chat system over http though sockets would be more suited for the job. There are also websockets which are in between.
Using PCRE seems reasonable, given that it is very commonly used when developing applications in non-c languages. Many languages even have it built into the standard library...
First, depending on your text editor of choice, you might not be seeing all of the compiled machine code. Some editors stop at the first null byte. Second, to explain what you're seeing, you have to stop thinking about what a text editor shows as the rawest representation of a file's contents. Fundamentally, a file contains a sequence of bytes, which are groups of bits with 256 possible combinations. Even thinking of them as numbers isn't quite correct, because we have multiple different ways to interpret a sequence of bytes as a sequence of numbers. (`[u8]`, `[i8]`, `[u16]`, `[i16]`, etc.) If I give you this sequence of numbers, it does not inherently say how to interpret it. `72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100` **Text:** In most forms of text encoding, it will be interpreted as the string "Hello, World" because the basic Latin characters are covered by a standard called ASCII which the other character encodings built on. The [Wikipedia article on ASCII](https://en.wikipedia.org/wiki/ASCII) has a chart if you want to try decoding that yourself to get a feel for it. The Wikipedia [UTF-8](https://en.wikipedia.org/wiki/UTF-8) page explains the most common encoding we use today for going beyond that basic stuff. Speaking of ASCII, given the kinds of questions you're asking, you'd probably be fascinated by Eric S. Raymond's [Things Every Hacker Once Knew](http://www.catb.org/esr/faqs/things-every-hacker-once-knew/). It explains all sorts of details about why computers are the way they are, including why ASCII chose to assign specific characters to specific numbers and what some of those weird entries in the ASCII table are. **Audio:** It could be a fraction of a second of audio. When they say a CD is 44.1KHz, what they mean is that it represents the audio as a sequence of 44100 numbers per second and, because of the Nyquist sampling theorem, that's enough information to uniquely identify audio containing frequencies up to half that. (22,050 Hz) These two [excellent](https://www.xiph.org/video/vid1.shtml) [videos](https://www.xiph.org/video/vid2.shtml) by the creator of Ogg Vorbis have more information on how audio and video are represented in computers. **Image:** It could be a sequence of pixels. For example, if you group them into sets of three, then you've got a 24-bit color representation of four pixels. `(72, 101, 108), (108, 111, 44), (32, 87, 111), (114, 108, 100)` ... though you still also need to specify what the order is. (RGB? BGR? Something else?) Assuming RGB order, that first set of three would be the color `#48656c` when written in the hexadecimal notation you might be familiar with from web development. **Code:** Each number could be an instruction for the CPU to execute or an argument to a previous byte. Typically, bytes are represented in hexadecimal so that the range of possible values from 0 to 255 fits neatly into two characters: 00 through FF ...so let's show the string as hex instead: `48, 65, 6C, 6C, 6F, 2C, 20, 57, 6F, 72, 6C, 64, 21` According to [this chart](http://sparksandflames.com/files/x86InstructionChart.html), the hexadecimal `48`which would be "H" if interpreted as ASCII text would mean "DECrement the eax register" if interpreted as Intel/AMD x86 machine code. The reason these things seem so separate is that, in order to make things "Just Work", non-text files have identifying headers stuck at the beginning. For example: * A PNG file begins with `89 50 4E 47 0D 0A 1A 0A` (`\x89PNG\r\n\x1a\n`). * An EXE file begins with `4D 5A` (`MZ`) * etc. When you try to open a file with a program, the program runs down the list of the formats it knows to see if any of the headers match. If not, it refuses to open it. The only thing special about a text editor is that text files have no special headers, so text editors have to accept anything, even if the result is nonsense. Classic MacOS actually wouldn't let a novice accidentally open up non-text files in text editors because the filesystem itself imposed extra structure on files, so the file format was declared independent of the raw contents of the file and text editors had the information necessary to impose the same kinds of "Does this declare itself to be a text file?" checks as other formats have.
*fwiw* when similar questions were asked about Free Pascal, the answer was — Does not use `libc`.
Given that they're using the same algorithm, i think it's almost certainly a bug.
Explicit SIMD. 'nuff said.
Two interesting facts: - on my machine, the difference.is not as big (2.7s for rust 1.32 vs 3.1s for gcc 8.2). - I was wondering whether the difference could be that LLVM deals with the code better than gcc. But no, the C.code built with clang 8 is actually slower! (3.7s for clang vs 3.1s for gcc)
Yeah but thats for 50 million bodies
I mean it's less about Rust and more about LLVM. By the time x86 codegen (i.e: SIMD instructions being emitted) starts happening, the IR is a very, very long way from the original Rust.
Yeah but 20 hours is for 50 millions step simulation
Presumably it helps reduce the number of operations needed elsewhere to scale things.
Sorta yes, sorta no. See the below comments about clang vs gcc.
Beautiful description. Thanks for this
Is it not the case that LLVM IR permits the explicit notation of more invariants than standard C does, thereby permitting Rust, with its invariant-rich type system, to produce IR that tells LLVM, in more precise terms, what it's permitted to generate compared to a language like C?
Then, again, you benchmarking gcc vs llvm instead of rust vs c/c++. I think when you benchmarking something it. Should have a many things common as possible.
That can't be the whole story though, right? Otherwise projects like fearless\_simd wouldn't be attempted. 
Explicit SIMD allows the compiler to make more assumptions about things like alignment, requires it to do less work to vectorise (and therefore is more likely to vectorise), etc.
Thanks!
Always glad to see more projects like this! I have implemented multitasking in `dumb-exec`, but I'm not entirely sure how to test it. I also have an implementation that should support multithreading with alloc, but again, testing, and there's more unsafe code than I'd like.
This is very cool! (for context on the rest of this post, my job is to ship async/await in Rust) I also wrote an executor a few weeks ago (not alloc-only because it uses a threadpool). I want to try to find good benchmarks for comparing different executor implementations, and get a meaningful public conversation about the trade offs of different design decisions. There's tons of different ways to implement executors with different trade offs, and not enough information about the choices is easily available. The main thing I've been blocked on is figuring out a good set of benchmarks that are revealing. Just doing hello world examples is not very great because even with async IO they end up being IO bound since theyre just writing some bytes in response to each request. I'd really love ideas for good benchmarks for executors so we can kickstart that conversation.
&gt;You can also get rid of recursion if you get a pencil and piece of paper. What's that? Also, you're right, it was several times faster. I'll remember what you said. Thank you.
Yeah but [5.7 secs](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/nbody-rust-7.html#log) is for 50 millions step simulation.
I've seen this in a number of experimental academic projects. It's entirely possible someone was too lazy to read up stdlib definitions for their language, and that the result stuck around through ports.
[Machine code and information required for running it, organized down to an exact science.](https://en.m.wikipedia.org/wiki/Executable_and_Linkable_Format)
Desktop link: https://en.wikipedia.org/wiki/Executable_and_Linkable_Format *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^234643
Maybe some details didn't notice, and, unconsciously, it can be executed on the web. However, I believe that more tests are needed. This is my [git](https://github.com/pili2026/cassandra-rs-driver).Currently executable code.
Yeah, benchmarking is definitely going to be a challenge, even moreso for embedded environments with which I have exactly zero experience regarding benchmarks. I guess it really depends on what you're actually trying to measure, e.g. spawning new tasks, waker efficiency, etc. Different benchmarks for each characteristic will probably be required. Something like "spawn as many no-op tasks as possible within a certain time" or "see how quickly you can pull this future that just calls wake immediately."
My bad didnt see that!
Thanks, that's a very good post!
That's pretty cool! I'm excited to see more experimentation about whether we can use futures on embedded systems. Regarding executors the biggest challenge is avoiding dynamic memory allocation. We need to either spawn all futures at startup, or the executor should be configurable to have memory for exactly 3 instances of Task A, 4 of type B, etc. I'm currently working on a crate which aims to offer lots of async primitives for `no-std`. Hope this will also become useful for things like this. I'm also interested in making those generic over underlying locking systems. I might look into `lock-api` and `RawMutex`, as you did. 
&gt; What's that? Every recursion could be rewritten using loops. Church thesis, you know. But the main point is algorithm. They say, compiler may optimize away extra conditions or variable creation, but it can't make a bubble sort a quick one. So, try to optimize your algorithm. The best way is make it declarative, as I did. E.g. in this case I described an algorithm as a permutaton of alphabet concated recursively on itself. Because of that, compiler have done very good guesses about how code should lay and it resulted in good performance (beside extra allocations on each step). Iterators are always better in describing data pipeline. When you join several pipes you get the whole processing chain. And it's free, so use if if you could.
Oh hey! Your dumb-exec was one of my inspirations 😁 Have you had a chance to run it on any actual hardware? I've been testing mine with a little async/await-style serial echo server: https://gitlab.com/polymer-kb/firmware/polymer/blob/wip/src/echo.rs
Yeah, I'm actually using embrio for a lot of my other exploration in the async embedded space! Particularly the read/write traits and async/await macros. I was pretty bummed to discover that the versions in nightly still rely on TLS! I'd love to have a variant of my executor that works with `heapless` collections instead of `alloc`. That's originally what I was going for before I hit a few walls and decided to take the easy way out. Now that I have this version working, I might take another crack at it.
I inspired someone! \^_\^ `Spawn` and `LocalSpawn` are parts of the `futures` crate rather than the core functionality. It should be possible to get away with not implementing them, but why not implement a trait if you already have the needed method? Funny story, I'm not actually that big on embedded development. I'm mostly interested in `no_std` parts of rust for operating system development. (Like Redox on a much smaller scale.) I've been planning an x86 keyboard driver that sends key interrupts along an asynchronous stream. There are also parts of the disk driver that require waiting for certain interrupts, which would also probably be a good usage. My furthest attempt at an OS died at the disk driver in part because I didn't know how to suspend the function while waiting for a needed interrupt.
where is your exercises codes?
&amp;#x200B; So I'm going to use a simplified explanation of how GCC tooling and how loading with execve works as an example. So you got object files, they are generally assembly or some sort of intermediate byte code, that has different memory regions for things like functions, static variables, etc, they contain symbols which are functions, static, and constant variables. So essentially each module has it's own object file, that is generally assembly code, that has two sets of symbols (defined, undefined). defined is basically functions and variables defined within that module, undefined is calls to functions and variables in other modules. During compilation the process has two sets (defined, undefined) it will iterate through all the object files and append their undefined sets and defined sets. It then creates one big object file. if diff(undefined, defined) is empty then you can go on to linking, which produces a big object file with all of the symbols that contains a relocation table to help redefine the addresses of all the functions and variables at runtime . I'm going to keep the explanation about the loader short, since if you get me talking about kernel shit I won't shut up. Then the loader reads the file, sets up the addresses of functions and constants by reading the reloc table, sets up the stack, etc. If you're talking binary formats it's not rocket science, unless you're super into information theory, and want to over optimize shit. For example here's an http like binary format. \`\`\`\` status = u8 str = {len: u16, inner: string} method: u8 = GET | POST | PUT | DELETE header = { key: lfp, value: lfp } headers = {size: u16, data: \[header\]} frame\_type: u8 = Request | Response packet = {len: u32, kind: frame\_type, body: buf} request = {method, path, headers, body} response = {status, headers, body} \`\`\`\` 
Alright. I understand some of the ways we can use generics and things like `dyn Trait` and enums, and how they compile to machine code, but I'm not an expert on performance. With that said, I think benchmarking was a great idea. Results on my computer match yours. But I went ahead and ran some more benchmarks, this time on a smaller size, but different configurations: enums/(100000, ("e", "u", "u", "u")) time: [5.1429 ms 5.1543 ms 5.1630 ms] slope [5.1429 ms 5.1630 ms] R^2 [0.9895317 0.9898052] mean [5.1307 ms 5.1557 ms] std. dev. [38.101 us 88.541 us] median [5.1632 ms 5.1664 ms] med. abs. dev. [4.0137 us 8.9099 us] enums/(100000, ("u", "e", "u", "u")) time: [4.6021 ms 4.6118 ms 4.6179 ms] slope [4.6021 ms 4.6179 ms] R^2 [0.9944983 0.9948594] mean [4.6045 ms 4.6335 ms] std. dev. [27.102 us 114.37 us] median [4.6183 ms 4.6245 ms] med. abs. dev. [10.297 us 16.975 us] enums/(100000, ("u", "u", "e", "u")) time: [15.878 ms 15.899 ms 15.925 ms] slope [15.878 ms 15.925 ms] R^2 [0.9963715 0.9962631] mean [15.879 ms 15.907 ms] std. dev. [37.770 us 100.75 us] median [15.872 ms 15.886 ms] med. abs. dev. [33.294 us 50.007 us] enums/(100000, ("u", "u", "u", "e")) time: [4.6504 ms 4.6632 ms 4.6759 ms] slope [4.6504 ms 4.6759 ms] R^2 [0.9736116 0.9736065] mean [4.6156 ms 4.6674 ms] std. dev. [63.160 us 194.18 us] median [4.6716 ms 4.6952 ms] med. abs. dev. [30.824 us 60.909 us] single_generics/(100000, ("e", "u", "u", "u")) time: [2.6180 ms 2.6232 ms 2.6282 ms] slope [2.6180 ms 2.6282 ms] R^2 [0.9891193 0.9891905] mean [2.6165 ms 2.6296 ms] std. dev. [21.571 us 44.124 us] median [2.6260 ms 2.6372 ms] med. abs. dev. [15.183 us 27.430 us] single_generics/(100000, ("u", "e", "u", "u")) time: [2.3041 ms 2.3322 ms 2.3535 ms] slope [2.3041 ms 2.3535 ms] R^2 [0.8145892 0.8199274] mean [2.2997 ms 2.3343 ms] std. dev. [48.814 us 121.78 us] median [2.3315 ms 2.3402 ms] med. abs. dev. [19.115 us 40.702 us] single_generics/(100000, ("u", "u", "e", "u")) time: [15.851 ms 15.884 ms 15.920 ms] slope [15.851 ms 15.920 ms] R^2 [0.9883505 0.9882329] mean [15.828 ms 15.890 ms] std. dev. [133.45 us 179.79 us] median [15.767 ms 15.839 ms] med. abs. dev. [70.262 us 154.57 us] single_generics/(100000, ("u", "u", "u", "e")) time: [2.7672 ms 2.7732 ms 2.7784 ms] slope [2.7672 ms 2.7784 ms] R^2 [0.9901803 0.9903436] mean [2.7677 ms 2.7781 ms] std. dev. [19.948 us 32.357 us] median [2.7779 ms 2.7827 ms] med. abs. dev. [7.9616 us 16.670 us] (The repository doesn't seem to be in the same place for me to submit a benchmark PR, but I can share the code if that would be helpful) If I'm reading the results right, this shows the third "e" - the one configuring MonoGray or not, is the slowest, and also the part with little to no speedup from generics, while all the other parts get about twice as fast! With these benchmarking results, I think that your `HashMap` access is bottleneck. Maybe something like https://github.com/sfackler/rust-phf or https://github.com/Amanieu/hashbrown could speed this up to a point where the enums vs. generics part actually matters? Let me know what you think - this is definitely interesting to look at from a performance-of-different-things perspective. 
I recognize that tutorial, I've gone through it before! Great tutorial.
Yeah! I'm using a different stack than the wonderful dev who wrote it, but the although I know OpenGL enough to use it, I needed some good examples on how to use the gl crate because I'm fairly new to Rust. &amp;#x200B; I'm using OpenGL and GLFW as a baseline for a gui crate which I'm then going to use to make a 2D space adventure game, but I want a fairly robust 2D GUI library underneath to help me along
Feel free to checkout my progress: [https://github.com/00benallen/gui\_core\_00](https://github.com/00benallen/gui_core_00)
Cool!
To really understand executables you should probably start with learning something about assembly since that's the closest thing to executable code you can really work with. The next thing after that is to learn about the ELF file format which is used by Linux, and Unix, as their binary format (Apple uses a format called Mach-O, and Windows uses one called PE). It will probably take a while to understand, but this is a really great tutorial that should answer a lot of you questions: http://www.muppetlabs.com/~breadbox/software/tiny/teensy.html
Why do you need a common interface for a Map?
The original `perf` has more features than simple CPU profiling; by "casual profiling" I meant needing only such simple CPU profiling without any of those more exotic features. :) `nperf` is probably somewhat easier to use than `perf` (e.g. it works by default, while `perf` doesn't use DWARF based unwinding by default so you won't get proper backtraces by default) and it also has some minor enhancements, e.g. there is a bug (at least I think it's a bug) on AMD64 where Linux will just send you incorrect values of the EBP register. `nperf` has a workaround for that while `perf` will just grab an incorrect backtrace. &gt; Would you recommend nperf for benching http requests? Well, if you need to figure out what uses the most of your CPU time then maybe. (:
The reason is I've two sets of String data one that needs to be sorted by key and the other does not and I want to perform the same operations on them.
Something along these lines `macro!(fn map_op(a: MapType, b: MapType) {...}, HashMap, BTreeMap)` would produce 2 functions
Welcome to Rust! &amp;#x200B; Working on a game as well here. Did you look at glium or gfx? I'm kinda on the fence whether to move to pure OpenGL because glium is dead and all wrappers end up becoming dead it seems :/
Not really. The original C version uses explicit SIMD in a similar way, and is significantly slower. It might be a gcc vs. llvm thing, or maybe something about Rust, but it's not nearly as simple as just SIMD.
While impressive, it's a shame the winning solution is riddled with `unsafe` blocks in its `advance_bodies` method. I can't wait until Rust gets better SIMD support and we don't have to deal with that kind of jankiness.
Could you give me a link? :)
Sorry, updated with link to the same
Would leveraging virtual machines help with benchmarking?
Sounds like you need to roll your own trait. This might sound like a lot of work, but some thinking and code has already been blazed before: * https://users.rust-lang.org/t/no-common-trait-for-map-types/8275 * http://apasel422.github.io/eclectic/eclectic/map/trait.Map.html * https://crates.io/crates/multi_mut
Is there any reason that you couldn't simply use a `&amp;'static [&amp;'static str`]? `Vec` seems unnecessarily limiting, especially since I imagine most of these handlers will be fixed for a single callback in typical use cases.
So while I love the ability to one-up C, the code reads a lot like C-style rewritten in Rust. I wish there was a benchmark in each language for the *idiomatic* way to write in that language, even though I know this is a very subjective thing.
Compared to the C variant, the differences are: \- Using rustc \- Moving the loop from outside into "bodies\_advance(..)" (SSE pipelining(?)) \- Bundle intermediate variables/arrays as struct NBodySim (caching) \- Fit array-sizes within struct NBodySim to the number of bodies (caching)
This is a fantastic resource for questions about GUIs in Rust right now: http://areweguiyet.com/ The short answer is...well, not really. The ecosystem is getting there, but there's nothing I'd call "production-ready" yet. Not-Yet-Awesome Rust (which I maintain) has a [section](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust#user-interfaces) dedicated to documenting the holes in the UI area of the Rust ecosystem right now too. :)
I noticed that regex redux is only faster than rust when using the jit functions of pcre. Do you think there is any hope in getting the rust regex crate to match the performance? Do you think that reintroducing and optimizing the regex!() macro would help at all?
I was thinking that a programmer might want to conditionally trap signals, but it does seem like a very unlikely use-case now that you mention it.
&gt;Nobody really cared about readers gender before you appeared. Looks kinda double-faced. How ironic. You are so intent of making sure everyone who reads this be a guy. I seem to be the only one here who doesn't care about the readers' gender.
References *are* pointers, but Rust ensures the validity of references using borrow checking. Raw pointers can be created from references, but doing the inverse requires the use of `unsafe`.
[How to draw an owl with OpenGL and Rust](https://cdn-images-1.medium.com/max/800/1*ps1LThoFav5Joqyd1mPAyg.jpeg): 1. Draw a triangle. 2. Draw the rest of damn Owl. ^(A fun and creative guide for beginners)
OpenGL is dead, move to `gfx-hal` ;3
It is basically the C version with a few extra intrinsics sprinkled in, such as the `dsquared` calculation. Don’t get me wrong, it is fun to reach the top, however, I would argue they are both a mixture of higher-level and assembly code (intrinsics) rather than idiomatic versions of their respective languages. 
What tutorial is it?
Thank you for the answer. I'LL use the links provided in your comment and find something to use.
Hi congrats! For my information, why is the `fn offset_momentum` implemented with `for` loops instead of `map`
Enjoy your downvote m8
Some things just need to be explained like three times by three different people for me to grok it, and it seems like this task-executor-wake-thing is one of them, so thanks for being the third one writing about it :-) I'm still not sure exactly what a `poll(&amp;mut self)` method can do that a `poll(Pin&lt;&amp;mut Self&gt;)` can't, especially since it seems like every type in the world implements `Unpin`, but maybe it's just me...
The typical wisdom is that LLVM is better at optimising maths-heavy code, whereas GCC is better at optimising code that spends more time jumping around a larger codebase. Not sure how true that continues to be though.
Yes, it should never be deemed shameful to change your mind and be open about it.
We're definitely working in this direction. Except hyper PRs as soon as we have HTTP 3 half-way working.
I was surprised to see both of you talking about this, yet I could find no issue in the rustls repository about this. &amp;#x200B; I've filed one now: [https://github.com/ctz/rustls/issues/227](https://github.com/ctz/rustls/issues/227)
If the keys are relatively stable, you could use [indexmap](https://crates.io/crates/indexmap) for both, and just sort the one that needs it. But you wouldn't want to do this if there's much churn from inserts and removals mixed with accesses that need sorted order.
Answering myself as I found [this comment](https://github.com/rust-lang/rust/issues/55766) : &gt; Generators and the types of async functions are !Unpin. Types that could contain a generator or an async function body (in other words, types with type parameters) are also not automatically Unpin unless their type parameters are. 
Yep, that's really ironic, since you're the only one who did a big deal from someone's gender when everybody else just respectfully don't care. But you will never rethink your decision so it's kinda waste of my time. So I won't, I have more interesting things to do.
&gt; fwiw &gt; when similar questions were asked about Free Pascal, the answer was — Does not use &gt; libc &gt; . But the Fortran program prints the output, I'd expect whatever Fortran uses to need to link to libc for I/O. Or does it use syscalls directly ?
I don’t get why this comment gets downvoted. It’s a good point that this Rust code is not safe, and hence is not idiomatic daily Rust code, but highly optimized Rust code.
The benchmark game doesn't support nightly Rust, but `std::simd` has an `n-body` benchmark.
&gt; e compiler will downgrade intrinsics the target architecture doesnt have sometimes It is undefined behavior, so yeah, sometimes it might do that.
TCP + `serde` will be the way to go.
How does this compare against the `std::simd` implementation ?
An executor that runs in the same single thread, does not do/require any allocations, and has no concurrency (no multi-threading), is probably going to beat all of those benchmarks. 
This is interesting. Based on your results I changed the implementation and tested both libraries. The results are as follows: Hashbrown w/ MonoGray Size: 563.6631 MiB Enum: 5.8268 | 5.5825 | 5.5977 Generic: 5.5578 | 5.5494 | 5.6228 Hashbrown w/o MonoGray Size: 563.6631 MiB Enum: 3.5159 | 3.7454 | 3.6437 Generic: 2.3640 | 2.4013 | 2.3851 PHF w/o MonoGray Size: 563.6631 MiB Enum: 2.7836 | 2.7082 | 2.7034 Generic: 2.3418 | 2.3435 | 2.3760 PHF w/ MonoGray Enum: 13.2034 | 13.2956 | 13.2821 Generic: 13.0261 | 13.1651 | 13.1152 ~563.66 MiB | w/ MonoGray | w/o MonoGray | Generics ----------------------------------------- PHF | 13.1152 sec | 2.3435 sec Hashbrown | 5.5578 sec | 2.3851 sec ----------------------------------------- There is a huge slowdown for using `phf` if `MonoGray` is set. This is weirder than before: * Generics are faster than Enums. Nothing contradicting here. * Slowdown `phf`: For generics the `phf` crate seems to create a huge overhead if `MonoGray` is used. While `hashbrown` works only at half the speed (\~5.5 vs 2.4 sec); the `phf` crate seems to have a 5.7x slowdown (\~13.1 vs 2.3 sec). * Slowdown `hashbrown`: While `hashbrown` seems to be the better choice, it also shows a weird slowdown if MonoGray is not used. The HashMap is only declared and not used at runtime and the enum version is 1/3 slower than generics. This might be due to the characteristics of enums not get monomorphed(?) at compile time, but it still seems dramatic espacially if this part of the code is not executed.
There are some nice diagrams of common header formats like elf here https://github.com/corkami/pics/blob/master/binary/ELF.png
That doesn't seem to be true in this case: https://www.reddit.com/r/rust/comments/akgmef/comment/ef50c67
Note that there are already 2 frontends using gtk-rs: neovim-gtk and gnvim.
Do you want to be friends ? 
Can someone explain why Go takes almost 60% more time than Java, despite being compiled to machine code? Is the JVM JIT just that much better than the Go compiler, or are there just more people willing to optimize Java?
I've been using glium recently and it's just a delight, everything works, uniforms, attributes and textures are not a complete PITA, and you pass all options in a single struct instead of setting them all over the program. Especially nice after C++ with its raw stateful OpenGL (Or C, I guess, since it's the same library) and JS with its raw stateful WebGL.
Go’s compiler is not that smart in comparison with others. It doesn’t do lots of optimizations. Another thing is that some programs are written badly without considering the “best practices” for writing go code.
Maybe you should look at some game engine at http://arewegameyet.com, but as many point out, OpenGL is dying, so better go with a modern alternative gfx-hal or a vulkan wrapper.
Care to share the tutorial? :)
I made a little crappy game for a college project in Rust + OpenGL, its not exactly great code or anything but maybe itll be useful I dunno https://gitlab.com/iksf/sokoban_5
OpenGL won't die, until there are GPU which does not support Vulkan. Like my GTS 450
Thanks to put a word on this fallacy. I learn something today.
Yes, short compilation times is one of Go's selling points.
need to check benchmark methods: java JIT C2 compiler optimization mostly effective with few cycles of calling.
The journey to first person shooter is a long one..
Go's regular expression engine as well as FFI are very slow.
C# standard library is slower than libpcre
The author of nperf replied in this thread. See it for reference. BTW, main can now return a Result&lt;(), Err&gt;
I know what the point of the borrow checker is (obviously), my point was that it gets in the way when you do something non-trivial. Often in such cases I start missing pointers which would make the problem at hand simple (as I know exactly where an object is created and destroyed and know that a pointer is safe), in Rust I end up using something like `Rc&lt;RefCell&lt;&gt;&gt;`.
Whomever is writing the Go variant of a performance benchmark is going to use optimized code
To be honest, I found this article too exhaustive and not goal-oriented enough. I mean every information is interesting on it's own, but when the pure rust solution is 10 LOC, I don't have to learn about CubeMX. (Which would nicely fit into a separate article.) Otherwise it was still a good read about how the clock is setup. The [previous tutorial](https://nercury.github.io/rust/embedded/experiments/2018/04/29/rust-embedded-01-discovery-vl-flipping-bits.html) he linked was a very good read about the internals of a MCU and how to get it running with Rust.
The Tower API has a very Tokio-ish feel to it. IMO not a good thing.
Thanks for posting this. It looks like a fun project to me. What I'm missing is a how to contribute section and/or "Good first" labels for issues. 
Could you please post the script as a gist? On mobile, clicking on the link gives an obnoxious "pop-up" for signing up with no obvious way to remove it.
Wew, congrats! What are you using? The `gfx` or `piston` crates? 
Good point. As a quick fix, capping each benchmark at no worse than 3x of the best time to reduce influence of outliers, significantly shortens the gap: Time score: c: 1.0 rust: 1.0378 cpp: 1.0647 cs: 1.7704 java: 2.0855 go: 2.2062 A more proper fix would be to reverse scores so that best score is 1 and others range from 0 to 1, so that a very poor performer on one benchmark won't do worse than get no points in that specific benchmark, rather than cascade the bad score in an unbound fashion to the overall score. I'll look at it tomorrow.
Median and geometric mean are other tools for this kind of problem.
Maybe the C benchmarks aren't very ergonomic either but I think a lot of the appeal of the language gets lost when every other line is you writing an assembly instruction
This is a result of my mistake of not reading enough documentation and using `write` where I should have used `modify`. The `write` function, of course, overwrites other register bits with zeroes. I did not immediately realized what is going on, because it sometimes worked. I kind of knew I was not setting bits right somewhere. In a desperate attempt to validate my sanity, I have searched for the official way to configure MCU, and found this Cube MX tool. Which generated the code that worked. But I am actually glad for this detour: the reference manual is huge, and having a tool that generates _correct_ and _working_ code is very valuable, even if it is all in C. That's why I left this in.
Well, that's why you should use glx-hal. You can't really develop for just OpenGL anymore, because important platforms have dropped support and its development is slowing, but at the same time you can't use Vulkan yet. It's a sad state of things that reminds me of the mess that web development is. Damn you, Apple!
Minor thing, but I think the geometric mean is [better](https://en.wikipedia.org/wiki/Geometric_mean#Properties) for this sort of comparison of normalized values. However, I changed the definition of `avg` to `array.inject(:*).to_f ** (1.0 / array.size)`, and it seems to give pretty similar results: Time score: c: 1.0 rust: 1.0354 cpp: 1.0527 cs: 1.7971 java: 2.4204 go: 3.025 Memory score (no floor): c: 1.0 rust: 1.1436 cpp: 1.1789 go: 1.6675 cs: 7.6834 java: 8.9363 Memory score (floor 50k): c: 1.0 cpp: 1.0491 rust: 1.0886 go: 1.245 cs: 1.4327 java: 1.7018
Or even written according to “best practices” for writing code in a situation where smart choice is doing something else. People love replacing thinking with best practices. “Best practices” is a starting point, not a finishing line.
This depends on your use case. If your goal to to assess qualitative properties of performance with diminishing returns, geometric mean makes sense (e.g. you win as much by going from 1x to 2x, as from 2x to 4x). But the arithmetic mean still allows some kind of qunatitative understanding of how much "on average" language X is faster or slower than language Y. The average's goal here is to normalize differences across *benchmarks* with different running times, not to normalize differences across *languages*.
It's not slow **despite** of being compiled ahead of time, but **because of that**. - Java is JIT compiled, so it can take input from the runtime and run optimizations on the hot parts of the code. The good thing with JIT compilation is that it allows really quick compilation and still decent performance. - Rust or LLVM languages are compiled ahead of time and they spend a lot of time optimizing everything so it can get the best possible performance. It has the best performance profile, but the compilation is really slow. - Go compiles everything up front, and doesn't benefit of any JIT opimization, but it still wants to have fast compile-time. It does so by only performing the minimum amount of optimizations. The compile time is great, but the performance are less good. If your main issue is compile time, JIT is a good option but an AoC compiler like Go's is much simpler to implement than a JIT compiler. 
I wonder which portion of the C/Rust difference comes from LLVM/Gcc and which directly comes from the language themselves.
I don't know. Compile time regexes seem orthogonal to me. I haven't looked at the benchmark in a while, but last time I checked, the benchmark involves a very large number of matches that require the start/end of each match. A backtracking library can resolve the start/end offsets in a single pass through the text. `regex` does it with a [forward scan followed by a backward scan](https://github.com/rust-lang/regex/blob/master/PERFORMANCE.md#only-ask-for-what-you-need). This extra backward scan, if memory serves, seems to have been the key difference. So unless I'm missing something, the way to make it go faster is to figure out a smarter way to do start-of-match handling that doesn't require the backward scan. This isn't possible in general, so you wind up needing to do it for only some cases. Regex benchmarks are notoriously difficult to construct. The regex crate actually has a benchmark harness that includes several regex engines (D, Rust, PCRE1, PCRE2, Tcl, Oniguruma, RE2, Boost regex and C++'s `std::regex`), but I've never published them in any meaningful way because it would be irresponsible to do so without a thorough analysis, and at least an attempt to check my own biases.
Yes. At the very least Rust knows a lot more about pointer aliasing. But I remember that information is not passed to LLVM because of a bug in LLVM. Not sure if it’s been fixed.
That's not why. This analysis uses the top-scoring benchmark for each language, and Go's [top scoring benchmark uses PCRE](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/regexredux-go-2.html). However, it appears to be using PCRE1, although the Go import is [now a 404](https://github.com/tuxychandru/golang-pkg-pcre). Moreover, it does not appear to be enabling the JIT. That means there is potentially is an easy win out there for Go to use PCRE2 and enable the JIT.
The geometric mean can still be used to normalize across benchmarks. For example, you can peg C as 1.0 for every benchmark, compute a relative number for a language, and take the geometric mean of this relative number across benchmarks. This corresponds to the intuition that if (for example) C is 2x faster than Rust on one benchmark and Rust is 2x faster than C on another, then "on average" C and Rust are equally fast. Taking the arithmetic mean does not satisfy this property.
Is `image` the recommended crate for doing graphic work, or is there another library? Because I was trying to mess around with it and apparently there was an issue with a different crate that stopped it from compiling.
Asking the real questions.
The only thing `&amp;mut self` can do that `Pin&lt;&amp;mut self&gt;` can't is safely mutate self when `Self: !Unpin`. Since mutating self *could* constitute moving the value, you have to make extra sure that you're not going to do that when unsafely getting a mutable reference out of the `Pin`. It took a while for it to really click with me as well. I think it was trying to wrap my head around the safety of /u/Nemo157's [embrio-async](https://github.com/Nemo157/embrio-rs/tree/master/embrio-async) that finally did it for me.
Well, mostly what I'm doing this week is setting up a bunch of self-hosted services ([Gitea](https://gitea.io/en-us/), [Laminar CI](https://laminar.ohwg.net/), etc.) to manage my non-open-source personal projects. I probably don't really need any of this stuff - I was getting by just fine doing it by hand as needed - but it's a good excuse to learn about Docker and I can maybe get some use out of a new computer I bought. In the meantime, though, I'd appreciate if Criterion.rs users could weigh in on the [0.3.0 Roadmap](https://github.com/bheisler/criterion.rs/issues/261).
Off topic, but I'm a little shocked at how fast C# has gotten. I've been used to hearing that Java is messier but faster, but perhaps it's not true anymore. Also, Go performing worse then both C# and Java is surprising. But yeah Rust is awesome, great to see it's performance has caught up to C and C++
I think this mess is fault of the open source state of web and graphics, because they are too many ppl involved that is so hard to get an consensus quickly in a subject, just take as an example html5, how many years it took to get where is now, and javascript too, is just too hard, that’s why I sometimes appreciate closed or proprietary implementations (as a consumer, not as a developer), because you can see in short periods of time innovations. But yes, I mean by OpenGL is dying because is the old implementation that will eventually get dropped, not soon but later. Can you elaborate more about your Apple citation?
Your battery is running low and I love that background color ;) Have fun, but also check out gfx-hal.
&gt; for another MCU, I found out that this tool sets different flash latency depending on the system clock speed. Thats why I think CubeMX rather deserves a separate tutorial. But I know I'm a choosing beggar here. Thanks for your time to write these tutorials. 
\&gt; nor indicative of expected performance on real-world idiomatic code &amp;#x200B; sure it is &amp;#x200B;
\&gt; Java is JIT compiled, so it can take input from the runtime and run optimizations on the hot parts of the code. &amp;#x200B; That doesn't really matter in this case, the programmer can do those optimizations since they know exactly what the runtime will be, and these benchmarks are too short for hotspot to do anything useful that is worth the time spent doing it &amp;#x200B; &amp;#x200B;
C# has been faster since dotnet core 2, and platform intrinsics are coming, its going to get faster still. &amp;#x200B; however, for the regex benchmark, C# now uses PCRE, anyone else not doing that is losing that benchmark mostly because of not using PCRE &amp;#x200B;
[learnopengl.com](https://learnopengl.com) &amp;#x200B;
I have questions about the following pseudo-code let (sender, receiver) = mpsc::channel() send_to_another_thread(move || { let x = get_from_external_source(); sender.send(x).unwrap(); } let x = receiver.recv().unwrap(); OTHER_CODE_USING_x In other words, I need to get some data from an external source in another thread, and then a callback will be invoked on that data (the callback would just be the `sender.send` call in this example). I don't have direct access to that other thread, but I need that data to continue within that function. I figured I could make the callback send the data through a channel, and then in my own function just wait on receiving it. Am I setting up myself for a race? Will the sender wait for the receiver to get the data before the closure ends (which will drop the sender and probably close the channel)? I'm not sure if that can really happen in my application, but theoretically `get_from_external_source` could be very fast, right? Is this a performance hazards? The code part in question is one I care very much about, but I'm not sure I can really set up a proper benchmark for this part. Is there a better pattern? The `x` is a very cheap small thing (wrapping an integer)... Thanks for any pointers :)
Just now I got macros working in my dialect of Lisp. I will release it once it's (partially) compatible with r5rs. I do not plan on making it source-compatible with r5rs, but the syntax, standard library and forms are inspired by r5rs (with the exception of macros, which are more closely related to Clojure). I'm also working on a tmi.js-inspired Twitch API in pure Rust. It's partially functional and I'm thinking I'll make a 0.1.0 release quite soon. Other than that, I'm still working on my 3d game in amethyst, which I develop [live](https://twitch.tv/walterpi) 4 days a week.
apple deprecated opengl like a year ago. so at some point in the future opengl could stop working on mac/ios &amp;#x200B; its mainly proprietary interests causing the fragmentation. directx on windows, metal on ios/mac. People have had to fight tooth and nail to keep opengl on those platforms. &amp;#x200B; &amp;#x200B;
That's a very good educational resource. Thank you!
Also see https://users.rust-lang.org/t/why-is-a-lookup-table-faster-than-a-match-expression/24233 for discussion.
Fortran uses it's own io routines which are (very shitty to use) thinly wrapped syscalls. I joke, but gosh I still love Fortran.
Ah that makes sense. FWIW Go does the same thing. This approach has Pros and Cons.
But there is MoltenVK, where you can use vulkan API in mac/ios to work with metal. I kinda like that move, because opengl lack a tons of features and isn’t suitable for multi thread, it was not designed for that in mind when it first comes out. And yeah, fragmentation is bad, but you can build a common library on top of all those graphic libraries, like gfx-hal, and major engines I believe do the same thing. But as I stated, open source implementation that need consensus takes a long time to get a new feature added and agreed by everyone, when proprietary implementation (dx, metal) let you implement new features in a short time, and companies like apple and microsoft need to deliver new features for their own products to stay relevant with the competition (ios devices, xbox consoles).
How does one recreate header-definition separation from cpp in rust? I want to have declarations that you can understand at a glance without reading their entire code.
Well diagnosed. You know you're dealing at the pointy end of things when C's I/O is relatively intuitive and feature rich. 
&gt; why not implement a trait if you already have the needed method? Exactly! Though I'm not the hugest fan of the current API of these traits. The `SpawnError` type only has the one `is_shutdown` variant, where I think that, for executors with a static capacity, it also needs a `no_capacity` as well. *But*! that introduces another problem. If you're trying to spawn a future within another future and you hit the "no capacity" error, what do you do? Return a `Poll::Pending`? What happens with the `LocalWaker`? Do you just call its `wake` method before returning your `Pending` result? That seems inefficient to me, since the task will just get polled again immediately, even if no capacity has opened up. It almost seems like we need `Spawn` (&amp; co.) to be defined more like trait Spawn { type Error; fn poll_spawn( self: Pin&lt;&amp;mut Self&gt;, lw: &amp;LocalWaker, // Not sure how best to represent the fact that the // spawner will only take ownership of the future // on a `Ready` result. `take`'ing it from an `Option` // seems like a decent way to do it. future: &amp;mut Option&lt;FutureObj&lt;'a, ()&gt;&gt; ) -&gt; Poll&lt;Result&lt;(), Self::Error&gt;&gt;; }
Woohoo, more content in Phil Oppermann's Blog OS!
If you care are going to make a GUI crate, please release it to the rest of the world! Put a bit of push behind it and you will find many people interested in it.
https://github.com/rust-lang/rust/pull/50744
For me, it's a lot of domain-specific bugs that aren't really "errors" the way `Result` would describe them, but are incorrect results. Math in graphics code, mis-implemented instructions in interpreters, that sort of thing. I used to be frumple about Rust's insistence that integer overflow is an error by default, then I had it save me from a few stupid bugs and now I won't live without it.
I heard that it's called [POBOS](https://www.reddit.com/r/rust/comments/ahap4n/is_it_time_to_rewrite_the_operating_system_in/eecx5jm) now :D.
&gt;But there is MoltenVK, where you can use vulkan API in mac/ios to work with metal. and gfx-hal 
Definitely use glium over raw OpenGL, even now. I agree it isn't as well maintained, but it's still going to save you so much time. You might still want to consider gfx though, depending on your needs. I haven't played with the new gfx API yet.
You can `impl` any type in any module (as long as it has access, I guess), so you could put the type definitions and traits into one file, and then `impl` it in another one. Generallyl though, I don't see why you'd want that, given that rustdoc easily puts out a good overview.
Good thing to know for next time I write an average program :).
I've been wanting to re-work my gfx-prell renderer for my game Energy Grid to use batching. Though I am also tempted to convert it to using gfx-hal, so long as the OpenGL backend is working well :)
Surprised me too. Looking over the benchmarks, they seem to be heavily numerics based. If there were some that involved lots of string processing, I think it would not be so impressive (due to more string allocations and UCS-16 string implementation).
[Here you are](https://gist.github.com/dmitmel/7cbbc505fd00b09818402dc2060ef402).
PM me for details. And I’m on US FP east 1 
I once spent 1 hour looking at a 200 lines prolog program because i had a function name starting with lowercase (a 'fact') instead of uppercase (a function). Humans are crap at seeing what's in front of them when their stubbornness against going back to basics interferes.
Oh well, I already opened my computer and wrote a Python version. Thanks though :).
That's not strictly true. Going from unoptimised code A to optimised code B requires about the same effort regardless of compiler. The difference between a JIT and an AOT compiler is that a JIT guesses at run time, while an AOT compiler guesses at compile time. JITs like HotSpot rely on heuristics to measure the currently hot code, and guess based on that. But these heuristics can be wrong, and [can misoptimise code](https://tratt.net/laurie/blog/entries/why_arent_more_users_more_happy_with_our_vms_part_1.html). Because AOTs don't know what the hot code is, they are much more conservative and usually try to optimise everything. This means they're usually *redundant*, but *never wrong*. Neither JIT or AOT is a panacea.
I don't see any issue with your code as long as you're doing something between sending x to another thread and waiting for recv(). If not, you're essentially writing single-threaded code with a little less performance. It will block on the recv() 
godbolt.org doesn't contain any code.
Wrong Rust...
It's indeed basically single threaded code, but I have to use the function I called `send_to_another_thread` which takes a callback and executes it in another thread. Thanks for looking at it :)
Is there any good crates for currency manipulation?
So it specifically uses `cpu-cycles`, or some performance counter?
How does PCRE compare to RE2? 
It is already available as a crate — [`inferno`](https://crates.io/crates/inferno) — but for now it only has the `stackcollapse-perf` equivalent in it. It should be pretty easy to tidy that up so that you can give it any `Read` and `Write` pair to work with though. However, it would still expect its input to be `perf script` formatted, which I don't know whether `nperf` does? As for the functionality of the `flamegraph` program, that's something we'll likely port in the next part. You're unlikely to feel a big performance impact there though, because `flamegraph` usually operates on pretty small inputs anyway (compared to the full output of `perf script`). That will also become a part of the `inferno` crate though, so `nperf` should be able to use it!
Can't vouch for it, but https://crates.rs/crates/steel-cent seems pretty recent.
no idea, it seems like all the fast implementations in benchmarkgame are using PCRE though. 
So not really just a straight "port of fastest C SIMD variant".
I actually thought that the entry API (for [`HashMap`](https://doc.rust-lang.org/std/collections/hash_map/enum.Entry.html) and for [`BTreeMap`](https://doc.rust-lang.org/std/collections/btree_map/enum.Entry.html)) was a trait until I just looked it up now. Their methods don't seem to differ. Has it been considered combining them in a trait, even external maps could implement? I think `Entry` combined with `Index` could be pretty powerful.
&gt; In the meantime, though, I'd appreciate if Criterion.rs users could weigh in on the 0.3.0 Roadmap. This seems slightly relevant: https://kevinlynagh.com/notes/match-vs-lookup/
The type system can only get you so far (still pretty far). A lot of times with Rust I can "feel" my way through a new library by just lining up the type signatures until it compiles and it will be what I wanted! But sometimes that doesn't work. When I was first learning to use Futures, I was very confused about how to consume Futures and when I was able to `wait()` on a Future without locking up my application. Likewise, I usually prefer to use lower-level SQL libraries, like the `mysql` and `mysql-async` crates. Unfortunately, when you pull a row out of the database with a bunch of integer columns, you're going to have a tuple like: `(u32, u32, u32, u32)` and there's not much to save you from forgetting which column was which.
Are you using Rust stable or nightly? I had problems compiling image for nightly a while back; looks like its dependencies keep fairly up-to-date with nightly, so if you're using that you'll need to update to the newest version.
Depends what you mean by "graphic work". In any case, `image` seems to be the most prominent pure Rust library for loading and saving common image formats. Do you have more information about the compilation error? Which version of Rust are you using?
/r/RestOfTheFuckingOwl
&gt; I used to be frumple about Rust's insistence that integer overflow is an error by default, then I had it save me from a few stupid bugs and now I won't live without it. This very much, thanks to whoever made code panic on overflow in debug mode!
Note whether the answers you are given are: 1. speculation 2. analysis of specific programs
Neither, crates are: glfw = "0.26.1" gl = "0.11.0"
What is that?
&gt; … normalizes each benchmark so that the best time of any language is 1.0, and the other times are a ratio of that. https://benchmarksgame-team.pages.debian.net/benchmarksgame/which-programs-are-fast.html
There's another explanation that might help explain the results, but I haven't done the quantitative work. Many of these benchmarks rely heavily on FFI to optimized libraries (gmp, pcre, etc). Go's FFI has definite [performance issues](https://about.sourcegraph.com/go/gophercon-2018-adventures-in-cgo-performance). Most real-world applications of Go are pure Go, but this is a consideration. For example, OpenGL requires high FFI bandwidth, so I think this is one reason Go has made so little headway in the games space, in spite of its other advantages.
[Median and geometric mean](https://benchmarksgame-team.pages.debian.net/benchmarksgame/which-programs-are-fast.html)
Sorry, it is https://learnopengl.com/
Sorry, it is https://learnopengl.com/
Is better to say "nor indicative of expected performance" that WILL MATCH YOUR CODE. &amp;#x200B; Most codebases are much larger, have much more code paths and, VERY importantly, NOT MADE WITH PERFORMANCE AS MAIN GOAL. Also " real-world idiomatic code" means, in the \*real\* world: "Made by a bunch of smart morons that are tired, have not a complete set of requirements, and the few it have are mostly wrong, not have designed anything upfront, have not good tests, not have skills for how make performant code, neither how properly benchmark it (or analyze the results) have tested not with real data, was under a completely unreasonable set of demands, have a crappy machine (or use a laptop with under-performant components), haven't code in a system language, not know what a pointer is used, or how the computer work (really), ... " and so on. &amp;#x200B;
Life is not a competition
I will be so happy if Rust ever gets it's compilation times down into the same ballpark as Go compilation times (presumably via a mode that also does fewer optimisations).
Thank you too :)
I'm surprised by the number of people in this thread suggesting to use GFX instead. Similarly people saying, skip OpenGl and learn Vulkan. Both are much lower level APIs and if someone is getting started with graphics dev it's the equivalent of telling them to give up on Python and learn C++ or Assembly instead. OpenGL still has a place, especially when you consider WebGL and ES, and is a hell of a lot easier to learn than GFX and Vulkan. I'd posit many of the people suggesting the alternates are suggesting it because they have simply also heard of them and their advantages without actually trying them. They're fantastic alternatives but not suited for beginners by any means. -------- As a semi ironic aside, if you did want to learn a lower level graphics API and are new to graphics programming, consider Metal since you're on a Mac. It gives you similar low level access to the above mentioned APIs but there are a lot of convenience methods built around it so you can code with it in a style that I find even easier than OpenGL. It's not free, but this book is great for metal, https://store.raywenderlich.com/products/metal-by-tutorials There are also some free blog post tutorials on their site and lots of apple developer docs.
It’s there a metal binding for Rust?
It may still be better to use a match than a lookup table, for readability. It should be possible for the Rust compiler to generate more optimal IR for this kind of matching. I can imagine that there may be a lot of scenarios that the Rust compiler could potentially optimize better, before it reaches LLVM.
I believe .Net 3.0 is adding a Utf8String type, and a non-allocating JSON parser based on Span&lt;T&gt;, so string manipulation in C# might be getting faster soon.
all we have to do is require an interface that allows a *dynamic* number of futures to be polled concurrently, which can't be done by that executor.
/r/playrust
&gt; Many of these benchmarks rely heavily on FFI to optimized libraries (gmp, pcre, etc). Specifically which do you include in *etc* ? Or do you claim 2 (pi-digits &amp; regex-redux ) out of 10 is somehow "many" ?
Rust graphics programming is standardizing around gfx-hal at the moment, from the gfx-rs project. It provides a Vulkan-like Rust API that provides a hardware abstraction layer that can target any graphics API. Vulkan, D3D12, Metal, OpenGL. https://github.com/gfx-rs/gfx
Hmm... I thought the `regex` crate was normally faster than PCRE. Do you know why that doesn't apply here? Side note: It's fun to see PHP neck and neck with Rust here. Their approach of closely integrating C libraries into the standard library certainly has it's advantages.
Maybe I’ll give that a shot, now that I’ve got something going 
I imagine there is still a pretty good correlation. Benchmark game results aren't exactly out of line with normal expectations. C/Rust/C++ are fastest, Java/C#/go a little slower, node a little slower than them, Ruby slower than node. I think the primary wrongness is niche languages that don't have enough contributors doing good implementations. Like F# and Julia rate slower than they are capable of, because they are so niche. 
Programming language runtime is not life. Programming language runtime **is** a competition.
Making simple graphics like fractals. I'm using the latest stable version of Rust 2018. The problem was that there was an `extern crate flate` somewhere in the library and Cargo couldn't find it.
False False False False False False False False False False False False False False False False False False
Thanks!
I'm using Rust stable.
I disagree that the open source nature is what's causing issues for the web, in fact, ancient versions of closed source browsers (and even their slow adoption of standards in more up-to-date ones) like IE are what usually makes it hard to develop. Firefox/chrome/-ium, whose engines are very much open, tend to have self-updating mechanisms, and in general are very on top of the standards, so implementing for those two is rarely hard. Even the experimental APIs tend to be really similar, and are always quickly implemented. Besides, it's debatable how much influence anyone has on the standards. With something like 70% Chrome market share and the recent adoption of webkit in edge webkit basically *is* the reference implementation.
&gt;The Benchmarks Game is neither scientific… ? "[Scientific](https://dictionary.cambridge.org/us/dictionary/english/scientific) also means using organized methods, like those of science" &gt; … nor indicative of expected performance on real-world idiomatic code. How do you know? 
guy (gaɪ) n. 1. a man or boy; fellow. 2. **Usually, guys. Informal. persons of either sex; people:** *Do you guys want to go out tonight?* The word means what it means. "Doing better" is a noble goal, but you can't hope to achieve it by asking people to ignore their history and language; to step out of their comfort zones at the behest of your criticism, especially on the matter of the meanings of words where the choice is arbitrary. The letters `g u y s` are not inherently masculine letters -- they can be used for any purpose -- and you are surely not ignorant of the very useful and correctly applied gender-neutral meaning of them? Be careful that you're not the one declaring that those letters are privileged for only male use.
The only issues I've experienced frequently in Rust are logic errors. You still have to think about handling every possible corner case when developing software, regardless of the language. Although Rust does make side effects obvious when working with APIs, which majorly simplifies error handling. It is possible to devise means to avoid certain kinds of logic errors through the type system, once you master it. Move semantics is particularly useful for conveying state changes and enforcing proper order of API usage. The builder pattern is useful for safely constructing complex types, which may result in constructing different end types, with different restrictions and capabilities.
https://www.reddit.com/r/rust/comments/akgmef/rust_nbody_benchmark_ranks_1/ef61j38/ &gt; I thought the regex crate was normally faster than PCRE. I'm pretty sure that I've been fairly consistent in saying that Rust's regex engine is typically on par with PCRE. Performance of regular expressions is exceedingly complex and varies heavily depending on the work load. This is why I've never properly published any regex benchmarks, even though I have *many* of them for use with debugging performance problems. See for example [this exploration I did for ripgrep](https://github.com/BurntSushi/ripgrep/blob/master/FAQ.md#pcre2-slow). And I'd rank that as fairly pedestrian.
Clever programmers convert tasks that involve "lost of string processing" into tasks that are "heavily numeric".
My point was that this kind of crate is a misuse of `Deref`: &gt; Implementing Deref for smart pointers makes accessing the data behind them convenient, which is why they implement Deref. On the other hand, the rules regarding Deref and DerefMut were designed specifically to accommodate smart pointers. Because of this, Deref should only be implemented for smart pointers to avoid confusion.
That's not really an option. When `await` is a keyword, a postfix macro can't have the same name, and `.r#await!()` is not very ergonomic.
It uses the `perf_event_open` syscall (same as `perf`), and supports [a few different sources](https://github.com/nokia/nperf/blob/705f15e3f2b038fa4fe5ee3a48a3327e14c441aa/perf_event_open/src/perf.rs#L559) exposed by `perf_event_open`.
Plus, Rust integer types come with a suite of methods to handle overflow however you like. `checked_*`, `saturating_*`, `overflowing_*`, and `wrapping_*`. I love it.
Yes, hopefully we will get a language level support for this one day, see for example this [proposal](https://internals.rust-lang.org/t/pre-pre-rfc-target-restriction-contexts/7163).
To cover the different usage requirements we probably are going to need benchmarks for each case (static number of futures, dynamic number of futures, etc.).
&gt; Very generally speaking, the binary is composed of two sections, text and program data. Note: The section that holds the program code is usually called `.text` in ELF files. That makes your use of the term "text" somewhat confusing here.
it's just not NECESSARILY indicative of performance on any given real-world code. i.e. the performance characteristics of languages are domain dependent.
Cool! Yes, `nperf` can emit `perf`-compatible output, however it has its own internal `stackcollapse-perf` equivalent (it can output both already collapsed frames or it can output them in the raw `perf script` format.) What I would be mostly interested in is having a pure Rust flamegraph generator. Not because of the performance, but because I'd like `nperf` to be self-contained and be able to natively generate flamegraphs without depending on external tooling.
That's basically what the Cranelift is designed to do.
If you are interested in the real low level details, [here](https://wiki.osdev.org/X86-64_Instruction_Encoding) shows how Intel/AMD x86(-64) processors encode the many instructions of a program into binary. Since it's complex and not-human friendly, assembly is used as a textual deserialization of that data.
Which version of C#? I think latest Core has started doing hotspot style JITing as well, or maybe that's still coming and lands with 3.0
If we require the curly braces, we could just as well stick with the macro: `await!( ... )` `await!{ ... }` `await { ... }` That's not a big difference. If the curly braces are **optional**, and `await` just takes the next expression, then what is the operator precedence supposed to be in these examples? `await foo.bar()` `await { foo }.bar()` In the first example, one would expect low precedence, so `await` applies to the entire line. In the other example, however, most people would expect the following precedence: `( await { foo } ).bar()` This is inconsistent and would probably be confusing.
That's strange because `flate` is deprecated. Did you use the latest version of `image`?
&gt; [...] normalizes each benchmark so that the best time of any language is 1.0, and the other times are a ratio of that. Then, I added up all the times, averaged, and re-normalized the averages same way. You don't specify which average you use but just wanted to let you know that the arithmetic mean is *not* the proper way to summarize normalized values. You should use the geometric mean. You can read more [here](https://www.cse.unsw.edu.au/~cs9242/18/papers/Fleming_Wallace_86.pdf).
I have heard no news of this. If anyone knows of it I would be interested. 
&gt; JITs like HotSpot rely on heuristics to measure the currently hot code, and guess based on that. But these heuristics can be wrong, and can misoptimise code. &gt; &gt; Because AOTs don't know what the hot code is, they are much more conservative and usually try to optimise everything. This means they're usually redundant, but never wrong. You're right that AoT systems _tend_ to be faster because they optimize much more aggressively. However: JiTs rely far less on heuristics than AoT systems; JiTs don't need to "guess" what code is hot—they _know_ what code is hot, because they can directly observe the running code. So, while it's _typical_ for AoT systems to optimize much more aggressively, it has nothing to do with the JiT making its guesses at runtime.
Even if both the executor and and futures themselves never leave their originating thread, the current way the `*Wake*` structs/traits are written, you'll always need to at minimum support waking from other threads. Since the futures will be able to call `lw.clone().into_waker()`, they'll be able to send it to other threads or interrupt handlers, which act kind of like threads as far as `Send`/`Sync` are concerned. You could make the `wake` method simply panic and only define `wake_local` to actually do something, but that kind of breaks the contract between futures and executor.
RE2 (like Rust's regex engine) guarantees to complete all search operations in linear time on the text you're searching (where the regex is itself treated as a constant). PCRE, on the other hand, can take exponential time in the worst case. This is because, to a first approximation, RE2 uses finite automata in its implementation where PCRE uses backtracking. In practice, if you aren't hitting pathological cases in either regex engine (which exist, even for RE2, they just don't result in exponential time), then they should be in roughly the same ballpark performance wise. Rust's regex engine has similar performance characteristics as RE2, although, Rust's regex engine has many literal optimizations that RE2 doesn't have which can make it quite a bit faster in many common cases. You can either compare them on the benchmark game (there should be RE2 submissions, and of course Rust submissions use `regex` currently), but a single benchmark is effectively useless for judging the performance of a regex engine. The biggest *open* benchmark suite I know of is the one I maintain as part of the `regex` crate, and can be run with [this script](https://github.com/rust-lang/regex/blob/60d087a23025e045ae754a345b04003c31d83d93/bench/run). You'll need to draw your own conclusions though. (I am aware of the existence of non-public benchmark suites which likely put mine to shame, but I've never seen them and probably never will.)
I meant many of the implementations, different languages and different implementations within a language may use different libraries. Sorry my wording wasn't clear, I can see how it sounded like I was talking about Go only, in which case, yes, it's just those two. That said, looking more deeply I do think the FFI overhead is an issue. Looking at pidigits, both [Rust #3](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/pidigits-rust-3.html) and "Go #2"(https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/pidigits-go-2.html) use GMP, but the Go one take 1.69x the time. Even the PHP one is competitive here, suggesting that FFI is indeed a source of problems. Looking at [Go's regex-redux](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/regexredux-go-2.html), I think FFI is part of the problem, but it also looks like maybe they're recompiling the regexes? So maybe the problem here is that the entry is just nowhere nearly as competitive as it could be. Again, a lesson that this "benchmarks game" is a very poor source of information for evaluating actual language performance. It's a shame that so many people take it seriously.
Oh I read the title as "gnuradio" (enterprise-grade software defined radio suite) and got very excited, now my hopes have been dashed quite expertly.
PCRE will be faster for most uses but has a worse worst case, RE2 doesn't support backreferences.
The link to godbolt experiments is in the discussion linked above
[Contribute](https://salsa.debian.org/benchmarksgame-team/benchmarksgame/blob/master/README.md) your "*idiomatic* way to write in that language" program.
I think that Go program you linked does actually have the minimal number of regex compilations, but I can see why it looks suspicious at first glance. And yes, I would definitely speculate that Go's FFI overhead is a factor, and they could have some easy wins there. I've found it is actually better to create an allocation for the matches in exchange for pushing the loop down into FFI code, e.g., https://github.com/BurntSushi/rure-go/blob/578414ce0ddebaed31bc511f503ef4d8ba89a7f0/rure.go#L9-L44 Whether that's allowed in the game or not is something someone else will need to discover though. :-)
Just to refresh your memory and make sure that I understand what you're saying. These are the 5 different string replacements that the whole benchmarks depends on. And these are replacements happening on a very large, &gt; 5 million character string. 1. `(regex!("tHa[Nt]"), &amp;b"&lt;4&gt;"[..])` 2. `(regex!("aND|caN|Ha[DS]|WaS"), &amp;b"&lt;3&gt;"[..])` 3. `(regex!("a[NSt]|BY"), &amp;b"&lt;2&gt;"[..])` 4. `(regex!("&lt;[^&gt;]*&gt;"), &amp;b"|"[..])` 5. `(regex!("\\|[^|][^|]*\\|"), &amp;b"-"[..])` My personal benchmarks for these are: Pattern | C | Rust -------|-|----| 1|**72ms**|80ms 2|160ms|**14ms** 3|254ms|**239ms** 4|**148ms**|393ms 5|**98ms**|240ms It really just hinges on these last 2 patterns. So what you're saying is that since these are replacements, the beginning and end of each match must be known, and `regex` currently finds that a pattern exists at the end of the pattern, and then searches backwards to find the beginning of the pattern? And you are under the impression that it's the key difference? Do you know why PCRE2 doesn't have this problem? I'm curious because I might poke at this later.
&gt; … I can see how it sounded like… It sounded like you were talking about more **tasks** than just pi-digits &amp; regex-redux. 
When the CPU cache comes into play, an isolated test like this one is not the best benchmark in my opinion. I guess that in real life code, the cache may be filled with some other, possibly more performance critical data. Still good to know that compiler not always produces lookups and it may be good idea to manually implement them and benchmark.
Is dev on cranelift -&gt; release on current compiler a realistic goal? I'm not sure if there's anything that the current compiler can do that the cranelift compiler isn't planned to do, such as supporting certain features, since it skips LLVM.
Wouldn't cranelift need to basically become the size of LLVM for it to have the same performance, platform support and security? Even with a massive increase in people working on cranelift, that would take a very long time.
https://blogs.msdn.microsoft.com/dotnet/2018/08/02/tiered-compilation-preview-in-net-core-2-1/
If they'd said that the benchmarks are indicative of expected performance on real world idiomatic code, you probably would've gotten on their case about that, too. How about considering whether your interests would be better served by being less cryptically peevish whenever folks discuss the benchmarks game? 
That sounds familiar, yes. PCRE doesn't have this problem because it uses backtracking. It's a completely different implementation strategy. For the first 3, I'd actually expect the regex engine to do a bit better there. So I'd need to look into that too. There are relevant SIMD optimizations at play here. Some of them use `memchr`, which just relies on SSE2 (or AVX if available, but those aren't available on the benchmark game's target which uses `core2` I believe). The other relevant SIMD optimizations rely on SSSE3 (or, again, AVX2), and I'm not sure whether `core2` has those or not. I think so.
Well put.
&gt; The primitives of map and reduce, by definition, make up MapReduce, they do, but implemented by separate primitives, is that going to be as efficient as something with dedicated logic for partitioning and sorting, fed directly from the map into the reduce part. I am not so sure. I think there are many ways to do this. However it's also possible that a specialisation mechanism could substitute this for you (does rust have it / is it on the roadmap?) &gt; Rayon also has a parallel sort, though if you want it as part of the MapReduce operation, you typically implement this by having the map function map in to bucketed queues to be merged at the reduce stage (essentially implementing MergeSort with mapreduce). right now you're talking what I imagine is a single unified MapReduce abstraction where the implementer has done the R&amp;D and picked the best internal implementation 
How about (you) not name-calling. How about (they) not making unsupporable claims.
&gt; also, C# doesn't do runtime hotspot stuff at all but is faster than java I'm actually pretty sure that all major implementations of the CLR support JIT compilation. Unless you meant something else by that.
&gt; https://blogs.msdn.microsoft.com/dotnet/2018/08/02/tiered-compilation-preview-in-net-core-2-1/ cool! thank you! coming soon!
The intention is to use cranelift for debug builds (where compile time is more important) and LLVM for release builds (where the things you mentioned are more important).
I am an idiot 
&gt; Yeah, benchmarking is definitely going to be a challenge, even moreso for embedded environments with which I have exactly zero experience regarding benchmarks. Many embedded targets (and I think the STM32F in particular) have a DWT register which, when activated, will count the elapsed cycles. As long as you don't overflow this register, benchmarking is rather easy (unless im missing some hidden pitfalls, but it was certainly good enough for my bachelor's thesis). 
what about this - a version that minimises temporaries the mapper-function is given a (framework generated) function to accept a key-value pair. i.e. the mapper would iterate and call the given "push_key_value(k,v)" function in it's own loop, rather than generating a return value. the framework would have allocated a new key if need be , and call the given reducer one by one. i.e. there is *no* allocation for temporary key-values.. just allocation for the final k-v's I have to think a bit more to write the signature out.. I suppose it might also be possible to do this by having the mapper return a *generator* for the key-value pairs, instead of actual key-value pairs.
Yeah, I noticed that, which is really cool! The fact that it can _also_ produce `perf script` output is also neat, but you're right, that does obviate the need for what `inferno` _currently_ provides. But as mentioned, the plan is to also port `flamegraph` itself, and that then sounds perfect for your goal of making `nperf` self-contained! I'm hoping to get around to that either this weekend or next, so stay tuned. Separately, I wonder if `nperf`'s internal collapsing might give it some serious performance speed-up compared to going through `perf script`. If you watch the stream towards the end, you'll see that `inferno-collapse-script` reaches a speed of ~200MB/s (`stackcollapse-perf` maxes out around 24MB/s), but `perf script` is now the bottleneck with its sustained output of (from memory) ~70MB/s. Of course, `nperf collapse` makes all of `inferno-collapse-script` unnecessary, but it'd still be interesting to see what the performance of `nperf collapse` is. Also, you wouldn't happen to have any particularly large collapsed performance profiles laying around? That would be very handy for when we're porting and optimizing `flamegraph` in the next stream.
What we could do, is stabilizing the postfix syntax `expr.await`, but keeping the `await!()` macro around. Of course, we would have to rename it, like `wait!()`. This macro would then be trivial to implement: `($s:expr) =&gt; { $s.await }`
And what authority do _you_- oh, well okay 😛
If indeed the discrepancy is due to FFI performance, then yes, that is important to know.
Thank you for the feedback. &gt;I think I'd probably take the time to look at how Iterator is implemented for ranges and slices Based this, I dug into the implementation details of Range and now have a better understanding of how Iterators work and how to add that functionality via Traits to user created structs. &gt;I'm not even sure what you're saying here. I know only one way to create a struct value in safe Rust: initialize every field. You are absolutely correct here. I did not understand that it is a compile time error to construct a struct without initializing all elements. I am used to the idea of a default constructor from other languages and was confused. Your note made me start digging into variable initialization and the work that Rust goes through at compile time to have safe code. &amp;#x200B; I did have a question on this as far as best practices. It would seem that you should add "constructor-like" functions to your structs for encapsulation reasons so users do not need to know implementation details that may change. Is that correct? &amp;#x200B; Like I said before. I am still very much in a learning phase and I really appreciate you taking the time to read my blog and comment.
&gt; and there's not much to save you from forgetting which column was which. I don't think this is solvable by a programming language, is it? You would need to generate the correct types from the db schema, which is arguably out of scope for most if not all languages (not that you said anything of the like).
Except that method-like macros aren't available now, and might not be for another couple of years. So this is not practical.
 It's only syntax. It isn't necessarily a requirement to implement general purpose method-like macros first. At one point in Rust's history, there was no support for custom macros at all, but there were still a few macros made available in std. I don't see this as any different.
&gt; C# If you can get past the "eww Microsoft"-ness of C#, it has become a fantastic managed language that is surprisingly fast.
Looking as familiar as possible was never a concern when Rust was designed. Many language features differ significantly from other languages: Error propagation with `?`, block expressions, if/else require curly braces, closures use `||`, etc. etc. Anyone who learns Rust has to learn new syntax and new idioms. But most Rustaceans I've met don't think that's a bad thing, because these differences make Rust much more expressive than, say, Java.
I'm confused about how to do the activity for [the Display section in the book](https://doc.rust-lang.org/stable/rust-by-example/hello/print/print_display.html). Is it asking to implement [num::complex::Complex](https://autumnai.github.io/cuticula/num/complex/struct.Complex.html) from another collection/crate or am I just not understanding it at all? 
surely given that rust and c++ can both represent C programs, I'd expect all to be the same speed.. of course the state of the compilers may fluctuate. &amp;#x200B; I suppose you could also mean idiomatic rust vs idiomatic c++ vs plain C, but couldn't you just build some nice abstractions specific to the use cases of each benchmark to make the code for these benchmarks give you re-useable components?
This isn't that surprising to me, the match statement uses branching while the lookup doesn't. If the processor makes an incorrect branch prediction, the entire pipeline will need to be flushed.
&gt; the current way the *Wake* structs/traits are written, you'll always need to at minimum support waking from other threads. Not really, you can just panic if that happens, and generally, just do nothing. You can write an executor that manually polls multiple stack allocated futures in the same thread until completion.
That is good, however, is easy for some to draw wrong conclusions if don't have a clear picture of what, exactly, are the benchmarks measuring.
Of course, this is a **special case**. If you want to do something else with the returned result, like `unwrap()` or `map_error()`, this sugar doesn't help. That's why I prefer `expr.await` or `expr await`. These require neither parens, nor another syntactic sugar.
Isn't that a distinction without difference though? Every language's performance depends in large part on the compiler or interpreter. C with a trash compiler will perform poorly, but so will Rust.
A postfix `.try` would have exactly the same downsides as `?` when introducing a prefix `await`.
You could use \`clippy\` - it is awesome, and has quite a few suggestions for you. The lints usually suggest something that makes sense. In your case: clippy suggests to not pass \`char\` by reference. \`char\` is 4 bytes big, \`&amp;char\` ... depends on the architecture, I guess? 4 or 8 byte. I don't know if this is the reason for the lint. Also, \`cargo fmt\` may be useful.
It sounds like you're looking for [`fold`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.fold). You can do something like let aggregated = vec.into_iter().fold(FooAggregated::new(), |acc, v| { acc.a.push(v.a); acc.b.push(v.b); acc.c.push(v.c); });
&gt; But as mentioned, the plan is to also port `flamegraph` itself, and that then sounds perfect for your goal of making `nperf` self-contained! I'm hoping to get around to that either this weekend or next, so stay tuned. Yeah, that would be really awesome! &gt; Separately, I wonder if `nperf`'s internal collapsing might give it some serious performance speed-up compared to going through `perf script`. Well, my guess is that it should be faster, although by how much I'm not sure. I haven't really optimized the internal collapsing because it hasn't been a problem. I do it pretty naively; for every stack trace I generate a `Vec&lt;FrameKind&gt;`, where `FrameKind` is defined as: enum FrameKind { Process( u32 ), Thread( u32 ), MainThread, User( u64 ), UserBinary( BinaryId, u64 ), UserSymbol( BinaryId, u64, bool, StringId ), Kernel( u64 ), KernelSymbol( usize ) } Then I count them using a `HashMap&lt;Vec&lt;FrameKind&gt;, u64&gt;` (I'm not even reusing the `Vec`s when the `HashMap` already contains a given stack trace), and then I iterate over the `HashMap` and output the data to the stdout. I imagine the fastest way to generate a flame graph would probably consist of building a tree, and then generate a flame graph directly from that tree without going through any intermediate steps. &gt; Also, you wouldn't happen to have any particularly large collapsed performance profiles laying around? That would be very handy for when we're porting and optimizing `flamegraph` in the next stream. I do have relatively large ones at work, but unfortunately they contain a lot of info about our internals so I can't share them. ): I guess what you could maybe do is to profile several different apps, and then concat their collapsed traces? That should generate a hefty amount of data, I think. 
The geometric mean is also the one used on the Benchmark Game's "which programs are fast?" page. 
Ah, excellent! I'll definitely be taking a look at that soon! Is there anything else needed to enable it besides calling the `enable_cycle_counter` method [here](https://docs.rs/stm32f103xx/0.11.0/stm32f103xx/struct.DWT.html)? There's nothing else obvious that I've seen in the docs, but in my experience, it's not that uncommon for there to be a switch to enable some prerequisite in some far away interface that's not mentioned where you'd expect it to be.
How do you know to distinguish "FFI performance" from "actual language performance"? Will you know that a language implementation provides arbitrary precision arithmetic by wrapping GMP? 
Although it's not exactly the same, Scala has `expr match { ... }` also for the reason to improve chaining.
Geometric mean [used to normalize across benchmarks](https://benchmarksgame-team.pages.debian.net/benchmarksgame/which-programs-are-fast.html).
I'm still using glium, but we're stuck on 0.17 because of breaking changes to the API after that. \`gfx-hal\` looks \_really\_ low level and frankly simply doesn't work on my system. &amp;#x200B; I really liked glium too, it was simple and easy.
No, because C can be compiled by both LLVM and GCC, but Rust can't (yet).
Rigorous performance evaluation is hard work. The point of my comment was to shine a light on FFI specifically, which it looks like my parent comment was not considering. I think that's valid, and not sure why you're being so combative.
Personally I would rather have a slower compiler with better performance. If we can make the compiler faster without sacrificing performance than I would be all for that, but I don't think that compromise should be made.
It is asking to write a struct named `Complex` with implementations for `Display` and `Debug`.
Except that putting a brace-delimited block at the end is a well-established pattern and, even in Scala's version, doesn't seem significantly odder than traditional control flow constructs like `if expr { ... }` or `while expr { ... }`.
If I wasn't clear before-- what is most important is that a decision is made. I will be happy either way. Both solve the problem of having to use "raw futures" for a large number of tasks. I would prefer prefix, but as I've spent the last year learning Rust I am confident that I will be able to adopt postfix if that is the solution. :) 
The match statement doesn't need to use branching though, the compiler could compile it to do a lookup instead.
They are basically identical but no common code. Even the iterators are the same but incompatible with each other
I'm almost ready to publish my IRC bot framework! It uses a rocket-esque API to make adding bot commands really easy.
To be fair to AoT compilers, they can take back some of that "knowing what's hot" advantage by using Profile-Guided Optimization.
This usually happens when you use other crates in your public API. You can use multiple versions of a crate in a single application, but the types they expose won't be compatible. There's no language that has a solution to this (except maybe for COM).
Yes, they all JIT, but just once. They don't analyze runtime statistics then JIT again based on them. But something a little bit like that is coming soon. 
Why does rust allow a function share its name with a module inside the same module? (like the rectangle function in piston::graphics)
It sounds like someone in the crate hiearchy updated a public dependency to a semver-incompatible version in a semver-compatible release of their own library, which _will_ break things. I'd contact the application owners (since I assume you aren't based on the post?) and inform them of this situation so they can track down the dependency that messed up and fix it.
Really programmers convert every task into something that is numeric, so you’re not technically wrong, but you are missing the point. There is a difference in how memory and operations for strings are managed across languages, so it’s a meaningful distinction. 
Yes, I did send an email to the company concerned. What I downloaded was not a crate, it was a full app that needed compiling. I did do the decent thng, with copies of what I did, how I repeated it, what the errors were, and the level of diagnosis I had attempted before getting lost. As far as I can tell it was in their custom build code to compile the proto buffers, but why this should cause problems for some features in rand was beyond me. 
Quick note: write() is initialized with the reset value of the register, which is usually, but not always 0!
The "eww" part of both Java and C# for me is that they force you to put everything in a class. I'm really happy that Rust doesn't even have classes. Granted you can accomplish similar things in Rust that classes help you accomplish in other languages but I'm personally glad we don't have to deal with complicated inheritance hierarchies.
How to generate sequential UUIDs?
&gt; But AFAIK nothing prevents an executor from polling pending futures (am I wrong?), so such an executor can completely bypass the waker API. Well yes, that's an option as well, I suppose. But that would only be for the case where you don't support waking *at all* and your waker is just a no-op. But if you support any waking, it needs to support multithreaded wake, otherwise someone could roll up with a future that sends your waker to another thread and be a bit miffed when it doesn't work or otherwise behaves badly. &gt; Is this contract written somewhere? Not formally as far as I've seen. It's all a matter of setting expectations. A future shouldn't expect to get polled if it doesn't call wake. An executor can therefore expect to not have to poll any tasks that haven't had their `wake` called. It's fine for an executor to poll things that haven't been awoken, just as it's fine for a task to call `wake` multiple unnecessary times. If one side doesn't meet expectations, it's doing it wrong and probably won't work properly, or at least will only work properly under very specific circumstances. &gt; If it were a hard requirement, well, that would be very bad I guess because the async fn syntax would be tied to it. That would mean one could not use async fn to generate state machines for applications that do not support multi-threading of any kind. I'm not sure what you mean by this. All the `async fn` syntax does is build the generator state machine and pass the `LocalWaker` down to the *real* futures that are responsible for ensuring that `wake` gets called. The waker remains a `LocalWaker` as far as the `async fn` is concerned, so it doesn't care one way or the other if threading needs to get involved. The [current state of the RFC](https://github.com/aturon/rfcs/blob/future/text/0000-futures.md) states the following regarding the `Waker` (`LocalWaker` is no longer a thing): * They must be cloneable. * If all instances of a Waker have been dropped and their associated task had been driven to completion, all resources which had been allocated for the task must have been released. * It must be safe to call wake() on a Waker even if the associated task has already been driven to completion. * Waker::wake() must wake up an executor even if it is called from an arbitrary thread. 
It does sound like someone in the application development messed up and packaged the wrong stuff. Without knowing the actual details, nobody can really diagnose it. It sounds like a version mismatch, but any time you introduce build code the ways things can go awry exponentially increase.
Cheers think I'll go for bigdecimal-rs instead due to support in diesel
\*nod* I actually have a DOS retro-hobby project where, to save space, I've been wrapping BIOS `int 10h` calls using Open Watcom C/C++'s best-in-class support for inline assembly and I can certainly attest to that. (It's going to be an install wizard builder for DOS which a retro-enthusiast may want to use on floppy disks, so I'm pinching bytes wherever I can.)
 I mean I hate to be a downer but I doubt that'll ever be the case, even without the optimizations, the type system is more complex, and the ecosystem is grounded in favoring compile time complexity over runtime complexity. But it doesn't really bother me, even for big projects, if you split your code into sub crates, while developing only using the deps you need one and a time, and use cargo check, you won't spend a lot of time waiting for shit to compile, then again I spent a few years using sbt at it's worst. &amp;#x200B;
&gt; Then I count them using a HashMap&lt;Vec&lt;FrameKind&gt;, u64&gt; (I'm not even reusing the Vecs when the HashMap already contains a given stack trace), and then I iterate over the HashMap and output the data to the stdout. My _guess_ would be that `nperf collapse` should be faster since it doesn't have to write out every line and then read them in + parse them, but some benchmarks may be a good idea here! Keeping a running tree structure would _definitely_ be faster, but you're also right that it'll be a lot more work. &gt; I do have relatively large ones at work, but unfortunately they contain a lot of info about our internals so I can't share them. ): Ah, fair enough. Well, should you come across some that could be public, I'd love some test cases! Combining multiple perf results is a good idea in the meantime though, thanks.
Check out the [soa\_derive](https://crates.rs/crates/soa_derive) crate. It seems that it allows you to automatically create a `FooAgregated` struct based on definition of `Foo`, along with `push(Foo)` and all the other niceties.
FWIW, I have plans for improving this significantly in 0.2. It is mostly a time constraint at this point.
Ideally you'd want the fast compiler for rapid development and the slow one for testing/deploying.
ahhhh, that does make more sense
Is it really that simple? That's not much different from just copying out he Point2D struct but changing the variable names.
The difficulty (as always) is finding a way to maintain that "meaningful distinction" without creating a bias towards particular language implementations or restricting comparison so-much that it's actually a data structure / algorithm comparison.
That sounds like the name of some comet, cool!
You'd have to change the `fmt` implementation slightly, but yep!
I really like C#, I use it at my day job, but I completely agree. Inheritance trees, even in a non-C++ language that avoids The Diamond™, can be a nightmare to manage and parse in your head.
I think so? I put `image = "*"` on Cargo, which I think should get the latest version. But when I get home I'll try giving the actual version number and see if solves itself.
Is the source for the app you're trying to build public? It might help if we could see for ourselves exactly what's going on. It sounds to me like they didn't ship it with a `Cargo.lock`, or it got inadvertently updated, which led to some funky version interactions between things that were thought to be semver compatible but actually weren't.
He was describing your behavior. Short, pithy retorts don't necessarily offer a lot to the discussion, but if you might positively represent your position overall in a more complete, thorough (and probably less barbed) response then it might be better received. 
For reference, the application is exonum, a Rust implementation of a blockchain. The getting started is [here](https://exonum.com/doc/version/latest/get-started/install/)
Its called Exonum, a Rust implementation of a blockchain. Getting Started is [here](https://exonum.com/doc/version/latest/get-started/install/)
FFI is explicitly allowed for 2 tasks — pi-digits &amp; regex-redux — mostly because in-practice "Rigorous performance evaluation is hard work" is not an adequate response. &gt; …not sure why you're being so combative "a very poor source of information" 
This is exactly what [diesel](https://github.com/diesel-rs/diesel) is doing. It can be a bit cumbersome to use, but it provides awesome type safety.
It may be a good idea for the developers to provide source tarballs that contain an inner vendored tarball, so that the application can be compiled offline without Internet access. If they use CI, will be guaranteed.
The uuid crate supports different kinds of UUID types, does some of them work for you?
&gt; But I stand by the caution to not just uncritically accept benchmarks game numbers as some kind of definitive statement of language performance. "[Non-motivation](https://benchmarksgame-team.pages.debian.net/benchmarksgame/why-measure-toy-benchmark-programs.html): We are profoundly uninterested in claims that these measurements, of a few tiny programs, somehow define the relative performance of programming languages."
It's pretty unlikely we'll reach this goal; but you know what they say: "Shoot for the stars, at worse you'll land on the Moon."
I retracted that, I wrote it because at the time I thought the Go example was recompiling RE's, something that burntsushi later suggested wasn't the case. *None* of this stuff is obvious, which is why I'm adding so many caveats. I think we're more on the same page than you think.
Oh nice, thanks :)
You may be right, but I'm still optimistic about this one. I feel like cargo-check being relatively fast demonstrates that it's not necessarily the type system that is causing the bulk of the slowness. So if we could replace the slow LLVM and linking stages with something faster (for debug builds), then there ought to be scope for pretty significant improvements. Macros might throw a spanner in the works, but my hope is that we can be very aggressive with incremental compilation of macros (so that for example the 10 derives you have on each struct only need to recomputed if you modify the definition of that struct, which presumably wouldn't happen all that often).
/r/rustjerk
I started to work on [Stochastic Progressive Photon Mapping (SPPM)](https://github.com/wahn/rs_pbrt/issues/86) ... Let's see how far I will get this week ...
Thanks, I will look into them.
In fact, I've seen very smart strategies from LLVM converting large match / if-else ladders into **bitsets**. For example, testing whether a character is an hexadecimal digit (0-9, a-f, A-F) was turned into a simple range check (`c &gt;= '0 &amp;&amp; c &lt;= 'f'`) followed by checking whether the bit `c - '0'` was set in a pre-computed constant. And that's super nice code: - no data dependency between the 3 operations, - few assembly instructions, - single immediate constant (no separate data segment to load in cache). That's an obvious winner compared to a look-up table!
&gt; I mean I hate to be a downer but I doubt that'll ever be the case, even without the optimizations, the type system is more complex, and the ecosystem is grounded in favoring compile time complexity over runtime complexity. I realize that makes intuitive sense, but if you actually measure the time spent in `rustc`, *for most crates*\* type checking, borrow checking, etc are a miniscule amount of compilation time. If you pass `-Z time-passes` to the compiler, you can get an idea of where the compiler is spending its time. \* There are of course outlier crates, usually involving huge amounts of macro expansions, type level number tomfoolery, or both. But for 80-90% of the general ecosystem, this above statement is accurate. 
I understand that, I guess I was confused by the author's confusion? 
Recently I've been grumbling that - Integer overflow *isn't* an error in release mode - Debug mode is too slow - Integers default to i32's ^(I kinda liked the old days where they didn't default at all) - i32 is too damn small
I am confused on the new mod path naming on Rust 2018. Already read the docs but still confused. If I have |-main.rs |-libA |--libA1.rs |--libA2.rs |-libB |--libB1.rs |--libB2.rs How should I write the module naming on each file and on `main.rs`?
The graph there for C++ faster than C faster than Rust is a bit surprising given dbaupp's earlier comment [here](https://www.reddit.com/r/rust/comments/akluxx/rust_now_on_average_outperforms_c_in_the/ef60uve). Maybe they're not all taking the same set of benchmarks into account.
*Wow* that took an impressive amount of time/memory to compile. Anyway, it all worked fine for me. It's not actually an "app" per se, it's just a library, so not including the `Cargo.lock` was actually the correct thing to do. Try a `cargo update`? It's possible that the problem that you encountered was temporary and has been caught and remedied upstream now.
I was going to ask you how that's coming along, but I decided to have a look myself. Conclusion: There's definitely progress, but it will probably be at least 6 months to a year until there is a CraneLift backend that can compile `std` and decent proportion of common crates. See the [CraneLift backend for rustc](https://github.com/bjorn3/rustc_codegen_cranelift) and the [tracking issue for compiler `std` with CraneLift](https://github.com/bjorn3/rustc_codegen_cranelift/issues/146).
This is great! It's exactly what I was hoping to exist (but didn't want to write). I feel like it really shows off the power of procedural macros
&gt; &gt; Question: Why is the condition implemented as v %2 = add i64 %s.1, -1 %3 = icmp ult i64 %2, 3 &gt; i.e., length-1 &lt; 3? Why not just length &lt; 4? The %2 register doesn’t seem to be used anywhere else Add will underflow. i.e. its shorthand for length &gt; 0 &amp;&amp; length &lt; 4.
My bad. 
Points lost for not calling it PHOBOS
`expr.await` or `expr await` is also a special case! `break`, `continue`, `return` (and `yield` if it is ever implemented), also have this problem that they cannot be called mid-chain. For this reason, I prefer prefix-`await` (for consistency with the other control-flow keywords). And then a general postfix-macro syntax to solve this problem for all the effected constructs.
As a library author this is frustrating from time to time. Creating a library you cannot rely on Cargo.lock thus you cannot pin your dependencies onto a specific version. When one of your dependencies makes a semver incompatible change this can break your library and you can't really do much about it other request the author of the dependency to fix this. So a working and published version of my library can break by no wrongdoings of my own. This is really unhelpful because there is a version of the dependency out there that does work, cargo just wont let my library user to use it because cargo ignores Cargo.lock for crates and this is very unfortunate. If i think about that, this could lead to the same problem as the "npm left-pad fiasco" by dependencies yanking specific versions :/
&gt; I'm pretty sure that I've been fairly consistent in saying that Rust's regex engine is typically on par with PCRE. I believe you. I guess I just got carried away.
You want [profile.release] overflow-checks = true in `Cargo.toml`. See also https://doc.rust-lang.org/cargo/reference/manifest.html for full options
That seems deadlock-prone.
Yep, definitely. If the executor is full of futures that are all waiting for capacity to get freed up, it's deadlock-city. But if that condition were to occur, it should be pretty trivially detectable by the executor. For an executor supporting N concurrent tasks, it would need N slots for `Waker`s corresponding to tasks that are waiting for capacity to get freed. If it ever reaches a condition where all task slots *and* all waiting-for-capacity waker slots are full, then it could take drastic measures to make some sort of progress, like killing off a task, or just terminating the program. It's not really all that different from something like a [goroutine leak](https://medium.com/golangspec/goroutine-leak-400063aef468), just on a much smaller scale. 
hasn't c# *always* been faster? I mean the entire behind-the-senes reasoning for Java lawsuit was that Microsoft had in 1997 a faster version of Java (because of their JIT expertise) than Sun? 
&gt; Quick question: Does anyone know how to make overflow error while still compiling with optimizations? You can opt-in to `checking_*` operations or `std::num::Checked*` types. I don't know a way to tell the compiler to rewrite the code to be checked but it sounds like an antifeature to me.
Any support for pcapng? I'd love to be able to make Quinn's unit tests write pcapng files with embedded cryptographic keys directly rather than having to generate a bunch of fake UDP traffic to be manually captured with dumpcap.
Add D if you can
It's not in the benchmark list, unfortunately. I do wish it was there, since (other than Rust) it's one of the better-known relatively modern languages that tries to compete in the C/C++ space.
I've only ever heard of vulkan. WHat is GFX?
https://github.com/gfx-rs/gfx It's a low level abstraction layer around various graphics APIs. There's also a compatibility component that can be a drop in replacement for MoltenVK to translate vulkan calls to other formats like Metal at runtime
Building on other points, Cranelift only needs x86/amd64 and ARM support for the short term as that's the most common dev platforms. Actual deployment is done with release builds anyways.
I wrote a small library to read pcap-ng files a while ago: https://github.com/kornholi/pcapng
Yep, definitely! But it's no worse than the situation today with fully dynamically-allocated executors that allow tasks to spawn more tasks. Both are susceptible to the [goroutine leak](https://medium.com/golangspec/goroutine-leak-400063aef468) problem, just manifested in different ways. The dynamic executor will just keep allocating and allocating until eventually killed by the OS. The static one could deadlock unless it's somehow able to detect that there's no way it's ever going to make progress.
Should I use the parameters of my functions directly or should I borrow them when I have the choice? I have to choose between writing one of the following functions: ``` fn manhattan_distance(a: Point, b: Point) -&gt; i64 fn manhattan_distance(a: &amp;Point, b: &amp;Point) -&gt; i64 ``` Personally I would tend to write the borrowed version because there is no need to demand the ownership of a point when I can just borrow it and the compiler will make sure I don't manipulate anything by excident. Only take what you need, right? But when I look at [other peoples code](fn manhattan_distance(a: Point, b: Point) -&gt; i64) it seems they are not very converned about borrowing sometimes. What is the rust-way to go here?
I'm rolling out the release of [Reducer](https://github.com/brunocodutra/reducer) v1.1, which will ship with experimental support for async/await through [futures-preview](https://crates.io/crates/futures-preview).
Okay, who's up for writing a procedural macro for converting simple match statements into lookup tables?
This is an interesting result! Honestly I would have expected `phf` to behave better, but I guess it isn't too "perfect". With hashbrown, I think your intuition is correct - something is changing with code generation when the possibility of accessing a `hashbrown::HashBrown` is added, even if it isn't accessed. If we really want to find the cause of this, looking at the generated assembly would be a sound idea. https://rust.godbolt.org/ is great for this but requires self-contained code. Without looking at the assembly, though, it could definitely be something like `hashbrown::HashMap::get` being inlined into `Byte::{to_u8,from_u8}`. That could slow down the access if it's not needed and/or prevent `to_u8`/`from_u8` from being inlined into `calculate`. Maybe this is where the monomorphisation is showing its strength? Nothing about the `MonoGray` code path will ever contaminate or effect the non-MonoGray code path. 
That's pretty cool, do you think that cargo will make use of cranelift within the next two years?
A just-in-time compiler does have one advantage over an ahead of time compiler - it can watch how the code is actually used. This means when it compiles it can provide the appropriate optimisation to make it run as fast as possible. Normally the extra optimisation work an ahead of time compiler can do wins out, but for some things the focused optimisations can win out. That's because many optimisations make some things faster and some things slower and a JIT compiler can know whether the tradeoff is favourable. &amp;#x200B; There is a way you can have the best of both worlds - it's called profile guided optimisation. You compile the code, run it while recording metrics about how it runs, and then compiler it again using the same info. Google Chrome (which is C++) is compiled in this way for the best performance.
&gt; Perhaps Criterion doesn’t sufficiently isolate the functions under test? Criterion doesn't do that for you automatically, you need `test::black_box` for that, or `criterion::black_box` hack if you're on stable Rust.
I just pushed some changes reducing the memory usage of [Zola](https://github.com/getzola/zola) by a lot, only really noticeable if you have thousands of pages though. Going to look at what is missing for [Tera](https://tera.netlify.com/) v1.beta, which I am guessing is some bug fixes and https://github.com/Keats/tera/issues/379 If anyone is interested in helping, 379 is a nice one. Also looking for feedback on https://github.com/Keats/tera/issues/340 before releasing v1.
&gt; I'm not sure what you mean by this. All the async fn syntax does is build the generator state machine and pass the LocalWaker down to the real futures that are responsible for ensuring that wake gets called. `async fn` returns an `impl Future`, and iff `Future`s are required to provide an implementation of `LocalWaker` that "work"s, then you can use this to wrap async operations for which this model does not make sense (e.g. because there is no meaningful way to implement `LocalWaker`).
no it hasn't. to the extent that one can even say things about "what language is faster" C# tended to be a little behind Java most of the time until .net core, at least in recent history. Back in 1997 I have no idea.
You're not using it in your implementation though :) You can either add a `PhantomType&lt;T&gt;` to (one of the) enum members, or you could make the element type an output of `Semiring`, rather than an input, [like so](https://play.rust-lang.org/?edition=2015&amp;gist=ee938c2ef30b7392b6ba9b587389c01a).
do you have any source on this?
If you dont need multiple Semiring trait impls (parametrized over S) for a given type, you could change the generic parameter S to an associated type.
this thread is about The Benchmarks Game Java beat C# on most of the benchmarks in the benchmarks game until .net core 2 I don't know if they have historical data to look at or not. 
Perhaps. I would assume that's the goal. The Cargo devs would be the ones to ask.
benchmarks game is not definitive lol. what is your background? bootcamp?
Attempting to port a [transistor-level simulation of the 6502](http://www.visual6502.org/JSSim/index.html) to rust so that I can target web assembly. I'm actually porting someone else's port to C++ that also includes a simulation of the NES PPU, but is a native app that doesn't target the web. I'm exciting to see the performance differences between: - The C++ native version vs. the Rust native version - The C++ native version vs the webasm version - The javascript version vs the webasm version
This. The type system works great if you're using very specific types, but the moment (for example) you're using `Vec&lt;usize&gt;` to describe a path, you can get many logic errors such as: Is the path reversed? Does it correspond to an actual object? Which usize's can you append to an existing path such that it is still a real path? etc
At this point, based on your username and your discourse I don't think you are having a discussion in good faith but trolling. Just previous in the thread I said "to the extent that one can even say things about what language is faster" which should be a clear clue that I understand the question has no definite answer. However it has also been the experience of many professionals in various domains that the JVM tended to do a little better. This of course depends on many things and your own projects could be different. The lack of value types will make some things painful to make fast in Java for instance, though you can still do it. Some of the key things that improved recently with .NET were escape analysis and devirtualization, things that the JVM has had for a long time. The JVM can also do some auto vectorization and .NET still doesn't do any (unless that changed very recently) &amp;#x200B; &amp;#x200B; &amp;#x200B;
Do `Point`s implement `Copy`? Then it doesn't matter, so you should pick the owned version. Otherwise (for types that aren't `Copy`), default to the least powerful version; reference, then mutable reference only if you need to change something, and owned as a last resort.
The core issue is that Rust cannot tell "how you are holding a `T`", as in "it's like I own one" or "it's like I have a reference to one" or "it's like a have a mut ref to one" etc. The reason this matters is that Rust needs to be able to work out the "Variance" of `T` in `Regw` so that it can manage "sub-typing" correctly. "But (I hear you cry) Rust doesn't have inheritance!" - that's true at a type level, but it totally does have inheritance at a lifetime level: fn foo(input: &amp;'a str) -&gt; &amp;'a str { "hello" } This function take a parameter of lifetime `'a` and returns a value of lifetime `'a` according to its definition. In practice it actually returns a `&amp;'static str` regardless of `'a` but this is fine, since `'static` is a "subclass" of `'a`. Rust knows the rules for where subclasses/superclasses are valid to be used in place of the original type, but to do that it needs to know how the original type was referenced (since the subclassing rules for `&amp;mut T` are very different to those for `&amp;T`). For a good write-up of this problem, see [http://troubles.md/posts/why-phantomdata/](http://troubles.md/posts/why-phantomdata/) and then, if you're feeling brave have a read of [https://doc.rust-lang.org/nomicon/subtyping.html](https://doc.rust-lang.org/nomicon/subtyping.html). If you're not feeling brave, then I suspect you want a `PhantomData&lt;T&gt;` in the body of your `Regw` enum somewhere so the compiler knows you conceptually have a `T` inside you (it's inside the `S` that's inside you so I think this is correct).
Please provide a "port of fastest C SIMD variant" without new optimizations.
Thanks. For the moment, it's not an issue; I've "worked around" it by finding mention of Semirings in the core implementation (https://github.com/rust-lang/rust/issues/27739) and then reading a hell of a lot from there on.
Agreed. It's not quite got the sheer dynamic power that \`gfx-hal\` has, but for 90% of programs, Glium does an \*amazing\* job of hiding the complexity of pipeline initialisation unless you really need it. A triangle example is just a few tens of lines of code. A superb crate, really shows the power of Rust macros and builder patterns.
I started working on a Rust native D-Bus [implementation](https://github.com/marcelbuesing/dbus-native/) with a permissive license (MIT/Apache 2.0). Still quite a lot to do though.
How are intra-workspace dependencies handled by `cargo` and crates.io when referring to a crate inside a workspace from the outside? Suppose I have a core library and a number of dependent libraries in the same workspace, with `path = "../core"` in their dependencies' section. The whole shebang is pushed to crates.io, and I add one of the dependent libs to some other crate - in the usual way, by version. Will that even work? If it will, which version of the core lib will be used?
[removed]
At-least they made some link to that disparaging initial sentence. I choose not to characterize your remarks; they offer nothing to that discussion.
Note the date stamp.
Could you clarify what kind of borrow checker issues you're running into? It's not really clear to me how a `fold` or simple `for` loop falls short here.
Thanks :)
&gt; Again, a lesson that this "benchmarks game" is a very poor source of information for evaluating actual language performance. It's a shame that so many people take it seriously. That was very categorical — no caveats at-all.
Yes, some people who work on large and complex codebases in Rust often have to wait ages to be able to try a small change they made. This is really annoying and slows down people's workflow. This is why people want the compiler to be as fast as possible for debug/development builds. Of course, for the final release build, you want the compiler to do as many performance optimizations as possible to produce the best binary.
SOrry, you seem reasonably knowledgeable. I am somewhat cynical because these days I end up arguing with bootcampers with dunning-kruger who parrot what they've heard elsewhere and end up wasting my time. 
Well, you can pin by specifying a complete version in your Cargo.toml. Just do it only for crates where the author never heard of SemVer.
In that case, wouldn't it be fair game to submit a rust implementation that just uses PCRE as well? There is already a crate of pcre bindings so it would probably not be that hard.
That's fair, I don't know why I assumed the CLR would use a similar compilation scheme to the JVM.
That's not how Cargo.toml versions work. `rand = "0.6.1" does not specify that specific version, it specifies anything semver-compatible* with 0.6.1. If you want to pin a version you need to say `rand = "=0.6.1"`. But that still permits cargo to make (compatible) changes to the dependencies of rand, which can break things if you leak types from your dependencies. If you need to control versions, you must use a lockfile.
It would be even better if we had a release overflow check that would collapse multiple overflow checks into one to be faster. Something like replacing c = a + b if it overflowed, panic with "Overflowed at spot A" d = c * e if it overflowed, panic with "Overflowed at spot B" ... return d with c = a + b did_overflow = did_overflow | overflow_flag d = c * e did_overflow = did_overflow | overflow_flag ... if did_overflow, panic with "Overflowed somewhere in here" return d It cuts down data dependencies between instructions and removes all but the last branch, but still lets you see the general region where the overflow happened.
You could try. AFAIK, this is the only way to find out whether it's fair game or not. See also: https://www.reddit.com/r/rust/comments/akgmef/rust_nbody_benchmark_ranks_1/ef4rwqb/
Sure. With a little gymnastics, you can do this with simple imperative code. Fold has another issue I'll mention at the end. But the main issue is not that it _can't_ be done, but that it requires too much similar, mechanical code, which is a smell (leads to typos, copy paste errors, etc.). I was doing quite a lot of the same thing over and over. So take the `Foo` struct mentioned in OP, say I had a vec of them. What I wanted to do was to make an "aggregated `a`, an "aggregated `b`", and an "aggregated `c`". The obvious code from other languages (and this is where you hit the borrow checker problem), is to do three loops through the `Foo`s, one to make `a`, one to make `b`, and one to make `c`. The problem with this is that the `Foo`s all move in the first loop when you move `foo.a`, so you can't do this. The imperative solution has three phases: declare a `Vec&lt;_&gt;` for each field, loop through all the `Foo` structures to populate those vecs (which moves each `Foo` once, which is legal), then have an aggregation line for each field. Unfortunately this is three statements per field (four if you don't do struct literal with aggregation), and only one of them has "content"; the other two are completely mechanical, and that's not good. It makes your function three times longer, and my actual use case had many more fields. So two thirds of the long function was more-or-less content-free, but a casual reading wouldn't explain that. The fold solution is slightly better at first: it has one line per field to declare the accumulator struct, another to do the accumulation per field. So this is an improvement, although my structs didn't implement Default, so you still have one content-free line per field, which is not ideal. This has another downside which is that I honestly did want that `Vec` per field (to do `Vec`-level things); to do this with fold, I have to declare a new type of struct which has `Vec` fields, and fold everything into that, then still do the aggregation, so you're back up to three lines per field, with code reviewers having to look up what `fold` does because they're not as familiar with FP. Really what you want is a macro to make that vectorized struct for you, so you can create that with one or two lines (not one line per field) and then do one aggregation-line per field, so that code density matches up with idea density. So `soa_derive` worked amazingly.
Unfortunately this only works if `FooAggregated` is already declared. If you have to make it (just for this one function!) then you've got one line of code per field to declare the struct, and another to implement `FooAggregated::new()`, so we've got the same amount of mechanical code, but in more places.
We just wrote a couple of blog posts on this, for \[iOS\]([https://medium.com/visly/rust-on-ios-39f799b3c1dd](https://medium.com/visly/rust-on-ios-39f799b3c1dd)) and \[Android\]([https://medium.com/visly/rust-on-android-19f34a2fb43](https://medium.com/visly/rust-on-android-19f34a2fb43)). Hope you find it interesting!
I thought I might note this — only because it's interesting — but the Unison language (http://unisonweb.org) has features designed to solve this specific problem. The caveat being; it's not finished yet :)
I see they have quite a lot of posts in their blog. Do you have a link to something that talks about this in particular?
This is likely related to the recent `rand_core 0.4` release, which used the semver trick to make it compatible with `rand_core 0.3.1`. The problem is that if you have both `rand_core 0.3.0` and either `rand_core 0.3.1` or `rand_core 0.4.0` in your build, then you're liable to get the errors you encountered. It's not clear to me how folks are getting into this state, but it's usually resolved by doing a clean build or by running `cargo update`. See also: https://github.com/BurntSushi/quickcheck/issues/228
Hello! I'm trying to traverse a tree that I've parsed with `Comrak`, and fold the result into something. During the fold I want to access the actual nodes of the tree and extract their value, but the iterator I get from traverse gives me a `&amp;comrak::arena_tree::NodeEdge&lt;&amp;comrak::arena_tree::Node&lt;'_, std::cell::RefCell&lt;comrak::nodes::Ast&gt;&gt;&gt;`, which I cannot for the life of me find in the [https://docs.rs/crate/comrak/0.4.0](Comrak documentation). I am super new to the eco-system so I might be missing something. Any hints?
&gt; What types of errors are most frequent in Rust? Compiler errors :p --------------------- In general I'd agree with the other answers already here. Most of the bugs I see tend to be "silly" mistakes like using `-` instead of `+` or the domain equivalent. The only real "class" of error I can think of seeing relatively commonly is overuse of `.unwrap()`.
The formal semantics of what alias-based UB is is not totally nailed down yet (ongoing discussion [here](https://github.com/rust-lang/rfcs/issues/1447), one detailed proposal [here](http://smallcultfollowing.com/babysteps/blog/2016/05/27/the-tootsie-pop-model-for-unsafe-code/)). IIRC one of the proposals was to strictly make it UB if there are ever two values in scope that can both be used to mutably access the same memory location if at least one of them is a `&amp;mut`, which your implementation violates because while constructing the second element of the tuple you have access to `self[a]` through both the first element of the tuple and `self`. But [the standard library's implementation of `split_at_mut`](https://github.com/rust-lang/rust/blob/d8a0dd7ae88023bd09fa4b86c9ca1f6ed8095b43/src/libcore/slice/mod.rs#L991) does the exact same thing. I suggest using the `split_at_mut` version so that if the final UB model decides that that pattern is unacceptable, your code will be automatically updated to avoid it when the standard library fixes its implementation.
No worries! As I said, intuitively it sounds like type checking would be expensive but in practice, there's enough other parts of the compiler that are slow that it doesn't even make a blip usually. I personally am excited for the day type checking is the most expensive part of a debug build. 
I was using C for my thesis, but to turn it on you just need to set a certain bit, so this looks like it's doing exactly that.
Wow! Care to share your Memory Sanitizer setup? That one is notoriously tricky to get to work, it requires recompiling the Rust standard library and all other dependencies. Or did you mean Address Sanitizer? That one is conspicuously missing. Also, have you done any fuzzing?
Thank you. Yes a cargo update did get over this problem. The build proceeded, quite quickly on my machine. I now have issues with the custom build protoc execution, but I can work these through. MANY THANKS FOR ALL HELP. 
&gt; async fn returns an impl Future, and if Futures are required to provide an implementation of LocalWaker that "works", then you can't use this to wrap async operations for which this model does not make sense (e.g. because there is no meaningful way to implement Waker). This is where you've gone wrong (or we're not communicating effectively for whatever reason). The `Waker` isn't implemented/supplied by the future implementation, it's tied to the executor. From the Future's perspective, it's just an opaque object with a `wake` method that the type system says can be safely sent to other threads. It's up to the executor to actually provide the implementation of the waker.
Author here. We are using [rust-san](https://github.com/japaric/rust-san) on Travis-CI. Please see out .travis.yml [here](https://github.com/japaric/rust-san). All the test cases were also scanned with Valgrind on mu dev machine. Fuzzing is a future work; and rustls has done it. We'll probably add fuzzing in the next 0.8.1 release.
Thank you for the links and the good advice regarding using the `split_at_mut`-solution instead is clever.
I think I am the one communicating ineffectively. When I implement Future, I get as an argument a LocalWaker, and have to do "something" with it. Ignoring it for async operations that cannot be awoken feels wrong.
= means equals. Cargo will not override it.
I needed this exact same thing for some code I wrote a little while ago. My implementation: /// Provides simultaneous mutable references to two distinct elements in the same slice. pub fn get_refs&lt;T&gt;(array: &amp;mut [T], mut idx1: usize, mut idx2: usize) -&gt; (&amp;mut T, &amp;mut T) { assert_ne!(idx1, idx2); // Ensure idx1 is to the left of idx2 let swapped = if idx1 &gt; idx2 { mem::swap(&amp;mut idx1, &amp;mut idx2); true } else { false }; let (lhs, rhs) = array.split_at_mut(idx2); if swapped { (&amp;mut rhs[0], &amp;mut lhs[idx1]) } else { (&amp;mut lhs[idx1], &amp;mut rhs[0]) } } 
Ah, it's *address* sanitizer after all. Makes sense.
What do you mean by "cannot be awoken?" I believe that the intent of the interface is that you have zero guarantee that your future will ever be polled again unless you call `wake`, and should generally treat it as though you're guaranteed to *never* be polled again if you never call `wake`. Otherwise, someone will inevitably try to use your future in an executor that behaves in such a manner and be confused when it never gets run. I guess none of this is a problem if both your `Future` implementations and executor implementation are entirely internal and relegated to implementation details of whatever it is that you're building.
&gt; If you need to control versions, you must use a lockfile. Exactly! But lockfiles only work on binaries and not libraries. I would find it really helpful if there was an option to force it – either as a library author or as a library user – in a case of breakage. I feel like this could really fire back in the future as Rust programs/libs heavily use libs from crates.io. If a leaf/deep lib screws up, we can easily have our own "left-pad"
I think when you try to publish to crates.io with `path` dependencies, you'll get rejected with an error about that. You'll have to explicitly publish each dependency first, and you'll need a fallback `version` alongside each `path`, which anyone not building inside your workspace will use.
&gt; I was still wondering if this is truly UB? Yes. I think the rustonomicon is sufficiently clear here? Mind you that UB does not mean provably unsafe. It simply means your code has no defined meaning. Even though the generated assembly may be provably safe, that may change at any time. Even for the same compiler-version as a result of changes in optimization triggered by unrelated code-changes.
Your implementation is a bit more clever regarding bounds, as it is smart to rather split on the highest index to prevent my `b - a - 1` calculation. 
You can also have a macro with the same name as a function, and a member with the same name as a method. I think the idea is that the calling syntax makes it clear in each case what sort of item you're asking for (`foo()` vs `foo!()` vs `foo::bar` vs `x.foo` vs `x.foo()`), so the compiler doesn't need to be more restrictive about it. One of the interesting consequences of this is that `use mymodule::foo` imports _all_ of the items named `foo`, which could be more than one.
Let's go back to the DMA example. To see if the async op completed, you need to poll whether a byte is set somewhere. If the Future is not ready, then it returns Pending. If the Future does not call `wake` before returning `Pending`, it will never be awoken again. If the Future calls `wake` before returning `Pending`, the executor will block on every such Future because it will be schedule right after.
I released a crate that allows you to write generators-&gt;iterators very simply, can someone take a look? I didn't really played with macros enough so I would love some feedback. [https://github.com/vova616/simple\_generators](https://github.com/vova616/simple_generators) &amp;#x200B; Ex: #[generator] fn test_macro(n: u64) -&gt; impl Iterator&lt;Item = u64&gt; { let mut num = 0; while num &lt; n { yield num; num += 1; } }
Really? You `=0.6.1` doesn't lock it on this version?
&gt;We should compete where we are best positioned to do so, and monolithic Web applications is not that. While this might be true in the general case, Rust is ideally positioned for the subset of monolithic Web applications that require high performance (such as games).
This 100%. When micro benchmarking, your LUT is always hot and will be sitting in cache. In a real life run, you'll be fetching from main memory much more often. On top of that, you could have unintended side effects with cache thrashing, where your LUT is potentially evicting more important data.
Just to clarify on what I'm saying (because I think something may have gotten lost in my horrid mobile formatting): If I have this: [dependencies] rand = {version="=0.6.0", features=["log"]} I get this dependency tree (from `cargo-tree`): scratch v0.1.0 (/home/ben/scratch) └── rand v0.6.0 ├── libc v0.2.48 ├── log v0.4.6 │ └── cfg-if v0.1.6 ... more dependencies follow But if I change to [dependencies] rand = {version="=0.6.0", features=["log"]} log = "=0.4.0" I get this dependency tree, where now `rand` uses a different version of `log`: scratch v0.1.0 (/home/ben/scratch) ├── log v0.4.0 │ └── cfg-if v0.1.6 └── rand v0.6.0 ├── libc v0.2.48 ├── log v0.4.0 (*)
It locks the version of `rand`, but not the versions of `rand`'s dependencies. I'm bringing this up because the top reply to OP notes this: &gt; This usually happens when you use other crates in your public API. You can use multiple versions of a crate in a single application, but the types they expose won't be compatible.
Do you mean running examples of crates that you have not yet cloned on your machine? That seems kind of strange to do. If you already have the crate with an example folder you can just use the --example flag on cargo run like: cargo run --example [name of example]
Meta note for everyone: please share feedback in the RFC thread! That's the only way to guarantee everyone will see it. Thanks :) -------- &gt; Rust is ideally positioned for the subset of monolithic Web applications that require high performance (such as games) Agreed that it can certainly make a lot of sense to write a whole web app in Rust in some scenarios, but I don't think it makes sense to focus *only* on that for the roadmap (for the reasons outlined in the RFC). Running with your example of games, I think using something like unity is probably more practical for most shops. However, let me reiterate: (1) it is already possible to do pure-Rust web apps without any JS and always will continue to be, and (2) the toolkit will only make it easier. This is the "yes and"-style, positive sum, rising tide lifts all boats approach I think we should pursue. It is a lot easier to grow from supporting tiny modules with no assumptions, to larger modules, to whole web apps that control "`main`" than it is to start with the whole web app and work backwards to little modules with no assumptions.
There's also the `unzip` method, but I think it only works with 2 elements.
You can use a tuple instead of making a new struct
I haven't taken a really *deep* dive yet, but I like the way this project's code is structured. It makes it a lot easier to understand than some other existing Gtk-rs applications our there (Garta, Fractal, and several others). This is something that there doesn't seem to be very much consensus on at the moment. Each Gtk-rs application handles initialization and doles out ownership of responsibilities in different ways. It's nice to see a fresh approach.
Ah, I indeed misunderstood, yes. Thanks!
You can also use `&amp;mut self` instead of `&amp;self` here: ( &amp;mut *(&amp;mut self[a] as *mut T), &amp;mut *(&amp;mut self[b] as *mut T), ) I couldn't tell from your post if you've tried that approach or not, so I'm just making sure you're aware it works :)
I am very suspicious about using Web Workers for WASM multithreading. Not only because it requires non-trivial amount of JS glue and limitations it ensues, but also because I believe that WASM should not be tied to web platforms. It will be a great shame if it will become "vendor-locked" into JS engines, while it has a great potential (and momentum) to become #1 embeddable sandboxed environment. So I hope Rust WASM WG will push hard for some sort of "[native threads](https://github.com/WebAssembly/threads/issues/8)" and will look out for such JS-isms in future.
That optimization is already allowed by the [RFC about overflow behavior](https://github.com/rust-lang/rfcs/blob/master/text/0560-integer-overflow.md#delayed-panics), though I don't know if rustc actually performs that optimization.
It's kinda strange, that's why I think it's more of plugin territory.
Hoping to get my [Clippy PR](https://github.com/rust-lang/rust-clippy/pull/3652) landed!
&gt; So a working and published version of my library can break by no wrongdoings of my own. Because of how cargo works, when you pick a dependency, one of the things you are doing is picking to trust the owners of the dependency on crates.io.
I'm not familiar, but if you read their "about", it's a static typed language that's intended to encode distributed side affects. One of the ways they suggest they do it is by packaging and shipping code at the function level to other instances to execute, where the function is identified by a hash of itself and all its dependencies. I presume the parent comment was referring to this -- so you could export public types from the same dependency at different versions because they'll have different hashes.
&gt; If the Future calls wake before returning Pending, the executor will block on every such Future because it will be schedule right after, making the operation sync. Not necessarily. That's up to the executor to decide and isn't something the future should have to worry about. If the executor has a queue for determining what to poll, then you won't get into a situation where one task can monopolize the executor because when it calls `wake`, it goes to the back of the line. You could in theory have *all* of your futures operate that way and still be perfectly fine - they'd all just get polled every time. Calling `wake` before returning `Pending` is a perfectly fine approach in the situation you described IMO, and one that every executor implementation will have taken into account.
My single most consistent bug source lies in array/vector indexing (nested loops are evil). Clever use of iterators and a couple of really nice crates make this less of an issue, but it still pops up from time to time.
How the f did you install a version *BEFORE* 1.0??? 
Is there any performance difference between something like let port: u16 = if let Ok(p) = var("PORT") { p.parse().expect("Could not parse $PORT") } else { 80 }; and let port: u16 = match var("PORT") { Ok(p) =&gt; p.parse().expect("Could not parse $PORT"), Err(_) =&gt; 80 }; I was just wondering if there's any performance advantage to using \`if\`-\`let\` expressions as opposed to \`match\` expressions for this kind of pattern, which comes up quite often. I would look at the MIR to compare the desugared versions but I'm not very good at figuring out exactly what's going on with that yet (and also I'm lazy) Any answer is appreciated! Thanks!
Ah, that explains it! I was really surprised that a 1:1 port could have such a result, but it sounds like core aspects of the algorithm are different. That's totally fine, too, I'm honestly always impressed at how much people can whittle away at already highly optimized code to find new, significant wins.
I also ran their `split_at_mut` version using a copy of the standard library's implementation, and Miri didn't report any errors. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b17faac8af8af5d8ad0f85fb6d0912cf
Because it was 2014 and that's what the available versions were.
&gt; Similarly people saying, skip OpenGl and learn Vulkan. What?? Are you saying they shouldn't write 1000 lines of boilerplate then spend the next several months tracking down obscure synchronization bugs??
One skepticism I have is that there is no real interest in making the C++ benchmarks fast. There is an interest in making the Rust benchmarks fast. That said being within a percent or two sells Rust as being as fast as C++.
It's also worth noting that the n-body benchmark is actually a 5-body benchmark, and the rust code even goes as far as passing fixed-size arrays to functions, rather than slices. Not that it would change anything to the result if it used slices, but it's worth noting that I did try to make things more generic, using Vecs for NBodySym.r and NBodySim.mag, and /that/ made things worse, presumably because of the indirection that causes (not the bound checks, I did try removing them but that didn't make a difference).
Well, that's a very old version (before there was a real push towards proper installer behavior I assume) I'd suggest finding the install script, reading it and figuring out what it installed where. If you're still at a loss, join us on IRC or Discord (irc.mozilla.org #rust) and someone there should be able to help clean up the old installation and get rustup working.
Yes, I believe this is a realistic goal.
&gt; Is the JVM JIT just that much better than the Go compiler Yes, JVM JIT is light years ahead of Go AOT compiler.
The topic here is optimization. Rustc performs Rust-specific optimizations to the code (including MIR magic and so on) before passing it on to LLVM, which performs more optimizations of its own. Often they can do some pretty smart transformations to the code, such as compiling match statements or long if/else ladders into lookup tables. (In fact, the incredible abilities of the rustc and LLVM optimizer is sometimes the only reason you can use very high-level abstractions in Rust, such as iterators or other functional concepts, and still get machine code that's just as fast as the equivalent low-level C code)
Maybe not what you had in mind, but I'm currently writing a library crate for an SCTP wrapper over the FreeBSD SCTP implementation, using libc. (The only "complete" rust SCTP crates are wrappers over a Linux-kernel specific library, as of now.) There is an insane amount of boilerplate involved in doing very simple networking things from the C APIs (which are effectively what I have to target). On the bright side, I'm getting to expose a much cleaner/simpler client-side API.
They shifted focus to a realtime vcs subproject called memo. The last update was posted on october 2nd, with sporadic commits since then
People always misunderstand when we say to use Vulkan. To do an analogy: OpenGL is like C, Vulkan is like assembly. We're not saying to use Vulkan over OpenGL as in using assembly over C, we're saying use a language that is better than C and that compiles to assembly. In other words, use a library that works on top of Vulkan, instead of using that awful thing that is OpenGL.
It's anecdotal but I've switched to from C to Rust for pretty much all utility programs I need to write (processing/modeling very large datasets), and Rust's performance is rarely an issue. When I have encountered a slowdown it was typically due to me misunderstanding a concept in Rust (e.g. &amp;str != &amp;[u8]). Even better, straightforward Rust tends to be fast. So it's fast and readable/maintainable. C is my all-time favorite language but I'm convinced Rust is the future. 
I'll be driving the first Rust discussion group at work tomorrow. I'm not sure what to expect; there seems to be a lot of interest from different people with different backgrounds. We decided to read the book and discuss any issues or questions. It's going to be interesting to see how others approached the subject, what was their motivation, their pain points, what excites them, etc.
Yes, webworkers are closer to multiprocessing than multithreading IMO.
With all due respect, I think that's a retcon and semi fallacious. What higher level libraries are these? Noone in this thread so far had listed any. Afaik there's only AMDs V-Ez which doesn't have much adoption or learning materials https://github.com/GPUOpen-LibrariesAndSDKs/V-EZ On the rust side there's Vulkano https://github.com/vulkano-rs/vulkano and ash https://github.com/MaikKlein/ash but it's still pretty low level, and GFX as an abstraction like I mentioned. All of these are still very low level, and not something a beginner should try. So I go back to the point of people suggesting Vulkan as being either misguided or uninformed That's not to say Vulkan isn't worth learning but you're throwing them the extreme deep end of the pool. If a beginner did want to learn one of the newer low level graphics APIs I'd direct them in order to metal and dx12 before I'd suggest Vulkan. The tooling, documentation and learning resources are just a lot better 
So, I'm not sure what resources you've turned to besides old links, but a LOT has happened with Rust's installation method(s) since then. I'm not using this to excuse this bad experience you've had with trying to uninstall an old version of Rust -- I'm saying that that's probably why there's not much documentation out there to help. Also, it means if and when you do resolve this, you'll have a much better experience to look forward to. Feel free to PM me so I can try to help you, if you still need it! :) &gt; Honesty.. after typing this I am very soured on this language. Never have I found a less helpful and less knowledgable group of developers. Search the web, you'll only find broken answers that don't work or ass hats like myself. Please keep things constructive, according to the [Code of Conduct](https://www.reddit.com/r/rust/comments/2rvrzx/our_code_of_conduct_please_read/). Anecdotally, my experience has been the total opposite of yours. From what I've seen, the Rust community has been immensely helpful in pushing others to success with their issues with both Rust itself and the domain-specific challenges people are trying to solve to get things done in it. I have found Rust *very* enjoyable, and expect to use it for a while now simply because recent releases have been a total pleasure to work with.
Doesn't SharedArrayBuffer basically give you shared memory?
yeah, and you can (sort of) get shared memory in multiple processes using `mmap()`. I say close enough...
Yeah, but you have to ask for a piece of shared memory explicitly, as with processes. Threads share all of thei memory by default.
Ladies and gentlemen *step right up*, step right up over here we got [learn-gfx-hal](https://github.com/Lokathor/learn-gfx-hal), the very latest in `gfx-hal` tutorial technology. We got it all, I'm tellin' ya: * Opening a Window * Clearing the screen * Drawing a triangle * Shading that triangle into a wild rainbow of colors * Texturing a quad with an image * Transforming a cube across the major coordinate systems! All the things you could ever need to start your *very own* 3d game framework that you abandon 6 months from now! * ***How To Contribute*** * **Read the tutorials!** Spoiler: they're each *far* longer than a simple blog post tutorial like you might normally see. I'm aiming to be detailed enough that you can go from "zero knowledge" to "at least somewhat capable", which means that each lesson is in the "thousands of words" range. * **If you're a beginner** file an issue when you get too lost. Some temporary amount of being lost mid-lesson is normal and okay, but if you hit the end of a lesson and you don't kinda know what just happened that's probably my fault. * **If you're an expert** file an issue when you see anything that's totally wrong. Some amount of being vague is okay because there's a whole lot to `gfx-hal` and you can only introduce things so fast as people are learning, but I don't want to actually have factual errors in the document. PS: Join the Rust Community Discord and chat with me and the others in the `#gamedev` channel!
I wonder if this is somewhat dependent on the access pattern by which the input successfully resolves.
The gfx team uses https://github.com/gfx-rs/metal-rs for Metal bindings
You da real MVP
Okay so I finally stopping being lazy, and turns out the MIR for both are identical. This was on the rust playground in debug mode, but I don't think there's much optimization to be done there. But yeah, overall no difference. I know it was a question but I guess here it is in case anybody comes looking 😂
Isn't it a big worry that cranelift doesn't maintain the exact same semantics as LLVM, thus introducing subtle differences between release and debug builds?
Scala is one of those languages that had so much promise, but is held back by a few key things.
I read that once, went "wait" and read that again... &amp;#x200B;
https://docs.rs/resiter/0.3.0/resiter/ https://docs.rs/insideout/0.2.0/insideout/
As someone who has never used C, what are some of the things that you like about it? Also, what things do you like about C compared to Rust and vis versa?
&gt; * First `filter` line: instead of calling `unwrap()` on the result of `metadata()`, I would like to log the error and skip to the next item. However, it feels like too much code for a closure, and I don't know how to replace this with a `for` without sacrificing lazy evaluation. If it's more than one expression you should put it on multiple lines, but long closures are fine. What I'd do is `filter_map(|de| de.metadata().unwrap_or_else(|_err| log!("Thing broke"); None))`. &gt; * Second `filter_map` line: same, I would rather not `unwrap()`, but log. Too much code for a closure, but I don't know how to make it a `for`. Same thing here. `unwrap_or_else` for logging errors if you want to, or `filter_map`, `flat_map`, `and_then`, and `unwrap_or` if you want to skip things that aren't `Some`/`Ok`. &gt; In general, I would like this function to return an iterator rather than call `collect()`, so that it will be lazily evaluated when called.. That's what [this](https://doc.rust-lang.org/book/ch10-02-traits.html#returning-traits) syntax is for. Although lazy evaluation might not be the best design here, since you could end up with inconsistent results if you do a lot of work between reading each element.
They also JIT immediately when loading a type, there’s no bytecode execution phase like the JVM.
`break`, `continue` and `return` don't need to be chainable, because these expressions don't return anything: (return foo)?.bar() makes no sense, so a postfix `return` wouldn't add any value. You're right that a postfix `await` would need some getting used to, but a postfix `.await_!()` macro is completely unrealistic. It requires a new syntax that is **impossible** to justify just for this special case. Also, it is unnecessarily verbose. On the other hand, if postfix `await` is stabilized, creating an `await_!()` macro that works like before, becomes a one-liner. The macro can't be called `await` when it becomes a keyword, of course.
The code (for Criterion, md is the content of the Rust project README): ``` c.bench_function("pulldown_cmark", |b| b.iter(|| { let mut result = String::with_capacity(md.len() * 3 / 2); let parser = pulldown_cmark::Parser::new(md); pulldown_cmark::html::push_html(&amp;mut result, parser); })); c.bench_function("comrak", |b| b.iter(|| { comrak::markdown_to_html(md, &amp;comrak::ComrakOptions::default()); })); ```
Doesn’t seem at all strange to me; I’ve been wanting to do it regularly when evaluating GUI crates, for example; but instead, I’ve been cloning the crate repository, and *then* running the examples I desire.
Makes sense, thank you.
Thank you
"... at worst you'll burn up on reentry."
Still working on [Eko](https://github.com/ravernkoh/eko), a simple scripting language written in Rust. I’m in the middle of reworking the interpreter to use bytecode instead of walking the tree.
Kind of an aside, Java is capable of being much faster than Java typically is. If you use arrays of primitives and array indexing for everything, Java is every bit as fast as C. Nobody programs Java this way. I suspect if you added a garbage collector to Rust and started using references everywhere and vtables for lookup on all your functions, it would be about as fast as Java.
Why?
``` c.bench_function("pulldown_cmark", |b| b.iter(|| { let mut result = String::with_capacity(md.len() * 3 / 2); let parser = pulldown_cmark::Parser::new(md); pulldown_cmark::html::push_html(&amp;mut result, parser); })); c.bench_function("comrak", |b| b.iter(|| { comrak::markdown_to_html(md, &amp;comrak::ComrakOptions::default()); })); ``` Ftfy
I want to create a struct which may have a reference to another struct. If I write, ``` #[derive(Debug)] struct A; #[derive(Debug)] struct B&lt;'a&gt; { x: Option&lt;&amp;'a A&gt;, } impl&lt;'a&gt; B&lt;'a&gt; { fn set_a(&amp;mut self, a: &amp;'a A) { self.x = Some(a); } fn unset_a(&amp;mut self) { self.x.take().unwrap(); } } ``` then the following code compiles ``` let a = A {}; let mut b = B { x: None }; b.set_a(&amp;a); b.unset_a(); ``` and the following does not (as intended). ``` let mut b = B{x:None}; { let a = A{}; b.set_a(&amp;a); } // `a` does not live long enough b.unset_a() ``` but the following code also does not compile. ``` let mut b = B { x: None }; { let a = A {}; b.set_a(&amp;a); b.unset_a(); } // `a` does not live long enough println!("{:?}", &amp;b) ``` So is there any way to do such a thing? [This](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=aaedc91819de285b33f594220b801036) is the link to the playground.
The different axes makes it very hard to compare the first row to the second row, but it appears that pulldown\_cmark is somewhat faster?
It seems the rust community is getting a little obsessed with benchmarks. 
I'm obsessed too.
I think having to explicitly request memory be shared is a feature ;)
A cross platform (mobile/desktop/server/embedded) wasm runtime would be at the top of my wish list with the web side of things the least interesting. 
here's some help: go to r/playrust and ask there.. 
Thanks man!
Could you normalize the graphs so they're more directly visually comparable to one another?
If compilation is according to spec, the only differences would be undefined behavior in unsafe code.
`NodeEdge` is indeed not visible in the docs, because it isn't exported itself, but is only made visible because it is part of `Iterator` implementation of public type (`Node`). The module structure in question looks like this: mod arena_tree { pub struct NodeEdge; pub struct Node; impl Iterator for Node { type Item = NodeEdge; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { panic!() } } } pub mod nodes { pub type AstNode = crate::arena_tree::Node; } So `&lt;comrak::nodes::AstNode as Iterator&gt;::Item` is `NodeEdge`, but otherwise there's no way to name that type. This is an error on their side, so I suggest for you open an issue.
Lookup tables aren't necessarily a win over branches though, which is why a small number of branches aren't usually turned into a lookup table.
Thanks everyone for your answers. I'll think about a way to make a pull parser (ideally using the same code as the existing one) and update the docs to explain what is a pcap. I want to wait for the 2.0 for the pcapng parsing because it's far less trivial than a pcap parser. I someone is motivated to implement any of this or have an other idea don't hesitate to open an issue and/or a PR.
Why does this work? The [`IndexMut`](https://doc.rust-lang.org/std/ops/trait.IndexMut.html) trait does not guarantee that different indexes map to different data. Hence the first `&amp;mut self` should prevent the second one from occurring, because we cannot guarantee that the indexes point to unique data through the type system. 
Ok that's my bad, I thought it was clear enough. pulldown_cmark took ~180 us per iteration, vs ~380 us for comrak, so there's a big difference here.
I mean, of course they try to conform to spec, but maintaining two implementations doubles the surface area for mismatches right?
Oh alright, I've never had that need, but seeing other people are having the same thing means you should probably make a request on NYAR (Not Yet Awesome Rust), or something like it. I don't know if it has a lot of traffic, but you can at least submit it if you think it has potential. https://github.com/not-yet-awesome-rust/not-yet-awesome-rust/blob/master/README.md You can of course always try to make it yourself if it's something you desperately need soon.
I can understand what you mean. You would need some temporary caching though, so the program wouldn't have to clone the crate again when you want to run the same or a different example soon after.
It should be stylised as PhOBOS, surely? 
&gt; That just serves to reinforce the notion of the male default. We strive to do better here. What's the problem with a male default? The majority of us are male, so the default is logical. We can't see people on the internet, so we address based on the largest demographic.
I've used https://github.com/kernelmachine/cargo-profiler . I visualise it with http://kcachegrind.sourceforge.net/html/Home.html . You can get pretty detailed info, and you can group per class, which looks like it's roughly equivalent to modules. It's not entirely ideal always because in my case things like tokio are always topping the charts because they are doing most of the "work". core::ptr will also probably feature highly. But based on the call graphs you should be able to figure out what is going on. [profile.release] debug = true to profile in release mode if that is something that you want
If you want to help a comparison of two things i think it's better if those two things are as close together as possible. In this case i would put the curves in the same diagram. Example: A: |----------&gt; B: |-------&gt; &amp;#x200B; vs. A: |----------&gt; &amp;#x200B; and &amp;#x200B; B: |-------&gt;
This is interesting. In the project I've started recently I have similar issue. I do not use references though, as I need to have muscle acess to a place in a byte buffer, so I use two or more mutable pointers. Everything is wrapped in safe abstraction of course, but now I'm wondering, whether casting shared reference to several mutable pointers is UB too?
RUSTFLAGS=-Ztime-passes cargo run
I don't know if it makes a difference but I *think* you should return the result here or add a black\_box to it to make sure there's no difference in how the compiler optimizes the functions. [https://doc.rust-lang.org/1.5.0/book/benchmark-tests.html#gotcha-optimizations](https://doc.rust-lang.org/1.5.0/book/benchmark-tests.html#gotcha-optimizations)
Counterpoint: http://aras-p.info/blog/2018/12/28/Modern-C-Lamentations/ Aras P and others have a workflow. They like their workflow and they're effective with their workflow. They feel that modern c++ no longer works well with their workflow Others are saying "your workflow is wrong. You need to do all these other things that you're not doing because that's the best way to use C++". Those people are missing the point. They aren't doing anything to address the complaints, just saying "no you're wrong and you need to do everything differently" That doesn't seem very helpful
It might sound counterintuitive but the C language is so simple! You dont have to bother with paradigms or advanced language features or garbage collector performance penalties or anything. If you dont mind the increased code size and stick to well known C idioms its maintainable too. Of course package management is horrible and cargo is amazing
I'm surprised that rustc does loop unrolling itself. I'd have thought LLVM would take care of that.
&gt; a lot of the modules are very tightly interdependent, so splitting them is a non-trivial task. TBH if this is making is hard to refactor, then it's making the argument to refactor now rather than later. Module/crate dependence should only occur one way. If modules have 2-wau dependency, then they shouldn't be separate modules or they should be refactored.
&gt; We could go deeper on this explanation — after all, we still don’t know why Rust/LLVM doesn’t automatically convert the match into a lookup table. Probably because the lookup table is 2kB of mostly zeros. 
I have a local benchmark for both as well but it is important to consider that comrak generates an AST and therefore is going to be slower. Also comrak fully respects the spec while pulldown_cmark doesn't pass it.
Would compiling both in non-optimized mode give an accurate comparison?
The error message seems fairly clear. You are attempting to derive `Debug` for `FnStruct`, and this works by calling the `Debug` implementations of all of its fields. However, `Box&lt;Fn(i32)-&gt;bool&gt;` doesn't implement `Debug`, so you can't derive it. If you remove the `#[derive(Debug)]`, then it compiles and runs.
This is UB because the compiler is allowed to assume it. Even with the checks, it's UB. You don't need to transmute from immutable to mutable references to achieve your goals here, you can just extend the lifetime, which isn't always UB. `&amp;mut *(&amp;mut a[k] as *mut _)` should suffice, and it isn't UB as long as the final returned lifetime is properly constrained and you check for overlap.
In this instance, it does seem like the output type (aka associated type) is better. It's hard to imagine an implementor of `Semiring` where the `T` would not be `Self` or something else uniquely derived from `Self`.
Both those times look really great to me!
&gt; That's up to the executor to decide and isn't something the future should have to worry about. That's problematic, because it means the Future isn't usable with the executor. Basically, this Future calls `wake` with the intention of being awoken after the executor tries to poll all oder Futures that are also ready to be awoken first. Executors can't differentiate whether `wake` means "right now" or "try to make progress with other Futures first". Arguably, a `Future` that calls `wake` before returning `Pending` could just have continued to do the work itself, without needing to yield back to the Executor and then immediately back to the same `Future`. &gt; they'd all just get polled every time. Note that the problem isn't that they get polled every time. The problem is that all Executors widely in use handle this case as "this Future returned pending and I haven't scheduled another future yet, did this Future call wake? If so, I'll schedule it again".
The main thing game devs want is fast builds and good performance even in debug builds. Everything else really follows that. Modern C++ is often hated Not just because the language is a mess bur because it violates these requirements. 
Hi, sorry for your bad experience, but be aware that 0.12 is an alpha version, coming with no guarantees at all. The original pkg files are available here: https://mail.mozilla.org/pipermail/rust-dev/2014-October/011267.html. I'm not on OS X anymore, but if I remember right, they should also provide uninstallation. You might also be able to use the binary release for your platform, which _does_ have an `install.sh` script (which ships with an "uninstall" subcommand). Finally, you can also scrub your computer manually, the list of files can be gotten from the binary released (it's mostly rustc, cargo, the rustlib directory and a couple of libs in the "lib" directory). All library files carry a hash in their name. I know that a manual process is annoying, but it gets the job done.
Oh, nice to see that my comment made it into the RFC. I want to move from JS to Rust for my web application, and the lack of debugging support is the one thing that makes me hesitate.
Tbh Rust compilation time is the worst of all languages I ever tried
So you effectively want a sandboxed high performance VM runtime? Fair enough, if so. For what purpose? (I'm curious)
IIRC it only show times for each step of compilation, not individual modules?
Hi, &amp;#x200B; im a Wev-Dev trying to get some private projects done in Rust (keep it interesting learning new stuff :) ) But i love my HTML/CSS/JS frontends and don't want to Draw in QT or OpenGL &amp;#x200B; I saw that i can write native Node.Js modules with rust wich is cool but it is kind of "heavy" and the main App will stay in Electron and im looking more for a pure rust backend. Is there a good libary / tutorial for writing Web GUIs with full backend in Rust? &amp;#x200B;
I came to rust because of this obsession.
This is great! thanks
These sound like a perfect use for UnsafeCell as a way of completely avoiding undefined behavior to duplicate a mutable reference. Would you happen to know a reason why not?
&gt; it means the Future isn't usable with the executor I'm not sure why you would think that. Imagine this: the executor is full of futures that exclusively call wake before returning pending. It's entirely unaware of this fact. They're all in the poll queue. When it polls each one in turn, each calls wake and puts itself in the back of the queue, so they all keep getting polled in the same order, not each immediately to completion. Now throw one in there that has a real event-driven wake calling. The worst that happens is that it gets stuck in the queue behind all of the others. But it *still gets polled* because all of the "greedy" futures still go to the back of the queue *behind* the evented future when they call wake because that's how the executor schedules things.
&gt; They're all in the poll queue. When it polls each one in turn, each calls wake and puts itself in the back of the queue, so they all keep getting polled in the same order, not each immediately to completion. Which executor does that? I haven't found a single executor in the ecosystem that does this.
In those aspects, Rust is not a good contender.
It is LLVM doing it: the IR visible in the article is after LLVM has run its optimisations.
Read subreddit description before posting. And do this everywhere, not only in reddit.
Is there a reason why fn item types can not be named explicitly?
Reading this one thing that comes to mind is the Jai programming language. I've been very unimpressed by it. Reading blog articles like this really helps me to understand the motivations behind it.
Aha thanks! I mistakenly thought this was the output from rustc to be fed into LLVM. 
&gt; whether casting shared reference to several mutable pointers is UB too? That sounds a lot like UB. Do you mean that you have a single mutable reference that you change into several mutable pointers, or that you have a function that takes a shared reference and makes several mutable pointers? Having a function that does the latter without some form of runtime checking/locking would not be safe.
I don't see how you would encorporate `UnsafeCell` into this example. If you could give a concrete example of using unsafe cell without changing the signature of the function, that would be great.
Not really. Rust generally relies more on optimization than C does.
For each folder that needs be recognized as a module, you need to create an additional file at either `./&lt;folder&gt;.rs` or `./&lt;folder&gt;/mod.rs` which re-exports its submodules as the help message „name the file either libA.rs or libA/mod.rs inside the directory ""“ for E0583 suggests if one tried otherwise. main.rs lib_a.rs lib_b.rs lib_a/ lib_a1.rs lib_a2.rs lib_b/ lib_b1.rs lib_b2.rs `lib_a.rs` (or `lib_a/mod.rs`): // if you want to keep the hierarchy: pub mod lib_a1; pub mod lib_a2; // otherwise if you'd like to flatten the structure: // mod lib_a1; // mod lib_a2; // pub use lib_a1::*; // pub use lib_a2::*; `lib_b.rs`: analoguous to `lib_a.rs` `main.rs`: mod lib_a; mod lib_b; fn main() { // if nested: lib_a::lib_a2::foo(); lib_b::lib_b1::bar(); // if flattened: lib_a::foo(); lib_b::bar(); }
The graphs here are especially problematic because they use completely different time scales, so a quick lookup gives the impression that pulldown is quite a bit slower and less predictable than comrak, until you check the scales and realise that cmark barely reaches 300µs while comrak's peak is around 360 and averages 380.
I know it's unsafe, and there are runtime checks, and no, I do not return those values to a user, it all happens completely inside one function. Maybe, I should consider using `UnsafeCell` instead.
At the moment yes. There is hope this can change. 
Yeah if all of this is happening within the method of a struct, you probably want to mark those fields using `UnsafeCell`, or perhaps even `Cell`/`RefCell` if you want to be a bit safer (but they do add some overhead because of runtime checking). When properly encapsulated and commented, this is normally safe.
Porting my company's (medium data) ML service from Python to rust!
That's true, but i guess we kind of already have this problem with mrustc.
A hardware-level sticky overflow flag would be super convenient there. Sadly x86 doesn't have one, and apparently ARM's Q flag is only present in E variants, and only acts as an overflow flag for some operations (it's really a sticky saturation flag).
Yeah, I just didn't want to use `RefCell`, as I already do all necessary checks by myself and the code is hot, so I didn't want to spend additional resources on non-needed checks. Anyway, thanks for help.
You're creating a dangling pointer. You can't store a pointer to a point in memory that will get freed without using unsafe. You either have to move a into b or use a heap allocation and store the pointer to that in b (a Box, for example).
That's a pretty big program, so could be a useful data point. What's the build time, out of curiosity?
Here is my take on paging, with generics for tables and macros for bitflags. [https://github.com/pczarn/rustboot/blob/master/arch/x86/cpu/mmu.rs](https://github.com/pczarn/rustboot/blob/master/arch/x86/cpu/mmu.rs)
Criterion also creates a violin plot, combining all benchmark results. I think this would be more appropriate to show here.
Struct field inheritance would make the language a bit faster for graphical/AST cases, but it looks like they don't care for gfx. perf. And verbosity is also high on Rust, too.
"You're gamers, aren't you supposed to enjoy steeper difficulty curves than Dark Souls?"
Rust without optimizations can be slower than Python...
Exactly, well, I assume this optimization is done in LLVM. So, it would allow you to compare the efficiency of what the language itself produces.
I'm wondering just how much faster it can get. I don't know anything about the internals so I can't really comment on it, but it seems like it would need order of magnitudes improvement to compete in that regard. Will it ever be *that* much better??
I expect that's the point they're making.
Could you give an example which does not compile?
I am excited! &amp;#x200B; The Rust ecosystem has so far lacked reliable AtomicReference and concurrent hashmap, and I have been expecting crossbeam to implement it. &amp;#x200B; I am also expecting to the Hazard Pointer because EBR seems to be plagued by inability to reclaim memory in a timely manner. &amp;#x200B;
You have to define `a` in the same scope as `b`. Otherwise the compiler cannot prove your code is correct. Imagine `unset_a` has a bug and does not clear `a`, then printing `&amp;b` would result in undefined behavior.
Using `*` as a version requirement will break your code if there is a breaking change in the library.
I'm not at all familiar with it, but I've seen cranelift mentioned in various threads an experimental backend with faster compile times. According to the comments I've seen would be something you use for debug builds and then you would use the regular compiler for release builds to get all the optimizations.
[simplest example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=c5c266606b91d83e317d0b0b63de1fc7) It is possible to bind fn to a variable, but the type itself can not be named ([fn item types in docs](https://doc.rust-lang.org/reference/types/function-item.html).
This is awesome thanks! I'm interested in the implementation of the [Trieber Stack](https://docs.rs/crossbeam/0.3.2/crossbeam/sync/struct.TreiberStack.html). u/raphlinus has an implementation he's working on for the [synthesizer](https://www.reddit.com/r/rust/comments/a0m49g/fearless_lowlatency_audio_synthesis_video_slides/) project, and it requires careful control of allocation so that the audio-thread never blocks. Could Crossbeam be used for audio? 
Is that right or an exaggeration? Just curious. Do you have concrete examples where that happened? Maybe for heavy string manipulations, where most Python functions just directly call the corresponding C routines.
OK, I understand why the author of the blog post wanted to understand the difference, but IMHO if this is the real code why not just optimize it to "out = in - '@'; if ((unlikely)) out &gt;= 5 then out = 0;" Probably faster than both of these implementation, and cache friendly too..
&gt; vks_ Thanks for the explanation. It makes sense. But is there any workaround..?
Thank you. I heard that Rust 2018 does not need the `mod` file anymore? How do we do it then?
&gt; A special Injector queue was introduced, which integrates nicely with Worker queues and supports similar operations. I looked at the API and it does not appear to offer any blocking `pop` of any kind. When writing some sort of scheduler you eventually want workers to go to sleep when there is no work, instead of spinning. How would one best implement this using this Injector queue? When using synchronised structures (e.g. a `Mutex&lt;T&gt;`) you can use a condition variable, but that seems like it would be overkill. Using a channel would work, but it means you'd have to send a message to every channel if you want all workers to wake up (instead of just using something like `Condvar::notify_all`). It would also be nice to see what the overhead is of pushing into the Injector. When using a crossbeam Worker, pushing is fast (I measured it to be consistently around 200 nanoseconds). This makes it ideal when you want to quickly push new jobs somewhere, without many threads contesting over a single shared resource. The backstory for these questions is that I'm rewriting the scheduler of [Inko](https://inko-lang.org). Right now it uses mutexes (using parking_lot at least, but it's still expensive at times), which I'm replacing with a completely new scheduler. One of the main issues is that multiple threads may produce work, and I would like to schedule that work as fast as possible. Using a synchronised queue would allow for other sleeping workers to wake up and pick up that work, but it comes at the cost of many threads (potentially) fighting over the same resource(s). Right now the setup is as follows: 1. Pop from a local Worker 2. Try to steal a bunch of jobs from another Worker 3. Perform a blocking receive from a `std::sync::mpsc` queue The mpsc queue is used when scheduling jobs directly onto a worker (my scheduler unfortunately has to support task pinning). This however means that one thread may end up with a lot of work (if it keeps producing new work), while other threads are asleep. The blocking part of the last step is important, as I don't want threads to spin forever when there is no work. How would one tackle this using the new Injector type?
How long does it actually take? You're already taking advantage of `cargo check` and incremental builds to reduce the work required? IIRC incremental builds are the default for dev builds but disabled for release. And same w/LTO: this can take a lot of time (disabled by default for dev builds, enabled for release). Another option is `sccache`.
Great writeup - the "What is crossbeam' alone is good to know. &gt; Every such improvement in crossbeam-deque has a ripple effect on the library ecosystem. By bumping dependency versions and leveraging new features, Tokio’s thread pool gets faster, and therefore every application using Tokio gets faster, too! The knowledge that people used to dealing with these low-level performance issues are working steadily on them and making them safe for mere mortals like me to use, and furthermore that it is percolating steadily through the Rust ecosystem, gives me a warm fuzzy feeling.
This is amazing work. Thankd so much! 
You could always declare your reference with a static lifetime. `let a: &amp;'static A = &amp;A {};` It will let the code compile and run, but I'm not sure if this will create a potential memory leak (if `mem::replace` will actually let `a` die after `unset_a` is called).
You still need `mod` in Rust 2018. They stepped back from the plan to get rid of it because the community found it too unintuitive.
The only architecture I'm aware of that has anything like this is Itanium, I think. It would be really handy if more hardware had such things.
Sorry, on mobile so I can just give a hint: 'foo' is not a type Bute merely a name. You want something like 'let x: Function = foo' which should work if you replace Function with the actual type for a function (Fn which takes no argument and returns nothing). Or you omit Function and just have it inferred by the compliler: 'let x = foo' 
&gt; Which executor does that? I'm pretty sure that this is a very common way to do it. If you look at [local_pool](https://github.com/rust-lang-nursery/futures-rs/blob/master/futures-executor/src/local_pool.rs), it uses a [FuturesUnordered](https://github.com/rust-lang-nursery/futures-rs/blob/master/futures-util/src/stream/futures_unordered/mod.rs) as its registry/ready-to-poll queue. It maintains a [ReadyToRunQueue](https://github.com/rust-lang-nursery/futures-rs/blob/master/futures-util/src/stream/futures_unordered/ready_to_run_queue.rs), which is used by the [Task](https://github.com/rust-lang-nursery/futures-rs/blob/master/futures-util/src/stream/futures_unordered/task.rs) struct to implement `UnsafeWake` in pretty much the exact manner I described. The `LocalWaker` gets forwarded around a bit via the `AtomicWaker`, but ultimately, it's a simple queue of task IDs that determines what gets polled when. &gt; If the future that just yielded Pending can make progress, chances are that all the data it needs is still in the CPU caches of the current thread of execution. If a future is calling `wake` before returning `Pending`, it's not because it can actually make progress, it's because it doesn't have a better way to get its task woken up. If it could actually make progress and is so CPU/cache bound that maybe having to wait in line to get polled again is a problem, then... Why is it in the hot path of an IO-bound event loop in the first place? And why not just go ahead and make its progress rather than yielding?
Have you looked at the [rayon task scheduler](https://github.com/rayon-rs/rayon/blob/master/rayon-core/src/registry.rs) for inspiration?
There's (somewhat) limited support for `SharedArrayBuffer` due to security concerns: https://caniuse.com/#feat=sharedarraybuffer
`fn foo() {}` doesn't have a type `fn()`, but an unnameable type that in the error messages the compiler prints out as `fn() {foo}`. Having such special types allows stuff like `stuff.iter().for_each(foo)` to monomorphise just like with closures, instead of doing virtual calls on each iteration. So I understand the question as "why can't you name those special types" - but I don't have a good answer to that myself.