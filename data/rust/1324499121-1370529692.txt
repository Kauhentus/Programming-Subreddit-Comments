Someone should change line 188 in raytracer.rs to use pow.. I had to write that terrible abomination of code because I couldn't figure out how to get pow into scope at the time. :-)
Nobody has ported to ARM yet. It will definitely happen and would be an awesome project for somebody to undertake.
That would be awesome. I suggest talking to nmatsakis on #rust on irc.mozilla.org. He did the first port from x86 to x86_64 so will have a good idea of how to go about it.
 some_u32 as float used to be the syntax for type-conversion i think
Thanks! How could I specify `floor` or `ceil` when converting a float to an int?
Yeah, it's not showing in the docs. http://doc.rust-lang.org/doc/core/files/float-rs.html
Why does `int::to_str` use a radix but not `int::from_str`? That's just inconsistent.
Ah, vec::head and vec::tail do the trick. use std; fn main(args: [str]) { check vec::is_not_empty(args); let program : str = vec::head(args); std::io::println("Program: " + program); }
So far it will syntax highlight your code, provide markers for compilation errors, and allows you to launch your application from the IDE. These features still need a great deal of refining. Also, the new project wizard will create an .rc file for you.
Maybe crust is a bit too cute. Why not "foreign"? Or overload the native keyword, I suppose.
So far nobody has voiced a strong opinion for or against, but I agree it's questionable. "foreign" sounds ok to me. "native fn" has some vague meaning in the language already.
Install MinGW and put it on your path.
Nice.
See also https://mail.mozilla.org/pipermail/rust-dev/2012-July/002000.html
&gt; ##Removing the `cont` keyword in favour of something else &gt; Either `next`, `loop`, `again`, or similar. Unlikely to go with `continue` since all other syntax changes are converging once more on sub-5-letter keywords. Totally cosmetic change. Why not Perl’s `redo`?
Yay; I dislike maintaining export lists. That seems like work an IDE should be doing anyway.
Maybe because it's not the same as `continue`? The "redo" command restarts the loop block without evaluating the conditional again. The "continue" block, if any, is not executed. This command is normally used by programs that want to lie to themselves about what was just input. PERLSYN(1) 
Quoting the `continue` part of `perlsyn` was little useful, as Perl’s `continue` is not other languages’ `continue` either. It was the `again` example that lead me to think that `redo` was an appropriate keyword.
I can't even begin to say how excited I am for this. 0.3 looks like the release where I'll really start digging into Rust. Thanks for posting!
Here's the announcement on the mailing list: https://mail.mozilla.org/pipermail/rust-dev/2012-July/002087.html
Why, hello, I know you from somewhere... Feel free to stop by the #rust IRC channel on irc.mozilla.org . We're friendly folk :-)
How does the lifespan impact allocation/deallocation logic? Is it enforcing, that the pointer you provide to some function, is no longer accessed, after the function call, by any of its code? Ifso, how is this enforced? 
Will `do while` come back in another form?
There was a short feedback on the weekly meeting: https://github.com/mozilla/rust/wiki/Meeting-weekly-2012-07-17
Yes, and yes with caveats. There can be many impls for a single type. impls have to be defined either in the same module as their self type or the same crate as the trait they implement. So you can define impls anywhere, even if you didn't define the type, as long as you are also defining a trait. This is so common that there is going to be syntax for defining a trait with an implementation, so if you want to tack on some extra methods to a type you can write something like `trait helpers for mytype { fn foo() { ... } }`. It isn't implemented in the language yet. What you won't be able to do is take a type from one module, and a trait from another crate, and implement the trait for that type.
Nice, sounds like interfaces but more flexible. What I don't understand is this: &gt; What you won't be able to do is take a type from one module, and a trait from another crate, and implement the trait for that type. Lets say there is a trait 'ToJson' that is defined in the standard library of Rust. I have defined a struct 'Customer'. What you are saying is that I cannot implement ToJson for Customer?
&gt; Can I "inherit" from DateTime and create my own myDateTime that implements ToJson? I think this is basically what you'd have to do. It's fairly easier to do with the newtype syntax: enum myDateTime = DateTime; This creates a single variant enum that wraps a DateTime. You could then define an impl on myDateTime instead of DateTime.
Awesome! From your reference to "newtype" I assume this is inspired to Haskell, which is a very good thing in my book. Also I see that all the answers here come from core rust developers, so thanks a lot and I'll be not wasting more of your time, guys: I'm looking forward to seeing rust 1.0!
Why does passing a value on the stack generate a warning? Isn't it a fairly common thing to do? I looked at the Rust website and it talks about classes. Are they now completely replaced by structs? Is there a concept of private members?
I'm not sure exactly what you refer to, but passing a value on the stack doesn't itself generate a warning. Copying certain types (those with mutable state or that require allocation) gives a warning, either because it's likely to be a logic error or a performance hazard. So if you pass a mutable type by value then the compiler is going to complain - in many cases this is a logic error because if the callee mutates the value then the caller won't see it. Classes will be removed from the language soon, their functionality replaced by structs + impls. Private members are in the design ('pub' and 'priv' are reserved words), but not yet implemented.
Docs are lagging, yes. How private members work isn't ironed out yet, but I think the most likely scenario is that the module that defines the struct has access to the private members. So any impls defined next to the struct do as well. Constructors will likewise just be functions defined alongside the struct (the naming convention has not been determined yet). Destructors will be implementations of the `drop` trait (several previously-independent features are being unified with traits - destructors, operator overloading, type kinds). In the end, creating something that behaves like a typical OO class will involve a bit of convention - a mod with access to the private fields, a constructor function with a recognizable name, an impl of `drop` and impls for the other methods and traits. As an OO programmer it makes me a little sad, but I try to remember that Rust is not an OO language - it is a multi-paradigm language. It offers a lot of powerful tools but the best way to make use of them isn't clear yet. We do have a powerful macro system in development so if one wanted to wrap everything up in a macro like `class! my_class { ... }` it will be doable. 
Sorry to be off-topic, but i would just like to thank you guys for sticking to it. I love how easily you guys throw out languages features and introduce new ones. You are still exploring, and you aren't being dogmatic about any aspect. It almost feels like this has never been done before. As if, most of the time, language designers (a bit like artists) start with some 'choices' that are never up for discussion, just to limit and guide the search-space. All I want to say: is take your time, and keep breaking Rust whenever you see fit please. The process is incredibly interesting to follow, and i'm very excited to see, what happens when people actually take the time to first get it all right .. before it takes up and you have to deal with legacy support.
Just a quick reply with the link: http://lambda-the-ultimate.org/node/1277 They have a link to the PPT file there, but here is a PDF version: http://www.st.cs.uni-sb.de/edu/seminare/2005/advanced-fp/docs/sweeny.pdf The talk is about the "next mainstream programming language -- from a game developers perspective". The stuff about framework inheritance starts at slide 21. Couple of notes: - this is from 2006 - he assumed the hardware race would not wait for software to catch up (he was expecting 20+ core machines in 2009) - the statistics about concurrency used in the Ureal Engine are very interesting - the troubles, focus and performance areas may indeed be very relevant for Rust But i'm not really a mailing list guy. But it may indeed be very interesting to bring these slides up there, just to consider how well Rust solves some of the problems mentioned here. Just to be clear: you may not be aware. Their game engine is by far the leading industrial engine, with no other engine comes close in market share at the high-end feature set. Older versions of the Unreal Engine even had their own crossplatform scripting language built (UnrealScript) which was a pretty interesting scripting language in its own regards (taking inspiration from finite state machienes).
&gt;Type parameterized modules sound similar, and that probably isn't going to happen, though there has been some discussion about merging some aspects of modules and types. Type parameterized modules are quite a different beast though. Consider a framework like this: class Actor {} class Scene { void createPlayer(){ return new Actor("the player"); } } class Camera {} Normal inheritance gives us: class BetterActor &lt; Actor {} class BetterScene &lt; Scene {} class BetterCamera &lt; Camera {} The problem is: both scene.createPlayer() and BetterScene.createPlayer() return an instance of Actor, instead of BetterActor. The simplest form of framework inheritance would look something like this: framework GameEngine{ class Actor {} class Scene { void createPlayer(){ return new Actor("the player"); } } class Camera {} } framework MyGame &lt; GameEngine { class Actor { // inheritance is implicit // we can overwrite methods if we want } } Now, if we create a scene in MyGame and call .createPlayer() we would get the subclassed Actor. There are a couple of workarounds in traditional object oriented languages, but they tend to add a lot of verbosity, and run-time complexity. The most obvious workarounds without changing anything to these languages would be: - have some factory method that can be overwritten (compile-time) - have a central factory class that allows a consumer of a framework to just configure their preferred constructors (run-time) - make the constructor classes generic types of the class (compile-time) But that all seems very cumbersome and verbose. I'm not sure how to implement this in Rust. For me the language is too unfamiliar and changes too rapidly to have even have a clue in what kind of direction you would be looking. But from a top-level point of view: you have this common set of code split in multiple files. Now is there a way to bundle all that logic, and extend it, without forking it. It could fit Rust very well, to also look at inheritance as a feature of OOP that can be applied completely separate from the other OOP features. Much like how encapsulation and multiple-dispatch, the other two cornerstones of OOP, are handled. 
I think reference counting and cycle collection in Rust is interesting. Rust knows more about types than many other languages. E.g. it knows that a reference can't be seen in a different thread, so no need to use interlocked ops. It also knows that immutable values are guaranteed to be acyclic, so no need to trace. For performance, maybe you can switch some parts of the code to use deferred reference counting (which adds some latency), instead of immediate reference counting. This could well be the sweet spot here (esp. in the future with large heaps - cost proportional to amount of garbage is better than cost proportional to amount of live objects).
It's fascinating to watch the Rust development process. The team seems to have incredible momentum. I was looking forward to a 0.4 release next week, but it's been [pushed back](http://irclog.gr/#show/irc.mozilla.org/rust/245996) for a week or two. Keep up the great work, Rust team!
video by any chance? 
I haven't heard of any audio or video being captured, but I will post it if something turns up.
This reads nicely. I'm excited for the programming language of the future. Does Rust not have multidimensional arrays?
Yes in retrospect there are a few places I could have replaced loops with folds or maps. I've been stuck in procedural mode this week :-/
The video release schedule has been posted: https://thestrangeloop.com/news/strange-loop-2012-video-schedule Unfortunately it won't be available until January!
 match curr.survives(&amp;p) { true =&gt; next.set(&amp;p, true), false =&gt; next.set(&amp;p, false) } should be: next.set(&amp;p, curr.survives(&amp;p)) or am I missing something?
Also https://mail.mozilla.org/pipermail/rust-dev/2012-October/002467.html
So, I wanted to try Rust. Went to the home page, downloaded the installer, installed it, copied the example from the home page (“A very small taste of what it looks like”), compiled it, ran the resulting executable, and... --------------------------- primer.exe - Entry Point Not Found --------------------------- The procedure entry point _ZN4rand14__extensions__10meth_1231414gen_uint_range17_fbb3337cf235b5b13_04E could not be located in the dynamic link library core-c3ca5d77d81b46c1-0.4.dll. --------------------------- Just that. Maybe somebody wants to look into it... **EDIT** [Solved](http://www.reddit.com/r/rust/comments/11j36d/rust_04_released/c6npp3y): The "runtime" core DLL was wrong, copying the "compile" DLL over it solved the problem. 
Does this help? "Note that the Windows installer still requires a somewhat specific version of MinGW and Msys to operate; recent builds of MinGW provide versions of GCC that are incompatible. Rust presently builds and tests with release 20110802, containing GCC 4.5. [Details] can be found on the wiki." https://mail.mozilla.org/pipermail/rust-dev/2012-October/002489.html
Oh, that's bad. I thought GCC was only needed to build rust, but, if you didn't want to do that, `rustc.exe` was enough. I think now that it is using my newer version of GCC and that's why it doesn't fail completely but only on certain methods. I would suggest for Rust to just include in the installation the C compiler and libraries it needs, and put them in a subfolder. Disk space is cheap these days. **EDIT** Apparently no GCC problem after all. [See here](http://www.reddit.com/r/rust/comments/11j36d/rust_04_released/c6npp3y).
&gt; “do not communicate by sharing memory; share memory by communicating.” Isn't that from [Tony Hoare](http://en.wikipedia.org/wiki/Tony_Hoare) ?
That is not normal, but the errors seem to give a good indication of what is wrong.
The goal is to eventually switch to MSVC rather than relying on MinGW, but I'm not sure what the timeframe on that is.
lol reboot and refresh fixed it .. doh, shouldn't leave my machine on so long
So the plan is to have Rust 0.5 on Dec. 18th
Though keep in mind that the devs tend to consistently overrun the release milestones by a month or so.
Announcement: https://mail.mozilla.org/pipermail/rust-dev/2012-October/002497.html
I have not really been privy to the conversation (just joined the dev list recently), but have there been any suggestions on better syntax to deal with this?
It looks quite an awesome tool. Once again shows the advantages of not designing your compiler as just a compiler, but building it one re-usable block at a time!
Any justification for managed pointers being the "default" type (get their own builtin syntax) and shared_ptr being the "opt-in" type (have to go a library, more verbose to use)? The determinism in debugging I think makes shared_ptr a saner default, unless you're specifically not supporting finalizers/destructors for objects behind managed pointers.
I have been following the condition system discussion on the mailing list and it looks well thought out... but something is nagging at me... ... when is it actually useful ? Does anyone with a background in Lisp or other languages using conditions have an example of when it's useful to call a condition rather than just failing ?
Thanks for sharing this. I have a style question. This may be personal preference or maybe there is a Rust best practice. Why pass-by-pointer instead of pass-by-value. And in Rust what should drive our decision one way or the other? I'm primarily a C++ developer; the main questions as to pass-by-reference vs. pass-by-value are how big the object is, and how expensive it is to copy (putting aside C++'s pass-by-pointer since there isn't really a Rust equivalent). Since Rust doesn't have copy constructors, copying a struct is always just field-by-field or bit-by-bit copy, right? It sounds like this would be an opportunity for the compiler to estimate whether it's more efficient to "under the hood" actually pass by pointer or by value, so for immutable values it would always be preferable to pass them in such a way that the compiler could choose. Will Rust take advantage of having that flexibility? If so, would it be in pass-by-value cases only, or can it make that call even for pass-by-pointer? On a related note, one of the patterns pass-by-value seems to create is that if you use a for-loop you need to dereference the parameter to the function all the time, since the vec::each expects a function that takes a pointer. It also unfortunately seems to preclude use of the standard "map" which expects to pass a pointer.
Perhaps, but they're on such a tight schedule right now that I'd expect them to call out to C libraries wherever possible. And until Rust code can run without a runtime ([#3608](https://github.com/mozilla/rust/issues/3608)), any library rewritten in Rust wouldn't be usable from any other language anyway. The good news is that this means they'll be motivated to ensure that Rust -&gt; C interactions are as fast as possible.
Thanks! The more I look at it and the more it looks like an error-policy mechanism.
Oh, that's really unfortunate. I'm not sure if gluing things together will provide the necessary data to evaluate Rust. It's a bit over-engineered for that. But it's interesting that there is a ticket for run-time-less Rust. If such a thing existed, it would be possible to rewrite particularly exposed areas of code for an increase in safety.
I havn't personally seen one... I guess I'm not wholly certain what are you looking for in this sort of tutorial. Pointer explanations? Type system explanations? Functional programming explanations? Rust is a pretty new language, as these things go. 
Rust probably isn't the best kind of language for absolute beginners, many of its assumptions rely on knowledge of lower level languages such as C. For example, every explanation of borrowed pointers you will find will say they are "C pointers" that are checked at compile-time for validity. The thing about the languages that around rust-level and below is that you need to have an understanding of the underlying machine in order to make sense of any of the design tradeoffs and behaviour they have. Additionally, Rust is quite a new language, and it's hard enough to find resources for it if you DO know an existing low level programming language. I would suggest finding a tutorial for C that will explain all of the base concepts of the language. You don't necessarily have to be good at the language, but understanding why it does the things it does is very important.
Very appreciated, thanks. Is there a blog article explaining how it works/how to use it?
"A REPL". Pronounced Repple
Still didn't work. However when I did this extern mod std(vers = "0.5"); That allowed it to find std::net::ip::v4::parse_addr I don't remember what place i found that in, but it was aweful hard. I am finding it hard to really toy with rust due to hte lack of documentation on libraries that _do_ stuff. 
while documentation is scarce, you will come out better off when you get past these hurdles no? With all new languages, it will take some time before functionality is stable, and in turn have sufficient documentation. just the opinion of an outsider who is also looking through screens of source code
Somebody should really be posting these important messages on [@rustlang](https://twitter.com/rustlang) twitter account.
That's a good question, and in fact that operation doesn't work at all right now! fn main() { let y = "bar"; let z = "qux"; let x = y + z; // error: binary operation + cannot be applied to type `&amp;static/str` } (Notice that the error message confirms that `"foo"` lives in the static memory region, written as `&amp;static`.) I actually have it on my list to implement this before 0.5 (due within a month or so), but it's still unclear to me what the return type would be. Most likely the result of concatenating two `&amp;static/str`s would be a `~str` (pointer into the owned heap), but I'll need to consult with one of the devs to figure out the best course of action. Let me also note that, yes, strings in Rust are pretty weird (vectors are also weird, but they're weird in basically the same way as strings). It took a long time and a lot of discussion to arrive at the current design, which seems to emphasize efficiency and ease-of-use over design consistency. There really needs to be a tutorial to discuss these differences.
The 0.5 priorities were always a bit ambitious, but even if you reduce your expectations they've gotten less done than they would have liked. Two of the five primary developers were otherwise occupied in this cycle: Graydon on relicensing and setting up the (rather awesome) new buildbot (poke around http://buildbot.rust-lang.org/ to check it out); Niko on Rivertrail (a Mozilla/Intel project on data parallelism in Javascript, see https://github.com/RiverTrail/RiverTrail/wiki) and ParallelJS (a more general extension to parallelism in Javascript, see http://smallcultfollowing.com/babysteps/blog/2012/12/05/self-hosted-parallel-js/). 0.6 should be more productive, because not only will Graydon and (hopefully!) Niko be back in swing, but they're getting a temporary new developer: John Clements, a contributor to DrScheme (DrRacket?), who is taking a sabbatical from his university position next semester to work on Rust.
I like it. I'd like to see it go oven further one day and so something like a UFCS in D where a non-impl fn's first param can be kinda the instance. E.g. fn foo(something: string) -&gt; int can be called let i = "hey".foo() but that may get away from the core of the language too much I know.
Neat, I hadn't heard of UFCS before: http://www.drdobbs.com/cpp/uniform-function-call-syntax/232700394 Distilled: *"Methods can be 'added' by third parties without changing the original class definition. Additions from multiple third parties can be used simultaneously. And payment will only be made for the methods that are actually used."* Note that Rust *already* has this capability, in a form that is similar to the "extension methods" discussed at the end of that article. Amusingly, the "method call syntax as sugar" RFC here would achieve the exact same result as UFCS, except in the exact opposite way: in UFCS you define a function and get a method for free, and in this proposal you would define a method and get a function for free. So in the sense that the two features overlap to such a degree, I don't think UFCS would be very useful in Rust (assuming that this proposal is accepted, which seems likely).
They're called anonymous traits now, and you can define them for any struct or enum that you declared in the same module.
Whoever thought of that was a bloody genius. Don't like the gradient on the logo though - ick.
Posted this because I've seen a *lot* of people frustrated over Rust's current (and rather primitive) networking libraries. I myself would love to write some server applications in Rust, but the libs need a lot of work before they're suitable.
&gt; we get high level languages that throw out performance considerations left and right for minor convenience reasons (e.g. C#, Java). Uh, what? "Run anywhere" binaries are a minor convenience? And the performance considerations are about allocating extra heap objects for class members instead of having them inline, but you *can* have them inline with `struct`s in C# (although I agree that it wrongly forces either all or no instances of a class/struct to be stored inline, rather than letting the consumer of the class/struct decide).
Thanks. I just wrote my first code in Rust today and I'm still a little fuzzy about the memory model, but this definitely helps. Let me know if my understanding is right: * a plain Java array is like a `@[int]` vec which doesn't allow appending (`push()` takes a `&amp;mut ~[T]`) * a C++ array is like a `~[int]` – you can't alias it and in Rust you'd need to do `let b = copy a;` * an ArrayList/Python array is like a `@~[int]` – essentially, just a level of indirection over a C++ array (and `push()` works because it's perfectly legitimate to borrow a `&amp;mut ~[T]` pointer from an `@~[int]`) Looking forward to the next post!
Frankly... not sure why I would want to write the latter of these two options, doesn't rust already have the preferred method of writing the example? The article gives a reason at the end with the animal namespace but isn't that what overloading is for in OOP? If Dog does not have is own impl then it calls the Animal impl (assuming extend not interface) dog.speak(); Dog::speak(&amp;dog); // this seems redundant and lots to type I don't really buy the advantage given, this feels like it is reverting to C style where you always do func(&amp;object, params) since C doesn't allow for the object.func(params) style.
You wouldn't actually type `Dog::speak` in practice. I just wrote it that way to make it clear where the method name lives. You're right that writing `Dog::speak` is annoying; that's why the method call syntax is supported in Rust (one of the major differences between Rust and Haskell/ML).
Understood, might want to make that clear. I was a bit confused like you were trying to say it was better or something. Thanks for the explanation. edit: maybe I was just being dense and didn't understand .. seems a bit obvious now in retrospect
I really like reading about Rust. I say so to encourage you and the other developers to continue writing articles like this. While I don't have time now to devote to writing Rust code, I do like to keep informed and these little articles are perfect for that.
But what if the application had a legitimate data dependency between network calls? (And needed to run 10000 parallel logical threads, all doing that). I don't think that blocking calls are going to cut it for such applications.
Note: the author states: "I think it is premature yet".
How does this work exactly? Does it use ndk under the hood? I guess it doesn't use the GCC compiler the ndk comes with?
According to the changes to README.md one needs to have the NDK installed
Right. Not sure what name to give to that though. The paradigm of "not necessarily functional, but safe, unlike C++". Maybe it's just "type safety"?
Yep, based on his ["The Next Mainstream Programming Languages" POPL talk](http://www.st.cs.uni-saarland.de/edu/seminare/2005/advanced-fp/docs/sweeny.pdf), he's looking for the language to help with: * Performance * When updating 10,000 objects at 60 FPS, everything is performance sensitive * Modularity * Very important with ~10-20 middleware libraries per game * Reliability * Error-prone language / type system leads to wasted effort finding trivial bugs * Significantly impacts productivity * Concurrency * Hardware supports 6-8 threads * C++ is ill-equipped for concurrency I think Rust is well on it's way to help with all those points.
no
&gt; § By 2009, game developers will face... &gt; &gt; § CPU’s with: &gt; &gt; – 20+ cores &gt; &gt; – 80+ hardware threads &gt; &gt; – &gt;1 TFLOP of computing power Heh.
Though the video was only just released, the presentation actually took place in September 2012, when the compiler was at version 0.3. Keep in mind that a lot may have changed in the interim. EDIT: Just finished the video, it's definitely worth watching even if it is slightly out-of-date. Especially the specific discussion of Rust's zero-cost abstractions is something that I haven't seen elucidated elsewhere.
Indeed. Mostly this disclaimer pertains to the questions at the end, as many of the things mentioned have now been implemented. You'll also see quite a few explicit moves in the slides - these are no longer necessary. I'm pretty sure the `move` keyword is being deprecated, but I could b wrong.
Thanks, added to the description: http://ix.io/3Tr
First off, you can get rid of all those capture clauses and move keywords, since both are deprecated (moves happen automatically now). So that brings you to this: https://gist.github.com/4480070 Still working on slimming it down further, I haven't done much with tasks in Rust either. :) EDIT: Note that my version of Rust is the bleeding-edge incoming branch. Not sure which version you're on. EDIT 2: You don't actually need that `extern mod std`, since you're not actually using anything from the stdlib. I'll be updating that Gist as I go. EDIT 3: You don't need to explicitly note the types of `result_port` and `result_chan`. EDIT 4: Likewise for the type of `result`. EDIT 5: Just a style thing, but I like to define helper functions like `double` near where they're used. Moved its definition into `main`. EDIT 6: Okay, this next part is cheating a bit. :) Basically I wanted to abstract away all those `.clone()` calls using a macro. But, I ran into a bug in the macros system (and filed it here: https://github.com/mozilla/rust/issues/4375 ). Once that's resolved, you *should* be able to do something like this: https://gist.github.com/4480491 . That's probably the best that I produce without extensively consulting the IRC channel. I suppose the point of all this is to emphasize that there really ought to be a built-in library for this sort of thing! :)
does rust have something similar to ruby's i, j, k = 3.times.map { chan.clone() } ?
&gt; I saw you posted an even shorter version on the original challenge page Yeah, I noticed that the original author dropped that criterion for his most recent Go example and decided to follow suit. Also note that while Github currently doesn't support Rust syntax highlighting, you can approximate syntax highlighting by explicitly declaring which language's highlighting rules you want to use. The example there right now is using the Javascript highlighter, though depending on the specific code example the C++ or Ocaml highlighters may look better.
Great, this is awesome! I learned some C++ from this :) 1:1 and N:1 threading is actually supported today. See the various options in the `task` module.
So... turns out Rust had a futures library after all. :P I've updated the challenge with the new code, but might as well stick it here too: extern mod std; use std::future::spawn; fn main() { let p1 = do spawn { (|a: int| a*2)(10) }, p2 = do spawn { (|a: int| a*2)(20) }, p3 = do spawn { (|a: int, b: int| a+b)(30, 40) }; let (x, y, z) = (p1.get(), p2.get(), p3.get()); io::println(fmt!("%d + %d + %d = %d", x, y, z, x+y+z)); }
there's no dynamic typing involved - `3.times` returns a lazy iterator over `0..2`, and you just map over that. the haskell equivalent would be `map (\x -&gt; clone chan) [1..3]` that mlet macro is neat but i don't know how well it would play with a function returning multiple values!
The problem is moving the values into the closure; channels are not automatically copyable (they have copy constructors), so you have to copy them explicitly and move each copy into the closure. Fixing this would require adding copy constructors to Rust, which some on the core team don't want to do because of the comprehensibility danger of hidden function calls not reflected in the source.
ah - interesting point! as a c++ user i definitely find myself in sympathy with the "don't add copy constructors" camp.
I've seen someone post it on IRC. Maybe the generated output is hosted somewhere.
Gotta love the extensive discussion on borrowing `@mut T`!
Very exciting stuff coming in the next year! It's really been a pleasure watching the language evolve, I know you all can hit your goals. Once the core language is more or less stable (which it sounds like it will be with 0.6) I think that contributing will be a lot easier and hopefully you'll get lots of community help. I know I intend to, no price is too high to finally have a nice modern language I can use in lieu of C++! :P
I wrote this last week and linked it to a few people, and I've turned it into a page for Rust's wiki now. I'm glad people found it useful :). https://github.com/mozilla/rust/wiki/Rust-for-CXX-programmers
Thanks for the kind words :)
Very exciting! I'm basically waiting for the build system to be sorted out then will jump in. Can't wait!
The function is given a borrowed pointer, that is to say `&amp;(a value)`, and it receives `&amp;x`, which effectively binds `x` to the value. If, on the other hand, the function received `x`, it would be bound to the whole `&amp;(a value)` and thus be a pointer.
It can be regarded as syntactic sugar for: |tmp| { x = *tmp; ... } To put it another way, it is like the pattern matching one can do on structs and enums in a `match` match an_option { Some(a) =&gt; { .. } None =&gt; { .. } } but, in this case, the "constructor" is the `&amp;` . The confusion might come from the fact that this sugar is not a type signature. Written in full, the closure should be |&amp;x : &amp;int| { ... } so the `&amp;x` term has type `&amp;int`.
Thanks for the help!
I'd like to expand on dpaupp's point a bit, because there's something *really* cool going on here. Look at this program: fn main() { let x: &amp;int = &amp;4; log(error, x); // &amp;4 log(error, *x); // 4 match x { &amp;0 =&gt; io::println("nope"), &amp;1 =&gt; io::println("nope again"), y =&gt; log(error, y) // &amp;4 } match x { &amp;0 =&gt; io::println("nope"), &amp;1 =&gt; io::println("nope again"), &amp;z =&gt; log(error, z) // 4 } } Each log statement is annotated with its output. The first log confirms that `x` is a borrowed pointer to the value 4, and the second shows how you'd normally access the value by dereferencing `x`. The first match expression tests if `x` is 0 or 1. We have to write `&amp;0` and `&amp;1` because `x` is an `&amp;int`, and the compiler will effectively dereference `x` and `&amp;0` at the same time and compare their values. If `x` is neither 0 nor 1, the final pattern in the statement binds `x` to `y` and lets you do whatever with it. But `y` is still an `&amp;int`, as you can see from the output of the log statement. In the *second* match expression, we do one thing different. The final pattern now looks just like the first two patterns, and instead of dereferencing the pattern to *compare* it against `x`, it dereferences it and *binds* it! So now `z` contains the value of `x`, which the log statement confirms. There's just one more thing you need to know: patterns like `y` and `&amp;z` are important because they're "catch-all" patterns (known in techno-jargon as "irrefutable patterns"). Remember that a match expression like this is illegal: match x { &amp;0 =&gt; io::println("nope"), &amp;1 =&gt; io::println("nope again"), } ...because what happens if `x` fails to match either of the first two patterns? That's just no good, for various reasons. So you need a catch-all pattern. Think of them like `default` in a switch statement in other languages. **Here's the cool part: you can use *any* irrefutable pattern in this way!** To be more specific, you can use irrefutable patterns in assignment statements and in closure and function arguments. This rule is why Python-style tuple-destructuring assignment works: let foo = (1, 2); match foo { (a, b) =&gt; log(error, a+b) } let (x, y) = foo; log(error, x+y); It's also how you can ignore variables that you just don't care to name: let _ = some_function_whose_result_I_dont_want(); // ^ this is technically unnecessary since ignored return values are dropped anyway And how you can abstain from naming unwanted closure arguments: for [1,2,3].each |_| { io::println("yo world"); } You can also combine these: let foo = (&amp;1, &amp;2); let (_, &amp;b) = foo; log(error, b); // 2 It's super neat.
I opened an issue https://github.com/mozilla/rust/issues/4417
In the Rust repository, see the src/test/bench folder (online here: https://github.com/mozilla/rust/tree/master/src/test/bench) for some benchmarks. The ones with "shootout" in the name are implementations of the benchmarks from http://benchmarksgame.alioth.debian.org/, where you'll find implementations in several other languages. Note that it may have been a while since anyone took a look at those benchmarks, so they might be using outdated features. Tuning the benchmarks and peeking at the generated assembly would probably be quite instructive; pcwalton spent a while yesterday tuning a Perlin noise benchmark and found that one of the compiler's low-level C-functions wasn't inlining properly (with that fixed, the benchmark is in the ballpark of the version written in C). As for the general speed of Rust code, there's still a lot of legwork to be done before it can be rightfully called "as fast as C++". I'm confident that it'll get there, but development focus has been mostly on language completion up to this point.
Some notes: * I've shown it in #rust and Graydon and others were pretty happy with it, so I'm going ahead and implementing it (as a replacement for Cargo, with the final name being "rustpkg" -- not a fan, but I agree that consistency is needed). * The original concept was all imperative. Graydon wanted it to be declarative by default with imperative supported but not suggested. I originally disagreed but after a decent discussion I agree with him now, plus declarative seems to look pretty good. * I'm making the concept as solid as possible now but won't be able to start implementing it until next monday. I don't think it will take that long to implement, I'm on holidays so I might have it done in a week with luck. If anyone has any suggestions or can see any issues (especially regarding conforming to the purely functional description) please tell me.
Also the performance of the benchmarks over time are tracked on the build bot page: http://bot.rust-lang.org/ (one has to wait a bit initially).
Indeed, the author of the ATS language once said, that the main reason that ATS is so fast on the Alioth benchmarks is that it can allocate most stuff on the stack. Can't find this quote, though.
* Is it really a good idea to use Rust for package metadata? It seems for example Python is moving away from this practice ("setup.py") in favor of a dumb text file ("setup.ini") for meta data that can be parsed without an interpreter. This is also useful for package indexes because it is much easier to safely figure out for example dependencies and lost those in a web interface, for example. But maybe those Rust pragma thingies can be similarly parsed without interpreting any code, and maybe there's a Rust library to parse those easily? Still, harder to parse in other languages compared to a standard text format, although I guess you could write an executable in Rust that spits out JSON or whatever. Or even have a command for that in "rustpkg". * I also read [this post](http://jackkelly.name/blog/archives/2013/01/06/imprisoned_by_the_haskell_toolchain/index.html) recently about Haskell which seems to suggest it is valuable to directly use, or at least play nice with, autotools, and that you maybe shouldn't roll your own build tools for your own languages. I'm not sure I agree with everything in that post, but it might at least have some useful points to consider.
1. I would also prefer to use JSON or YAML for this. It's just that Brian and Graydon would prefer the metadata to be extracted from Rust code and I can see where they're coming from - having a separate metadata file is kind of ridiculous when you can have metadata and the build process described in the one source file. Rust provides `libsyntax` to parse crates and the attribution syntax (which is what you're referring to) but it suffers from the issue of compatibility because it will probably fail to parse older syntax and new syntax, so I'd imagine upgrades could be painful. On the otherhand, using a metadata format would never have compatibility issues (and there would be a field saying what versions are supported). I'll talk to Graydon and see what he thinks. Of course, there's no concept of a central repository so there's no requirement for compatibility - the repository that you're installing from should specific to one Rust version anyway, and might have branches for different versions. There might turn out being both a central repository and installing from URL support, but people seem keen to move away from the central repository. 2. The whole point of the new build system is to get away from the current autotools build configuration for both Servo and the Rust suite which is hell. Haskell's FFI system does not integrate with C or autotools as well as Rust's, so that link is not quite as relevant. I am (personally) not a fan of autotools at all, but there's no reason that it couldn't be used like it is today. As to the link's message of "don't make your own package manager", the system package manager can still install Rust crates to a global directory which will be searched for by the Rust compiler (e.g. `/usr/local/lib/rustc/x86_64-unknown-linux-gnu/lib`). The package manager / build system will still be completely optional, but incredibly useful. At the end of the day, if there's not an official well-made package manager and/or build system in the Rust suite then there'll be a lot of competing ones which is not at all what we want.
There is no need to solve version conflicts because differently versioned (if you mean compatibility wise) packages can coexist. Due to the fact that all packages will be installed (including dependencies) from a bare URL, there's no way to see a history of versions and hence at the URL level there's no concept of a version. You would depend or install packages from a URL (such as a packaged version of the package in a tarball, or a Git repository on a specific version tag) and the name, version and other data of the package would not be discovered until it starts parsing the `package.rs` file. This is how the Go toolset does it and I think it works quite well. Of course, there is quite a big benefit to having a central repository (like NPM does for example) because you can use older versions of packages with ease, which would work a lot better with the purely functional system. I'll have to talk to Graydon because I really think that package sources should stay. Installing and depending on a URL would still be an option, it'd just be less reliable then the repository because there's no way to tell the package manager what version of Rust it uses. I think this is quite important right now because the Rust language is going to continue to be quite unstable syntax wise until at least version 1.0 which is planned at the end of this year and if there's no way to see metadata without parsing I can imagine the package manager being quite annoying usability wise for an entire year (assuming the syntax is frozen post-1.0).
Version could be made part of the URL, I guess.
Not sure yet about the second question, but I can confirm that Rust appends hashes to all of its symbol names. Try these test files: /* choychoy1.rs */ #[link(name="wasabi", vers="0.1")]; pub fn ahoyhoy(x: int) -&gt; int { return x; } /* choychoy2.rs */ #[link(name="wasabi", vers="0.2")]; pub fn ahoyhoy(x: int) -&gt; int { return x; } $ rustc --lib choychoy1.rs # produces libwasabi-68a2c114141ca-0.1.so $ rustc --lib choychoy2.rs # produces libwasabi-68a2c114141ca-0.2.so $ diff &lt;(nm libwasabi-68a2c114141ca-0.1.so) &lt;(nm libwasabi-68a2c114141ca-0.2.so) 4c4 &lt; 00000700 T _ZN7ahoyhoy16_5bff46218c276b03_01E --- &gt; 00000700 T _ZN7ahoyhoy16_5bff46218c276b03_02E # &lt;-- look closely!
Also interesting: the long hash in the symbol itself is dependent upon the library name: #[link(name="horseradish", vers="0.2")]; pub fn ahoyhoy(x: int) -&gt; int { return x; } $ diff &lt;(nm libwasabi-68a2c114141ca-0.2.so) &lt;(nm libhorseradish-68a2c114141ca-0.2.so) 4c4 &lt; 00000700 T _ZN7ahoyhoy16_5bff46218c276b03_02E --- &gt; 00000700 T _ZN7ahoyhoy17_2c25a3631efe514b3_02E Maybe this answers your second question?
Showed Graydon the updated version over IRC: &lt;@graydon&gt; sweet, perfect &lt;@graydon&gt; yeah that's right in line with what I was thinking &lt;@graydon&gt; +/- whether or not tav or others complain about having a name + uuid rather than "just a URI" &lt;@graydon&gt; but I couldn't care less. I can live with either :)
Looks promising, I hope you'll keep us informed... especially if/when you spot short-comings :)
You're spot on. Every symbol is uniquely versioned by the crate metadata (`#[link(...)]`) so multiple instances of the same crate can be used at the same time, in theory at least. There are a couple of basic tests for this in the suite but it has not been used in the real world at all.
Is lower better? These graphs seem extremely noisy, but two of them have clearly shot up at some point.
With its unique types Rust can also guarantee that many or most pointers are not aliased, something that C can not do but Fortran is known for.
Lower is better, yes. If you mouse over the graphs you'll see the compiler commit hash for each data point. But I don't know if all of those graphs are benchmarks, exactly. This whole page is going to be replaced soon-ish with a new built-in benchmarking facility; just like how today you can use `#[test]` to automatically get unit tests, you'll be able to use `#[bench]` to automatically get a simple benchmarking test suite (similar to how Go does it, I hear).
The configuration flags are just to define configuration specs in the build file. It's just so you could make your build file have extra things to build that the user has to explicitly turn on. e.g. `haul build --cfg platform=toaster` so you could do this in the `package.rs` file: #[cfg(platform = "toaster")] #[pkg_build] fn build_toaster() { // build toaster specific things }
There's no way to escape names and UUIDs (or some sort of unique ID like a reverse domain mentioned further down) when designing a purely functional package manager. The hashed directory needs the ID to allow packages with the same name but different origin to coexist and the name is also needed for ease of using (as in `haul use name@version`) and removing. Haul installs things globally (to the user or the system), so there has to be a way to remove packages nicely. tl;dr - tav and others who don't like the concept of just installing from a bare URL with no metadata defined in the source code will have to deal with it because it doesn't play nice.
https://github.com/felix-lang/fbuild Basically, parallel compilation and digest-based memoization of the build process. So it will only compile crates when their digest has changed. You'll notice that the API is similar to it to. Not much is similar beyond that, although seeing as Erick developed fbuild, I can see it looking a lot like it in the future.
Brian already spotted one. Windows won't be able to find the libraries that a compiled Rust executable was linked to if they're stored in a unique directory (`/store/&lt;hash&gt;/lib`) whereas Unix can using RPATH, so I'm going to make libraries get installed to `/lib` and binaries to `/bin`. I'm going to parse binary crates before compiling them to find their UUIDs, name and version and hash the binary name (rather than in a hashed directory) so everything will still be able to coexist nicely.
Possibly, how does Java handle it? I don't think it will matter, as long as Rust has the ability parse it everything will be fine. It won't be used besides being stored in a locally installed packages list.
This might be of interest: http://blog.omega-prime.co.uk/?p=138
Unless I'm missing some automatic way to do this, editing the executable directly is probably not very practical. And I don't think it works the same as rpath, because rpath does not necessarily need an absolute path to a library to work.
In the second section ("In practice") he shows how to do it without editing the executable, using an import library instead (and editing that). Presumably doing this automatically would be the compiler's job (or some other part of the toolchain). You seem to have a point that unlike RPATH, this doesn't fall back to the usual locations if it's not found at the "RPATH" (though I didn't try it). I'm not implying this is the right/best way, just wanted to flag it as a possibility.
I don't like that. Then you have to have a domain name, or use something like github, and then it's awkward to move the code. Just make a command to generate an initial `package.rs` that does the UUID for you.
I'm not really a fan of the reverse domain names either, but AFAIK it's basically just a glorified, human-readable UUID, it doesn't actually map to anything in DNS. You can give your package the ID "com.google.foo" and it will work just fine, and there's nothing that can stop you.
Then maybe not call them domain names... Did I mention OCD? :p
Not yet. LLVM has the necessary annotations to tell it about aliasing but Rust codegen is not using them.
Looks very useful; thanks for putting it together. A few comments: 1. "Pointers and references": maybe explain what (immutable) means, since as it says there's no C++ equivalent. 2. Same section: does an owned pointer convey the same optimization potential as C99's restrict? 3. "Strings"/"Vectors": are the contents immutable by default too? 4. "Exceptions": examples would be great. What does "can be dealt with at task boundaries" actually look like? 5. "Nominal types": no mention of (the lack of) inheritance or of traits, which are both kind of important. The swathes of copy'n'pasted impl code in e.g. libstd/json are really unsightly; is there any mitigation on the horizon?
C99 did add the [restrict keyword](http://en.wikipedia.org/wiki/Restrict) for that, but I guess in practice Fortran's not aliasing by default makes a big difference for most code.
I wrote a little test last night, spawning 1 million tasks with stateless closures (lambda which captures no state). On average the memory usage was around 1.2-1.6GB, that's about 1.6 kilobytes per task. 
Interesting, was it just something like `for 1_000_000.times { do spawn {` or was it more involved? I'm also curious how that size-per-task compares to Erlang and Go.
Yep pretty much, very simple test just see how far I can push the concurrency run-time, no channels, no state. I have some ideas in my head I wanna test out soon. 
Interesting, I wonder how the same test would fare in C++. Do you think you could throw the code up in a Gist or Pastie for the purposes of comparison?
I was kinda hoping for more than just three paragraphs out of a piece that was actually about something else...
I'd also love to hear a response to point 5. No disprespect intended to the libstd/json author's intended, but I had the same visceral reaction when I looked at the code. The DRY principle has become such second nature to me that this kind of code just kind of jumps out at me as something begging to be refactored. In a more familiar OO language I'd probably reach for inheritance and polymorphism to cut down on the repetition, but in Rust I couldn't think of a way to approach it (I'm a complete Rust newbie, however.) Would any rust developer with a bit more experience care to comment as to whether libstd/json in its current form is a good example of idiosyncratic Rust? Could some of that duplication be avoided even with Rust in its current state or would it require some language changes? 
I’m not sure I see the point, as needing a finally block often means having a resource that you want to free, which you might as well wrap in a struct that has a destructor.
I have a daily cron job that auto updates the plugin if there are any changes.
&gt; If we gave it the name d Rust would emit an unused variable warning, but prefixing the name with _ turns that warning off. Sounds annoying. In C++ both clang and gcc turns that warning off for types with a custom destructor, I think copying this in Rust (ie disabling on types with a finalizer) might help alleviate the use of `_` (and consequently the risks of just using `_` and running the finalizer too soon). On the other hand, I'd like to point out that the idea of sending closures to a function for "safe" work is definitely sounder than those pesky `try/catch`, and allows much more variety; so it's worth investing in making painless syntax-wise. For example, one can implement `if/else` by taking two closures and only executing one based on the result of the test.
This is not necessarily the truth, as the example with the failing tasks shows. For sure, if you don't want to leak resources they are better wrapped in RAII types; however some times you just want arbitrary actions to be executed on unwind. Of course, one could wish for a simpler syntax (`defer` statement); but that a similar result is relatively easily achieved *as library code* bodes well for Rust's future.
For the sort of low-level stuff for which you'd use C, you'd probably be striving to avoid Rust's garbage collection entirely. I believe the point he's trying to make here is to contrast Rust's approach (default-off, fine-grained per-item GC) with Go's (mandatory GC) and D's (default-on, switch-off/switch-on GC). Because Rust always presumes that you want to avoid the GC entirely, it makes working without one very safe and relatively easy. Regions and unique pointers go a long way towards alternative means of memory management without forcing the programmer to manually allocate and free memory.
I'm having a really hard time grasping managed boxes. Are there resources for wrapping my head around them besides the tutorial? (And not the manual, that's even more complicated)
I'm with you. This blog post has helped me most so far: http://tomlee.co/2012/12/managed-and-owned-boxes-in-the-rust-programming-language/ Even after reading this, though, I only think I'm about 70% of the way to fully understanding them.
Do you know about automatic reference counting, like std::shared_ptr in C++? This is basically the same thing. The object lives on the heap, can have any number of references pointing to it, and is kept alive as long as there are any references to it, and is destroyed/deallocated when there are none left. The differences are that the under-the-hood implementation may or may not be different, mutable boxes have special treatment (as in the blog post) but this is independent of the memory-management part, and unlike C++ they are accessible only from the same task.
What programming languages are you familiar with, and with what in particular are you having difficulty?
Either pcwalton or brson would be the best to ask about this, but they weren't on IRC this morning so I asked nmatsakis instead: &lt; kibwen&gt; nmatsakis: do you know what the embedding story for servo will be? I hear that gecko isn't really embeddable at all nowadays, wondering if webkit-like embeddability is a goal for servo &lt;@nmatsakis&gt; I believe it is a goal 
Thanks for the info. While D hasn't implemented (or finished designing) `scope`, a memory-safe subset is planned. You can do `RefCounted!T` and `Unique!T` for manual memory management without GC. How does Rust distinguish between refcounting and garbage collection? Does Rust's standard library use GC?
Do I have to be afraid to accidentally write code with mutable managed pointers that looks reasonable if I'm coming from Java-style references or similar that is reasonably safe but will fail those runtime checks?
It depends on what you're trying to do. The example he gives at the end: for self.monsters.each |monster| { // pointer created here self.total_gold += monster.gold; } is afaict no different from Java's ConcurrentModificationException (except that I'm not sure if ConcurrentModificationException is guaranteed or just a heuristic... in Rust the failure here is guaranteed). I guess it's not a perfect comparison because you can't ever concurrently modify managed things in Rust (no shared mutable state), but w/e. The implicit assumption with all of these changes is that normal users *won't* be hitting this error. As in, the semantics now reflect how users expect to be able to use managed pointers; whether or not this is true in practice remains to be seen.
What are the parts that confused you? I'm very interested in editing the tutorial to make it clearer.
&gt;There is one gotcha here, however. As implemented, if any pointer exists to any part of an @mut box, then the entire box cannot be mutated while that pointer exists. Isn't this a pretty severe restriction in practice? If I understand correctly, this means a reference counted object can never be modified through a mutable reference? It seems likely that if an object requires reference counting/garbage collection it is because there are multiple owners; so when would a @mut box be useful? 
There are three cases for ability to modify or borrow a mutable managed value: 1. *No part of the value is borrowed:* The value can be modified or borrowed via any copy of the @mut pointer to it. 2. *Any part of the value is mutably borrowed* The value can only be modified or borrowed via the `&amp;amp;mut` pointer to it. This is dynamically enforced. 3. *Any part of the value is immutably borrowed* The value cannot be modified. It can be immutably borrowed via any pointer to it, but cannot be mutably borrowed. This is dynamically enforced.
Java, Go, C, C++, Visual {Basic,C++}, Haskell. I'm just having a hard time understanding when you use different ones. Part of this is due to the very limited amount of full-scale examples. 
Maybe I mis-understand "if any pointer exists.." Does this mean any any pointer -- or just any borrowed pointer (&amp;)? In your case (1), if there are multiple @mut managed pointers to the same object but no borrowed pointers to the value -- is that valid? Can the object be mutated via any of the @mut pointers?
So if I want multiple pointers I have to use managed boxes (which means requiring the runtime and GC, right?) edit: By the way, thank you. Your blog and this post have been very helpful!
To a first approximation, yes. However, you can temporarily borrow owned boxes and take as many pointers into them as you would like, as long as the compiler knows those pointers don't outlive the owned box. Roughly speaking, this means you can pass those pointers to other functions and store them in data structures as much as you like, as long as those pointers are all dead by the end of the lifetime of the owned box. For example: fn f() { { let x = ~3; let y = &amp;*x; // pointer 1 let z = &amp;*x; // pointer 2 ... use y and z ... } // &lt;-- x is freed here, at the end of the block // if there are any uses of y or z down here, it will be a compile-time error } You could also build reference-counted smart pointers if you prefer reference counting to tracing GC.
Sorry, these are really noob questions, but in the case that you take a borrowed pointer to an owned box, they're immutable, correct?
I believe it means any borrowed pointer. As I understand it, objects are "frozen" for the lifetime of a borrowed pointer. So if you were to have several @T pointers to the same object, and you were to call a closure that borrows one of them, the value cannot be mutated by *any* other managed pointer to that object for the lifetime of the reference. This is mostly evident in closures: let a = @mut [1, 2, 3, 4, 5]; let b = a; let c = @mut []; a += 6; // No problem, modifying the original b += 7; // Modifying the same value through b, totally valid for each(a) |e| { // Here e has type &amp;int, a has been borrowed // by each c += (e + e) // Okay, no problem here, c hasn't been borrowed // and e is only being read a += (e * e) // Nope! a is "frozen" for the duration of the borrow b += (e * 2) // Also nope! b is also frozen as a result of the borrow } I think that's how this works (someone please correct me if I'm wrong). Also I constantly get mut T[blahblah] and T[mut blahblah] mixed up lately, so I might have it the wrong way around. **edit** Turns out I was wrong on both counts. Updated with correct syntax for mut.
Also, I don't have anything against the tracing GC (.5 is still ref counting though, I thought) but my rough understanding from a Github issue thread was that using managed boxes -&gt; GC -&gt; prevented embedding in C apps.
Sorry about any typos I might have had. I thought I'd fixed them right away, but submitting has been slow on my phone and I accidentally submitted early. I think what I meant is clearer now. And yes. You can have as many @mut pointers to a value as you want and you can mutate the value via any of them, as long as the value hasn't been borrowed, in which case *none* of the @mut pointers can be used to modify the value.
I don't think it's true that GC would be an insurmountable hurdle for C embedding. You would probably have to start a Rust scheduler though (as allocating GC data requires a task). The reason for this is that we need a per-task place to store the list of allocations so the GC knows what to sweep.
You want `let a = @mut [1, 2, 3, 4, 5];` Other than that this is totally correct. I'm really relieved to see that the borrow checker seems to have gotten more understandable :)
Hah go figure neither of the two was right! I just keep getting the order of mut jumbled. I blame language hopping too much as I eagerly await Rust's next release :P And yes, the proposal for the new borrowing semantics really helped straighten things out in my mind. I especially like the idea that passing an &amp;T to a function that takes an &amp;T will result in an automatic "re-borrow". It's definitely an intuitive idea, I'd gotten bitten by it before and it took me a while to figure it out.
Maybe this helps? https://github.com/mozilla/rust/wiki/Rust-for-CXX-programmers
That seem pretty different to me : In Java, ConcurrentModificationException only happens when you modify a collection while iterating over it. Here the modified value is not the array but a field of the struct containing the array. And ConcurrentModificationException is not a language feature in Java but an exception rose by some standard library objects. It is possible to write a Collection that don't rise this exception.
I'm sorry to abuse reddit like this, can you tell me why this seems to move instead of copy to the parameter in the function call? struct TestStruct { mut data: int, } fn test1(x: TestStruct) { x.data = 1i; } fn main() { let w = TestStruct{data: 10i}; test1(w); io::println(int::str(w.data)); // ERROR, it seems calling test1 moved **w**, rather than copied it I expected? } (I would expect it to print 10.)
+1 To Option 8 &gt; &amp;{self} val It seems least difficult to mentally parse. when I see &amp;self/val I think divison for some reason.
That's fair, it's been years since I've used Java. :) It may be possible that this restriction could be lifted in the future, but it would require more runtime machinery. Right now there's only a single bit in each struct to represent whether a borrowed pointer exists to any part of that struct, and afaict adding a similar bit for each field in the struct would require either arbitrary space in the struct header or a single word in the header and a pointer indirection every time you wanted to mutate the struct. More expensive, but if the current design doesn't pan out perhaps it would be worth it.
Here's why: `w` is allocated on the stack. Just like `~` pointers, stack-allocated values are considered *owned*. All this means is that the compiler can track their usage precisely, and know who "owns" them at any given time. When you pass an owned value, such as when you pass `w` to `test1`, the values aren't copied--they're *moved* (as you probably inferred from the error message). If you want to force a copy, you have to be explicit about it: struct TestStruct { mut data: int, } fn test1(x: TestStruct) { x.data = 1; } fn main() { let w = TestStruct{data: 10}; test1(copy w); // &lt;-- look here! log(error, w.data); // output: 10 } This gives you the output you expect. In the future the `copy` keyword will be going away, to be replaced with a `.copy()` method that will be available on all copyable objects (a copyable object == an object without a destructor). So like I said, owned values are moved by default. So the stack-allocated code above has the same behavior as the owned-pointer code below: struct TestStruct { mut data: int, } fn test1(x: ~TestStruct) { x.data = 1; } fn main() { let w = ~TestStruct{data: 10}; test1(copy w); log(error, w.data); // output: 10 } On the other hand, managed pointers aren't owned, so they don't get moved by default, and so the `copy` in this next example is useless: struct TestStruct { mut data: int, } fn test1(x: @TestStruct) { x.data = 1; } fn main() { let w = @TestStruct{data: 10}; test1(copy w); // we're just passing a copy of a pointer to the same data log(error, w.data); // output: 1 }
Yes, it is similar in spirit but is less precise than Java's ConcurrentModificationException. (Of course, if you represent the array as `@mut ~[int]` then it is closer.)
New type bounds: `&lt;T:Foo+Bar&gt;` ! The thing about destructors is slightly annoying though: limiting to owned types means that it is no longer possible to create "revert" operations. In C++ I can do: auto var ... ; auto reverter = set(var, newval); reverter.commit(); // if an exception is thrown var is automatically reset to its previous value.
Cool!
If I understand correctly, you'll be able to replicate the current destructor behavior by using `unsafe` blocks. Which, considering the current rules, isn't any more unsafe than you are today.
Indeed it is! I think it would be awesome if this changed so that you could annotate the unsafe block with exactly the mechanism you wanted to turn off. Would be a lot more self-documenting as well. I don't know that anyone's ever actually proposed this, but I can't see any reason that the devs would object to it.
It was mentioned in the thread that they take the cognitive budget seriously wrt. adding and removing features. Is there a list of features that have been removed or rejected anywhere, ideally along with the reasons? If there isn't one, it might be nice to keep track of it on the wiki. For one thing it's an easy reference for people wondering "why doesn't Rust have X" or thinking of proposing a feature, so they can check whether it's been considered already. It could also provide some useful insight into the thinking of the developers, what kind of things they look favorably/disfavorably on and for what reasons, etc.
I feel obligated to point out (as is done later on in the thread) that much the code there is actually working around bugs in the current implementation. Here's a more representative example from Niko of what the code would look like once the bugs are fixed: pure fn each(&amp;self, f: fn(&amp;(&amp;self/K,&amp;self/V)) -&gt; bool) { match *self { Leaf =&gt; (), Tree(_, ref left, ref key, ref maybe_value, ref right) =&gt; { left.each(f); match *maybe_value { Some(ref value) =&gt; { f(&amp;(key, value)); } None =&gt; {} } right.each(f); } } } The region syntax is still bad, but it should be changing shortly.
There's a syntax proposed later in the thread that would turn: &amp;rb/A/c/d/e into &amp;'rb A&lt;'c, 'd, 'e&gt; with the new rule that a leading apostrophe unambiguously denotes a lifetime. I'm not ecstatic about it, but it's definitely better than the current syntax.
No references for the second point, it's usually mentioned in passing when people are like "hm we could do this if we had higher-kinded types", or when people ask if Rust has monads and they're like "nope, we'd need higher-kinded types for that". Maybe try a search on the mailing list via Gmane: http://thread.gmane.org/gmane.comp.lang.rust.devel/ Some of the devs do visit Reddit from time to time, but I hardly expect them to religiously read every thread like I do. :)
Can you give an example of what you're trying to do that `fn foo&lt;T: TraitA TraitB&gt;(param: T)`doesn't solve?
You're probably looking for trait composition. https://github.com/mozilla/rust/issues/2616 I have no idea if this works or not in current Rust implementation, but I'm aware of how trait system should work and this is the solution.
So far option 8 or **&amp;{self}** (IIRC) was most soothing for me. The **'** syntax is a bit easy to miss. Did they consider squiggly lines **'~'**? Like **&amp;rb~A~c~d**? Or are squiggly lines off limits? What information does lifetime syntax tells?
They're actually needed because garbage collection is fully optional (only used for @) and Rust still has to guarantee that there are never dangling references/pointers to provide safety. In a fully garbage collected language, all objects just stay around until they aren't needed.
Borrowed pointers are essentially the same as C pointers or C++ references - there's no boxing, overhead or runtime checking. The lifetime annotation is needed to prove to the compiler that the region they point into will outlive them.
Option 8 *is* the best syntax, yes. But sadly this syntax is ambiguous; it doesn't work without either creating a whitespace dependency or other weird hacks.
&gt; Structural records (in favor of nominal records (structs), but I'm not sure of the exact reason) I'm actually really sad about that change. I'm starting to learn ML on [coursera](https://www.coursera.org/course/proglang) and am really struck by the beauty of how records are built into the language and are used as the basis of tuples and function arguments (in ML, functions only take one argument, a tuple, and tuples are actually sugar for records).
Hm, maybe dynamic dispatch problem? In this case, type T must be known at compile time. But maybe I need to use trait inheritance in this case.
Wait until you get to currying.
Have you checked out trait objects? http://static.rust-lang.org/doc/tutorial.html#trait-objects-and-dynamic-method-dispatch It would help to have a code example (even a broken one) showing what you're trying to do.
Regardless of the sigil chosen, I have to admit to finding lifetime modifiers almost impossible to grok. I suspect a lot of C++ people will. &amp; and @ and ~ are fine; they're not obvious to a reader coming in cold, but once you map them to references and shared_ptr/GC and unique_ptr/move constructors you're fine. Lifetimes, not so much; I get the concept in the abstract, but I can't model the meaning of the code in my head - it's crossed the line into syntax soup. Maybe the tutorial will help once it covers them, I dunno. Here's hoping.
This is my very broken example - https://gist.github.com/4637894 It's based on tutorial.
I could be wrong, but I believe the proposed apostrophe syntax would look like this: struct StringReader&lt;'self&gt; { value: &amp;'self str, count: uint } Any better? Worse?
nmatsakis (the designer and implementer of borrowed pointers) wrote a borrowed pointer tutorial a while ago. I'd link it here right now but it's a bit dated by this point, so I've asked him to update it. Once he does I'll be sure to submit it here in /r/rust (and send you a pm to remind you, if you'd like).
Yeah, I'm disappointed too.
Thanks, that'd be great. I'm subscribed to Niko's blog, so I'd imagine it'll show up there.
see my post in the parent: http://www.reddit.com/r/rust/comments/178t57/intimidation_factor_vs_target_audience/c83p6v3
As I mentioned on IRC, it's because (a) we don't have coinductive/recursive type unification (so people kept having to rewrite their code when they put a record inside a record), and because (b) they don't work very well with typeclasses. To expand on the typeclass problem, if crate A defines `type Foo = {a: int, b: int}; impl Foo : Eq { ... }` and crate B defines `type Bar = {a: int, b: int}; impl Bar : Eq { ... }`, then crate C links against A and B, which version of Eq do you get? ML doesn't have this problem because it doesn't have typeclasses (except for Eq). Haskell has nominal records (I believe) for this reason.
Most of the features that have been removed were removed because other features overlap with them. There's a philosophical argument about whether it's simpler to implement features A, B, and C directly or whether it's simpler to implement feature D that allows the programmer to implement A, B, and C themselves. Usually we err on the side of implementing features that give a lot of "bang for the buck" and removing specialized features, but reasonable people can differ as to the right approach.
I read lifetimes as "lives as long as" (where "lives" means "isn't deleted"). So, for example: struct MyStruct { x: int } fn f(x: &amp;&lt;'a&gt;MyStruct) -&gt; &amp;&lt;'a&gt;int { &amp;x.x } The return value *lives as long as* the parameter `x`, because we notated them with the same lifetime. Similarly: struct MyOtherStruct&lt;'self&gt; { x: &amp;&lt;'self&gt;str } This structure contains a pointer to a string that *lives as long as* the structure itself. Does this make more sense?
It just takes one more line, and you won't need to implement the trait for each type: impl&lt;T: Foo&gt; T : Bar {} Now all types that implement Foo implement Bar as well.
In a sense C language runs anywhere where decent C compilers exist as well. There are going to be a lot of changes for it to run on different architectures, but same can be said of any language (including Java, where is some amount of effort, but much lesser than in C) that aims for cross platform compatibility. Only thing that differs is amount of effort to achieve it. I do hope rust is mostly cross platform compatible (more than C but probably not as much as Java).
So how much of this applies to Rust? This blog isn't connected directly to rust, iirc.
&gt; There are going to be a lot of changes for it to run on different architectures, but same can be said of any language (including Java, where is some amount of effort, but much lesser than in C) that aims for cross platform compatibility. Yeah... the number of changes required for unmanaged languages like C which use system libraries simply dwarfs the changes needed for a JVM/.NET program. You can somewhat mitigate this by using cross-platform libraries to abstract details away, but it's still a much bigger issue in languages like C. Furthermore, C binaries are **not** cross-platform. Period. You seem to have completely missed what "run anywhere binaries" means - it means you compile your code to a binary, and then can just distribute that binary to users. You simply can't do that with languages that compile down to native code.
Right. They don't refer to a preexisting identifier. (Early proposals for lifetimes did, but nobody liked them much.) `self` isn't actually a preexisting identifier; it's a magic lifetime variable that's always defined for every struct/enum. This is probably confusing, and I believe Niko's proposals change that. I suspect that they will be rare. You typically only need them in one of two cases: (a) when returning a region pointer; (b) when defining a struct or enum that itself contains region pointers. In general we've been hesitant to add keywords, however.
Dependent types should refer to types that depend on a value, like List&lt;String&gt;&lt;2&gt; might indicate a list of 2 strings. It would be *so* weird to have those without having higher-kinded types first.
Doesn't this resemble the well known diamond problem that C++ (or any other language with multiple inheritance) suffer from? wasn't a viable option to try to resolve this at link time, and tell the dev. "you gotta specify if you mean crate A or B Eq({a:int, b:int})" ?
As far as I understand it, at the moment this has a (probably unintended) side-effect: it actually means there can be no other implementations of Bar, any attempt to impl anything else will give a "conflicting implementations" error (i.e. impl int : Bar {}, with or without impl int : Foo {}). This is because rustc doesn't look at the type constraits when checking trait/impl-related things, which is required to uphold the "open world assumption" (that's what it is called in Haskell-world, not sure if that's the rust terminology), and it exists for good reasons... although I don't quite understand it enough to be able to explain. (This might be the behaviour you intend, and in fact is actually slightly useful in other ways too: it allows one to add methods to a trait without requiring them to be implemented by hand (i.e. when they can be derived from methods already implied by the "super"-trait), although this use-case will probably/hopefully disappear as default methods get more stable.)
Yes, but we were talking about the case where the trait just represents the intersection of other traits. It has no methods of its own, so there is only one possible implementation.
Description is right, example is not quite. You can do List&lt;String, 2&gt; in C++ if you want to. That's just a compile-time fixed length list. Nothing dependent about it. Where it gets dependent is with things like: fn makeList(n: Nat) -&gt; List&lt;String, n&gt;; i.e. the compile-time type of the list depends on the runtime value passed into `makeList`. And if that blows your mind (it does mine), read up. (This might be what you meant as well, it wasn't clear.)
Indeed, that was part of the plan. I was looking for reference to start my game off, but couldn't find any. It's funny to now find a post about someone else creating a game. Expect it to be rudimentary: I would really like some feedback on it, once it's somewhat of a game. I'll post it on /r/rust once it is done.
It was not obvious to me either, but once someone pointed out the parallel to `Foo: Bar` bounds in type parameter lists, I don't think I'd forget again. :)
It seems like there's always more on the agenda than you get around to discussing... :)
The parallel with type parameters seem even more confusing to me: * In *MyType&lt;P:T&gt;* the expression "P:T" means "P implementing the T trait". * In *impl P:T* the expression "P:T" means "T for the P type" In the first case P is the subject. In the second case T is the subject. When you write **impl P:T** you really have the felling you are adding things directly to P, but you aren't. [This pcwalton's blog post](http://pcwalton.github.com/blog/2012/12/30/the-two-meanings-of-impl/) explain it clearly
I'm a bit nervous of about the lifetime decisions. First off, having explicit lifetime parameters on functions just doesn't work. If I have: fn foo&lt;T&gt;(x: T) -&gt; T { x } And I want a complete function value, I write: foo::&lt;T&gt; However, if I have: fn bar&lt;T, 'lt&gt;(x: &amp;&lt;'lt&gt; T) -&gt; &amp;&lt;'lt&gt; T { x } Then I write: bar::&lt;T, ???&gt; What do I put in the `???`? There's no lifetime to put there. You can't even infer it since you don't actually know the lifetime until the call site. The lifetime simply isn't a property of the function value, thus it should not be a parameter on the function name. Not to mention that there's no equivalent way to introduce the lifetime names in literal function types. I also don't really like the angle brackets in `&amp;&lt;'lt&gt; T`. They aren't consistent. Consistent would be `&amp;&lt;'lt, T&gt;` or `(&amp;T)&lt;'lt&gt;`, but these aren't desirable. Thus, evaluating the angle brackets on their own merit, they don't serve any syntactic purpose (they don't delimit a list) and they're noisy. The only possible benefit is if you don't think `&amp;'lt T` is visible enough, which I personally don't agree with but is obviously going to be subjective. My personal preference is to always need to include the parameters on a lifetime parametrized type. This isn't incompatible with being able to elide the lifetime of an `&amp;` since you don't need to infer the parameters to be the same as the lifetime of the value. Anytime you reuse a lifetime parameter, you're imposing a constraint, and I think all constraints should be explicit. I'd be in favour of being able to say "don't care" using `'_`. It just seems wrong to me to be able to elide the parameters on a type in this one specific case.
I still don't like these Perlisms that seem to infest Rust's type system. That alone would make me choose OCaml over Rust.
As bachmeier implies, you can do this with macros: fn main() { macro_rules! borrowed(($thing:expr) =&gt; (&amp;$thing)); macro_rules! owned(($thing:expr) =&gt; (~$thing)); macro_rules! managed(($thing:expr) =&gt; (@$thing)); let b = borrowed!(1); let o = owned!(1); let m = managed!(1); io::println(fmt!("Borrowed thing: %?", b)); // Borrowed thing: &amp;1 io::println(fmt!("Owned thing: %?", o)); // Owned thing: ~1 io::println(fmt!("Managed thing: %?", m)); // Managed thing: @1 } ...though I'm not sure how robust this particular implementation is.
Don't even need macros: type Managed&lt;T&gt; = @T; type ManagedVec&lt;T&gt; = @[T]; type ManagedStr = @str;
Nice! Although I would rather expect that be part of the language. On my line of work I don't need to squeeze every ms out of the hardware, so I am usually fine with GC. Rust would be a nice addition for the remaining use cases, but it brings back Perl memories (last used in 2005). Note this is nothing against Rust's design per se, I also tend to use Python over Ruby for the same reason.
They are used often enough that it makes sense to have a sigil there. They are quite clear and with obvious purpose, nothing like Perl.
Good post. One comment/confusion (I'd post it there, but after ten minutes of trying I can't get Tim's host to accept any Google URL as an OpenID provider) about the discussion of the first, @-pointer version: &gt; The first version says [snip] these pointers might exist in different tasks". I thought managed boxes couldn't cross task boundaries? Am I missing something important? The "time now versus time later" story is a strong one. It's even stronger in the context of multithreading, since unit testing (the traditional dynamic comeback in the eternal static-versus-dynamic debate) instils a lot less confidence there.
Yes, you're right about managed boxes. Don't know what I was thinking when I wrote that, but I edited the post to clarify. Sorry about OpenID not working -- someone else commenting on my blog had problems with that a month ago or so. I filed a Dreamwidth support request about it, but apparently it hasn't been fixed (the support person at the time said it's an intermittent bug).
Are there any other explanations on the lifetime concept? I think I understand that it means the borrowed pointers can't outlive the ~ pointers lifetime, but how does rust enforce this? The code without copying and &amp;L/IntList doesn't really make this clear to me. I think the L is arbitrary character and links the argument 'l' to the returning int by the "L" lifetime designation, is this correct? | "The code for head_of_tail shows this claim is true." What would be an example that it is not true and this could not be used?
&gt; Are there any other explanations on the lifetime concept? I added links at the end of my post to Niko's blog ( http://smallcultfollowing.com/babysteps/blog/categories/rust/ ) and the Rust tutorial ( http://static.rust-lang.org/doc/tutorial-borrowed-ptr.html ), which have more details. &gt; I think the L is arbitrary character and links the argument 'l' to the returning int by the "L" lifetime designation, is this correct? That's right. &gt; What would be an example that it is not true and this could not be used? Great question! For example: fn bogus() -&gt; &amp;L/int { /* any code *except* fail */ } This function would fail the borrow checker because -- in the terms I used in my post -- it is *not* true that with no assumptions, you can produce a pointer to an int with an arbitrary lifetime L. In general, to make a pointer with a lifetime L, you need to be given a pointer or pointers with that lifetime (as in my example in the post, often this means returning a field of a record that you're given). The one way you *could* conjure up an &amp;L/int "out of nowhere" is if the body of the bogus function is just a call to fail, or a call to a function whose result is declared as ! (basically meaning it always fails). Then, the claim is true because *if* you ever return to the caller, you return an &amp;L/int -- but you never return. By the way, there's an analogous concept when it comes to polymorphic (generic) types -- that basically says that what a polymorphic function does with an argument whose type is a type variable A is highly constrained -- called "parametricity".
I purposefully chose a simple example, though :-) It can get more complicated than that, but in my post I hoped to give a broad sense for why you'd care in the first place.
Here's the gist of how it works: Lifetimes refer to *code blocks* where a value is live, and you can't have a pointer to data leak outside of its code block. The Rust compiler associates a lifetime with each `&amp;` pointer to make this work. Here's a simple example of a lifetime error: fn main() { let dangling; { // start of lifetime let x = ~"hello"; dangling = &amp;x; } // &lt;-- end of lifetime; "x" destroyed here io::println(*dangling); } You'll get a "value does not live long enough" error at compile time if you try to write this, which is exactly what the problem is: `x` does not live long enough to survive all the way to the `io::println` call.
It would be interesting to consider the combination of GC and regions. I.e. make the GC aware of regions, but once a region goes out of scope eagerly reclaim the space associated with the whole region. For long-lived regions the GC would reclaim space occasionally. There's been some research in this area, but I can't find the papers right now.
The syntax for lifetimes is going to be changed: http://osdir.com/ml/rust-mozilla-developemnt/2013-01/msg00138.html
New screenshot: http://i.imgur.com/7i691Vy.png Old Screenshots: * http://i.imgur.com/OqxOo6q.png * http://i.imgur.com/nSRf5uS.png
Nice! :)
Wow! That's amazing :D
Yeah, there's a lot of stuff not implemented yet. Attributes aren't implemented, sprite palettes are unimplemented, sprite 0 hit is implemented wrong, scrolling is wrong, nametable mirroring is unimplemented. I didn't mean for this to get publicity so early (that's the reason it has no README), although I suppose it's my fault for tweeting about it :)
I'm wondering if in Rust explicit concurrency is as encouraged as in Go. I guess, the SDL part (especially graphics) could run in separate task. I'd be glad to see how Rust channel primitiveness are used. 
I've thought about how to incorporate concurrency into this design; Rust does encourage its use. I'm worried about the performance issues with running the CPU and PPU in separate tasks though. They have to communicate a *lot* in the NES. I worry that the huge amount of locking you'd have to do would negate all parallelism gains.
Unfortunately the CPU can actually query the current state of rendering in the NES. There's a thing called "sprite 0 hit" whereby the PPU exposes details of its rendering to the CPU, and almost all games rely on this. The CPU and PPU therefore have to run in near-exact synchronization, synchronizing after every CPU instruction. If you ran them in different threads I don't see any way you could get away from taking a mutex lock every instruction (or exchanging messages, which would be even slower). :(
Sorry, did not mean to embarrass you. I just think this is pretty cool.
This is very interesting. I understand that one of the nice-to-haves that would be made possible/easier from this line of work would be a runtimeless rust. Combined with the fact rust is designed to be binary compatible with C I assume this would make it possible to extend scripting language's implemented in C, like Python or Ruby, using rust rather than C or C++. This would be very attractive to me at least, I don't know how wide the interest would be. My question is, is this kind of use case a part of the teams vision for rust? Or is it more of a "Hey this would be cool, but we're not really working towards it". Edit: I suppose this same improvement would also be interesting to people writing libraries that are loaded with a simple dlopen for whatever language, so I'm going to guess this probably is on the roadmap.
&gt; I assume this would make it possible to extend scripting language's implemented in C, like Python or Ruby, using rust rather than C or C++ Indeed, afaict any language that exposes a C-compatible interface would be capable of calling into Rust code. &gt; My question is, is this kind of use case a part of the teams vision for rust? Or is it more of a "Hey this would be cool, but we're not really working towards it". It's part of the vision, definitely, but there's no guarantee that it'll make it into the language before 1.0. Having the runtime be written in Rust is the only reason the devs themselves need this feature, so it's less of a priority than others. Although I agree that it would be a huge game-changer to be able to write libraries in Rust that had the universal compatibility that C libraries enjoy.
i'd be very interested in this. one of the things i wanted to do in some language other than C is develop a good, fast library for wordgame programming that scripting languages can bind to. i was eyeing ats, but that has a very steep learning curve; rust looks a lot more promising.
This is great, kudos!
Well, you have to make sure the PPU doesn't run too far ahead of the CPU. How far is OK is something we'd have to experimentally determine. Definitely the final blitting can be done in another thread though. That's a great idea and could result in a nice speedup.
Can you talk about your experiences with rust-sdl and bindgen?
Yes, if you know SDL, rust-sdl should be easy to use. It doesn't have all bindings implemented yet, but it's easy to start once you know how the event-loop should be implemented using 'match'. You can find an example of that in the game: https://github.com/FrozenCow/rust-airhockey/blob/master/main.rs#L287 Mouse-events and doublebuffering weren't yet implemented when I started rust-airhockey, but it was not too much work to add them when you follow the keyboard-events as an example. The fix got pulled quite fast and I could use the fix through cargo within a day. As for bindgen, it does a very good job, though it requires some fiddling. I created a file 'opengl.h' which I used to extract bindings: typedef long ptrdiff_t; #include &lt;SDL_opengl.h&gt; Then I called bindgen: ../rust-bindgen/bindgen -I/usr/include/linux opengl.h -match gl.h &gt; gl.rs I had to include /usr/include/linux for stddef.h. After that I had a file with all functions in there, but the #define's that gl.h had weren't included as constants. So I added those by hand (copy from gl.h and search-replace with regexp to get from #define to rust-syntax).
Thanks for the help on IRC, awesome that it is indeed the first game in Rust! Hopefully more games will come.
Again, thanks for the help on IRC!
Overall great little meetup, lots of interesting detail, but one thing sticks out to me - Why focus on backwards compatibility and syntax now? Shouldn't you worry about that after 1.0 release?
Popular demand, and trying to ease our burden. I've seen a lot of complaints that Rust changes too often to be useful, and I think we're approaching the time when we need to take these issues seriously. Also, there are over 100,000 lines of code written in Rust now, and keeping them up to date with basic language changes is becoming more and more of a burden.
Dunno if it might've been intentional, but the name can also be read as a contraction of Bot for Rust. That's how I interpreted it at first, not knowing about the science fiction reference. It also means pepper in Hungarian. (The kind that's usually next to salt, not the kind that's usually red, yellow, or green.)
At first i thought it was some sort of pun on "brson" (https://github.com/brson)
I have been using this kind of *Continuous Integration* in the past, and falling back to *Continuous Build* was oh so painful: - Continuous Build tells everyone: "Your tree is broken, go and fix it!" - Continuous Integration tells to the submitter: "Your patch does not integrate cleanly, go and fix it!" Well, the latter works so much better: - No tragedy of the commons: the "Cannot be my PR" syndrom - No trashed tree, ever I guess I don't have to stress the importance of a *clean* tip ? The only question, then, is that of scalability. Continuous Build is scalable in the sense that it will just integrate bigger chunks of code at once; however building and running tests for each and every PR may not scale (as the number of PRs grow). Of course, I guess that it would great for Rust to run into this problem :D
Actually, Graydon Hoare himself would like to see the grammar formalized, if only to check that there is parse ambiguity (that has been missed until now). I believe one of the member of the Rust core team has kinda "volunteered" to do it in his spare time (at least that is what I got from one of the latest status meeting).
Thanks, I didn't know that. Looking forward to a progress update! :)
The problem with those videos is that on sluggish connections (such as mine), it's quite difficult to get them. I wish I could just get the slides (bonus: with transcript).
I believe jclements is going to be working on this.
If you right click on the video element in chrome (or Firefox?) then you can download the talk as a video file. That worked for me.
Had not even thought of that, too used to have Flash players. Thanks for the trick.
This was mentioned on #rust today. Was pretty amazed. Edit: I Hope this doesn't lead to English discussions spamming their channel – I thought it'd be nice to share just in case we have any Korean users on Reddit.
You will be more amazed if you learn that there is a dedicated channel about programming language development (and more occasionally, general programming) which has more than 100 Korean users. As discussions about Rust had been frequent, the separate channel has been formed. (I'm one of founding members of that channel BTW.)
Sure, it was removed in commit [6997adf76...](https://github.com/mozilla/rust/commit/6997adf76342b7a6fe03c4bc370ce5fc5082a869), so the newest version is that in the previous commit: [ef75860a...](https://github.com/mozilla/rust/commit/ef75860a0a72f79f97216f8aaa5b388d98da6480) (the actual OCaml code is in [src/boot](https://github.com/mozilla/rust/tree/ef75860a0a72f79f97216f8aaa5b388d98da6480/src/boot).)
Thank you sir!
A big thank you goes to JensNockert for starting this bike shed. A well-designed numeric trait system would be incredibly useful.
I'd very much like to work on anything Rusty. There's a bunch of "Mozilla student projects" [here][1], not sure how relevant they are. [1]: https://github.com/Yoric/Mozilla-Student-Projects/issues?labels=Rust&amp;page=1&amp;state=open
I find it hard to believe that you didn't see the answer to this in the IRC channel, considering that we were talking about this yesterday. :P http://irclog.gr/#show/irc.mozilla.org/rust/371467 Here's the solution that bjz arrived at: https://gist.github.com/bjz/b4a5099f5bd632d71e7c ...which doesn't actually work right now because of a bug when using lifetimes with ~~@-pointers~~ `@fn`s. It's also full of syntax that will eventually be changing. Here's my blind stab at how it will eventually look, for Rust 1.0: https://gist.github.com/bstrie/4770441 I've included two files because I'm unsure about some of the future semantics. Consider them glimpses into possible alternate futures. EDIT: Oh right, you had other questions too. :) &gt; What do those compile errors mean? `error: moving out of dereference of mutable ~ pointer`: note that in `accgenT` you're attempting to return the closed-over value `n`. Because your closure has the `~` sigil that means it's *owned*, which means that it can only close over data that is also owned. It also means that by default it will *move* anything that it closes over, rather than copying it. (Recall that after you move a variable, the previous owner can no longer access it.) This is a problem because if you want to call the function returned from `accgenT` multiple times, the second time you call it will no longer have ownership of the closed-over variable, because it was moved out after you called it the first time! So to fix this we put the `Copy` bound on the type parameter `T`, so that it looks like `&lt;T: Copy + Num&gt;`. Now, instead of moving the variable out to return it, it copies the value and returns that instead. In the future there will be a notion of *one-shot closures* which will be a way to tell the compiler "it's okay that this function returns a closed-over variable via moving, because I promise I'm only ever going to call it once" (but that wouldn't help in this case). Let me know if you're still confused regarding any of your other questions. And if you're more confused than ever, that's also fair. :)
What do lifetimes mean on @-pointers?
Create a whole lot of learning and/or introduction tools, like Go's tour, and many languages' `try&lt;lang&gt;.org`. As a complete newbie to Rust, I definitely feel that's the biggest thing it needs to get itself out there. --------------------------------------------------------- A package management system? --------------------------------------------------------- Some good libraries? To databases, GUI bindings, queue/job servers, math libraries, etc?
&gt; learning and/or introduction tools There was a post on rust-dev about people working on a "rust playground": https://mail.mozilla.org/pipermail/rust-dev/2013-February/003056.html I agree that it's a nice GSOC project. &gt; A package management system? There's cargo, and the "rustpkg" cargo-revamp that's just about to land: https://github.com/mozilla/rust/pull/4610 Also, the rust issue tracker on github has some relevant categories, like "an interesting project": https://github.com/mozilla/rust/issues?labels=A-an-interesting-project&amp;page=1&amp;state=open
 &lt; bstrie&gt; nmatsakis: re: bjz's accumulator generator from yesterday, what do lifetimes on @ things mean? &lt;@nmatsakis&gt; bstrie: they can only be applied to fns &lt;@nmatsakis&gt; bstrie: and they refer to the lifetime of the closure state &lt;@nmatsakis&gt; bstrie: so if you have an @fn() that closes over borrowed pointers, &lt;@nmatsakis&gt; bstrie: it's normally illegal &lt;@nmatsakis&gt; bstrie: but you could do it with such an annotation So in bjz's example, the lifetime annotation on the @fn enables it to close over the borrowed pointer that was the input to the `accgen` function.
Thanks for your reply! Actually, I am somewhat new to rust, and have never looked at the IRC channel... but thanks! As for the errors: thanks, I think I mostly understand. I actually tried `T: &lt;Num + Copy&gt;` before, but my 0.5 compiler didn't like the `+` symbol, so... that happened. I still have one question though: after adding the `+ Copy`, I have the following code: fn accgenT&lt;T: Num + Copy&gt;(n0: T) -&gt; ~fn(T) -&gt; T { let n = ~mut n0; fn ~(i: T, move n) -&gt; T {*n += i; *n} } This returns the following error: accgen.rs:3:30: 3:31 error: value has non-owned type `~mut 'a` accgen.rs:3 fn ~(i: T, move n) -&gt; T {*n += i; *n} ^ I'm confused now; `n` is an owned pointer, which I want the closure to take control of and then own, so that its lifetime is the same as that of the closure. I thought that by specifying `move n`, I was indicating that the function now owned `n`; Am I writing it wrong? Is there a reason one should need to specify lifetimes here, i.e., shouldn't the `move n` indicate to the compiler what the lifetime should be?
While I understand the problem and realize that it is a common mistake (that even very skilled programmers do), I think it is as easily avoided by simply not using integer constants as short-hand for floating-point ones together with maybe some sort of lint pass could be able to detect the issue with as much accuracy as some sort of fix in the semantics of Rust. I (personally) wouldn't want to lose the genericness (is that a word?) of the division operator and the possibilities of generic code supporting both int &amp; float division. So I would really prefer it to stay the way it currently is and I think in most cases people actually want C-style round-to-zero integer division when they type '/'. My assumption is that most Rust users come from either a high-level language with type-coercion and a proper numerical tower (Ruby, Python) a language with only a single numeric type (Lua, Javascript), or a C-like language (C, C++, Java, C#) with the current semantics of that operator. And none of those languages use a different operators for different types. Python 3 did a different change to solve the issue (with float-like division by default for all types and a separate operator '//' that rounds in mysterious ways), and I personally don't like it, but on the other hand I know some of my numerical analysis professors love to import that behaviour into Python 2 for exactly the reasons you presented above. What does everyone think about this, and/or other solutions to this problem? Another solution to the problem would be to not allow integer by float arithmetic operations, but I don't think people would be too happy about that either, but is definitely a change that I could support since it would remove a whole class of bugs. 
Thanks.
I see your points. Perhaps my suggestion belongs to a nonstandard debug library rather than the main code. The library could enable a restricted euclidean division function for integers and disable the standard division operator for integers.
The `move n` bit there is what was called a "capture clause". These are deprecated and are in fact entirely ignored by the compiler at this point. Nowadays, owned values always get moved unless you say `copy`. The issue there is that this function hasn't told the compiler that `T` is owned. So just like you added `Copy` into the traits list there, you can also add `Owned`, and in addition you should move the ~ up into the function type. And now you'll be left with this: fn accgenT&lt;T: Num + Copy + Owned&gt;(n: ~T) -&gt; ~fn(T) -&gt; T { let mut n = n; fn~(i: T) -&gt; T {*n += i; *n} } Note that you were saying `~mut` before, which is (sigh) deprecated syntax (are you seeing a pattern here? :P ). Instead of `let n = ~mut n;` you'd instead write `let mut n = ~n;`. And *now* compiling gives this error: wacky.rs:3:20: 3:22 error: assigning to dereference of immutable ~ pointer wacky.rs:3 fn~(i: T) -&gt; T {*n += i; *n} ^~ wacky.rs:3:21: 3:22 error: mutable variables cannot be implicitly captured wacky.rs:3 fn~(i: T) -&gt; T {*n += i; *n} ^ error: aborting due to 2 previous errors For whatever reason, Rust doesn't want to capture mutable ~-pointers in ~fns! What a pain in the ass. I'm not sure if this is a bug, but it brings an end to the idea of using ~fns for the accumulator (at least for now). So if nothing else, you've learned today that Rust is buggy, unstable, unpolished, and a rapidly moving target. Have I scared you off yet? :)
OK, that makes sense now; thanks! Sorry for wasting your time talking about deprecated syntax... it was quite interesting, though. Rust is a very interesting language, and despite the bugs, instability, lack of polish, and rapid movement I'm very much enjoying small bits of coding in it; its the first statically-typed compiled language I've enjoyed coding in in a while. Thanks again!
All the Rust projects on the list that you linked to are still relevant, with the possible exception of "Port Rust to ARM" (a lot of work on that has been done in the meantime).
For the Google Summer of Code 2013, Port the Mozilla Rust http://www.rust-lang.org/ programming language to GNU Hurd, Mach based operating system. GNU Hurd allows any language that can RPC Remote Procedure Call http://www.gnu.org/software/hurd/hurd/rpc.html to write ‘servers’ ie. components of the OS that run in User mode as opposed to Kernel mode. Allowing an existing modern functioning OS to be written, extended and potentially parts rewritten in Rust. OS writers would use a modern low level capable language, Rust, to extend a functioning OS and components in a language that was designed to preserve large-system integrity, availability and concurrency, some of the same goals of GNU Hurd project itself. GNU Hurd allows runtime installation of low level OS components that do not require root access or a reboot to install and if they crash do not bring the whole kernel down. Unlike most UNIX’s, GNU Hurd is designed to be modified and extended through system hooks. The OS can be developed and progress at a potentially different pace compared to most others. A lot of functionality is implemented as file system interfaces 'translators' which may be another way of providing further features to Rust or RPC capable languages without reprioritizing language features such as httpfs virtual filesystem and netio http://www.gnu.org/software/hurd/hurd/translator.html . Perhaps Hurd translators or Rust library features can be written with a layer such that you can add the feature to a GNU Hurd virtual file system 'translator' AND Rust library with a single implementation. Work on either is work on both. Potentially this could mean that one missing aspect of GNU Hurd, drivers, could be written in Rust. Also work is being done to recompile categories of 'C' Linux drivers to GNU Hurd Linux. 78% of the Debian archive builds out of the tree on Debian GNU/Hurd. FOSDEM slides: http://people.debian.org/~sthibault/hurd-i386/2013-02-02-fosdem.pdf GNU/Hurd Plans For A Future With USB, SATA, 64-Bit http://www.phoronix.com/scan.php?page=news_item&amp;px=MTI5ODM http://www.gnu.org/software/hurd/hurd.html Modern operating system writing, in a modern language.
That sounds nice. As far as I can see it allows me to convert all constants to a new checkable int-type. But how do I make sure that I haven't forgotten to convert any integer constants? Can I disallow int-variables completely? 
I'm actually not sure how the lang items really work, and it also occurs to me that you'd probably run into problems where the rest of the compiler expects integers to be divisible. You'd need to ask someone more informed than I.
A site like http://dpaste.dzfl.pl/ would be amazing.
Have you seen https://github.com/mozilla-servo/rust-http-client ? Also i remember the plan to use https://github.com/erickt/ragel to generate Rust code from Zed Shaw's HTTP Ragel description.
Is the point that it would support Rust syntax, or that it would be written in Rust itself? For the former, there is at least [this pastebin](http://paste.aerdan.org/).
The main feature is the online compiler. The utility of this cannot be understated.
Ah, didn't see that. Yea, something like jsfiddle or the [School of Haskell](https://www.youtube.com/watch?v=sW327nUDf5g) for Rust would be cool. I'm not sure it needs to explicitly be a "pastebin" though.
So [Issue 4064: play.rust-lang.org](https://github.com/mozilla/rust/issues/4064) and [Issue 2235: Make Rust work with emscripten](https://github.com/mozilla/rust/issues/2235) then?
...which I still need to do. So many things to work on :(
How are versions in dependencies handled? What about dependencies not available in any repository (file://)?
Currently you have to depend on a specific version of a package, so you can't do a range or wildcard (something like 0.3.* for example). It's been discussed how to tackle this, but in my opinion none of the suggestions have been solid enough. File URIs should work out of the box, but I haven't tested them. It just uses cURL for everything non-git atm.
That's awesome! Does that mean that the parser in libsyntax will be rewritten using Ragel?
Not as far as I know, although that would be pretty neat. To be honest, I'm not sure if Ragel can handle LL/LR/etc grammars, so it might only be useful for handling simpler things, like a JSON parser, or parsing http for a client or server.
Ah, yes, now that I read the description of it carefully... Regular languages only. Sorry.
I know. *sigh*
That's neat, thank you. One question: If i require version 0.5.6 of git://github.com/mozilla-servo/rust-xlib what do the mozilla-servo guys need to do when they checkin a 0.5.7? Will they break my requirement? Or is this going to use branches, tags, git history?
This is very nice, I don't know how many languages got it wrong not having ONE package manager (to rule them all) right from the beginning :-)
Though I think it only gives it to you if you ask for it via the ```rust syntax. For example, Gist knows that https://gist.github.com/bstrie/4947114 is a Rust file, but it still hasn't highlighted it.
Python has a nice statement named "with". It may not solve all problems, but it leads to very readable code. http://effbot.org/zone/python-with-statement.htm
Also relevant: [A rust tool similar to Go's go tool](http://www.reddit.com/r/rust/comments/18sxae/a_rust_tool_similar_to_gos_go_tool/). It's really great to see some work happening on the tooling front.
It's going to use branches, tags, whatever you want to use. You can even publish a source tarball for each version. Of course, tagging is the easy-peasy solution.
See [my reply](http://article.gmane.org/gmane.comp.lang.rust.devel/3134) for further simplifications. If you guys can think of any more ways to make this simpler I'm happy to hear them. I really want to help get this right so we don't have to change anything when it actually affects the ecosystem.
For a single file there might be no benefit. But I think it is intended to replace the use of make files / autotools and the like in which case it is very welcome! &gt; And it goes against the UNIX way : "Design programs to do only a single thing, but to do it well" It is a single program, designed to do a single thing: build rust programs.
How is the UNIX way relevant to a cross-platform programming language's command line utilities? Don't get me wrong, I only ever use Linux and prefer UNIX-likes, but I don't see why the Rust toolset should follow a UNIX specification (or more of a "ideology") just because it's being built to run on UNIX systems. The UNIX way as it originally was is arguably out-dated in some parts and sticking to an old ideology that has been changed for the better with decades of working with UNIX systems is a silly move in my opinion. However, I still love heaps of ideas from the UNIX way, especially "all things should have an integrated input and output". Of course that's just my opinion isn't really relevant to this discussion so I'm not going to ramble on any further.
That makes sense, but using lib/bin has the awesome bonus of being able to make an API and an executable that uses that API in the one crate directory. The executable would be statically linked to the API, though.
Even its creators find it outdated: &gt; “Not only is UNIX dead, it's starting to smell really bad. - circa 1991” ―Rob Pike 
I'm interested in making sure that the `rust` tool is structured somewhat like the `git` tool^[1], i.e. instead of a monolithic binary there should exist separate `rust-build` and `rust-run` binaries, and the `rust` command merely exists to aid discovery of which tools are available. It also means that installing any new tools would just require them to have a `rust-` prefix and be dropped in the right directory. ^[1] Note that Git isn't *entirely* structured like this; the core functionality is really all in a single binary. But Rust has many fewer commands than Git, so I'm hoping it can get away with the more modular approach.
I'm not sure I have a clear understanding of this proposal. Please correct me if I'm wrong. So, if a "pkg.rs" file exists at the top level, then the build system executes the code in this file. This code mostly calls rustpkg's functions. If "pkg.rs" does not exist, then some conventions are used to build the artefacts. This proposal is about what these conventions should be. Graydon suggests that the existence of a file "somedir/bin.rs" instructs the build system to build the executable "somedir". I'm not sure what the content of this file is supposed to be, though. Is it the same as the current *.rc files ? Should it contain a main function ?
That sounds nice. I can imagine home brew commands like * rust-benchmark-the-testsuite * rust-prettify-everything-aggressively * rust-deb-package (I wish... :-) I guess that that binary approach will allow for shebangs. ( See https://wiki.ubuntu.com/gorun )
Regarding the integrated GUI it could be supercool to have a standalone refactoring library similar to rope for python. http://rope.sourceforge.net/
I believe that both lib.rs and bin.rs are just the replacements for .rc files, yes.
I think it would also be interesting to ask the Go community what in general they _don't_ like about the Go tooling and what unexpected problems they've encountered with the approach. On in particular that comes to mind is the lack of support for indicating either remote branches or sha1s for particular versions, relying on the maintainers to commit to not breaking backwards compatibility. I know I've read some fairly large discussions on golang-nuts, so it shouldn't be too hard to find some downsides that might want to be considered and thoughts given to approaches to fixing them.
The biggest problem I met when trying to implement this is that Rust does not want to compile a crate that doesn't implement all lang items, which means that you really need to link libcore from `libmath'. And core would need to link `libmath', which means there wouldn't be much point due to circular references.
Here are threads on packaging golang for Debian. I will not try to summarize, but hopefully they can give some inspiration. http://thread.gmane.org/gmane.linux.debian.devel.general/179252 http://thread.gmane.org/gmane.comp.lang.go.general/81527 https://wiki.debian.org/MichaelStapelberg/GoPackaging#Why_should_I_use_.2BIBw-go_get.2BIB0_instead_of_apt-get_to_install_Go_libraries.3F
AFAIK distutils2 for python was partly created to help packagers. I don't know how much of the following should be included in the first version of the packaging system though. * http://pythonhosted.org/Distutils2/ * http://www.python.org/dev/peps/pep-0345 * http://www.python.org/dev/peps/pep-0376 * http://www.python.org/dev/peps/pep-0386
I love the [editor wars] note
The dev team seems very cognizant of what is and isn't a bikeshed argument. It very refreshing
I don't fully grok Servo's situation as discussed in the "Inheritance" section, but usually where an OO language will use virtual methods, a functional one will use closures. Is there any reason that's not appropriate here? Or specifically for something like is_block_context(), what's the counterargument against just storing a bool? You could say "it takes up storage", but so does a pointer to a vtable. So it only makes sense once you have several such properties. But then you could just as well put those properties as fields in a struct, and store a pointer to an instance of the struct (which could be allocated statically). I kinda like the single inheritance proposal just from an "ooh, shiny" point of view though. I wonder at what point if you go down that road you start wanting to have a trait-struct inheriting from another trait-struct (adding not just methods but fields), in which case you've basically implemented standard OO classes?
I did not quite understand your proposal of having traits inheriting from structs. Where do the fields actually live ? - Is it a way to have mandatory fields in the struct that actually implement the trait ? (sounds okish, but requires a virtual call to get to the field) - Does it mean the field lives in the trait ? (sounds like it could introduce the diamond problem) What am I missing ?
The idea is to be able to have single inheritance of struct types. Basically if struct B inherits from struct A, then A forms a prefix of B, so a pointer to B can be safely cast as a pointer to A. When you're working with object types, you basically have a pointer and a vtable. Normally, you only know that the pointer can be used as a receiver for the methods in the vtable. The idea is that you could specify that the pointer points to a specific struct type, so that you can statically access methods and fields of that type. Normally, this would be pointless, since if you know the static type, then you don't need a vtable. However, if struct inheritance were implemented, then this would be useful, since the vtable can then reference methods of B despite the pointer being a pointer to A, assuming a pointer to B was used to initialize the object and B inherits from A.
Hum... I thought the idea was having traits inheriting from struct.
Ah, I think I finally grok it. So basically this restrain the trait to being only implementable by those struct that have the appropriate "sub-struct" in place ?
If trait T1 inherits from traits T2 and T3, that just means that any type S1 implementing T1 also has to independently implement T2 and T3. If T2 and T3 inherit from struct types S2 and S3 respectively, that means that S1 has to inherit from both S2 and S3. This is only possible with single inheritance if one of S2 and S3 inherits from the other. Assuming S3 inherits from S2, then the instance of S2 visible in the context of T2 is the same instance as the one visible in the context of T3 (since they are both the instance inherited by S1 through single inheritance) so there is no diamond inheritance problem.
So is the following line required, or is Baz automatically implemented for Foo? impl Baz for Foo {} EDIT: Do I understand correctly that traits are not allowed to have diamond inheritance?
&gt; Gating releases of a language on readiness-status of libraries and maintaining legacy APIs is a pain, true, but users often care much more about library APIs than they do about language features. "How do I write $foo in Rust" is very often an API question, not a language question, and the absence of a good API is equivalent to saying "go away". https://mail.mozilla.org/pipermail/rust-dev/2013-February/003297.html
Yes, you would need to `impl` explicitly, like any other trait. (I'm not a Rust dev, but I would be very surprised otherwise.) Structs would only be allowed to inherit one other struct, but as before they can impl any number of traits (similar to the "single inheritance of implementation, multiple inheritance of interface" rule in many OO languages). Again I'm not a Rust dev, but just going by logic, here's how I think things would make sense: struct S1 { ... } struct S2: S1 { ... } struct S3 { ... } trait T1: S1 { ... } trait T2: S2 { ... } trait T3: S3 { ... } Nothing controversial yet. The following impls would be legal given the above: impl T1 for S1 { ... } impl T1 for S2 { ... } impl T2 for S2 { ... } impl T3 for S3 { ... } Now for less obvious things. trait Tx: S1, S2 { ... } There could be a simple rule that a trait may only specify a single subtype requirement, in which case writing the above would be illegal, but it could also be interpreted as "types implementing Tx must be subtypes of both S1 and S2" which, in this case, since S2 is a subtype of S1, is equivalent to "must be subtypes of S2", so Tx could be implemented by S2 and anything inheriting it. Following similar logic, you could also allow: trait Ty: T1, T2 { ... } // or T1, S2; or S1, T2... The subtype requirements would be unioned in the same way, so anything inheriting S2 could implement it (except that in this case they must also implement T1 and/or T2). You might also allow: trait Tz: S1, S3 { ... } // or S1, T3; or T2, S3; whatever Which means that any types implementing Tz must be subtypes of both S1 and S3. Which is impossible, because a struct can only inherit one other struct, and S1 and S3 are disjunct. But as far as soundness goes, there's no harm in allowing you to write the trait - you just won't be able to implement it. There's no problem with a struct implementing multiple traits that specify subtype requirements, if it satisfies all of them, so: impl Tx for S2 impl Ty for S2 Would be completely fine. Basically, if you restrict structs to only inheriting one other struct, the diamond inheritance problem doesn't exist. It's the same as if you restricted C++ to single inheritance. C++ doesn't have any analogue for Rust's traits*, and what might look like multiple inheritance in traits is actually something different. `trait Tx: S1, S2` requires the type to be a subtype of both S1 and S2, but not for it to have \_separate copies_ of them. And if you do require multiple inheritance with `trait Tz: S1, S3`, that's just a \_requirement_: you won't be able to satisfy it. I hope that helps! \* okay, except for SFINAE
What do you mean, exactly? Do you mean: trait T1 { s1: S1; s2: S2; ... } struct S3 { s1: S1, s2: S2 } And then the trait stores the offsets of s1 and s2 in S3? What benefits do you see getting from this?
Thank you for a very clear explanation. That was a good read. I guess that all this will work in generics as well trait T3 &lt;T1 : S1&gt; My only worry is that this will collide with the future syntax for using a struct ( rather than a trait ) as generics patameter. Something like the following might be nice to have in the future. struct S1 { N : int } struct myvec &lt; s1 : S1 &gt; { data : [ int * s1.N ] } trait T3 &lt;s1 : S1 &gt; : myvec &lt;s1 : S1&gt; { fn get ( i : int) -&gt; int{ if i &lt; s1.N { self.data[i] } else { fail " Out of bounds" } } } 
You don't have actually have a virtual lookup for each field access. You only need one look-up within the method scope to get the address of `s1` (for example), and then all accesses to its fields are just "normal". So, at most as costly as a abstracting behind methods, and at best nearly cost-free; it certainly is an advantage.
You're making a false comparison. Accesses to s1 and s2 would be comparable to methods returning s1 and s2, whereas you're comparing them to methods accessing *fields of* s1 and s2.
Ah, exact! Still there is a small, but important, difference. There is no reason a method should always return the same `s1` so its result cannot be cached; if you think about a loop, with a virtual field you can hoist the field retrieval outside the loop whilst with a virtual method you cannot without knowing that the method always returns the same field.
There's some discussion collected at https://github.com/mozilla/rust/issues/3608 , but I'm sure it's been discussed more than that. As far as I know, it's definitely on the roadmap.
Sure. Of course, you can also do that manually, which reduces the benefit of having the feature, and it no longer addresses the motivating use case.
Does avoiding order dependence gain you anything? Type families in GHC (prior to 7.8) can only be order independent, and if you're trying to write something analogous to a type-level 'match' it's a pain in the ass.
Excellent post! Thanks for the links. I want to add more stuff to the book, currently working on a Rust buildpack for Heroku... Rails 4 has been sucking up all of my time lately though. :/
The code will be easier to maintain since it's less fragile.
Stay tuned. This is being worked on.
I'm having trouble connecting the dots from "you can put the arms in any order" to "it will be easier to maintain". Why? Why would you want to reorder them? Saying "if you want to add an arm it doesn't matter where you put it" doesn't work, because that only holds if the arm you are adding maintains the order independence, so you still have to think about it. It feels like the kind of superficially appealing rule that OO people who don't know any better tend to think up, without an actual benefit. That an introductory blog post about Rust is the first I hear of it, after coming from Haskell with pattern matching everywhere, strengthens the feeling.
Submitter here. This snippet was produced while porting my (minimalistic) [music video game](https://github.com/lifthrasiir/angolmois/) in C to Rust. It is currently quite tailored to my project (no fixed length string etc.), but I think my approach itself is general enough. The list of issues relevant to my code and related approaches is outlined in the source code.
Nice! (Does it update automatically?)
Is it written in Rust? If people merge commits from others, will the changes be attributed to them too?
In general I think it's nothing but the desire to avoid catching future developers off-guard with unexpected order-dependence. For example, I recently added support for the new lifetime syntax to Rust's Vim syntax highlighter. The solution I came up with is regex-based, and requires that the lifetime highlighter execute before the character-literal highlighter. I don't expect that anyone will ever bother to reorder those lines, but I still felt compelled to add a big comment that made the dependence explicit. To clarify, are you saying that pattern matching in Haskell is required to be order-independent?
Definitely not. :) Normal value-level pattern matching is order dependent and I've never seen anyone suggest that it might be beneficial to write them in an order-independent way. (I don't mean to say that it couldn't be, but I personally don't see how.) I don't think the fact that pattern matches are order dependent is any more surprising than the fact that if-else and (in other languages) switch statements are. It's a convenience, not a burden. What \*is* still required to be order independent in GHC/Haskell is "pattern matches" at the type level (type families, type functions, associated types, whatever you want to call them), because the type system is a different beast (open world assumption and all that). That was pretty annoying for some things, so they recently came up with a language extension to support order-dependent matching at the type level as well, which will be in the next release of GHC.
I set up crontab but didn't checked it works well yet.
It is counted now. (See bors) I didn't thought I can exclude merge commits. But it is available easily. I will try one. Source code is here: https://github.com/youknowone/gitstat Unfortunatelly, it is python :p
No merge count version: http://ruststat.youknowone.org/test.html
Without giving us a date can you tell us at least how much time do we need to wait? Are we talking about weeks, months or years? I work in a computational biology environment where matlab, python, c and c++ are known. I can only experiment with rust if what I do can easily interface with some of these languages/runtimes. And thanks for all the hard work. EDIT: I'm following the [this](https://github.com/mozilla/rust/issues/3608) github thread and it will keep me updated about this
Holy shit Brian Anderson, he's some sort of coding machine. I'm guessing he's one of the lead devs?
Richard Bird, in his introductory Haskell book, is generally careful to write non-overlapping patterns. I believe he mentions it's in the interest of maintainability. That said, the un(der)-motivated and relentless focus on this property in the article (without a sufficiently big disclaimer that it's just an exercise) will probably lead to nothing good ...
I'm in the same boat, I just tried to build Rust yesterday and it failed! 
Given this dialogue, how likely is runtime-less rust to skip 1.0? And if that does not happen, how *harder* will it be to interact with other languages like C or python? &gt;* Dave: what about runtime-less Rust? &gt;* Patrick: needs to be decided for 1.0 &gt;* Brian: impacts design of core &gt;* Patrick: impacts a number of things &gt;* Dave: need decision and direction, not necessarily needed to release with 1.0? &gt;* Patrick: would really like to ship with it, but won't necessarily block 1.0 on it 
One paint point I can think of is brought up by Luca in the thread. In summary, Go is a pain point for repository maintainers because it is: * statically linked (when using `gc`) * doesn't enforce package versioning. (when using `go get` and `go build`) The latter is typically enforced idiomatically. Basically: if an update to your pacakage breaks API/ABI compatibility, those changes do not belong in that package; they belong in a separate VCS repo as a new package. Basically, version 1 and version 2 [or any API/ABI change] means that two packages are no longer equivalent, so idiomatically they do not belong in the same VCS. The go team dogfoods this idiom, in a way, with the go compiler `gc` and all the go tools. Any breaking changes in a Go 1.x.y release can [for the most part] be cleaned up with `go vet` and `go fix`, all other breaking changes are reserved for the infamous "Go 2.0". --- Someone else brings up that you could use a VCS' tag functionality to do versioning, a la ruby gems, which would also help.
I think it's a good idea to look at what the gnome guys did with gobject introspection. It's makes calling gobject libraries from languages like python really without the need for bindings for every library. https://live.gnome.org/GObjectIntrospection
Python does use the grammar for its parser http://docs.python.org/2/reference/grammar.html. I'm not sure any other language does so, but a custom parser can always be written if there are performance/error reporting issues. It seems to me that going with the grammar first is a good idea. 
(Not sure specifically what I was thinking of with regards to OO people... I know I've had situations where the OO folks had come up with these at-best-narrow-minded-at-worst-misguided best practices, rules to follow, or things not to do, and when I tried to write code in that language in a functional style it was like "oh no, your code is wrong, you should clearly get more experience, you are breaking the rules and doing these things that everybody knows it is wrong to do", and it was unpleasant. And I guess this had a similar smell for some reasons. But I can't recall a specific example.)
brson is one of the core team members, and yes, he's *phenomenally* productive.
You could probably make it work today. Ask brson on IRC for more details.
`goto`? Really? Or is it like the Java `goto` where it's reserved but can't actually be used?
I've noted that https://github.com/amuxtux is not counted as a Samsung employee.
If I'm reading the gist correctly, Go has `goto` and Rust doesn't. Which makes sense, because I'm pretty sure I've seen at least one of the main Rust devs saying it won't happen — and if it did, it'd mean doing potentially scary things to the definition of initializedness/borrow/etc checking to deal with unstructured control flow.
Yeah, I was surprised Go would include it. But [indeed it does](http://golang.org/ref/spec#Goto_statements). I guess they put some restrictions on it and then trust people not to abuse it. *shrug*
Do you know his commit name or email? I cannot find him in git log.
Specifically, Github now has Rust 0.3 syntax highlighting :P
Rust doesn't have `goto`, true. Instead, [like Java](http://docs.oracle.com/javase/tutorial/java/nutsandbolts/branch.html), it has [labeled break and continue](https://github.com/mozilla/rust/issues/2216).
I've separated merged commits to merger group. This is now mainpage.
My mistake. He is involved, but does not seem to have contributed code directly.
pcwalton is a fanatic about removing keywords (I think there's like 12 fewer keywords nowadays than there used to be), and he's sad that Rust hasn't gotten to less than 30 keywords. So here's a proposal: if anyone can think of a way to reduce the keyword list even further and get it accepted, I'll give you a sigil of your choice (or perhaps a `keyword slayer` flair).
&gt; Anyway, at least the compiler tells me when I get it wrong. This made me smile, and then made me unsmile when I realized that until [#4707](https://github.com/mozilla/rust/issues/4707) is done, the compiler will happily let you do a vast number of wrong things. But it's coming along... &gt; I'm going to implement a solution to this challenge problem. Heh, we've actually been here before: http://www.reddit.com/r/rust/comments/165aml/my_attempt_to_solve_a_concurrency_challenge_in/ &gt; As a note, use statements can also appear at the beginning of function bodies, I think (not just at the top of the file). Entirely plausible, though I'm not sure if this is intended or not. `use` statements inside of `mod`s are definitely intended, though. &gt; Functions are not closures. This means that they do not grab variables from the enclosing scope (you can only use the explicit arguments). To clarify: functions at the top level of a file aren't closures. There aren't any variables at the top level anyway, and this allows top-level functions to both call each other. But if you define a function inside of another function, it *is* a closure (though I think this is deprecated... see again #4707). &gt; I just guessed at punctuation marks until the compiler was happy Sadly I think this might be the best way to learn Rust at the present. :) The code itself looks good, I'm happy that someone was able to make this sort of thing work without having to ask for help. The only things I'd change would be these lines: let (out,in): (Port&lt;int&gt;, Chan&lt;int&gt;) = stream(); let (strout,strin): (Port&lt;~str&gt;, Chan&lt;~str&gt;) = stream(); Those types are a pain to, er, type, so I'd just let the compiler infer them: let (out,in) = stream(); let (strout,strin) = stream(); Finally, for comparison, here's the (semi-cryptic) eventual result that I came up with for that challenge, once people started breaking out their futures libraries and stopped caring about receiving the output in a separate thread: extern mod std; use std::future::spawn; fn main() { let p1 = do spawn { (|a: int| a*2)(10) }, p2 = do spawn { (|a: int| a*2)(20) }, p3 = do spawn { (|a: int, b: int| a+b)(30, 40) }; let (x, y, z) = (p1.get(), p2.get(), p3.get()); io::println(fmt!("%d + %d + %d = %d", x, y, z, x+y+z)); } EDIT: &gt; The changing ordering tells us that Rust really is parallelizing our puny tasks. There's actually an easier, and more amusing way to observe the parallelism: fn main() { for 10.times { do task::spawn { for [1, 2, 3].each |item| { io::println(fmt!("%d", *item)); } } } } Not only will the numbers be in a different order every time, but because each task can yield after printing the item but before printing a newline, the linebreaks will vary irregularly between runs.
I have trouble letting the compiler infer the types because then I feel like I don't understand what's going on, even if it does. I guess I should work on that, if leaving them out is better style? Could you elaborate on the relationship between top-level functions not being closures and top-level functions being able to both call each other? The language tutorial told me that use statements can appear at the start of ["crates, mods, fns, and other blocks"](http://static.rust-lang.org/doc/0.5/tutorial.html#modules-and-crates). The most frustrating parts of making it work were figuring out the right imports and picking the right punctuation mark for the ~str part. The imports were frustrating because [the task and communication tutorial](http://static.rust-lang.org/doc/tutorial-tasks.html#communication) used import statements that didn't work for me. I now think this might be because I was reading the documentation for Rust 0.6 while actually running Rust 0.5, but at the time it was baffling. I used the comment at the beginning about "the modern stuff being in pipes not comm" to guess at replacing comm with pipes in the use statements. The most helpful thing in figuring out the memory/pointers part so far has been attempting to explain it to 3 or 4 different people. In particular, explaining why you would want to use each type of pointer was helpful in crystallizing my understanding.
This is fantastic. Thank you for working through this. I hope it is helpful for others because I think it's some of the best documentation available for Rust concurrency at the moment. Given the sorry state of our docs I'm glad you were able to discover so much on your own. I would love to rewrite the Rust concurrency tutorial for 0.6, but sadly I'm not sure I'll have the time for it. Maybe some of this material can be integrated into it.
FWIW, I include a lot of type annotations as well, particularly in code with a lot of environment captures and unsafe code, or just any code that I'm not entirely confident about. I do not think it's a best practice to use inference wherever possible - sometimes things are clearer with explicit types. I almost always annotate calls to `stream` and `transmute`. On the particular issue of annotating pipe types, the primary reason is that there have historically been some problems with type inference through closures in a few important cases. In both cases I usually annotate the call itself though. Consider: ``` let (out,in): (Port&lt;int&gt;, Chan&lt;int&gt;) = stream(); ``` vs. ``` let (out,in) = stream::&lt;int&gt;(); ```
&gt; I have trouble letting the compiler infer the types because then I feel like I don't understand what's going on, even if it does. That's fine, the style guide says nothing about omitting types,^[1] though this would have saved you from having to figure out `~str`. :) &gt; Could you elaborate on the relationship between top-level functions not being closures and top-level functions being able to both call each other? No need, I was actually wrong about that. :P Function-looking things inside of a function are *not* closures, and can indeed call each other (mutual recursion). But to illustrate the concept in general, see this Gist: https://gist.github.com/bstrie/5111674 . The first two files demonstrate that the functions can call each other regardless of if they're defined inside or outside of a function. The third one fails because you're trying to close over a value. The fourth one fails because of scoping; there's no way to arrange the two closures such that they are in each others' scope (unlike the functions, which are capable of seeing each other). &gt; The imports were frustrating because Yeah, the docs are pretty thin right now. And in fact `stream` has once again been moved out of `pipes` and into `comm` as of 0.6. :) ^[1] The style guide says nothing about anything. The style guide does not exist.
Would this also work? let (out: Port&lt;int&gt;, in: Chan&lt;int&gt;) = stream(); I guess the question is whether declaring types with `:` is like type annotations in Haskell, or whether it's specifically a part of `let`. ...also, what's the reason you can't just write stream&lt;int&gt;()? I suppose some kind of syntactic ambiguity, but what kind?
Go has a keyword, *iota*, which is similar to *enum* in Rust. http://golang.org/ref/spec#Iota 
I tried to answer this question by citing Rust. I'm not confident about the details I gave, so I made it a community wiki answer. Feel free to improve it !
I am not that well-versed into Rust's checks but I believe that the borrowing checks ensure that you cannot borrow (`&amp;`) without making sure that the pointed-to variable will remain alive for longer than the borrowed pointer. Therefore, I'd wager that the GC need only care about `@`-pointers and mightily ignore all the rest; whatever the strategy used by the GC.
I don't believe so. Borrowing works a different way for every pointer type. For stack data, it checks the lifetime statically as you say. For @-data with the current reference counting GC, I believe it inserts a reference count increment at the beginning of the borrow and a decrement at the end. (Correct me if I'm wrong, this is what I remember reading.) With tracing GC that would translate to tracing the borrowed pointer.
I hope not (for Rust), as the premise of the borrowed pointer is that it can come from either a gc-pointer or a unique pointer; therefore having special treatment depending on the source would be awkward. I hope some of the Rust devs come by and explain how they do it (or plan to).
Yes, the problem is that this post is already dating (July last year is ages away given Rust's fast pace development) and I know Niko's been working on changing the borrowing rules/checks since... It's pretty hard to stay on top of Rust at the moment!
Nothing wrong with goto. It's just too hard to implement in rust unfortunately.
Right now, it's true that &amp; pointers on the stack are traced, but if and when the implementation switches to precise-on-the-stack GC, they won't be. There's no requirement that they be traced for correctness.
I confirmed it does!
That was a nice writeup, really helped explain the unique/managed pointers, but it doesn't address two more pointer types (&amp; and *). Maybe a part 2 is in order?
...but which first requires one to realize that ``` let foo = &amp;7; ``` is sugar for ``` let foo = 7; let foo = &amp;foo; ```
Ah I see, but then aren't * pointers, unsafe pointers that can do various unsafe operations? Also don't pointers always point to memory that's been allocated?
*Excellent*.
First question: what version are you using? From that impl syntax it looks like 0.5, but I really recommend git master if you can swing it (the language has changed significantly since the last release). Also, can you provide a test case? I'm really not familiar with Haskell, so I don't know what a functor really is or how you'd expect to use it... so I'm just going to throw things out there. :) First of all, just to get it compiling: trait Functor&lt;A&gt; { fn fmap&lt;B&gt;(@self, f: @fn(A)-&gt;B) -&gt; Option&lt;B&gt;; } impl&lt;A: Copy&gt; Functor&lt;A&gt; for Option&lt;A&gt; { fn fmap&lt;B&gt;(@self, f: @fn(A)-&gt;B) -&gt; Option&lt;B&gt; { match *self { Some(x) =&gt; Some(f(x)), None =&gt; None } } } This doesn't generalize over the container type, but let's go over the other differences for now. First, check out the bit of the method signature like `fmap&lt;B&gt;(@self,`. Rust is currently moving to a Python-ish explicit self for its methods (though unlike Python, the parameter name must actually be "self"). Here we're taking self by `@`, a.k.a. a managed pointer. I've also added a `@` to the signature of `f` to turn it into a managed closure (not strictly necessary, just gives us some flexibility for now). Also notice the 0.6-style `impl` declaration (sorry, I didn't have a copy of 0.5 on hand). Finally, notice the `Copy` bound on the generic type in the impl. I'm not actually sure if this is intended to be necessary, I'll have to ask around. Now let's try and get that Option out of the method return type. Ordinarily if you're returning the same type that you're implemented on you can use the special `Self` type in the trait, like `fn foo() -&gt; Self`, but our self type is actually different from the return type in this case because its contained type is different. So I'm not sure what the best way to handle this is, but let's try stuffing more `@`s in there: trait Functor&lt;A&gt; { fn fmap&lt;B&gt;(@self, f: @fn(A)-&gt;B) -&gt; @Functor&lt;B&gt;; } impl&lt;A: Copy&gt; Functor&lt;A&gt; for Option&lt;A&gt; { fn fmap&lt;B&gt;(@self, f: @fn(A)-&gt;B) -&gt; @Functor&lt;B&gt; { match *self { Some(x) =&gt; @Some(f(x)) as @Functor&lt;B&gt;, None =&gt; @None::&lt;B&gt; as @Functor&lt;B&gt; } } } Now we're casting the result of the method to a trait object. I have no idea if this is satisfactory for your use case, since they're somewhat obscure and I've never used them before. In fact, it's probably mostly useless in this state. :) I'll have to ask around to see if there's a better way to generalize over that container, but as far as I know there might not be. The only weird thing to look at here is probably the `@None::&lt;B&gt;` bit, where `::&lt;B&gt;` is how you specialize a type on `None` (which I feel like shouldn't be necessary, but it seems to be at the moment). Now, can you provide an example of how you'd like to use it? :)
fwiw I don't think what you are hoping for is currently possible. Traits are implented for types and I don't think there's a way to say "the same instance but resulting in a different actual type". But I'm not a type system wizard.
Rust doesn't support higher-kinded types yet. So you can't do what you want to do.
(What pcwalton said.) FWIW it seems like `trait Foo&lt;T&gt;` is the equivalent of Haskell's `MultiParamTypeClasses`, with the usual Rust eccentricity of giving the first parameter special treatment syntactically (so `T` in this case is the second one). This works with 0.5: trait Iso&lt;T&gt; { fn there(self) -&gt; T; static fn back(x: T) -&gt; self; } enum Bool { No, Yes } impl Bool: Iso&lt;bool&gt; { fn there(self) -&gt; bool { match self { No =&gt; false, Yes =&gt; true } } static fn back(x: bool) -&gt; Bool { match x { false =&gt; No, true =&gt; Yes } } } fn main() { let b: bool = Yes.there(); let B: Bool = Iso::back(b); io::println(fmt!("%b\n", B.there())); } Which is neat. Didn't know that was possible. Learn something new every day.
This cracks me up: There's a discussion about changing "static" to "const", since static memory is only used for constants. Then, later, there's a discussion about global variables; consensus seems to be that they're necessary, but there's disagreement about syntax. So Graydon goes: &gt; Clearly `const mut`! lol
Well that's easy then. I can stop trying now :). Since you say "yet", is there a timeline for adding support in?
Oh well. Thanks for trying though.
It's not on the roadmap, but it comes up from time to time. If somebody were to write a patch, I'd most likely be in favor of taking it.
Is there room for it in the syntax? Would it be possible and sufficient to just infer the kinds of type variables from the way they are used, i.e. the compiler sees `Self&lt;T&gt;` and realizes the self type of the `Functor` trait has kind (in Haskell terms - how would you even say it Rustily?) `* -&gt; *`? I'm having a hard time imagining syntax for declaring them explicitly, because "how do you even write a kind in Rust" on one hand, but also because the "Foo: Bar" syntax you would want is taken by trait bounds, and in the specific case of traits there isn't even an obvious way to talk about the self-type at an appropriate point... But I guess if inference works for Haskell (in most cases), it should work for Rust too?
If someone sneaks global variables in, I really want to see the syntax being available for context locals: https://gist.github.com/mitsuhiko/213892b73873f9d20920 :D
I think it'd just be based on inferring the kind of each type parameter based on the number of parameters it's "invoked" with, as you suggest. I don't think it's worth introducing explicit kind syntax unless it's needed, since the feature isn't used much outside the standard library and scalaz-like frameworks. (Just my opinion, of course.)
It is (slightly) possible. trait Functor&lt;A, FB, B&gt; { fn fmap(&amp;self, &amp;fn(A) -&gt; B) -&gt; FB; } impl&lt;A,B&gt; Functor&lt;A, Option&lt;B&gt;, B&gt; for Option&lt;A&gt; { fn fmap(&amp;self, f: &amp;fn(A) -&gt; B) -&gt; Option&lt;B&gt; { self.map(|&amp;x| f(x)) } } fn main() { let s = Some(1i), n: Option&lt;int&gt; = None; io::println(fmt!("%?: %?\n%?: %?", s, s.fmap(|x| (x as float) + 1.5), n, n.fmap(|x| (x as float) + 1.5))); } Prints: Some(1): Some(2.5) None: None Where `FB` and `B` are dummy parameters that allow the (inner and outer) return types to be specified, and allow the type-inference to handle things. It's purely convention that `Self` and `FB` are both the same functor (abusing this would lead to some unexpected behaviour). However, this technique seems to be something that will blow up in your face easily (fortunately there seems to be an implicit functional dependency between `Self` and the trait parameters, which saves a lot of type-inference explosions), and it results in impossible-to-understand error messages when anything goes wrong. Edit: an impl for `Either` impl&lt;T:Copy, A:Copy, B&gt; Functor&lt;A, Either&lt;T,B&gt;, B&gt; for Either&lt;T,A&gt; { fn fmap(&amp;self, f: &amp;fn(A) -&gt; B) -&gt; Either&lt;T,B&gt; { match self { &amp;Left(copy t) =&gt; Left(t), &amp;Right(copy a) =&gt; Right(f(a)) } } } fn main() { let r: Either&lt;char, int&gt; = Right(1i), l: Either&lt;char, int&gt; = Left('a'); io::println(fmt!("%?: %?\n%?: %?", r, r.fmap(|x| (x as float) + 1.5), l, l.fmap(|x| (x as float) + 1.5))); } =&gt; Right(1): Right(2.5) Left('a'): Left('a') 
So what bugs should I be tracking if I'm interested in following progress on embeddable/runtime-less Rust?
Scheduler rewrite: https://github.com/mozilla/rust/issues/4419 Freestanding Rust: https://github.com/mozilla/rust/issues/3608
Very nice. How good is the performance? Does Rust compile blur_rust down to four loops?
Here's the optimized asm: https://gist.github.com/brson/5156787#file-gistfile1-txt-L472 And a possibly easier to read annotated version:https://gist.github.com/brson/5156799#file-gistfile1-txt-L647 Almost everything is inlined into one function, in just two loops, with the two inner loops apparently optimized away. The innermost closure though (https://gist.github.com/brson/5156799#file-gistfile1-txt-L255) is not inlined and is called several times inside the loop. Not sure if that's good or bad.
Ha, unfortunately yes. At the beginning of this release cycle pcwalton and graydon had a duel to determine what was the language's most pressing issue; graydon alleged that it was the slowness of the compiler while pcwalton thought it was general lack of feature-completion combined with [the ridiculous number of deprecated features](https://github.com/mozilla/rust/issues/4707). pcwalton was more persuasive, but graydon is still obsessed with making the compiler faster, so I expect it to be a huge focus for 0.7 (along with language performance in general).
Ouch. Yeah I understand not going for Go's impressive 2min to compile everything, however 5hrs to check if something you made didn't break anything is just... too long (15min if possible would be ideal). I thought they already did some fixes on 0.6. Guess I was wrong.
It really depends on your machine. My gaming rig fully compiles Rust in approximately 12 minutes (which is still too long IMO), and Rust+LLVM in around 18. But if you're swapping I'm sure your speed goes right to hell. And 2-minute compilation times? Please, let's at least have some ambition and try for Lua's 0.1-second compilation time. :P
What do you mean by gaming rig? Sounds like it eats small children. I have a Virtual Box Linux, running amazingly smooth on a 8GB Core2Duo (I don't know specs on the top of my head). 5hrs to compile any language is unacceptable.
Dual i7 2600K CPUs, 8GB of memory. A few years old now, so nothing too spectacular. I make LLVM with -j8, which is what allows it to build in 6 minutes (at full 800% CPU), but memory usage will jump up to 5GB at times. Rust itself doesn't build in parallel at all right now, and memory usage is probably around 1.5GB at the highest. Are you sure that you're allocating enough memory to that VM? Multi-hour compile times aren't unheard-of, but usually indicate a lack of available memory (or a netbook).
It would go well with immutable variables. Like the old joke: "Constants aren't and variables's don't."
It "only" took me an hour or two (not exactly sure, but definitely less than 5 hours) on a 5 year old 2 GB Core Duo laptop to compile rust, so you've probably got a different problem than just rustc being slow. (I think it now l takes me about 20 minutes on an 8GB ulv i7 laptop.)
Amazing!
I think it makes sense... in C values are normally mutable, and you say `const` if you want them to be immutable. In Rust values are normally immutable, and you say `mut` if you want them to be mutable. So it makes the same kind of sense for Rust to not have `const` as it does for C to not have `mut`. The alternative is expressed by saying nothing. (And having `const` mean something different from C's `const` can be confusing, brought into sharpest relief when you try to combine it with the antonym of C's `const` for the oxymoronic `const mut`.) `static` feels more appropriate to me for talking about allocation lifetimes. Though I also have no problem at all with just using `let` for global variables, same as for local ones. You only really need a special word for the special lifetime, and for statically allocated variables inside of functions, as floated in the meeting notes - for that case I don't think `static` has a rival. OTOH it's also kinda nice if the things that have `static` lifetime are precisely those things that are declared with `static`. OTOOH `static` on global variables means something completely different in C (unlike on local ones where it happily coincides). So I dunno. Bunch of little pros and cons on everything.
The original blog post is quite odd. The OP uses old syntax like `alt`, `ret` and the `times` function. When he gets to error handling he doesn't even mention `Option`.
Agreed - doesn't really delve much into the semantics all that much, which is where rust really shines.
The comparison appears to considerably misrepresent both D and Rust. Not only are the Rust snippets using ancient 0.0 syntax, they are written such that they would never compile in the first place.
* D language is not full open source: http://dlang.org/faq.html#q5) * It is overly complex due to have implemented so many features; see the number of keywords in http://dlang.org/lex.html * It is a language that has not had much attraction although it has come to version 2
In all fairness, languages are rarely big hits right out of the gate. Python toiled in obscurity for 15 years before it blew up. :) All that really matters is that the language is usable and that the community is Large Enough to be self-sustaining. Everything else is just gravy.
DMD, the reference compiler, is open source (both frontend and backend). It isn't "Free Software" though because the backend is licensed from symantec. The D compilers LDC and GDC are both completely open source and free software. D is much more popular than Rust ([TIOBE](http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html); D is the #27 most popular language on GitHub, Rust is #37). Neither are particularly popular though. D is a pretty complex language though, as you say.
Well, let's be fair… * Judging the keyword list isn't quite fair since the primitive types are keywords in D and predeclared identifiers in Rust. * Rust hasn't had a lot of attention either.
Sure D is much more popular but D has released the version 2.062 while Rust is in 0.5 I usually am quite objective and I think that Rust due to its features and simpler sintaxis is going to attract more developers than D is doing now.
There's been some interesting discussion on IRC regarding what '@' and '~' types to be called in documentation to be as unambiguous and straight forward as possible. Comparing to C++ concepts like smart pointers works, but of course not everyone coming to Rust is familiar with C++ pointer types. A popular suggestion was owned and managed allocations for ~ and @. Also, people might want to take a look at strcat's draft wiki page: https://github.com/mozilla/rust/wiki/Rusty-semantics-draft
Your link goes to a "Create New Page" page. edit: seems it was github not liking my browser. In chrome it redirects to the rust wiki main page. Has the semantics draft been deleted?
Huh? Why? Are you missing a comma in there, maybe?
Yes, looks like it was deleted and reworked in to this pull request: https://github.com/mozilla/rust/pull/5428
Quite nice article. My main issue is having too many @ and ~ scattered around the code. Maybe having one of the following forms should be ok as well. let x : ~int = 5 let x : int = ~5 
I would say that '~x' could be refered to as 'unique scoped pointer to x' -- it pretty much explains how '~x' behaves. The obvious disadvantage is that it's a bit long-winded. But I do not see anything inheretly wrong with 'owned pointer' and 'managed pointer' provided the terms, when used, link to their respective documentation page.
Perhaps it might be beneficial to distinguish things that are true of the *pointer* from things that are true of the *pointee*. If you have `~int`, the `int` is *owned by* the pointer, while the *pointer* is unique. So these have a correct meaning, when referring to `~int`: owned int, owning pointer, unique pointer. This does not, taken on its face: owned pointer. It's not the pointer that's owned. For whatever reason, saying "owned pointer" still feels more natural, despite being wrong. Is that just because of familiarity? I honestly can't tell. I can't become un-familiar with it to find out. Another problem is that if you're talking about a pointer generally, without a specific pointee, it's kinda hard to use the formulations that refer to the pointee ("owned *something or other*"). And the remaining options aren't always satisfactory. While "owning pointer" merely sounds weird, with the others it's even worse. "Managing pointer" doesn't make any sense (it's not the pointer that's doing the managing), and "borrowing pointer" is just absurd. Humans are pretty adept at repurposing language, so I guess people quickly form the subconscious connection that "*whatsit* pointer" is verbal shorthand for "pointer to *whatsit* thing", so owned pointer =&gt; pointer to owned thing, managed pointer =&gt; pointer to managed thing, borrowed pointer =&gt; pointer to borrowed thing. So I'm not sure if accuracy trumps ease-of-use or vice versa... Though in the latter case I can't even tell whether it's more correct to think of the pointer or of the pointee as the one that's borrowed. (In case anyone's expecting specific advice hidden in this comment somewhere, there isn't any. Just a meditation on the things we mean by words.)
Yeah, in practice you leave off the types. I just included them for clarity.
In the future I think this should be mentioned at least once in every instructional blog post, since they tend to annotate the types at all times. Misunderstandings like the above are quite common, and are hard to shake if not advertised.
I do have quite some experience with ML family of languages, but it is easy to get that misunderstanding given how the blog posts, or the Rust tutorial is written.
So question, let's say I have a smart pointer in a function scope and I want to return it to the calling scope, is this possible? Basically the calling scope would now control the lifetime of that unique pointer and free it when it exits (unless it also returns said pointer) rather then in the scope of the function it was created, i.e. fn f() -&gt; ~int{ let x: ~int = ~1024; return x; } // x is NOT freed here because it's returned fn g() { let x: ~int = f(); println(fmt!("Your lucky number is: %d", *y)); } // x returned from f IS freed here I realize syntax is probably off here but will this work in general and if it does what is the actually syntax for it, does something else special need to be done?
Yes, that's the syntax for it.
D2 broke compatibility with D1, and lost some users. It's gaining users since (judging by newsgroup posters) and raised $30,000 for http://dconf.org/. Rust looks promising, but whether its concepts are easier to master than D's remains to be seen.
&gt; 29 Feb 2000 + 3 years + 1 year -&gt; 28 Feb 2004 (or 1 Mar 2004) Is there really no ISO standard that has a say on this? I think the real problem here is that the attempt to frame this operation as *addition* is a leaky abstraction.
The section of [libraries](https://github.com/mozilla/rust/wiki/Lib-datetime#3-research-of-libraries-from-other-languages) should add the [language Go](http://golang.org/pkg/time/) which has a great implementation.
Do you see high-resolution timing as being within the scope of this module? Something abstracting QueryPerformanceCounter, mach_absolute_time and the like. Pro: consistency. Con: very fine-grained resolution is likely to bloat the time/timespan representations.
Are timezones in the scope of this library? If so, [pytz](http://pytz.sourceforge.net/) is quite nice. Also, Python's [dateutil](http://labix.org/python-dateutil) has some neat features (parsing, and some timezone stuff too).
Presumably one could have a `HiResTime` type, but that loses some of the consistency (retains the abstraction though, which is nice).
I went with managed box and owned box for the updated tutorial. An owned box has the same semantics as an unboxed value in terms of mutability, destruction and ownership with the only difference being the indirection. Managed boxes are just owned by the garbage collector instead of continuing the ownership tree. Using "owned allocation" and "managed allocation" was also considered, but I'm fond of being able to use the term "unboxed" and several others weren't as keen on "allocation" as myself :). The pointers themselves aren't really important, they're a non-nullable, non-dangling handle. The handle for the managed box merely observes it, while the handle for the owned box owns it.
I am not sure. I've added it, since the Boost author's used those aspects to point out that most Date/Time libraries are not correct/complete: http://www.boost.org/doc/libs/1_53_0/doc/html/date_time.html#Motivation. That depends on the scope, of course.
[Ruby `Date` class](http://ruby-doc.org/stdlib-2.0/libdoc/date/rdoc/Date.html) is also worth looking at. Interestingly enough, it allows for different Julian-to-Gregorian transition points and does it in the single class (this design is unique among other programming languages I know of). Also note [SRFI-19](http://srfi.schemers.org/srfi-19/srfi-19.html) for Scheme implementation.
I think that it should go the other module (or at least a different inner module), due to the difference between the timekeeping (where we want the date and time components) and timing (where we want the duration between two instants). Both has a concept of time instant (point), duration and time interval (duration with two endpoints) but with different scales. For example, POSIX `CLOCK_MONOTONIC` and `CLOCK_REALTIME` has different (non-linear, non-continuous) axes.[1] As we expect the time instant for timekeeping should has date and time components but we don't expect it for timing, I think we should keep them apart. [1]: I consider this to be a fault of POSIX. They should not share the same `struct timespec` type.
&gt; Con: very fine-grained resolution is likely to bloat the time/timespan representations. If we are tralking about the duration type, Boost uses a template argument for this: http://www.boost.org/doc/libs/1_53_0/doc/html/boost/date_time/date.html Generics should only bloat the code if you need that bloat.
does 'let x = ~(5: int)' work, for cases where the type of 5 is useful documentation? Or when you have 'let x = someExpression(~some_sub_expr)', and the type of the sub-epxression is not obvious to the reader.
Erik Naggum's [The Long, Painful History of Time](http://naggum.no/lugm-time.html) is considered classic for this topic.
It don't seems relevant anymore since Rust code will soon be callable from external language, using C ABI. http://brson.github.com/2013/03/10/embedding-rust-in-ruby/
AFAIK no. The type annotation is part of the syntax for `let`. I would prefer if it were the way you suggest (like Haskell).
Indeed, this is from early last December. I'm very glad at how far things have progressed since then.
Though it handrolls the non-local returns rather than having language support for it, like the example languages given. Interesting post, thanks!
I suppose it should instead belong to a *calendar* library, but I've certainly never heard of such.
Excellent post! This article clearly summed up the different pointers in Rust for me. I particularly like the last example: I found myself thinking "well, what if the borrowed pointer is passed outside the function?" Given the last example though, I assume that would be caught by the compiler! Very, very cool! As someone who was struggling a little w/ the different smart pointer types, I'm very thankful for the post.
I'm not at all familiar w/ C++ or it's different pointer types, and I only have a passing familiarity w/ C. For me: this was one of the most straightforward explanations of Rust's pointers that I've read. (FWIW: My background is mostly Java, Ruby, and Go -- that is to say: languages &amp; runtimes that rely on garbage collection.)
would it be possible to have modulo return an int typed with its range? That's way, the int_to_Rem cruft wouldn't be needed.
The ending was a bit anti-climactic. It *did* end up using threads (well, ok, "fibers") for internal-&gt;external iterator conversion, just like we predicted in part 1.
I'm very happy to say that this article is now obsolete. :)
Well, the more Rust advances, the less patient I become. I WANT RUST NOW!!!!!!!!! Rust + JavaScript (only the good parts and asm.js) + Python seem to be the best tools for me. Hopefuly, after v0.6 is released, there won't be any more big changes, and the documentation will catch up. I tried several times to compile the master branch on Windows, but without luck. Also, are there any plans in making (a subset of) Rust to be able to target asm.js?
 fn fmap&lt;B&gt;(f: fn(A) -&gt; B) -&gt; Functor&lt;B&gt; Doesn't this correspond to, in Haskell, something like: fmap :: (Functor a, Functor f) =&gt; (a -&gt; b) -&gt; f b Which isn't really `fmap` at all?
Should be normal fmap: fmap :: Functor f =&gt; (a -&gt; b) -&gt; f b This was written in 0.5 which had implicit `self`. Written with an explicit `self` parameter it would look like: fn fmap&lt;B&gt;(&amp;self, f: fn(A) -&gt; B) -&gt; Functor&lt;B&gt; where `self` has type `Functor&lt;a&gt;`.
Ah, `self` of course. But that wasn't the only thing I was talking about. Isn't the `A` in the rust code more like an `f a` in Haskell? And then, isn't `Functor&lt;B&gt;` a different Functor instance, not really the same instance containing a different type?
Well yes and no. Remember that none of this is really valid Rust though :). The `A` is brought into scope by the trait declaration: trait Functor&lt;A&gt; { // &lt;-- Here fn fmap&lt;B&gt;(f: fn(A) -&gt; B) -&gt; Functor&lt;B&gt;; } And the `A` is the type parameter provided to an implementation of the `Functor` trait. Having said that, you are absolutely right in that `Functor&lt;B&gt;` may well be a completely different type of `Functor`. Rust doesn't provide any mechanism to indicate that `Functor&lt;A&gt;` and `Functor&lt;B&gt;` must be the same type of functor.
Kinda but remember that `Functor&lt;A&gt;` in Rust is not the same as `Functor f =&gt; f a` in Haskell since Rust does not have type constructors.
Hmm, what about borrowed *something or other* though? Obviously that's not a box.
I'm fine with calling that a borrowed pointer, but perhaps just saying "reference" would be better.
Cool! One thing that your blog didn't make clear—was it something in the language that caused you to struggle, or was it just a hard problem in general?
It's a little difficult to understand the code at a glance seeing as you're not using the standard naming conventions. This would help: enum CardSuit { Clubs, Diamonds, Hearts, Spades } struct Card { suit: CardSuit, rank: u8, // 1..13 next: Option&lt;~Card&gt; } Minor quibble though, thanks for sharing! 
I don't get what's going on in the ~[0, ..n] section...
Yeah, that doesn't read well. At issue is whether the literal `~[0, ..n]` produces the type `~[T]` (unique vector of T) or `~[T x n]` (unique pointer to fixed vector). Currently it's the latter, but most think it should be the former.
I think I've got it. (Compiles in the recent incoming) // [c1, ..., cN], cX -&gt; [cX, c1, ..., cN] fn place_top(pile: Option&lt;~card&gt;, newcard: ~card) -&gt; ~card { fail_unless!(newcard.next.is_none()); ~card { suit: newcard.suit, rank: newcard.rank, next: pile } } // [c1, ..., cN], cX -&gt; [c1, ..., cN, cX] fn place_bot(pile: Option&lt;~card&gt;, newcard: ~card) -&gt; ~card { fail_unless!(pile.is_some()); fail_unless!(newcard.next.is_none()); let ~card { suit: suit, rank: rank, next: next } = pile.get(); let next_ = match next { Some(next) =&gt; place_bot(Some(next), newcard), None =&gt; newcard }; ~card { suit: suit, rank: rank, next: Some(next_) } } // [c1, c2, ..., cN] -&gt; (c1, [c2, ..., cN]) fn pop_top(pile: ~card) -&gt; (~card, Option&lt;~card&gt;) { let ~card { suit: suit, rank: rank, next: next } = pile; (~card { suit: suit, rank: rank, next: None }, next) } // [c1, ..., cN-1, cN] -&gt; (Some(cN), [c1, ..., cN-1]) fn pop_bot(pile: ~card) -&gt; (~card, Option&lt;~card&gt;) { let ~card { suit: suit, rank: rank, next: next } = pile; match next { Some(next) =&gt; { let (tail, pile_) = pop_bot(next); fail_unless!(tail.next.is_none()); (tail, Some(~card { suit: suit, rank: rank, next: pile_ })) }, None =&gt; (~card { suit: suit, rank: rank, next: None }, None) } } Destructing ~-ptr is not fun, but I was unable to use `pile.next` directly due to the immutable field move error (even when `pile` itself is mutable!). I've also experienced the partially moved value error (when `pile.next` has been once moved, an access to `pile.suit` and `pile.rank` is blocked) but that's another story.
Thanks for the explanation !
Yeah, I know -- same with Haskell. That's why there's real serious value in the provided binary installer :-)
Funny how the author fails to acknowledge the existence of native compilers for Java and C#, with operating systems written with them.
Voodoo? I just downloaded the (old) MinGW/Msys toolchain that's recommended on the wiki and did a standard configure/make from the Msys shell. It did take more than an hour, though - make likes hanging with multiple jobs on Windows, so I had to do it "single-threaded". 
Kudos to the Changelog where I found the link: http://thechangelog.com/rust-antlr-an-antlr-grammar-for-rust/
I'm glad it worked for you. It didn't for me. Do you have other mingw's in the path? Git msys? Which version of gcc did the mingw installer download? Also Building Rust in an hour or so is pretty standard. On my Ubuntu x64 VM with 4GB of ram and 40GB SSD drive it takes about an hour and a half and that's without rebuilding LLVM (granted I usually have more than one VM running on the machine at the same time). make -j 8 doesn't appear to help much with build Rust itself unlike when building LLVM.
I had to rescue this from the garbage bin, presumably because Reddit took offense with the misspelling in the title. :P This looks neat though! More examples of Rust-powered graphical applications are great for giving others a base to work upon. I should also note that there's been some difficulty writing traditional, terminal-driven roguelikes in Rust because of the apparent difficulty of creating bindings to curses, which has inspired efforts such as Amulet (https://github.com/eevee/amulet) to create alternative terminal-control libraries.
Thank you, sir. I misspell "rogue" word 50% of the time and for some reason spell-checker does not highlight it. :) I enjoy ASCII art, but don't like curses. I've seen `amulet` before starting to work on `rustyhex`, however my love to hex is stronger than to ASCII and `rust-sdl` bindings are very nice too. :)
&gt; D language is not full open source The language is completely Open Source. The official compilers backend is not libre software.
Woo! I was working on a similar game application for learning; I think that there is a need for this sort of thing
Oh Jesus. I hope ANTLR has matured since I used it last. Its a really cool concept but horribly undocumented and unnecessarily inflexible and opaque But this would be useful for lots of tooling. Cool stuff. 
Why did Rust's devs decide to use an ad-hoc grammar? Everything else I've seen in Rust seems to be very carefully planned; it surprises me they didn't have a formal grammar from the start.
FWIW, there are no plans to replace Rust's existing parser with one generated by ANTLR. For one thing, it doesn't seem to have a Rust back-end yet.... :)
You can follow the Rust tag specifically here: http://thechangelog.com/tagged/rust/ I'm sure anyone subbed here will probably see a lot of the same stories, though. :)
Currently compiling here. Originally, 0.5 and then 0.6 were supposed to be nearly syntax final. How close are we to that today?
One thing I just found, but that has no mention in the changelog, and hadn't seen anything about: `pure` has been removed. Anyone knows the reasoning here?
0.4 was supposed to be pretty close too. It's a lot closer now. There is still one major known backward-incompatible change pending: casting to `@Trait` is going to become a coercion instead of being written explicitly with `as @Trait`. Beyond that there will surely be some minor bugs uncovered in the grammar.
Totally. Obviously the language won't be frozen, but a lot of people have asked me how close it is to "I won't have to keep re-writing my code over and over when a new release comes out," so that's a firm enough answer.
Congrats to the team on the (upcoming) release. So I have a directory full of small example programs (some by me, a couple by kibwen, a few from Armin Ronacher's slides on Rust, and some from Steve Klabnik's book). I'm trying to get all of them to compile on 0.6 RC. Is the "small taste" example on the [rust-lang website](http://www.rust-lang.org/) still supposed to work? It doesn't: $ rustc taste.rs taste.rs:4:20: 4:51 error: type `@core::rand::Rng` does not implement any method in scope named `shuffle` taste.rs:4 let v = rand::Rng().shuffle([1, 2, 3]); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ taste.rs:5:24: 5:29 error: the type of this value must be known in this context taste.rs:5 for v.each |&amp;num| { ^~~~~ taste.rs:5:24: 5:29 error: mismatched types: expected `[type error]` found borrowed pointer taste.rs:5 for v.each |&amp;num| { ^~~~~ What's up with that error message saying the compiler expected "[type error]"? Should it maybe just print the second error and not the third? This one is an example from Armin Ronacher's slides: fn main() { let s = [1, 2, 3, 4].map(|&amp;x| (x * x).to_str()); for s.each |&amp;item| { io::println(item); } } It doesn't compile because Rust can't decide which integer type's to_str() method to call. Candidates are core::{i32,i16,int,u8,i64,u64,u32,uint,u16,i8}. Bug? Or code needs updating? I've been trying to update [my solution to that concurrency challenge 3 months ago](https://gist.github.com/graue/5282602). Fixed a few of the errors, but so far, stumped by "the type of this value must be known in this context". Ideas appreciated. **Edit**: Solved, thanks to luqmana on GitHub - I needed to remove the capture clauses. I also thought it would be cool to get kibwen's solution [that implements its own futures library](http://www.reddit.com/r/rust/comments/165aml/my_attempt_to_solve_a_concurrency_challenge_in/c7ta0mx) working, but: future-adders-kibwen.rs:15:0: 18:1 error: trait bounds are not allowed in structure definitions future-adders-kibwen.rs:15 struct Promise&lt;T:Owned+Copy&gt; { future-adders-kibwen.rs:16 priv port: Port&lt;T&gt;, future-adders-kibwen.rs:17 priv mut value: Option&lt;T&gt;, future-adders-kibwen.rs:18 } Finally, does anyone else find `assert!(...)` a little confusing? It looks like "assert not", and the difference between `assert!(some_condition)` and `assert!(!some_condition)` is a little hard to see, for me.
The REPL seems to leave 2 "dangling allocations" for every (non-empty) input line here: $ rust sketch WARNING: The Rust REPL is experimental and may be unstable. If you encounter problems, please use the compiler instead. rusti&gt; 1 1 rusti&gt; 2 2 rusti&gt; 3 3 rusti&gt; exchange heap not empty on on exit6 dangling allocations$ Tried on OS X Mountain Lion, 64-bit. Built with Clang by editing the Homebrew formula (just updating the referenced source and removing patches).
Are the docs (tutorial/guide) updated for .6 ? I need some reading material for my flights tomorrow.
From a cursory look, yes, the "trunk" links on the home page seem to be up-to-date.
`pure` in Rust was never the same thing as referential purity in other languages (it definitely didn't prevent any side-effects), which became a neverending source of confusion. It also led to complications with several other mechanisms. Something like `pure` might re-emerge for Rust 2.0, but that's TBD.
Here's an issue to keep an eye on, then: https://github.com/mozilla/rust/issues/4707 All of those *should* be settled by 0.7.
You've got it good! On a recent checkout of incoming on 64bit Linux: WARNING: The Rust REPL is experimental and may be unstable. If you encounter problems, please use the compiler instead. rusti&gt; 1 rusti: $HOME/rust/src/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldELF.cpp:221: void llvm::RuntimeDyldELF::resolveX86_64Relocation(uint8_t*, uint64_t, uint64_t, uint32_t, int64_t): Assertion `RealOffset &lt;= (2147483647) &amp;&amp; RealOffset &gt;= (-2147483647-1)' failed. Aborted (Unfortunately, the REPL is absolutely full of problems that have to be addressed.)
No, it is supposed to be `assert!`. `fail_unless!` is just for the smooth transition (keep in mind that Rust compiler is written in an older version of Rust, and also `assert!(...)` may be parsed as `assert !(...)` in the presence of `assert` syntax) and should be gone soon.
Another I'm having trouble with is this: extern mod std; use core::num::Num; fn find_even&lt;T:Copy+Num+cmp::Eq&gt;(vec: &amp;[T]) -&gt; Option&lt;T&gt; { let zero: T = Num::from_int(0); let two: T = Num::from_int(2); for vec.each |x| { if *x % two == zero { return Some(*x); } } None } fn main() { io::println(match find_even(@[1,2,3,5,7,9,10]) { Some(x) =&gt; int::to_str(x), None =&gt; ~"No even numbers" }); } I forget who wrote this code or where I got it, sorry! With the `Num` trait gone, I'm not sure how or if I can use the [remaining numeric traits](http://static.rust-lang.org/doc/core/num.html) to accomplish this. Ideas?
In Vim, `assert!` is highlighted in the same color, while `!` operator is highlighted in another color, so I don't find it confusing.
We discussed the `assert!(...)` issue a *lot*. Eventually it was decided that syntax highlighting will usually solve the issue, and most of the time people will use stuff like `assert_eq!()`, which doesn't have that problem.
`Num` should return; it's only gone because generic trait inheritance is busted at the moment. Use the `FromInt`, `Modulo`, and `Eq` traits.
Got it. Here's where I am now: [failed to find an implementation of trait core::num::IntConvertible](https://gist.github.com/graue/5283546).
You don't have to recompile LLVM when you fetch commits or make changes, it's a one-time thing every few weeks when it gets bumped.
OpenCL-style vector API would be seriously awesome.
Is there any actual thinking regarding 2.0 (e.g. time frame), or is it just a hazy theoretical shiny object at this point?
No time frame, no. Right now it's just a hazily-defined bundle of features that might be nice to have but aren't important enough to make it in for 1.0, such as an effects system (the aforementioned replacement for `pure`), higher-kinded types, and datasort refinements: http://smallcultfollowing.com/babysteps/blog/2012/08/24/datasort-refinements/
Hmm. Will 2.0 be backwards compatible? Is it the first opportunity after 1.0 to make backwards incompatible changes, or the first opportunity after 1.0 to add features *at all*? (Correspondingly, if there's a 1.1, will it have new features or only bugfixes and so forth?) I had been working from the assumption that major version number bump =&gt; backwards incompatible changes, but the examples you cite in many cases seem like they could be added without breaking compatibility, so maybe that's wrong.
I don't see that for "floor" either. "sqrt" performance should be affecting a lot of real world code, though.
Uh, this is an interesting observation. I'll put a note in the bug report about this. We should consider carefully what these intrinsics are doing. They all may be using some unknown amount of stack.
Still no mention in the release notes about `pure`…
`pure` is dead, and good riddance. Something better (i.e. something that actually does what you think it does) may re-emerge for Rust 2.0.
I'm fine with `pure` going away if it's not helping, but a significant part of existing code won't compile without removing it (even if you didn't bother marking every pure function, sometimes you were required to), and the notes won't mention it. Fortunately, though, the compiler gives a fairly helpful message, rather than some cryptic "unexpected identifier".
I'm usually loathe to do this for fear of forgetting someone, but I really want to give props for some of the most memorable community contributions to 0.6. * strcat, for his complete overhaul of Rust's container types and refusal to be overshadowed by the STL. * z0w0, who took the initiative to create rustpkg and at last nail down the details of a system that we've done without for far too long. * pfox__, whose infinite patience is all that keeps libuv in check. * Luqman, for, among other things, the out-of-nowhere awesomeness that is `asm!()`. * bjz, for continuing to pseudo-coordinate the efforts to make Rust an awesome language for graphics programming. * sanxiyn and the whole Samsung crew, for their huge architectural contributions and contagious enthusiasm. I haven't slept in 37 hours, so that's my excuse if I've forgotten to mention you! You're all awesome! Now let's keep up the pace and make 0.7 even better. :)
Also it's available in arch linux: https://www.archlinux.org/packages/community/x86_64/rust/
Not really. Servo code is actively following the recent changes in Rust, and any production code should also do so at this moment. That said, 0.6 is said to be the last version to make the major language decisions and the situation will be hopefully improved.
Here is a follow up http://smallcultfollowing.com/babysteps/blog/2013/04/03/associated-items-continued/
What exactly will supplant *pure*? I do remember hearing something about some kind of assertion system (that can for example assert if function is pure), but do you know anything more than that?
I would probably prefer to have similar syntax for associated constants and normal struct members. That would allow me to switch back and forth at minimal cost. In other words I would like the implementations of Vector and FixedVector to be a similar as possible. trait Dim { fn get_dim (&amp;self) -&gt; uint; } struct Vector &lt; T &gt;{ N : uint, data : [ T ] } impl Dim for Vector &lt; T &gt;{ fn get_dim(&amp;self) -&gt; uint{self.N} } struct FixedVector&lt;T, N ?: uint&gt; { &lt;- Possible future syntax data : [T, ..N] } impl Dim for FixedVector&lt;T, N ?: uint &gt; { &lt;- Possible future syntax static N : uint = N, fn get_dim(&amp;self){ FixedVector&lt; N &gt;::N} } On the other hand I can see why the functional approach works well for generic traits. Perhaps the best solution is a hybrid approach. BTW: It would be supercool to have a trait that allowed me to refer to v.N even if v is a FixedVector. That would allow me to switch from Vector to FixedVector without having to do add getters and setters to the rest of the codebase. Python uses "properties" to overload getters and setters. It would be supercool to have something similar in rust. (Is this equally possible with both approaches to associated constants??) http://docs.python.org/2/howto/descriptor.html#descriptor-example
I'm not exactly certain myself, the most that I know is that it could be similar to Scala's "effects system", as described here: https://wiki.scala-lang.org/download/attachments/1310722/effects.pdf
There are also some breaking changes guaranteed for 0.7, but that's only because they ran out of time for 0.6. However, we're only talking about language-level changes here: I expect *library-level* breaking changes will continue to happen until 1.0.
Here's my disqus comment: I don't think I fully understand... what would accessing methods look like if I have a generic trait, with a generic type. For example (from lmath): pub trait BaseVec&lt;t&gt;: Index&lt;uint,t&gt; + Eq { fn from_value(value: T) -&gt; Self; fn to_ptr(&amp;self) -&gt; *T; static LEN: uint; } pub trait BaseVec3&lt;t&gt;: BaseVec&lt;t&gt; { fn new(x: T, y: T, z: T) -&gt; Self; } pub trait NumVec&lt;t&gt;: BaseVec&lt;t&gt; + Neg&lt;self&gt; { static IDENT: Self; static ZERO: Self; } pub trait NumVec3&lt;t&gt;: BaseVec&lt;t&gt; + NumVec&lt;t&gt; + Neg&lt;self&gt; { static UNIT_X: Self; static UNIT_Y: Self; static UNIT_Z: Self; } #[deriving(Eq)] pub struct Vec3&lt;t&gt; { x: T, y: T, z: T } Also, what would happen when you access the items via an alias? type Vec3f = Vec3&lt;float&gt; Honestly, for conciseness I'd still do a type impl directly on the Vec3f type alias using a macro (see [lmath](https://github.com/bjz/lmath-rs/blob/master/src/vec.rs#L1204) along with [an example of them in use in rray](https://github.com/luqmana/rray/blob/master/scene.rs#L101)). I don't see myself using such complex type paths in ordinary code, as it's simply too noisy. I don't have any solutions to this however.
I don't believe so, as anything to do with networking is going to be really grody until the new scheduler lands (and I'm not sure of the timeframe for that). You're welcome to give it a shot, though. :) Is there an existing mail lib that you were thinking of mimicking?
Not really, although for basic SMTP/POP I'd naturally gravitate towards mimicking Python's libraries since those are the ones I've used recently. The networking parts may be iffy, but on the other hand the actual 'putting it on the wire' is the small part, and it's all TCP/IP anyway; API design and implementing/testing the protocols are where the work is. I might as well get started as this'll be a 'learning Rust' thing for me, and I've been postponing it long enough now. 
That would be cool. See also https://github.com/mozilla/rust/wiki/Library-editing
Ok, I think I'll go ahead and do a bit of research first, and then I'll go fishing on the dev list for other interested parties. 
I'd prefer to have MIME before SMTP/POP/IMAP. As already pointed out, networking is uncertain, but MIME (MIME type, quoted printable, base64, internationalized header, multipart message, etc., basically what Python's `email` does) should be doable right now. PS. Oops, we'd need character encoding support before attacking MIME...
There seems to be some support for base64 in libstd. https://github.com/mozilla/rust/blob/master/src/libstd/base64.rs
Ouch, encodings is an area where I really have no experience beyond that of a user (of åäö). 
This isn't an apples to apples comparison - the C and Go code is explicitly choosing to use 8-bit and 16-bit integers (and 32-bit implicitly) where the Rust code uses 64-bit (on x86_64) integers. There are also two ~ sigils on arrays in the `solve` function that slow things down a bit and can just be removed.
Could not we keep the current syntax, that is: Trait::foo Trait::bar(x) Trait::AssociatedType But, if `Type` implements `Trait`, also allow this syntax: Trait&lt;Type&gt;::foo Trait&lt;Type&gt;::bar(x) Trait&lt;Type&gt;::AssociatedType 
The people over at programming [experimented with it](http://www.reddit.com/r/programming/comments/1bs479/performance_of_rust_and_dart_in_sudoku_solving/c99limx).
If you're going high level, look at APL.
Doener has done some optimizations too: http://www.reddit.com/r/programming/comments/1bs479/performance_of_rust_and_dart_in_sudoku_solving/c99s6fl
I haven't been following Rust as closely as I'd like, but could you briefly say what `pure` was and why it was removed? (I recall reading about it a while ago and thinking, "Oh that's cool." But I've since forgotten the details.)
Ah, OK. That clears it up---thanks!
This comment is now (mostly) deprecated :).
Curious about this code: if (by &gt; 0) { self.position += by as uint; } else { self.position -= by as uint; } This is a simple error in code that doesn't matter for the example and should just be `self.position += by;` right?
I believe so, yes. EDIT: actually, I misread self.position as an int. I actually have no clue. :P
It isn't at all natural, since each has values that the other can't represent. You end up having to specify a bunch of fragile and often counterintuitive-to-many rules for how to handle all the boundary cases. Plus, since it's all implicit, a maintainer is less likely to spot incorrect conversions. Making it explicit solves these issues, since you can always see *that* conversions are happening and *what* conversions are happening. Edit: Oh, I see what you mean now. 2's complement negative numbers will behave correctly under addition and subtraction even if they're represented as uints as long as the bit width matches. Casting to uint doesn't change representation, so the posted code was incorrect.
I'm confused mostly because I'm not sure how uint subtraction wrapping works. Casting a negative int to a uint results in the most significant bits being one and therefore a very large number. Subtracting will then cause a wrap. I poked at the options in rusti: rusti&gt; let mut tot: uint = 100; () rusti&gt; let x = -20; () rusti&gt; tot += x; () rusti&gt; tot 80 rusti&gt; tot -= x; () rusti&gt; tot 100 // Second Pass rusti&gt; let mut tot: uint = 100; () rusti&gt; let x = -20; () rusti&gt; tot += x as uint; () rusti&gt; tot 80 rusti&gt; tot -= x as uint; () rusti&gt; tot 100 Looks like everything works as mathematically expected.
&gt; else { &gt; self.position -= by as uint; &gt; } If `by: int, by &lt; 0`, then `by as uint` translates to `-by as uint`? That's kinda surprising-to-me behaviour. Doesn't mean it's wrong, it's non-obvious what `as uint` could possibly do in that situation if there's no option to signal failure somehow, and presumably the people involved have thought it through. But it would cause less face-scrunching if the code explicitly said `abs(by)`. 
It doesn't do `abs(by)`, it casts the representation which is defined as two's complement.
And I see what you mean, I was not suggesting to implicitly cast to `uint` to `int` (or the reverse) automatically, instead I was suggesting to have four overloads: uint + uint uint + int int + int int + uint (the result type is to be debated, but it's lot like you cannot have overflow/underflow with `int + int` or `uint + uint` anyway)
Signed integer overflow is well-defined in Rust.
Rust won't guarantee TCO, but in which cases will TCO be available? It's there for self-recursive calls now. I'd actually prefer a version of Clojure's loop/recur. It's much easier for non-functional programmers to grasp.
Patrick commented on the HN thread: &gt; Note that we do sibling call optimization per LLVM, and you can use trampolines like Clojure does. Sibling call optimization, most notably, includes all self-calls. This means that calls to the currently executing function in tail position will be tail call optimized, if there are no destructors in scope. &gt; The biggest problem here is ABIs: you have to use Pascal calling conventions instead of C calling conventions. It's also difficult when you have destructors: there are not many tail call positions when destructors are in scope. https://news.ycombinator.com/item?id=5527015
There is no support for C++ in the Rust FFI and I don't expect there to be any time soon. It's not impossible, but it's a huge rabbit hole. So far when we need to interop with C++ we first create C bindings, then create Rust bindings to the C bindings.
C++ FFI is difficult to do, but my hope is to implement C++ FFI to the level supported by D. Doing more than D doesn't seem useful (compared to effort). http://dlang.org/cpp_interface.html
ELI5? :)
"You can't have it" :P
The bit about what it is, not whether or not I can have it 
Does [this](http://stackoverflow.com/questions/310974/what-is-tail-call-optimization) answer your question?
Ah, sorry. Normally when you call a function "recursively", that is a function calls itself again and again and again ... at some point you run out of stack memory. Tail call optimization under certain circumstances alows the compiler to turn this recursion into interation, where the function does not call itself but is called one after another. Ommitting argument passing, something like foo(foo(foo(foo()))) would be optimized behind your back to foo(); foo(); foo(); foo()
Wow, those are great news. I think this implicitly means Servo is a probable future successor of Gecko.
I listened to this presentation in the room and thought what a curious project. What a world, what a world.
Interesting. Thanks :)
That was in april last year?
I will totally help in this effort. Otherwise, we're going to end up having to write a C wrapper for SpiderMonkey when we want to use any more recent version in Servo than we currently do.
Did you see the news about Mozilla and Samsung? Seems like it's very possible.
Cool. I'd thought this would be fun. Any comments on the good/badness of using the GC?
More info please :(
That would be worth investigating; I know nothing of its capabilities.
Very interesting! I'm excited to see where this goes.
Yes. Robert O'Callahan wrote about it here: http://robert.ocallahan.org/2012/04/korea.html
What problems did you find?
Sorry, i am not the author and have not tried to compile it yet. Maybe create an issue?
The freetype bindings has a #[cfg(target_os = "linux")] so it probably only works on Linux currently.
The fact that Only virtual functions are callable against C++ classes seems like a tricky limitation to me.
Rust does have a fully functional C++ compiler to leverage (libclang) so I don't think it's entirely the same situation as D.
Well, Rust *will* have libclang, once someone decides to actually use it for something and gets it added back into the tree. It's a shame, I was actually using Rust's libclang to avoid having to install my own copy of Clang for use with bindgen.
I'd love some help getting this running... :( I can't get it to build.
You could join the IRC channel, if you have not already.
Yes, one of the long-term goals is to get the windows port running on MSVC rather than MinGW.
A curious question, given that Obj-C has had closures (called blocks) for a while now, without GC.
How about webGL?
WebGL is based on OpenGL ES, not OpenGL.
Discussion seems to have ended up at /r/programming: http://www.reddit.com/r/programming/comments/1ca48z/a_hard_case_for_memory_safety/
&gt; a GitHub bot called Bors handles dispatching builds for patches that have been reviewed and merging pull requests that have passed tests Just so nobody gets confused, Bors isn't a service provided by Github; rather it's just a script by Graydon that scrapes comments on pull requests in order to automate the merging process. It's also a bit different from continuous integration: rather than running the test suite against each pull request, it merges pull requests one at a time and *then* runs the test suite. It's obviously a lot slower than traditional CI, but it ensures that the tree is green 100% of the time.
Great news all around.
For those not familiar with Haxe, one of the upsides of this is it gives us access to a whole host of awesome game-related libraries from that community. e.g. Nape physics: http://napephys.com/
It also provides for simple examples of the standard db API.
I think it's a mistake because there's no way to swap it out for another database 5 years down the road without dropping the whole API. In Rust, I don't think SQLite's query language and manifest type system would be very appealing. I would only end up building a layer on top of it that wouldn't be much different than doing it with a key-value store. LevelDB is also an indexed, ordered database with atomic commits and snapshots (a lower level than transactions), but you couldn't just drop it in to replace an SQLite module.
XML, JSON and HTML are data formats that will be widely used by lots of Rust programs. Having them in the standard library helps to make third party libraries that use them interoperable. I don't like the idea of having xpath or DOM standard in the standard library, because approaching it with the type system is arguably much better. SQLite is a query language and a dynamic type system in addition to the file format. It's not nearly as widespread as a data format so I don't think you would lose anything by having it a `rustpkg` command away. (g)dbm is just obsolete, because the implementations that Python wraps aren't thread-safe or performant. It actually requires a global lock for the entire process, since it uses the same errno across multiple databases.
Wouldn't it just smart to follow Haskells lead and just pre-populate the rustpkg repository with a few libraries that are considered "core" but are still available over the normal process? Thus you can add new ones and remove old ones, just when removing rustpkg needs to know "where to get them" if "legacy" (strange in the context of Rust) libs are needed :-)
wait, what? node.js has libraries? i thought node.js was a library. am i woefully misinformed about what node.js is? (i thought it was a callback based IO library.)
I believe the python sqlite lib follows the python database bindings pip, so it uses the same methods and whatever as MySQL, firebird or any other database that co forms to the pip, level db could be made to fit that too, but since sqlite and leveldb are not really the same type of database it would be hard to anyway
It's more of a platform/environment. It's the v8 runtime + some core C libraries + JS libraries built into it, which are all designed around the async IO you're describing. If someone was talking about a library for node.js, they would mean either a JS module designed to work with other parts of this platform, or a C/C++ library that has bindings that can be used from this platform.
I am not sure it is as clear cut though, the issue with having all those libraries is dependencies hell. Not the fact that you need to download a dependency (nobody cares about that), but the fact that you need to coordinate the dependencies: what happens when two different libraries suddenly use incompatible dependencies ? Oh, of course, *just fork it* right ? Except that I have a lot of code to work on already, so making a fork is not that desirable. First, it takes time (one-shot) to port, and then it takes time (regularly) to maintain it up to date. Notice a pattern there ? And I have other things to do! Now, it does not mean that I completely shun the idea of independent libraries (that would be silly), however I do feel that just being *minimalistic* is not a good idea: you're throwing the baby with the water there. As an example, let's look at C: do you know how many incompatible *string* libraries exist in C ? And how much effort (and CPU/memory) is spent converting from one representation to another when gluing together two libraries ? You don't see that issue in Java because `String` is included so everyone just settled for it. So let us keep in mind that developers also look for *convenience*, because they have a *problem* to solve, and those dependencies issues are just *getting in the way*.
If the theorem is something like "in the presence of sufficiently good package management, the incentive to minimize dependencies goes away", then Haskell's example can't be used in favor of nor against it, because Cabal is definitely not a sufficiently good package manager. (They say it's not even a package manager, though I don't know what the(ir) definition is.) Edit: I guess there are three questions. * Is sufficiently good package management possible, * Does/can/will rustpkg achieve it, * Does the theorem hold.
&gt; what happens when two different libraries suddenly use incompatible dependencies ? In Node, this Just Works. It loads both versions and each library uses the one that it needs.
cabal is indeed not a package manager, but ghc-pkg is.
&gt; Not the fact that you need to download a dependency (nobody cares about that) I care about that. I care that to use your utility I not only need to install your interpreter, I also need to install your interpreter's package manager and do everything through that. I care that instead of just using `apt` and being done, I also have to use `cabal` and `pip` and `gem` and `npm` and `pathogen` and who knows what else I've forgotten is sitting on my hard drive with half out-of-date packages that probably have security issues. I care about all of that. I probably even care enough to just not use your utility.
Many languages have standardized generic database APIs. Python [PEP 249](http://www.python.org/dev/peps/pep-0249/), Perl [DBI](http://dbi.perl.org/), Ruby [DBI](http://ruby-dbi.rubyforge.org/), Go [SQL](http://golang.org/pkg/database/sql/), Java [JDBC](http://www.oracle.com/technetwork/java/javase/jdbc/index.html). Most do not ship with implementations of these APIs, Python being the exception with SQLite. SQL has been around longer than XML, JSON, and HTML and will continue to be around for years after we've moved on to other data formats. To borrow your words, SQL will be widely used by lots of Rust programs. Having an implementation in the standard library helps...you get the point. That said, I agree with you that SQLite should probably be kept out of the standard library, but for different reasons. I think the standard library should be Rust-only code as much as possible, not bindings to code written in unsafe languages. I certainly don't fault Python for including SQLite though, from a useability perspective Python is the winner in this area.
What is 'liveness'?
Hackage and Cabal are good enough that you can auto-generate packages for a distribution with a buildbot instead of packaging everything manually. For example, [this project](https://github.com/archhaskell) generates Arch packages from hackage and has all of the building/updating automated. There's a certain extent of patching the names of external dependencies in the cabal files to match the system ones, but it's not too bad. The system package manager makes dealing with the dependencies a non-issue.
Whether variables are initialized and not moved from. I think it currently has a concept of uninitialized, alive and maybe-alive but not "dead". There are more details here on how it relates to moves: https://github.com/mozilla/rust/issues/5016
SQLAlchemy uses the DB API in PEP 249. While SQL dialects may vary, the DB API rarely does: connect, execute statements, get result sets. Why would you not standardize the DB API? What benefit do you get from every database having a completely different API? Having no standard would certainly make building things like SQLAlchemy or ActiveRecord extremely difficult.
What the beginning made me think of: http://xkcd.com/605/
Python has a really nice thing in the standard library called shelve. It's basically just a Python dict backed by a file. I use it all the time for scientific code: I have a small wrapper called persistent_memoize that you use like this: @persistent_memoize("filename") def fib(n): if n &lt;= 2: return n else: return fib(n-1) + fib(n-2) Basically it will memoize the function and it stores the memo table in the file. This means that I can experiment and re-run the function, and if the value has already been computed at some point in the past it goes very fast (my function calls can often take a few minutes or even hours). If you want to plot some memoized function you just plot it and the memoization will make sure that if you change the plotting code it will not recompute everything from scratch, and you didn't have to do any explicit DB management or manual file mangement: it's just one line and you're done. Just throwing this out there because I've found this very useful and the discussion here reminded me of this. Hopefully somebody else might find this useful.
Well, except if the libraries they depend on are exposed in the public interface. For example you have library Foo which returns a data type from library Baz v1 and a library Quux which takes a data type from library Baz v2 as an argument. Now you can't pass things returned from Foo to Quux. Of course this doesn't mean that a minimalist standard library is bad, but it does mean that you need some kind of coordination for the community provided libraries.
I must say, Rust looks pretty interesting from a performance standpoint... I've been absolutely head-over-heels for the go language for at least two years now (especially since go version 1 and the antiquation of makefiles). That being said, I am really excited about both of these new systems languages.
Rust will be doing away with the makefiles in the not too distant future, don't worry! It's a necessary evil at the moment though. :(
Rather, I think that the "systems language" moniker has become ambiguous to the point of uselessness in the past decade or two. Hell, they've ported Linux to Javascript, but I don't think most people would consider it a systems language. :) Better would be to mention a more specific criterion that the language needs to meet. For example, "embedded systems language" or "hard real-time language". People probably wouldn't consider Go for either of those, but then again they probably wouldn't consider Rust either (at least not yet).
I can imagine something like this in Rust using syntax extensions as attributes: #[memoize("filename")] fn fib(n: int) -&gt; int { if n &lt;= 2 { n } else { fib(n-1) + fib(n-2) } } Of course, until there exists an external API for writing syntax extensions you'd have to add this to the compiler itself. But maybe this is useful enough that it ought to be built-in to Rust?
What would be the alternative?
[rustpkg](https://github.com/mozilla/rust/wiki/Rustpkg) is in development.
The second (and worse) part is [here](http://maniagnosis.crsr.net/2013/04/letterpress-cheating-in-rust.html).
&gt; Cabal is definitely not a sufficiently good package manager. I may be hyper-sensitive to this point, because I've never actually gotten cabal to work, but problems with it seem to be a significant chunk of Haskell questions I see. And a goodly fraction of those are dependency management problems.
I'll just toss this out there: perl: 120,548 packages / 26 years = 4636 packages / year On the other side of the question though, quantity does not equal quality. A number of years ago, I worked on a medium-small open-source Perl project that used no less than four modules for sending email; one shelled out to sendmail and the other three were packages from CPAN. All of those three quite competently spoke SMTP but none of them looked up DNS MX records or (IIRC) dealt with MIME messages (admittedly, I didn't need that, so I'm not sure about MIME support).
I do bash Go design decisions in what regards generics, but for systems programming, one can say that even pure ANSI C without language extensions is one.
I've got no idea what you're trying to say.
That contrary to what people think, even C is not a systems programming language if you restrict yourself to the standard ANSI C, without using any of the common language extensions. Try to do systems programming in C without intrinsics, storage declarations, inline assembly, memory mapped IO ports, all language extensions. Same applies to Go, or any other GC enabled system programming language like Native Oberon or Modula-3.
FYI: Code golfing refers to a coding sport of which goal is to minimize keystrokes (as if you minimize strokes in golf) of the program. It is notable since it is one of a few public servers where you can try compiling the Rust code online (besides from rusti in #rust). Anarchy Golf already supports a fair bit of programming languages, so it gives also a bit of publicity. By the way, I have heard that Shinichiro Hamaji (an admin) had trouble compiling Rust in the limited due to the lack of Debian packages (AFAIK); this may be a good reason to have them ;)
That's exactly the reasoning I had for submitting the patch. :) I'm also planning to implement `deriving(Rand)`, so that it's even easier. (However, data structures often have invariants/substructure that's not in the types so this would only be useful in certain situations.)
I just opened [#5995](https://github.com/mozilla/rust/pull/5995) which makes the following valid: let _many : ((), (~uint, @int, ~Option&lt;~(@char, ~(@bool,))&gt;), (u8, i8, u16, i16, u32, i32, u64, i64), (f32, (f64, (float,)))) = rand::random(); (separate comment to give you a notification :) )
Nice
1) "Before the end of the year." Expect the release announcement to occur eight minutes before midnight on New Year's Eve. 2) "Most" of the backwards-incompatible syntax changes are expected to be complete by 0.7 (the upcoming release, due in a few months). But no promises of backwards compatibility are being made until 1.0. 3) Pick one of the following for your programming language: deterministic destructors, or guaranteed tail call elimination. You can have both, but it starts getting nasty. Furthermore, the code generation platform that Rust is using (LLVM) penalizes tail calls. Furtherermore, TCE introduces complications when dealing with anything in C. Furtherestmore, it's always possible to manually convert tail calls into either a loop or a state machine. Furtherestermore, trivial cases of TCE (such as calling yourself) are already supported in certain circumstances, just not guaranteed. 4) It depends on what your uses are. I see Rust as more of a back-end language (e.g. powering your http servers and databases) rather than a middle-end language (e.g. Ruby, Go). But I'm sure some people will have interest in such a thing, just as some people like to write web applications in Haskell. 5) It's done broke. JITs are hard. 6) No great tutorials so far, the best is probably http://static.rust-lang.org/doc/tutorial-tasks.html (which might be dated, for all I know). 7) `@` *can* often require you to think less about pointers, but you can run into problems if you want to use them with concurrency. There does exist a theoretical performance penalty (GC overhead). There do exist times when `@` is necessary, such as when an object can contain cyclic references (i.e. if it's possible for the object to reference anything that can ever itself reference the object). Try http://pcwalton.github.io/blog/2013/03/18/an-overview-of-memory-management-in-rust/ for a nice tutorial. 8) Yes! Look at the "Easy" bugs on the bug tracker: https://github.com/mozilla/rust/issues?labels=E-easy&amp;page=1&amp;state=open (don't be intimidated, some are easier than others) and come ask the developers questions if you need any help, either in IRC (#rust on irc.mozilla.org) or on the mailing list ( https://mail.mozilla.org/listinfo/rust-dev ).
If have not read that yet, you should find this interesting.http://winningraceconditions.blogspot.ca/2012/09/rust-0-index-and-conclusion.html 
I tried fiddling with RUST_THREADS, but with no real effect. The scheduler seems to be doing its job in the first, -wide, versions, since it keeps 6 CPUs at 100% for the duration. On the other hand, the CPU time (as opposed to wall-clock time) is about 6x the total sequential time: dictionary lookups may not be the bottleneck there. I expected poor behavior from the second version (anagrams-vectors-tasks), since it is pretty chatty, but the surprise was the -wide versions, since those kick off 6 workers that do all of the CPU work with 1/6 of the complete dictionary each. The only communication with the master task is when the workers are done and reporting their results. I wouldn't expect concurrent data structures would help much. Am I correct in believing that send() blocks until another task calls recv() on the same stream? There is no message queueing? There is good news there, though. The sequential Rust versions are already as fast or faster than the sequential C binary search algorithm, and they're much nicer to read. Further, I'm beginning to suspect that evil magic is involved in the C hashing version. Thanks for your help! P.S. Why are you wasting time on Reddit when you could be working on that new scheduler?! Get back to work! :-)
`send` does not block. I've also seen lots of loads where the CPU was only ~50% utilized when I expect them to be 100%. I imagine there's a major bottleneck in the scheduler somewhere. Last summer there was a huge multithreading performance regression that was never investigated. I'll build your test case today and see if there's anything obvious that can be fixed easily, but at this point I am focused on the scheduler rewrite and am not much concerned with fixing the current performance. I would like to know what is causing this big inefficiency though so we can not do it again.
If i start with anagrams-wide, make it not parallel by removing the call to `spawn` I get a real runtime of 23s. If I then insert the `spawn` I get a runtime of 6s. So in an apples-to-apples comparison there is a noticable speedup here. The user time does go from 23s to 33s though, indicating that there's a lot of overhead in spawning and synchronization.
You don't at the moment, it's simply a uniform(0,1) random variable.
I feel tempted to say "wow, this seemed a lot easier back in C++", which probably isn't a good sign.
It is easier in some sense to not have to worry about typechecking templates up front. (Though the disadvantages outweigh the upsides in my opinion—template error messages, SFINAE used to work around the lack of concepts, etc.). But when concepts come on the scene C++ is going to have to come up to solutions to all of these problems too.
Example: rustc --target-feature +sse2 -S test.rs Graydon said in the original issue: &gt; Currently we generate code for LLVM targets with target-specific features (subtargets, use of specific ISA features) on or off based on defaults. In some cases the defaults are ok, sometimes they're sniffed from the host (!) but in any case the user has no way to do something like "build me an x86 binary with no SSE". This matters for old hardware, and will matter much more when targeting **phones**. Another nice contribution by Samsung.
The memory management probably has a lot to do with it. You're never going to get the same sheer development velocity as a language where everything is simply boxed on the heap and you don't need to think about memory management for even a moment. Rust is really more focused on helping you prove that your code is safe and efficient rather than helping you rapidly prototype.
Yes, this was necessary to actually see NEON instructions generated for my SIMD patch.
&gt; The new `rt::io` module is being designed to avoid this. See https://github.com/brson/rust/tree/io
Great, will the whole rust stdlib eventually be made completely GC and @ free? I see that as a prerequisit if we want to able to replace a C project with rust for example.
I'm not sure all the stdlib needs to be completely GC and @-free, it can make sense for "higher-level" modules to use GC, managed pointers and tasks. But a (large) subset of the stdlib, especially the lower-level stuff which everybody'll use, should be managed-free and advertised-so. But that's mostly for when Rust itself will be able to run without @, which is not quite there yet afaik
Would it make sense to also have a `#[target_feature(foo)]` attribute, seeing as how keen Rust usually is on eliminating command-line compiler flags?
As much as we can. There are, however, things that are impossible to write without GC, reference counting, or unsafe code, such as doubly linked lists. I haven't decided whether we should use reference counted smart pointers or GC—reference counting is going to be slower in a managed environment, but it avoids pauses (sometimes).
Off-topic, but why is python AND perl required to compile? Why not just stick with one?
If I recall, it's LLVM that has the dependence on Perl and Python, not Rust.
I know Python is used extensively for the test suite... but I did not remember Perl was used too. Is it for output coloring or dependencies generation ?
See also Graydon's further thoughts: https://mail.mozilla.org/pipermail/rust-dev/2013-April/003694.html
All that http://llvm.org/docs/GettingStarted.html#software is willing to divulge is that Perl is used for "utilities".
This was probably a lost battle from the point on where trait bounds were introduced and put between the angle brackets, but I think function signatures are getting fairly unwieldy. In haskell or C++, the bits that declare that the function is gonna be polymorphic are a prefix to the signature, I think that's a bit easier to mentally parse than `fn name&lt;huge string of type babble&gt;(parameters)`. Also having to introduce auxiliary names out of line instead of having a convenient canonical way to refer to impls makes the whole thing feel a bit not-first-class to me. Those names sound like they're always going to be a bit awkward and ad-hoc, like all the good naming juice is already used up for the type and the trait in the first place and there isn't really any semantic information left to put into the name you bind the impl to. I suppose the need for this is already only going to be with weird edge cases, and there probably isn't a way out of this that isn't somewhat embarassing, and I certainly don't have a better proposal, so this is completely subjective but it's kinda putting me off generics more so than the weird bits in C++ did. Edit: I think I'd like to be able to refer to things in an impl by saying `path::to::Trait&lt;path::to::ImplementingType&gt;::trait_member&lt;MoreTypeParams, ....&gt;` and bind the `Trait&lt;Type&gt;` to a name with `use`, vaguely in memory of typedeffing C++ template specializations, but that's probably a kinda skewed perspective and also completely unworkable with the language as it is ;) Edit 2: Also doesn't consider parameterised traits
These are the .pl files in src/llvm: ./utils/profile.pl ./utils/UpdateCMakeLists.pl ./utils/findsym.pl ./utils/test_debuginfo.pl ./utils/GenLibDeps.pl ./tools/clang/utils/analyzer/update_plist_test.pl ./tools/clang/utils/analyzer/reducer.pl ./tools/clang/utils/TestUtils/pch-test.pl ./tools/clang/test/make_test_dirs.pl So a bit of a mixed bag, but mostly test suite.
Decent nice high-level article about Rust!
I'd be interested to hear about how this went and what you were talking about.
I need to write a report (in Korean) anyway, so I will try to post at least a summary in English. No promise though.
It amazes me that the parser can be quite *that* slow.
I'm just happy that a formal grammar (allegedly) exists at all. Many other languages that started out with bespoke parsers (e.g. C++, Ruby, Perl (and maybe C?)) aren't so lucky.
Yeah, but then it comes again for the actual definition. It just gets all the type shenanigans out of the way first. :)
Upvoted for your username.
Ah, I had forgotten about the analyzer :( It seems the dependency could be lifted easily enough if anyone thought it was worth it.
I'd vote for "util".
I think one of the benefits is that any ambiguities can be discovered and hopefully fixed. Syntactic ambiguity is definitely a problem in other languages – C++, Javascript, Ruby etc. It'd be nice if Rust could avoid these issues. I think it'll probably also help with tooling in the future as well.
Specifically, see https://github.com/brson/rust/blob/io/src/libcore/rt/io/mod.rs#L24 for some ideas of what some potential use cases might look like.
[Follow up](https://mail.mozilla.org/pipermail/rust-dev/2013-April/003764.html): "yapps2 reports that this grammar is LL(1)."
Can anyone explain the reasoning for stdin() being a function?
At some point in the future it would be supercool to have default parameters in rust. That would allow for something like File::open(Path("diary.txt")) File::open(Path("diary.txt"), WriteOnly) File::open(Path("diary.txt"), Create, WriteOnly) Here is a link to a discussion that is possibly outdated https://mail.mozilla.org/pipermail/rust-dev/2012-August/002241.html 
For me, read_to_str implies some memory allocation for a string as opposed to just casting to string for read_as_string.
I wonder whether there is room for an n-ary-`select`-like API exposed by the scheduler. Sometimes I think it's concise to be able to write "now wait until I can read from either of these four connections or until this other task is talking to me over this pipe" without wiring it up with a dozen of auxiliary tasks each only blocking on one thing potentially even requiring more cleanup logic. Is that a reasonable pattern or would that go against the grain of the whole system?
Yes. Also would hep with things such as cleaning up the masses of str methods+functions.
Note *untested grammar* is LL(1). However there is a discussion to what grammar should be considered *official* now. The LL(1) grammar that Patrick came up with is not nice for direct exploitation (need to introduce some artificial rules to make it LL(1)) and therefore direct-AST construction from it produces a strange-looking beast. Personally I would favor having a *natural* LL(k) grammar expression as the official grammar (for low values of k) rather than an artificial LL(1) one.
Or possible overload ? On the other hand, as mentioned the idea of using generics (`OpenSpec`) and having a number of *tuples* implementing `OpenSpec` could also work. I am afraid it requires a lot of boilerplate so users won't want to deal with this though.
I agree with you, `select` is *just* necessary. However I would go further than talking about n-ary; ideally I think you would want to be able to `select` from any *iterable* type. For example a `vec&lt;Chan&gt;`.
Nitpicking: I would like to place errors at the end of a try-catch block. Here is an idea that might allow for a finalizer (closing the file after an error). let mut error = None; do File::with_file("diary.txt") |mut file| { file.write_line("met a girl"); }.catch( | e: IoError| error = Some(e); ) https://github.com/brson/rust/blob/io/src/libcore/rt/io/mod.rs#L160
No http = :(
Wait is that valid syntax / an in-use pattern right now? Looks kinda nifty.
One of the first things I want to do is write a simple HTTP client/server module, since people want to see that. I haven't put much thought into its design yet.
This would require with_file to return a closure so you could attach a method to it. You could also do something like do (|| { File::open("diary.txt").write_line("met a girl"); }).catch |e| { error = Some(e); } Which is fairly punctuation heavy. I suspect we'll ultimately end up with a macro for this.
We could special case `do` so that you could write do { ... }.catch |e| { } 
Maybe a solution would be to have an official LL(k) grammar, more readable for documentation, and publish separately a modified version for tools that require LL(1). 
I am actually quite confortable with the current *core*/*std* scheme. But the rationale for renaming them makes sense. However, I don't think that *std* conveys the idea of being part of the language. *core*, *base* or *rust* are better choices in this regard. For renaming the current *std*, I also think that *util* is the most appropriate. It really reflects the role that this module fulfills. My second choice would be *platform*, which is unfortunately a bit too long.
You can do it now. (Although the syntax isn't quite so nice.) struct EachElseRet(bool); impl EachElseRet { fn else_(&amp;self, f: &amp;fn()) { if !**self { f() } } } fn each_else&lt;T&gt;(v: &amp;[T], f: &amp;fn(&amp;T) -&gt; bool) -&gt; EachElseRet { let mut i = 0; while i &lt; v.len() { if !f(&amp;v[i]) { return EachElseRet(true); } i += 1; } EachElseRet(false) } fn main() { for each_else(&amp;[1,3]) |&amp;i| { if i == 2 { break } }.else_(|| { println("hi"); }) } 
Using "do" in addition to "for" you can get a bit closer to the original. fn main() { do for each_else(&amp;[1,3]) |&amp;i| { if i == 2 { break } }.else_ { println("hi"); } } 
Yeah, I though about that... but decided that the confusion of `do for` was worse than some extra parentheses. (Each to their own, of course)
For the record, this is brilliant. I might do this everywhere now.
Well... struct Stupid; impl Stupid { fn foo(&amp;self, _: &amp;fn() -&gt; bool) -&gt; Stupid { Stupid } fn bar(&amp;self, _: &amp;fn()) -&gt; Stupid { Stupid } } fn main () { for do do for for do for Stupid.foo { }.bar { }.foo { }.foo { }.bar { }.bar { }.foo { }; }
Yes, this is precisely the sort of goofy thing that I love doing. :)
Unfortunately, you then have **2** official grammars, and the risk of inconsistencies. pcwalton mentioned it during the discussion, but a tool that can transform LL(k) into LL(1) (even assuming it's possible) would be worth a thesis...
I'd be interested in talking about this, it's something I'd like to see in Rust.
Reminds me of a The Police song
The problem is that there is no switch to flip to get the really effective countermeasures. Case in pint: The authors of the cited paper did not run most of the examined tools because they are not publicly available. Even for the things that are just a compiler switch away (like PIE), some programs still need patching.
Having started with Turbo languages in the old days (Turbo Basic and Turbo Pascal), before going the C and C++ route, I have always been a devotee from strong typed languages. I mean the Modula, Ada, Oberon family. Sadly, thanks to UNIX, C took over the systems programming world, followed by C++, thus leading to the current situation. C++ helps, if one is able to use all the libraries and constructs that make it a better C, but the language is complex and remains unsafe by default. Given that the Pascal family, except for Ada, has been a victim of C's success, I am looking forward that D and also Rust manage to win the heart of new generations of system programmers. Specially given the issue that many young developers seem to mix strong typing with VM based languages, not understanding the difference between language and implementation. For C and C++, I currently advocate "-Wall -Werror" + Lint in our CI systems.
What are you using for linting?
Wow. That even beats Java's `"constant".equals(string)` in terms of ugliness.
I agree, I'm hoping we can get this addressed before 1.0.
I definitely would be interested in the answer to the match question: https://mail.mozilla.org/pipermail/rust-dev/2013-April/003875.html
Note that, as noted later in the thread, you can write `"constant" == string`, so it's better than Java. We just need to make the reverse work.
Answered it in the thread. It should be straightforward to fix.
Rust compiler's current (and I think future) stance on optimization is not to do it, that is, to delegate optimization to LLVM. In general, turning allocating code to non-allocating code would be a surprising and non-transparent optimization, and I think Rust has no intention to be that sort of better compiler.
Very nice writeup. A couple of things to note: * The UV interface will soon change, and in particular `uv_global_loop` will no longer be necessary, as the I/O will be directly integrated into the scheduler. * It will likely be possible to convert `~` smart pointers to `@` smart pointers, to make it easier to just use `@` everywhere.
Well, thankfully LLVM does it. Yes, really. For example if you write: int sum(int n) { int* array = malloc(b*sizeof(n)); int total = 0; for (size_t i = 0; i != size_t(n); ++i) { array[i] = i; total += i; } free(array); return total; } LLVM can (and should) detect that `array` is never read and can be eliminated. And thus also eliminate the paired `malloc`/`free`. So there are cases where allocation can be eliminated, and it might be able to special-case strings.
I don't know rust, but this post is right above the one that says &gt;~"string" is probably not what you want Yet its one of the first things the tutorial does. If this is indeed the wrong way of copying a string then please change it so others like me won't start thinking its the correct way.
Nice catch, I thought a lint tool would catch that.
Ow.
As the instigator who pushed the issue, I'm grateful to him for writing out a nice, thoughtful answer.
This is a nice reference. I added it to the [wiki](https://github.com/mozilla/rust/wiki/Docs)
Very exciting. This would eliminate one issue that everyone hits from the language. It would also mean that, for those who dislike smart pointer sigils and want a more C++ look, you could write: type shared_ptr&lt;T&gt; = @T; fn shared_ptr(x: T) -&gt; @T { @x } type unique_ptr&lt;T&gt; = ~T; fn unique_ptr(x: T) -&gt; ~T { ~x } type vector&lt;T&gt; = [T]; type string = str; And things would generally work.
Nice write up. OCaml, F#, Lisp and other FP languages can do well without 100% purity. 
What was the rationale for allowing the lhs, rhs, and result types to be different (contrary to how e.g. Haskell does it, but similarly to C++)? IOW what are the use cases? The one that immediately springs to mind is pointer arithmetic, but Rust doesn't do that much. Multiplying a vector by a scalar kind of thing?
It would be nice, but unfortunately because C/C++ is about independent compilation units (TU), this requires a fairly complex setup as only the function definition might answer whether a parameter is returned and this definition might not be visible at the call site.
It's the correct way of making an owned string, but it's not the correct way of making an otherwise constant value to compare an owned string with.
From what I've heard it works pretty well for Go (although I haven't seen any statistics as to how many allocations can actually be promoted… it might actually end up being a low percentage).
At first, I didn't realized "Hoare logic" was referring to Tony Hoare, and not to Graydon Hoare...
Matrix, vector, quaternion operations are big ones. There are many operations on mathematical structures that can have different LHS, RHS and Result types.
There are plans for this: https://github.com/mozilla/rust/issues/5992
The author keeps referring to Go interfaces as "pointers". A Go interface can actually be assigned any value, including but not limited to pointers. The article also glosses over a major difference between the Go and Rust systems. Go uses structural subtyping for interfaces, and methods are structurally part of the implementing types. Rust has explicit nominal subtyping of traits, and methods are structurally part of the trait and overloaded for each implementing type.
Rust trait = haskell type class + Java interface; type class is static polymorphism, which is much like C++ Concept, while interface is dynamic polymorphism; Rust trait can be static or dynamic , however, you only need to define a trait for a type once .... (sorry for my poor English)
[The ISSAC RNG is actually only about 10% slower than C with the addition of an #[inline] attribute.](https://github.com/mozilla/rust/pull/6073#issuecomment-17107369) (This is a fair comparison to the C, because the C executable was a single .c file, so inlining could easily have happened there, while Rust needs the push to get it happening cross-crate (I think?).)
That's right.
So are the android changes positioning rust to work on Firefox OS ? 
Would you care to explain why Rust needs a push ? Is it a matter of not exposing enough information (like the function definitions) to LLVM unless they are marked inline ?
Perhaps, but it's more likely a case of simply wanting to port Servo to Android in general.
The Android patches have been submitted by Samsung, who as far as I know have no official interest in Firefox OS.
Rust is designed to allow function definitions inside shared libraries to be modified without necessarily requiring upstream users of the library to be recompiled. However, this doesn't work if the functions are inlined. Because of this, you have to mark functions that you want to be inlined across crates with `#[inline]`.
This is correct. I did some investigation: [`rustc::middle::astencode::encode_inlined_item`](https://github.com/mozilla/rust/blob/master/src/librustc/middle/astencode.rs#L80) outputs the information required for inlining (e.g. the AST of the function), but it is only called for functions/methods that have an `#[inline]` or `#[inline(always)]` attribute.
C doesn't have any inlining at all between libraries or object files without LTO. It's all done by including the inline functions in every file with the C preprocessor (from a header, where it's defined as static inline). So Rust's ability to output the AST to crates is actually a nice improvement. To be clear, Rust doesn't need any explicit instruction for inlining within or module or modules in the same crate.
Flow sensitive borrow checking is really a huge improvement in the language since it's definitely much easier to use borrowed pointers now. No more unsafe code needed to implement the map types (mostly `find_mut`) and they're a lot more usable. :)
Is there a small example giving a Rust user an impression what is now possible which wasn't before?
The post seems like a sort of advertisement for dependently-typed programming languages. An advertisement is not necessarily a bad thing, but I wonder if it is relevant to the current discussion at all. Experience with languages with a compile-time overflow check *and without dependent types* would be much more helpful for this discussion, but I'm not aware of such languages. I understand the benefit of dependent types when it is adaquately implemented (I've used Coq a bit myself, for example), but the language design is full of trade-offs and dependent type is only one of possible (mutually exclusive) choices. ATS definitely sacrificed one thing (much more complex type checker for dependent types) for another (compile-time integer overflow check and other niceties). Rust may have to sacrifice one thing for another, but that another may or may not be dependent type.
Ah! I can see where this comes from, given the binary-compatibility I usually trudge through. Nice :)
I would not say that C++ cannot have systematic overflow checking, especially in the light of templates... *Disclaimer: the code below is "buggy" because `std::common_type` is not a panacea as there are weird rules in integer arithmetic that allow the compiler to pick out a common type that is insufficiently large to contain all values of both types. Inherited from C, as usual.* Let's have a simple class: template &lt;bool B&gt; struct enabler {}; template &lt;&gt; struct enabler&lt;true&gt; { typedef char type; }; template &lt;typename T, T Mi, T Ma&gt; class BoundedInteger { static_assert(Mi &lt;= Ma, "The minimum should be smaller than the maximum..."); public: typedef T UnderlyingType; static T const Min = Mi; static T const Max = Ma; BoundedInteger(): _value(0) {} BoundedInteger(T t) { set(t); } // Hey mah look, no runtime range-check since we know it's all good! template &lt;typename O, O Omin, O Omax, typename = typename enabler&lt;Min &lt;= Omin and Omax &lt;= Max&gt;::type&gt; BoundedInteger(BoundedInteger&lt;O, Omin, Omax&gt; o): _value(o.get()) {} T get() const { return _value; } void set(T t) { assert(Min &lt;= t and t &lt;= Max); _value = t; } private: T _value; }; // class BoundedInteger And then let's define addition (as an example): template &lt;typename L, typename R, L Lmin, L Lmax, R Rmin, R Rmax, typename C = typename std::common_type&lt;L, R&gt;::type&gt; auto operator+(BoundedInteger&lt;L, Lmin, Lmax&gt; left, BoundedInteger&lt;R, Rmin, Rmax&gt; right) -&gt; BoundedInteger&lt;C, C(Lmin) + C(Rmin), C(Lmax) + C(Rmax)&gt; { return { C(left.get()) + C(right.get()) }; } And of course we also need narrow-conversion, with bound-check. template &lt;typename Dest, typename Source&gt; Dest narrow(Source s) { typedef typename Dest::UnderlyingType U; typedef typename std::common_type&lt;U, typename Source::UnderlyingType&gt;::type C; assert(C(std::numeric_limits&lt;U&gt;::min()) &lt;= C(s.get()) and C(s.get()) &lt;= C(std::numeric_limits&lt;U&gt;::max())); return { U(s.get()) }; } And finally, one would provide convenience headers: typedef BoundedInteger&lt;signed char, -128, 127&gt; Int8; typedef BoundedInteger&lt;unsigned char, 0, 255&gt; UInt8; // ... Granted, this would amount to some insanity in the library to take care of all the edge-cases that C semantics let slip in. And probably some slow-down in debug code. On the other hand, the insanity would be contained in the library. And with user-literals, I can perfectly imagine typing `32_u64`, it's certainly easier to read than `32ul` anyway!
There are two examples here: https://github.com/mozilla/rust/commit/ebe35f3873b0ad48d48b009a871843e583584379 The borrow checker is now smart enough to understand that taking two `ref mut` patterns in a `match` is okay, and it no longer encounters the common flow-based errors. Before, if you took a temporary &amp; reference and then later an &amp;mut one in the same block, it would fail because it had no concept of an order of evaluation (error would call the `&amp;mut` loan a "prior" loan despite being later). It was just block based.
It's not an advertisement for dependently typed languages, it's an appeal for bounded integers and compile time overflow checking. The fact that I've only used these features in C++ and dependently typed languages is the reason for the examples. I don't really see how that "experience with languages with a compile-time overflow check _and without dependent types_" would be much more helpful since it's the utility of having support for the compile time checking that is the point. I can't help it if the only examples handy are from languages you're not interested in. The post presents examples in Gecko that have been problematic (including a security issue), a project that uses compile time overflow checking (AOS) and found it useful and a simple example, a bsearch implementation, where an overflow is reasonably 'hidden' but can be detected given support for checking in the compiler. Try concentrating on that vs "dependent types". I'm not arguing for that in Rust.
Sorry for misunderstanding your points. Still, I think you are arguing about must-haves instead of trade-offs. It is a nice goal to check integer overflows in a compile time but it may or may not be feasible in Rust depending on how it must be implemented. My general assumption (please correct me if I'm wrong!) is that a compile-time overflow check necessarily needs a fair amount of type system modifications (maybe dependent types), and the Rust type system (and static analysis pass) is already too complex to apply such modifications. In order to implement such check, we need either i) a way to implement a check without much type system modifications or ii) a plea of Rust developers for more complex type system. I think the latter is highly unlikely, so I asked for the former.
I'm by no means an expert in this area, but doesn't "pure" have a well defined meaning in computer science? I had thought it was along the lines of "doesn't rely on any thing but explicit inputs to the function, doesn't mutate those inputs in-place, but instead explicitly outputs any results. I didn't think "pure" necessarily had anything to do with compile time computability or parallelization except that it made both easier depending on the inputs. Am I wrong in this impression?
This C++ solution is what I call an obfuscating solution. Let me quote Tony Hoare to make my point clear: There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult.
My reply was more oriented to the fact that the author says that it's unfortunately possible to "forget" to call those macros or have the arguments backward or whatever. The problem I see is that if the checks are duplicated all over the place whilst using a `BoundInteger` class they would be insulated and audited once and for all.
Obfuscating ? Really ? Well, I'll politely disagree. I *can* agree that the library code itself is bound to be a little ugly; however does it matter ? Audit once, test the heck out of it (with fuzzers and whatnot), hell might be you can even manage to *prove* it. And then reap the benefits! `Int8 x, y; Int8 z = x + y;` will yield a compiler error because there is no (automatic) conversion possible from `BoundedInteger&lt;short, -32768, 32767&gt;` to `BoundedInteger&lt;signed char, -128, 127&gt;` (for obvious reasons). The code written using the library is not only clear, it's also safe *as long as* the library itself is safe, which certainly cuts down on the amount of code that needs be audited! If you think that assert macros all over the place are simpler and less likely to go wrong... I raise.
"N: We have special rules about ~fn and what it can close over. Might need to add ~fn:Owned to be explicit. B: Same for objects" Hmm, I wonder if this would fix something I encountered. It surprised me when I could not close over managed pointers with an ~fn, or contain managed pointers in an ~Object (dynamic trait). I wanted to use owned pointers to avoid some garbage-collection. I would think that being able to send a pointer to a different task (which will then also receive all that is reachable from it), and the ownership semantics of a single pointer are different issues.
Then how should one do x + y for non-constants x and y? 
Nice! It would be more rustic to use a `match` on [line 19](https://github.com/flipcoder/treegrep/blob/master/treegrep.rs#L19): match ch { ' ' =&gt; { indent += 1; } '\t' =&gt; { indent += tabwidth; } _ =&gt; { return indent; } }
It's in! thanks for the tip
But that is the point here, `x` and `y` are not constants, however their value is guaranteed to be within a certain range, and thus basic mathematical operations give us a guarantee as to the range of the result, and thus the result can be expressed in a type suitable for it!
The virtual currency bitcoin also had a [major issue caused by overflow](https://en.bitcoin.it/wiki/Common_Vulnerabilities_and_Exposures#CVE-2010-5139) back in the early days.
int8 n, i; if (i &lt; n) i = i + 1; Is this okay using your C++ solution? 
So basically, the result of every add/sub/mul/div/... needs to be explicitly coerced? This does not seems much better than just inserting run-time checks. No?
Are there any X bindings yet? (Or will you just use XCB?)
Indeed, this is the unofficial official term for idiomatic Rust, as I do so unofficially decree.
Your best bet might be to use bindgen (https://github.com/crabtw/rust-bindgen) to generate bindings to an existing XML lib.
Nope. Semi-intentional. Why does python have to get all the fun?
&gt; doesn't rely on any thing but explicit inputs to the function But what are those? When you get down to it, any Haskell function that uses `Int` is impure, because of unpredictable overflow behaviour: It's guaranteed to be at least 31 bits, and in GHC on x86 it's either 32 or 64 bits. Sure you can send a pure `Float -&gt; Float` function to the GPU, but are you *really* sure that the GPU is going to handle everything according to IEEE? The answer to these questions is, quite often: "Well, they're pure *enough*", so you'd want the compiler to treat them as such. But there's enough cases where the differences are vitally important. tl;dr: In theoretical computer science, there's no difference between practice and theory.
I recently put [bindings for expat](https://github.com/pascalj/rust-expat) on Github. Works nicely, although there are currently no Rust wrappers as I don't really need them for my project.
Seems like we're all pulling together to ward off "rusty".
probably rust-xcb Here's my old c++ one (unfinished): https://github.com/flipcoder/gridtop I never finished this version, altho it had some cool features: vim-like key bindings, window/animation easing, etc.
&gt; probably rust-xcb Ah, OK. Not native bindings. Perhaps native bindings will be my first Rust project this summer. &gt; Here's my old c++ one (unfinished): https://github.com/flipcoder/gridtop Aha! Neat.
Not necessarily. auto z = i + 1; is perfectly fine. And the type of `z` should be something like `Integer&lt;short, -128, 128&gt;`. And of course you could perfectly write `i += 1;` too, with a check in the implementation of `+=`. It is better than inserting run-time checks in that it is *exhaustive* and *correct*: you can neither miss a check NOR write an incorrect test (or forget to update it when you change the code). Just reads the news about a tiny patch in Diablo 3 auctions systems and its overflow bug to see how "just inserting run-time checks" can fail spectacularly.
I meant inserting run-time checks by the compiler. It is exhaustive and correct. For instance, the SML/NJ compiler (for SML) does it for all integer operations. 
There are requests from Windows 3.1 and Windows 95. I wonder whether we should support them as well? :)
Have you, by any chance, read something I didn’t write?
Is the hail of downvotes really necessary? SciK's post can easily be misread to the following effect: "I suppose you think we should support any old rubbish that people ask for?" Given that this is a common sentiment to encounter in linux-centric programming communities, it isn't surprising that scramjam leapt to the wrong conclusion. Just a misunderstanding.
Both SciK's and scramjam's posts have failed to be constructive. In the future I'll simply take more of an initiative in deleting comments like this when I see them. To address lw9k's original point: it's true that Rust is currently rougher on Windows than on the other two tier-1 platforms. However, Graydon has always, always, always, *always* been adamant that Rust will support windows as a native citizen. It's simply a matter of doing the legwork at this point. Volunteers are appreciated!
&gt;Graydon has always, always, always, always been adamant that Rust will support windows as a native citizen. It's simply a matter of doing the legwork at this point. Volunteers are appreciated! That's great. The problem is I have no idea how to help, since it requires rather in-depth knowledge of the compiler and runtime. That's why I want the core developers to address it, since it is an IMO high priority issue.
Well, the *other* problem is that, beyond compiler internals, it requires in-depth knowledge of the Windows platform and all the associated toolchains that entails. :) For instance, right now Rust runs on top of MinGW simply because that's the simplest way to achieve a working Unix-like abstraction, so porting is (relatively) easy. But the eventual goal is to move from MinGW to MSVC like a proper Windows citizen... and I honestly can't say if that's *hard* or not because I really have no informed notion of how extensive the differences are. If anyone out there happens to be a Windows expert, it couldn't hurt to offer your services on the mailing list (https://mail.mozilla.org/listinfo/rust-dev). Bringing expertise together is certainly the most expedient way to tackle this issue.
**I was silly, so the premise of this submission is wrong [see below](http://www.reddit.com/r/rust/comments/1e262v/rust_can_be_faster_than_c_for_random_number/c9wd8qb).** Came across this as part of my work on [improving random number support](https://github.com/mozilla/rust/wiki/Lib-rand). There's an even bigger speed-up when comparing the 64-bit variant of Mersenne Twister, but I don't think the reference implementation in C is particularly optimised. FWIW, compiling with `clang -O3` is actually slower than GCC (about 4.8-4.9s), so it's not just Rust's LLVM backend being fast. Ok, I've put the current code up [here](https://github.com/huonw/rust-rand).
Does it have the exact same properties as the implementation you reference?
 2086620983 real 0m3.074s user 0m3.064s sys 0m0.000s (Note that this actually generates a different sequence to the non-SIMD variant.) I've been watching the progress of the SIMD types so that I can implement SFMT in Rust... it's almost there!
Yep! They generate exactly the same sequence for all the seeds I've tried (e.g. the sum of the first 1 billion numbers starting with seed 1234 (the sequence in the gist above) is the same for both: `1040076681`).
Could you explain why `rand() * (35 - 20) + 20` (where rand() is uniform(0,1)) gives a random variable that's not uniform(20, 35)? (I hadn't encountered this, but obviously I'd like to avoid it.) But yes, I agree that that the next most important thing after non-buggy implementations of the RNGs/distributions is making it easy for users to be right and hard to be wrong.
Thanks for the bug report, I've nominated it as a blocker for 0.7.
My bad, I meant to refer to "classic misuse" of whole-number generators and [naively] using the correlated function to limit them - modulo. http://eternallyconfuzzled.com/arts/jsw_art_rand.aspx
Incidentally, for realistic ranges, the following should be much faster than what is proposed in your link (in c#; I don't know rust yet): int RandInRange(int low, int high) { var unusableValues = int.MaxValue % (high - low); var usableRange = int.MaxValue - unusableValues; while (true) { var candidate = myRandom.Next(); if (candidate &lt;= usableRange) return candidate % (high - low); } }
&gt;by clipping the maxvalue to one compatible with your range Yup, that's what usableRange is.
I was thinking this would be because Rust would include a parallel random number generator in the standard library. That would be a big selling point.
~~Nope, faster due to pure numerics.~~ Parallel in the sense of multi-threaded, or SIMD, or something else? (Currently, using random number generators concurrently in multiple tasks is safe, and [SIMD-oriented Fast Mersenne Twister](http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/SFMT/) is on my radar. But that's probably not what you're asking.) Do you have links to some parallel RNG algorithms?
Ah, yes, of course. I am/was going to do something along the lines of jpfed's suggestion. (Also, the wip source is [on github](https://github.com/huonw/rust-rand).)
Do you need to pass the 2nd argument to grep as a unique pointer? grep(pattern, ~"", in, tabwidth); I think grep(pattern, "", in, tabwidth); should work, since you're taking the 2nd argument by immutable reference in the function signature.
I'm glad to see this. We're pretty close to being able to ease those restrictions on failure, logging, etc. Probably within the next month we'll be able to set up that stuff manually on demand using the new runtime.
yep you are right, i'll fix that
Not familiar with SFMT, but couldn't you run four independent instances of the normal MT algorithm? One in each SIMD lane? I'd imagine most applications really care about amortized cost (if you need one random number, you probably need more), and the cost of getting a single random number wouldn't be much higher anyway... So every four invocations you'd compute four new values, and the other invocations would just return the "next" one from the last compute-run. You'd probably want to make sure you recomputed after returning the last cached value (that way you don't have any data dependency on the random number generation, you're just using a cached value). For 16 bit ints you could maybe do 8 at a time... Also, you could do even bigger runs each time to hide pipelining latency etc. (i.e. use all the SIMD registers you have). So maybe cache 16 random values or whatever.
.....huh, yes. I swear I used to be able to grok code...
What do you mean? The tutorial and references aren't enough??
I don't think they are enough. This language aims to be very complete, to replace C++, so it needs a lot more of updated resources. As an example, I'm trying to read a text file and bump into several problems. Changes in language since some examples were posted, and many functions are not exemplified at all. 
Nit picking, but you might consider adjusting next32 so that it doesn't put the generate_numbers() on the data dependency path. In other words, keep one extra u32 storage slot to store the "next" random value. next32 would *always* return whatever is in this slot, and write a new value into the slot (preparing for next time). This last step may need to call generate_numbers() but any code that relies on the returned random number can keep executing out-of-order, because *that* number was already available. This would make the returned value not subject to the branch (it's always just a straight memory load), so hopefully causes no misprediction penalties, and maybe greater instruction level parallelism too.
The language isn't nearly done yet. Don't you think making books this early is a bit silly? That kind of material can be worked on when at least the language specification is done.
Wow, this is dumb. I'd even found and fixed exactly this problem in the C version (which Rust used to use) of the ISAAC rng. Thank you for being so polite about my stupidity. :) I get (with [this CPU](http://ark.intel.com/products/65714)): ** Rust ** 1040076681 real 0m4.363s user 0m4.352s sys 0m0.000s ** C [gcc] ** 1040076681 real 0m4.240s user 0m4.228s sys 0m0.000s ** C [clang] ** 1040076681 real 0m4.673s user 0m4.664s sys 0m0.000s I've pushed the corrected versions.
Hm, I'm not sure the implementation complexity of this would have any advantages over using the SIMD version provided/suggested by the official MT site. I haven't looked at the SFMT code at all, so this might actually be what they are doing, I don't know. I'll look into it. FWIW, MT does (semi)-cache the values it is computing, and just shifts and xors them to get the actual result. Also, I've got a [`.fill_vec`](https://github.com/huonw/rust-rand/blob/master/traits.rs#L13) method on random number generators, which is designed to allow them to choose whatever way is fastest to generate many numbers. (e.g. [a 32-bit generator](https://github.com/huonw/rust-rand/blob/master/rng/mersenne_twister.rs#L99) is about half the speed of [a 64-bit generator](https://github.com/huonw/rust-rand/blob/master/rng/mersenne_twister.rs#L198) since the latter can generate 8 bytes at a time; and, some generators might/do cache results, so I'll be able to use a `memcpy` to fill the output vector, which would be even faster!)
Hm, I haven't really put much effort into micro-optimisations yet, I'll consider this, but I have a feeling bigger wins might come from just precomputing as many values as possible, so that memcpy can be used in `.fill_vec` (see my comment below).
Do you think a book would be quicker to update when examples stop working and the language changes in subtle ways than the currently available documentation?
My comment was mostly making reference to using multiple cores. Maybe the approaches you mention would work and be fast, but I don't have experience with them. It is common to use a PRNG for scientific/statistical computing these days. A popular PRNG is [here.](http://www.iro.umontreal.ca/~lecuyer/myftp/streams00/) Recent releases of R have the ability to take advantage of multiple cores. You do cl &lt;- makeCluster(4) to (speaking loosely) set up four independent R processes, each with their own independent RNG. Then you run your embarassingly parallel problem (bootstrap, MCMC,...) and get close to a linear speedup on big jobs. There's a description of RNG in section 6 of [the documentation](http://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf). I realize that Rust is not targeting scientific computing, so that may not be of interest, but it would achieve your goal of blowing the doors off C when generating many random numbers, which was what drove me to make my comment.
There is ["Rust for Rubyists"](http://www.rustforrubyists.com/), which (supposedly) is still useful for people who don't know Ruby.
&gt; I'm interested in Rust for scientific computing (that's partly why I volunteered for redoing the random numbers)! I'm glad to hear that. I think Rust has a lot of potential in that area, but I haven't heard much discussion about it. Once the language stabilizes further, and it's easy to call from C to Rust, I plan to start making R packages that call into Rust. &gt; (possibly included in the standard library) It would be nice if it did make it into the standard library. It's so much easier to share your work that way.
&gt; I plan to start making R packages that call into Rust. Ooh, this would be interesting. &gt; It would be nice if it did make it into the standard library. It's so much easier to share your work that way. In theory, rust will have a state-of-the-art package manager, [rustpkg](https://github.com/mozilla/rust/wiki/Rustpkg) (and [this](https://github.com/mozilla/rust/wiki/Bikeshed-rustpkg)), which should mean that dependencies can be specified simply and the build system will manage fetching them. Of course, being in the standard library would presumably imply better compatibility, and would make it slightly easier. (I'm not arguing for or against inclusion of a `parallel`-like library, just pointing out that all is not lost if it isn't included. :) )
&gt; But the eventual goal is to move from MinGW to MSVC like a proper Windows citizen... I was under the impression that neither would be needed in the future (source: http://permalink.gmane.org/gmane.comp.lang.rust.devel/2974). Am I mistaken? :( If either one is required I hope that at least MinGW continues to be supported.
That's why an open wiki book might be very useful. The Rust tutorial is very fine! But, I think there's a need for more. Take the example of [dive into python](http://www.diveintopython.net/toc/index.html) Each chapter begins with some small program. Each line is then explained in a thorough manner. And there's a logic in building something from beginning to the end of the book. Accelerated C++ is also a book of this kind. There are also some online C books that are also very complete. Rust tutorial explains key concepts, but is neither as thorough nor as deep. Regarding your question. What about if we could build such a (a hands-on) book? Does the language change? Yes it does. But the logic of building some program and explaining some concepts in each chapter would be kept. If in one chapter, the book is presenting vectors and vectors change from version 0.6 to 0.7, then a short notice could be placed in that chapter. Many people could (try to) change the code to the new version. This way, we could have a deep, thorough, always updated version of Rust for newcomers. I'm no one in the coding world, couldn't help develop the language, have no knowledge for that. I just try to solve my problems coding. And I feel that Rust has the potential to (finally) get people in a modern, multicore and fast path. That's why I'm trying to learn it, but finding difficult because there's little online material available. I know that this is aimed to be a system language, for very knowledgeable people. But if people like myself could migrate to Rust from C++, Java, Python, etc, that would also help very much for the adoption of the language. That's why I wrote about the need for a deeper and more thorough resource than the tutorial.
Something like Learn you a Haskell or Realworld Haskell would be great, including the model of community input while writing. Just be knowledgeable and ask O'Reilly, they'll, in all likelihood, love the idea. One thing, though: It should be agnostic when it comes to prior knowledge of languages, and give all paradigms equal treatment, so no "Rust for Pythonistas" or "OOP with Rust".
That would be lovely, but at the very least you need to make use of the native Windows linker (at least until LLD (http://lld.llvm.org/) is usable). But I'm more of a web programmer than a systems programmer, so I really have no idea if MSVC will be required for linking alone.
LLD looks like a great project, I didn't know about it. More compilation/linking speed is always great.
Once the "embedded rust" becomes stable and function pointers (and unions, ref: https://github.com/mozilla/rust/issues/5492) are supported well in C I look forward to creating a similar PHP extension toolkit.
Followup to the followup: http://smallcultfollowing.com/babysteps/blog/2013/05/14/procedures/
I am excited about "Universal functional call syntax".
...with UFCS is there any difference left between `impl`s and top-level `fn`s, beyond the fact that `impl`s provide a nice way to deduplicate generics and visibility related stuff for multiple definitions? (Would top-level `fn`s also allow a `self` parameter, or would it only work in the other direction?)
IIRC from a long-ago discussion, you'd simply be getting top-level functions "for free" by implementing methods in an impl block.
This seems to be an issue with Go still, but rustpkg should avoid this problem as it will support pointing to labels and branches. Rust also separates importing from link arrangements. You need `extern mod foo = "github.com/metajack/foo#v1.1";` only once in the crate, not at every `use` site.
If `Copy`/`copy` are going away, is there any reason not to rename `Clone`/`clone()` to `Copy`/`copy()`? Seems like a more straightforward name. `&lt;/bikeshed&gt;`
This is a nice idea. I guess it could be done once Copy is purged from the compiler.
I thought I should alert people to this. Addressing issue [#6037](https://github.com/mozilla/rust/issues/6037), this Scheme-style conditional helps to improve code clarity in instances where the `if`, `else if`, and `else` keywords obscure predicates undesirably. Here is an example: let clamped = if x &gt; mx { mx } else if x &lt; mn { mn } else { x }; Using `cond!`, the above could be written as: let clamped = cond!( (x &gt; mx) { mx } (x &lt; mn) { mn } _ { x } ); The optional default case is denoted by `_`. Eventually we are aiming for the following syntax once macros have had some improvements: let clamped = cond! { x &gt; mx { mx } x &lt; mn { mn } _ { x } };
Why are the parens required around the expr right now?
(Do read the sidebar on that subreddit before you rage!)
So the first thing I got from this is concurrency is when lots of things want access to a smaller number of things, wheras parallelism is when lots of things want access to their own things and don't care what anything else wants access to. Is that the distinction he's drawing?
From what I can tell, the thrust of his argument is that: 1. Concurrency is a property of any problem that involves resource contention. 2. Parallelism is merely a performance optimization for certain classes of problems. 3. Tools that address the concurrency problem are capable of performing the parallelism optimization by recasting a problem as a resource-contention problem. 4. However, casting a problem as a resource-contention problem introduces *necessary* nondeterminism and *likely* race conditions, as these are inherent properties of the problem domain for which these tools were designed. 5. Instead, use a tool that is explicitly designed to address parallelism in a deterministic, race-free manner. I'm sad that the author leaves further elaboration as an exercise to the reader. Even a single concrete, real-world comparison of, for example, Erlang and checkedthreads would go a long way.
Kind of, although it seems like that misses something. Parallelism is entirely optional. I'm doing this thing, and I can choose to make it parallel to make it faster. Say I'm writing something that will render graphics; I can serially render the whole screen, or I can divide the screen up into parts and render those parts in parallel to take advantage of my multiple cores / GPU execution units. This division of labor can be entirely planned ahead of time so that every core/execution unit can operate on exactly the data it needs. Concurrency is more inherent to the task. I keep receiving these events that I must respond to within some maximum allowable latency! And these events might all involve some shared state. I can't plan ahead to allow each event-handler its own part of the state; I don't know when the events are going to arrive, or what part of the state the events will require access to.
Specifically, pcwalton's reply: https://news.ycombinator.com/item?id=5712758
I had a small chuckle at the notion that 'Rust is good at concurrency'. It's certainly designed to be so, but it's not yet. I'm glad the author mentioned us though.
The key thing is that parallelism is semantics-preserving. If you want to add 1 to every element in a vector, there's no observable difference between doing it in parallel and doing it sequentially. For values of "not observable" that don't involve runtime performance, of course.
Yes, yes, yes.
[The code is on github](https://github.com/huonw/isrustfastyet) if someone wants to help make it less horrible... I accept pull requests! (In theory it updates every 15 minutes, but I'm not sure if that will work reliably.)
Thanks. Does this mean that most tests are disabled for Windows? Is it known what caused the drop in Linux build and test time?
A large chunk of the test suite is unfortunately not run on the windows bots because windows process creation is slow. I think we should be running the same tests everywhere though. It's not doing us any particular good that the windows bots are running faster than the others.
There is an open issue for that: https://github.com/mozilla/rust/issues/2105
What's with the huge difference for different platforms. 106s (Windows) vs 770s (Linux) seems like a big gap to me. Are there fewer tests being run on Windows than on other platforms?
At some point in the future it could be nice to have an IDE-plugin (or rustfix) that could insert lifetime syntax in situations where there is no ambiguity. Rust is really easy to read, but sometimes it takes a little time to make the code compile.
I agree that the upsides of symbols are expressivity and conciseness, downside is that you have to learn how to read them. In my opinion, one point that the post mentions but doesn't stress enough is: the downside (learning curve) gets more acceptable the more standard those symbols become, but may be unacceptable otherwise. As the article says, music and mathematics usage of symbols is a great improvement over trying to express an equation or a symphony by describing it in words. But try to introduce new symbols in musical notation, that other musicians will need to learn just to play the music you composed... Out of metaphor, there are some symbols that have been used in various mainstream languages and the effort of learning those symbols "pays off". But very language-specific notations can turn people away from the language, no matter how much heavy users of that one specific language come to like them.
I'd say you're technically correct, but that the "independent" case is merely uninteresting. *Any* two programs that don't need to communicate are trivially concurrent by simply running them in separate processes (or even on different machines); no special tools required. So the author is limiting his definition to interesting cases of concurrency, where resource contention and synchronization are mandatory.
For the impatient: 6x faster than the old scheduler. Down to 2 microseconds. There's some low-hanging fruit to optimize too.
This is just extremely promising. More and more Rust seem to be delivering the promises of a safe high-level system language, keep up the good work guys!
Perhaps we could figure out how to use `quote_ty!` for that?
Bugger, seems it wouldn't be possible without the user supplying more information. According to pcwalton, the macro compiler runs before the typechecker, which is understandable.
I think it is a good opportunity to discuss [macro import/export](https://github.com/mozilla/rust/issues/3114). ;)
(The link appears to have broken, just after I submitted it!)
&gt; But this system has its downsides as well—most importantly, complexity of implementation and interface. Learning to use references and unique pointers poses a significant learning curve. But, once the system is learned, it’s remarkably flexible, with an attractive combination of performance and safety. I'm glad the developers are very open to acknowledging this. Rust will never be the most elegant language, but I think that's a reasonable price to pay if you work in the area of high-performance computing.
"bors seems to speak only one programming language: Rust. Maybe it's about time to branch out a bit."
&gt; Garbage collection with value types and references D falls under this category as well as Modula-3 and the whole Oberon family. Just as information. Nice article.
Thanks for highlighting some of these commits rustcvswvi, very handy seeing as the repo is so busy these days. :)
That is a nice speedup! And for going through a general scheduler it sounds pretty good. I must admit though, that it'd still be nice to have a blazingly fast way to switch the stack for the current task (for coroutine functionality, where one can in many scenarios jump between stacks _very_ often).
The comparison that's missing is that to region-based systems with early deallocation. Probably because that, just like Rust's system, never actually arrived in any production language so far.
Go falls into the "Garbage collection with value types and references" category -- Go code has lots of stack allocation. (They use escape analysis to decide whether to promote a variable to a heap allocation, but it's transparent to the programmer.) http://golang.org/doc/faq#stack_or_heap
&gt; It’s also easy to write a function that acts exactly as free does, so you can precisely choose when your objects die. How do you do this? Is it just writing a function that takes a unique/owned pointer rather than a borrowed pointer, or does it involve something with timelines? I've written about 1k lines of Rust so far and never actually had to do anything like this.
Here is the complete implementation: fn free&lt;T&gt;(foo: ~T) {} Now when I do: let willy = ~1; free(willy); ...the pointer is moved into the function and is no longer available afterwards. This is how passing owned pointers always works, it just so happens that in this case our `free` function doesn't do anything with the pointer, which signals to Rust that it can be immediately freed. However, there's an even easier way to immediately free an owned pointer: let _ = willy; Since `_` is the pattern matching operator for "ignore this", the pointer is essentially moved into a black hole. Since it can no longer be referenced, it is freed immediately (EDIT: at least, as far as I remember, it's possible that it doesn't get freed until the end of the block).
-pedantic is nice too. -Wall leaves gcc extentions enabled without warning. 
At last, the answer to [~"unique strings everywhere"](http://www.quickmeme.com/meme/3qj070/).
Mainly just to further my own understanding... The new `fn`/`proc` scheme looks like it preserves all the use cases of the old one (as in the notes, `&amp;fn` =&gt; `fn`, `~fn` =&gt; `proc`, `@fn` =&gt; `@proc`). What's lost is the current ability to auto-borrow `@fn` and `~fn` to `&amp;fn`. A three-pronged question on that front: - Will it be missed? Was it used anywhere? - In the blog post it's mentioned as a vaguely negative thing that you can't tell from its type whether an `&amp;fn` is a copying closure or a by-reference one. But do you want to? It's clear why the caller would care, but is there any reason why the call*ee* should care whether the closure it was passed is by-ref or by-val? - Is there any technical reason why `proc` couldn't similarly be borrowed to `fn`? (IINM the latter is strictly more restrictive?) &gt; N: The proc type would have inside of it an `~` pointer that points to an environment that it owns. &gt; &gt; N: The proc type could then be put in an `@` box or `~` box or whatever, as currently I don't understand this. If the environment is already boxed, why would you also want to box the whole `proc` again, instead of using it directly? What's the representation of `proc` - is it something other than pointer to function + pointer to environment? (And any reason other than syntactical simplicity why it's not function next to environment as a dynamically sized type, which you could then put in any kind of box?)
I want this too. Besides a little refactoring (and replacing the old scheduler) there's not much standing in the way of fast task-local coroutines.
I am glad they could finally agree on Rust being super great.
Borrowing ~fn to &amp;fn was broken briefly about a month ago and there wasn't much fallout. I think it was used twice in the codebase. For when it is needed it's trivial to wrap the call in a `fn` like `|| myproc()`. The issue about being able to distinguish between closure types is regarding the lambda syntax. When you write `do some_library_function { }` you have no idea if you are making a cheap environment or an expensive one. I believe the resolution was to require `proc`, like `do some_library_function proc { }` or `some_lib_fn(proc || ())`, with the option of later adding back the current inference. I don't think there's a technical reason proc can't borrow. I do occasionally use `~~fn` in order to get single-word pointers to a Rust function. Not sure all the reasons for the two word closure representation, but putting the function pointer in the environment allocation would necessarily require an extra pointer indirection to call wouldn't it?.
Glad to hear it! Hopefully it will get included at some point
&gt; Borrowing ~fn to &amp;fn was broken briefly about a month ago and there wasn't much fallout. I think it was used twice in the codebase. Oh, empirical data! That's nice to know. Makes sense why it's not so important to keep it then. &gt; The issue about being able to distinguish between closure types is regarding the lambda syntax. When you write do some_library_function { } you have no idea if you are making a cheap environment or an expensive one. Right, I get that completely. It makes sense to have some kind of indication at the point where you're making the closure whether you're taking the environment by-val or by-ref. But in theory you could do that entirely on the caller side, in the lambda syntax, without changing things at the (higher-order) callee. What wasn't clear is whether there's a case where you're a higher-order function, you're taking an `&amp;fn`, and it bothers you that you can't tell whether you're being passed a by-ref or a by-val closure (because of borrowing). But if borrowing wasn't used much anyways then it doesn't really matter. &gt; Not sure all the reasons for the two word closure representation, but putting the function pointer in the environment allocation would necessarily require an extra pointer indirection to call wouldn't it? Extra relative to what? As currently proposed it's `proc { pointer to function, pointer to environment }`. The alternative would be `pointer to proc { pointer to function, environment }`. The difference is that the client would get to choose what kind of pointer.
I think these are much more sensible names, especially when they are not mentioned in unison. When you talk to someone and tell them "it's in the standard library", it's easy for someone to misinterpret this.
See the thread at http://thread.gmane.org/gmane.comp.lang.rust.devel/3619
I didn't include it there because the stack-versus-heap distinction isn't really controllable by the programmer. In languages like C#, if you allocate a value type on the stack it is guaranteed to be on the stack, period. (There is no address-of operator, so this is easy to guarantee.)
Rereading your post, I think I understand that reasoning.
you should really be just feeding in a range and getting a uniform distribution that's the correct interface for a random number
It's a little strange that the Rust code given doesn't compile. (It doesn't even parse!)
Given this is from a paper, it could be that's just a publishing issue. Who knows.
Hopefully that explains the parsing, but the semantics (a `&amp;str` is shared via the futures) are still incorrect. Edit: nope, just looked at the paper. It looks like the Rust code is just the ParaSail code with the keywords changed (e.g. there aren't even braces around body of the `_` match).
Samsung's commitment to Rust/Servo is impressive. It seems that 15-20 employees are working on it (not all the time though).
Very interesting. It makes me a little sad they seem to have opted for the style `foo(bar:T1, baz:@mut T2)` rather than the more usual `foo(bar: T1, baz: @mut T2)`.
Eventually there will be a pretty-printer to save us from all the whitespace bikeshedding. Bonus: if you write the pretty-printer then you get to impose your whitespace preferences on the rest of us!
I'm not very familiar with the FFI, but it looks like `sdot` is simply returning a pointer rather than the float itself. Can you try dereferencing it when printing, like `println(fmt!("%?", *x));` ? EDIT: Actually, derefing that pointer will complain about unsafety. Rather than stick an unsafe block in `main`, probably better to change your `sdot` function to just return a normal rust float by derefing and casting in the existing unsafe block.
 hello.rs:27:23: 27:25 error: dereference of unsafe pointer requires unsafe function or block hello.rs:27 println(fmt!("%?", *x)); ^~ note: in expansion of fmt! hello.rs:27:12: 27:27 note: expansion site error: aborting due to previous error alright, so I tried pub fn sdot(a: &amp;[c_float], b: &amp;[c_float]) -&gt; c_float { unsafe { *openblas::cblas_sdot(3 as c_int, to_ptr(a), 1 as c_int, to_ptr(b), 1 as c_int) } } Curiously that changes x to 0. p.s. If it wasn't so risky it'd be neat to have a comment box interpreter. :)
Glad to see so much progress on this.
Hm, how strange! Like I said, I'm not great at the FFI, but I think some of the syntax in your example looks strange to me and might be deprecated, which might possibly be part of the problem. Here's how a more modern version of the FFI looks like (using libm rather than openblas, since I'm too lazy to install anything): use core::libc::c_double; fn log(n: float) -&gt; float { unsafe { cmath::log(n as c_double) as float } } fn main() { let x = log(6.0); print(fmt!("%f", x)); // 1.791759 } mod cmath { use core::libc::c_double; #[link_args="-lm"] pub extern "C" { fn log(n: c_double) -&gt; c_double; } } Perhaps try updating your code and see if that solves anything (doubtful, but worth at try).
How large, complex, and optimized are the C libraries that they're aiming to replace? Do you expect that these will become mature enough to be used with Servo, and do you think that Servo's other dependencies would benefit from the same sort of translation effort?
What is interesting is that (coming from scala and haskell) you almost never pattern-match on option types. The power of option, imho, is that you don't have to care whether it is Some or None, you write the same code regardless. If you are pattern-matching the whole time, you haven't gained all that much over checking for nil/null. Option is a container, and we can use *my_var.chain(...)* and *my_var.map(...)* to update the things in the container. And the joy of it is that these methods will automatically do the right thing, with repect to Some/None, so you can string a bunch of these calls together, and if my_var is Some(...) is will apply all of the specified functions. If it is None, it will remain None. Finally, when we need a value, we can do something like *my_var.get_or_default(...)* to either get the value out, or to substitute a useful default value. The [scala docs for option](http://www.scala-lang.org/api/current/index.html#scala.Option) have a nice discussion of this. Edit: just noticed that this has already been covered in /r/programming.
cblas_sdot does not return a pointer. This code prints 25 for me (on incoming): https://gist.github.com/brson/5639525
They are large and quite optimized, using some sort of bit packing to represent CSS attributes efficiently. The major problem with the C versions is that the string interner (wapcaplet) is single-threaded, and we had to put a giant lock around it to make it work in Servo. I'm not sure how this reimplementation addresses that issue. We have a general goal in Servo of replacing as much C with Rust as possible. Pretty much all the third-party bits except for SpiderMonkey and Azure (gfx) are considered temporary until they can be rewritten.
I didn't realized it changed. Thanks! Problem was solved by removing the pointer from the type sig.
See also the recent discussion: https://mail.mozilla.org/pipermail/rust-dev/2013-May/004182.html
The whole thing reads a bit like an ad for Rust to me? :-)
I wish people wouldn't arbitrarily classify null pointers as not memory safe for the sake of being able to use more inflammatory language. Memory safety has to do with whether you can access memory in unsafe ways, not whether a given action, which may or may not involve a pointer, can cause your program to crash. There a many other ways for a program to crash dynamically based on a programmer error, such as divide by zero, and in Rust, mutating a managed value while it is frozen. Certainly it is often beneficial to be able to statically detect certain errors rather than doing it at runtime, but that's a separate issue with no absolute right answer.
I have the following: fn word_count(s: &amp;str, separator: &amp;str) -&gt; uint { let len = s.len(); match len { 0 =&gt; 0, 1 =&gt; if separator.contains_char(s[0] as char) { 0 } else { 1 }, _ =&gt; { let half_len = len/2; let left_slice = s.slice(0, half_len-1).to_owned(); let left_separator = separator.to_owned(); let mut left_sum = future::spawn(|| word_count(left_slice, left_separator)); let right_slice = s.slice(half_len, len-1).to_owned(); let right_separator = separator.to_owned(); let mut right_sum = future::spawn(|| word_count(right_slice, right_separator)); if separator.contains_char(s[half_len] as char) || separator.contains_char(s[half_len+1] as char) { left_sum.get() + right_sum.get() } else { left_sum.get() + right_sum.get() - 1 } } } } It compiles, but there is a lot of string copying (by using `to_owned()`). Is there a way to improve this?
Nice! For `s`, I think that a single `ARC&lt;~str&gt;` and passing indices into the string might work. (And similarly for `separator`. Also, using a `~[u8]` (or `~[char]`, if the rest of the program supported UTF8) for `separator` would possibly be more rustic.)
Rust's GC *is* being [written in Rust](https://github.com/graydon/rust/tree/gc), as is [the runtime](https://github.com/mozilla/rust/tree/incoming/src/libstd/rt).
Is it just written in rust, or also part of the application side, not of the runtime environment? See, you can implement a garbage collector, compile it down to assembly, and have it collect the garbage of another program, doing things with memory and boxes that the program that is being collected could never do. That's the traditional, and common, way of doing things. Or you could have the program collect its own garbage, without support from a runtime, while still ensuring safety through language design.
I'm not sure I understand... To be clear, in Rust's runtime there will be a GC for @ pointers, and that GC will be written in Rust. (As will (most of) the rest of the runtime.)
It also works for Haskell and most MLs.
No idea, sorry. If you don't get a definitive answer here, you could ask on the mailing list. :) (I'd be interested in the answer too.)
I'm not sure quite what you're asking, but making a garbage collector without any unsafe code is beyond the capabilities of Rust's type system. However, you can expose a safe interface to it. We've talked about using different GCs in different parts of your program, if you're careful about it. (We would like to experiment with using the JavaScript garbage collector for Rust types in the script task, for example.) This needs some ongoing design discussion, however, because of the interaction with manually managed allocations that contain GC pointers. Mixing GC with reference counting, however, is straightforward and can be done today.
In rust, mutating a managed value while it is frozen terminates the task. It doesn't invoke undefined behavior.
He's at it again, that avid facepunch programmer. I remember poking around in jsos checking it out. This guy does some cool stuff.
Awesome. This was honestly the first thing I thought of after I saw zero.rs. Not skilled enough to pull it off though :)
This is so cool. The first thing it makes me think of is using Rust for teaching OSes. The separation of safe and unsafe code seems valuable for educational purposes. PS Love the reclamation of the name rustboot. Not sure how many people have been around long enough to know that the old bootstrap compiler written on Ocaml was called rustboot.
I don't think you can call it kernel. It's just a simple baremetal program written in Rust. But it's definitely an interesting project. I'd love to use Rust for embedded software, replacing C without sacrificing low level features, assembler and performance. 
Better link: https://github.com/mozilla/rust/pull/6737
If you follow a pointer to "invalid memory" (or "invalid target" or whatever you want to call it) and the program crashes, is it really "memory safe"? I mean yeah, it's not a security bug, and it doesn't corrupt the heap (because there's no time - your program has done something unexpected and will at best unwind), but it's certainly not perfectly safe to go ahead and dereference that pointer either.
Since you are prevented from actually accessing memory in an unsafe way, yes, it really is memory safe. Failing is a perfectly valid way to prevent unsafe memory access. For example, attempting to access out-of-bounds memory through a memory safe vector type will cause your program to fail in most languages. To push the analogy further, a nullable pointer type can be thought of as a vector type with a length of either 0 (null) or 1 (non-null), where dereferencing produces the same result as accessing the first element.
What's your experience with developing Rust code on Windows?
Angolmois is not particularly designed for Windows so there were no major problems. And Rust has already enough abstractions to avoid platform-specific behaviors: Angolmois performs a case-insensitive matching on file names for example and it was platform-dependent in the C version, but in the [Rust version](https://github.com/lifthrasiir/angolmois-rust/blob/b38009c447d074fcfb7ae7f5be545d760b1b395a/angolmois.rs#L4103-L4172) it is platform-independent. Still there are several obstacles: * It is incredibly hard to call Win32 API correctly in Rust. Angolmois does not use a lot of them but they still occupy a good chunk of codes, like [this](https://github.com/lifthrasiir/angolmois-rust/blob/b38009c447d074fcfb7ae7f5be545d760b1b395a/angolmois.rs#L943-L992). * Crash is harder to debug. I had to use my Ubuntu VM just for debugging the rustc problems. * Make does not work. ;) Consequently the build instruction became a bit more inconsistent. (Not to say that *nix platforms have a consistent one...) In the future `pkg.rs` should be able to replace Makefile in Windows, however.
Could `mod win32` be a condidate for `libextra`?
This is beautiful Rust code. Thank you for sharing!
&gt; So you believe any language that can fail at runtime as a result of a programmer error is an unsafe language? Even assembly would be a safe language under your definition, if you throw the "as the result of a programmer error" get-out-of-jail-free card in there. Do you seriously think that assembly is memory safe? I'm guessing not. Every bug is the result of programmer error at some level. The point is that the language should *prevent* the programmer from causing the bug. Safety is nuanced. You can't claim to be memory safe if accessing memory can randomly crash. You can't claim to be type safe if values are occasionally cast to incorrect types (see array covariance in various languages). You have to specify what kind of safety you're talking about. Very few languages are 100% safe (you'd have to be total, for one).
Is null not just another name for "invalid memory"? So accessing null should then be considered unsafe? Or would you say that accessing the memory address 0 (where the first page is unmapped) is memory unsafe, but doing an almost trivial rename-job and making null first-class citizen and throwing an exception instead of a page-access fault suddenly makes the language safe? I don't see a significant difference between the two. In either case you have a reference that points somewhere invalid, and you are allowed to dereference it (causing crashes).
Haskell and ML fail #4 pretty badly.
&gt; Is null not just another name for "invalid memory"? No. It's an overloaded word. "Null pointer" in C/C++ means a pointer to memory address 0. "Null reference" in most other languages means an optional-reference-to-an-object that is currently empty, implemented however. &gt; Or would you say that accessing the memory address 0 (where the first page is unmapped) is memory unsafe, but doing an almost trivial rename-job and making null first-class citizen and throwing an exception instead of a page-access fault suddenly makes the language safe? *Memory safe*, yes. Words should have meanings. "Safe" is ambiguous, but "memory safe" shouldn't be. In one case the program tries to perform an invalid memory access and is forcibly terminated by the MMU/OS. (While in userspace it happens not to be, in kernelspace it *can* be a security hole.) In the other case the program detects an error and exits cleanly. I'll grant that, given that the MMU/OS can be counted upon to terminate programs that try to access memory address 0, it wouldn't necessarily make sense to berate a (thankfully hypothetical) language that is memory safe in every other way except for allowing null pointer dereferences for not being memory safe. But then it makes even less sense to berate a language for it that *prevents* those accesses by failing cleanly. Again "memory safe" is only a specific idea and doesn't encompass the whole range of what being "safe" can entail. If you use plain "safe" to encompass static freedom from null dereferences of any kind, I can't really complain. Or come up with a new term. "Null safe". Whatever.
You've gone off on a wild tangent again because you keep conflating memory safety with correctness. There's nothing unsafe about a program failing. It may not be correct for a program to fail under a given set of circumstances. Assembly is not memory safe because you can access any memory available to the program without restriction, which raises severe security concerns, not because you can write a program that can crash. Your example of array covariance is a completely unrelated issue. That's a case where static type safety fails, and type safety has to be dynamically enforced. Notice that this is *type* safety and not *memory* safety, and that the code is still type safe, just not *statically* type safe. To conclude: Dynamically enforced != unsafe Program failure != unsafe Type safety != memory safety Memory safety != program correctness If you conflate these things, you render them meaningless.
&gt; "Null pointer" in C/C++ means a pointer to memory address 0. "Null reference" in most other languages means an optional-reference-to-an-object that is currently empty, implemented however. My point is that there's no real significant difference in practice between the two. In one case you get a page fault (which you an trap and handle), in the other you get an exception (which you can trap and handle). I don't get why one gets to be called "memory safe" while the other doesn't.
&gt; There's nothing unsafe about a program failing So why is it unsafe if a program fails due to a corrupted heap? Safety is not the same as security. I just don't agree with your fundamental premise. If you want to claim to have a safe variant of feature X, and yet using feature X can cause a program crash, I don't think we have the same definition of safety. Safety to me means that using the particular "safe feature" is actually.. uh.. *safe*. I.e. it won't let you fuck up in ways which crash the program. EDIT: Let me put it this way. "Safe" to me (and most dictionaries) means "without risk". Dereferencing a pointer that's potentially null is clearly not without risk - since it can crash the program. If you want to use a more generous definition of "safe" that only excludes security risks, or heap corruptions, then you're welcome to it. But that's not what the word "safe" means to me, and I suspect most people. If something randomly crashes your app, it's not a safe feature IMO.
&gt; But you're arguing the other direction. Sort of, I'm saying that since there's no real difference in practice I don't get why we make a distinction. &gt; Why on earth should a language that cannot ever perform an invalid access be considered memory unsafe? I would contend that accessing null (even if the runtime catches the error instead of letting the OS do it), *is* an invalid memory access. Bottom line: if something causes random crashes, I don't think it can be called "safe". That's not what the word safe implies in any other context, so I'm not sure why we should be generous here.
Build instructions for Servo are here: https://github.com/mozilla/servo/blob/master/README.md
I'm not really sure it's ready to be test driven now. Many things are not implemented currently. Passing Acid 1 is still a goal for the project and it'll take some time to get there, as the focus is probably on more fundamental things right now. The language itself is still changing in response to concerns brought up while Servo is being developed. Major parts of the engine will be changed and re-architected right now, and a bunch of C libraries will be replaced with rewrites in Rust. In short, things are in great flux now, such that test-driving probably doesn't make sense. All this I gleaned as an outsider looking in to how I could contribute and reading through a lot of the github issues etc. Still, try building and running it, and take a look at the source - lots to learn. Also, I don't think there's any intention of a fork of firefox using servo instead of gecko, at least not for now. Servo is still considered an "experimental" project, and the main goal now is to prove its viability and performance benefits. There will be a separate chrome layer, called crow, which will serve it at first, but I don't know if any work has been started on that - I don't see it when running the servo project anyways. 
... The pace of Rust and its community is pretty awesome: * 2 days ago: zero.rs * 1 day ago: rustboot * now: rust.ko
I've never before seen the name "Crow" used in reference to the UI layer, where have you heard that?
The origins of the name "crow" seem to be found on [this issue thread on GitHub](https://github.com/mozilla/servo/issues/111#issuecomment-9412179), and it looks like the "crow"-me (pronounced "chrome") layer was dubbed as such by /u/brson. According to /u/pcwalton on [dev-servo](https://groups.google.com/d/msg/mozilla.dev.servo/zifMS6yohtc/fdQ8fQxuluMJ), the purpose of crow is: &gt; A simple frontend to establish the multi-process model. The rationale is that getting multi-process working early is better than putting it off to avoid the situation we are in with Gecko. edit: Also, for general knowledge Crow (as well as Servo) is a reference to [Mystery Science Theater 3000](https://en.wikipedia.org/wiki/Mystery_Science_Theater_3000), a sci-fi comedy series from the late 80s/early 90s.
I'm talking about *memory safe*, nothing else. I think the "traditional" meaning of "memory safe" is misleading and disingenuous. If accessing memory (i.e. reading trough a pointer) can randomly cause catastrophic failure, calling it "memory safe" is quite a stretch, IMO.
So how viable is rust for kernel stuff? Is it better than C++? I was reading an email from Linus about why C++ is a poor language for a kernel and I was curious about other languages. I'm very much a beginner programmer, so give me the idiots version if you do have some information to tell me. :P Thanks!
Doesn't rust only use dynamic dispatch if you typehint on the base class, and methods are implemented differently on each implementor? 
Making a port of [openmirage](http://www.openmirage.org/) would be awesome.
Rust uses dynamic dispatch on values that have been cast to one of the traits they implement. Doing that cast typically requires the value to be heap allocated, so there's a tradeoff between dynamic polymorphism and allocation.
I'm something of a lurker in the Rust community, but I've been following this issue closely. I liked the proc proposal in the sense that I agreed with the reasoning for it even if I didn't like the idea of having a new keyword. This proposal is (IMHO) simpler and better.
This is very nice. I was already starting to lean towards a `spawn!()` macro in lieu of the hypothetical `do spawn proc`, and this proposal manages to not only obviate the need to introduce new language concepts, but also *eliminates* current language concepts without sacrificing expressivity (and manages to make the behavior more explicit without being noticeably more verbose). For reference, here's what spawning a future looks like today: let v = 2; let mut f = do future::spawn { v * 3 }; // the `mut` here is weird f.get() // 6 And here's how it would look assuming both this proposal and assuming that `{}` are allowed in lieu of `()` for macro invocations (which Graydon has already approved): let v = 2; let f = future! { v =&gt; v * 3 }; f.get() // 6
&gt; and assuming that {} are allowed in lieu of () for macro invocations (which Graydon has already approved): wow, didn't know that was imminent! sounds like a minor change in the grand scheme of things, but it makes me happy.
I don't understand, Niko just said that `fn` would now always be a closure, why does not it fit in a data structure ? Do you think closures cannot be shared by default ?
Those are stack closures, and can't outlive the enclosing scope (`&amp;fn` today). So you can't return them, can only store them somewhere if you set up the lifetimes right (again to ensure it doesn't outlive the enclosing scope), you can't copy them either (to prevent recursive calls), and so on. `@fn` would just straight-up go away and that's what I'm trying to emulate as a library above. Edit: Updated the example in the previous post so that it's actually something you couldn't do with `fn`.
Thanks to /u/sanxiyn for the reminder to post this too :)
Am I right that this is a more structured OS compared to rustboot? I didn't see keyboard input in the source, so I guess this would be the next step? These projects really interest me, however I have a hard time finding a practical goal to work towards. A dedicated webserver comes to mind, but that is still a long long way to go it seems. (Threading, PCI driver, Intel ethernet driver and for storage you'd need ahci driver). Is it already viable to implement such features, or just wait for rust 1.0? 
See also the Wikipedia article on The Book of Mozilla for how this screen has evolved over time in Mozilla-based browsers: http://en.wikipedia.org/wiki/The_Book_of_Mozilla Here's hoping that Servo gets its own verse one of these days.
I too did this, although given that I'm merely a lurker it doesn't surprise me that I missed the chance. :)
Are there any actual benchmarks to demonstrate this?
Very cool. I compiled the last two versions, and it seems that the issue with the 2nd from last one is that it does not match on `[_,..strs]`. Do you know if an issue has been filed for this?
You can also use getopt http://static.rust-lang.org/doc/std/getopts.html 
The post has been fixed.
Nice. But the question remains :) Was it a bug in the compiler?
That's for my next post :)
Hmm. The getopt module is pretty cool, but I have been thinking that it might be possible to shorten the syntax a bit. Would something like the following make sense? (or even compile) fn main() { let opts = ~[ Flag { names : ~["--help","-h"] }, Opt { names : ~["--output","-o"], kind:String}, Arg { name : "input", kind:String, required : true} ]; let matches = getopts(os::args, opts); // Note the first name in the list, "--help", is always used as key if (matches.is_bad() || matches.contains_key("--help")) { print_usage(os::args[0], opts); return; } let input : str = get(matches.get_str("input")); let output : Option&lt;str&gt; = matches.get_str("--output"); do_work(input, output); }
It seems [otherwise](http://huonw.github.io/isrustfastyet/). There was a dip in the graph for linux but now it is back up at around 1400. ?
There is a *lot* of noise in the build-bot build times, unfortunately. Seemingly inoccuous commits cause huge jumps (e.g. adding [this testcase](https://github.com/mozilla/rust/pull/6891) apparently caused the most recent slowdown), so everything there must be taken with a grain of salt. However, the commit this links to did appear to [make things faster](http://i.imgur.com/RjEY4Wm.png).
[IRFY](http://huonw.github.io/isrustfastyet/) seems to [show something](http://i.imgur.com/RjEY4Wm.png), as much as the buildbot measurements can be trusted.
I like the distinction between Flag, Opt, and Arg. 
Maybe I'm being ignorant here (I probably am), but what's the difference between the 2nd and 3rd versions, besides separating out some functionality / making the code cleaner?
Yes, I believe it was. bjz said it was a bug in pattern matching on lists.
From andydude's response below, I think the difference is that the 2nd version is hitting a bug in the list pattern matching code. EDIT: Actually, perhaps he was referring to a previous version of the code. The only real difference seems to be factoring out the `str::connect` bit to make it more DRY.
I would be sad to loose the brevity of `@T`/`@mut T` instead of `GC&lt;T&gt;` and `GCmut&lt;T&gt;` (?) when I just want to bash out some code without thinking about lifetimes. Also, auto-dereferencing and borrowing are quite nice.
Auto-dereferencing and borrowing will stick around.
Is this saying that auto-dereferencing/borrowing would become a lang-item and/or trait, or there would be some other mechanism for it? I'm working under the model that GC pointers would be a library-space `struct GC&lt;T&gt; { ptr: *T, .. }`. I'm quite possibly misinterpreting the post. (To be clear, I understand that this post is more of an RFC than a "this is what/how we're changing" announcement.)
But then if someone writes `x *y` you have no idea whether that's a pointer to y of type x or if it's multiplying without a symbol table. This makes the language harder to understand and compile.
It's fine in Rust, because the former is written `let y: *x;`. (`*` is actually a pointer type in Rust even now: unsafe/raw pointers for FFI and general pointer manipulation.)
Right. I meant that they can unify the two symbols since an unsafe pointer is known by the unsafe block it reside in. Unless the underlying representation is different.
And `*unsafe` would be an unsafe/raw pointer (currently `*`), much akin to the current `unsafe { ... }` semantics.
I'm not sure this is possible (I'll happily be proved wrong though), since the memory management is different despite the representations being the same (in theory, not in practice, yet), specifically, `~` gets freed automatically, `*` does not, and there are perfectly legitimate cases to use `~` in unsafe blocks. Also, on a syntactic note, how would one allocate on the heap? `*foo()` doesn't work, since that's dereferencing.
`Gc` and `GcMut` would be just as first-class as `@` and `@mut` today in every way except for syntax, with the same benefits also extending to `Rc`, `RcMut`, `ARC`, and so forth? Edit: ...unless we add pattern synonyms or something, putting them in patterns is one capability we'd lose. Edit again: IIRC if you write `@some_function()` or `~some_function()`, the result of the function goes directly into the box? Would that also be true with a library solution? Could they point to dynamically-sized types? 
Hey, that would free up `@` for name-binding in patterns again. Not that I'm in love with it, I think `as` or `=` are preferable on the merits in terms of clarity.
That's basically the current situation. `@` right now is reference counted. There's nothing very special about ARC (in the Objective-C sense) as far as I know. It only seems like magic because Objective-C programmers previously had to manage their reference counts explicitly (poor guys), but otherwise even C++ does it automatically, as a library no less (`shared_ptr`). Automatic reference counting is clearly preferable to manual reference counting, so the logic for Objective-C is impeccable, but the tradeoffs look different if you're starting from a blank sheet.
Ah ok, I thought @ used GC for some reason. ARC is a little more sophisticated than `shared_ptr` though, as it is smart enough to look beyond the local scope. But certainly, starting from a blank sheet, the view is quite different. As a very recent newcomer to Rust, I found the whole memory management thing by far the most difficult to grok, though I really appreciate what it's trying to achieve. So I figured it was worth the extra effort to come to terms with the sigils, semantics etc to benefit from the design.
&gt; Ah ok, I thought @ used GC for some reason. The reference counting is/was a temporary thing to get something working. Graydon has a true GC for @ in development.
If it's not a lot of trouble then I suggest changing the font of your blog. ~ is not wavy enough so it's difficult to differentiate it from hyphens and dashes.
It'll be a lang item/trait, so that you can implement it for `Gc`, `Rc`, etc.
Just what I was hoping! Thanks.
Nice! :)
I just realized the biggest fallout from this change: /r/rust! `Gc&lt;reply&gt;`? `Gc&lt;share&gt;`? How will we cope?
Oh dear. At least my functionality is intact, even if my syntax has been uglified!
Frankly, this just sounds awesome on paper. The ability to just *remove* the GC from the core language and leave only a lean and mean core is not only extremely cool on principle, it also means that sooner or later (when the "restrictions" are put in place) one could ensure that no accidental use of the GC sneaks in performance critical parts statically!
I'm way too excited about this. The prospect of low-level programming with safe manual memory management and high level language constructs is making me quietly giddy. I think moving garbage collection to the libraries will also really help the uninitiated to differentiate between Rust and Go at a glance.
I like the proposal, but I see a strong downside: if there is no longer a "standard" garbage collected pointer, then we might end up with libraries using different implementations and thus being incompatible with each other.
C++11 is missing capture-by-move and it sucks. Please take the time to get this right for 1.0. I like the explicit capture.
I can't comment on your proposal, but the lack of guaranteed TCE in Rust has more to it than just the different calling convention. Have you read https://mail.mozilla.org/pipermail/rust-dev/2013-April/003557.html ?
Clarification: Rust does TCO, but it does not guarantee TCO *in all cases*. [This was the answer I got when I inquired a month ago](http://www.reddit.com/r/rust/comments/1c353t/why_rust_will_probably_never_guarantee_tailcall/c9cko6x).
I suppose it all comes down to opinion. IMO, there are really just two error-handling strategies to consider: conditions and enums. The rule of thumb is likely to be that you'll use conditions when errors are rare, because conditions are much easier to ignore in the common case. When errors are common or expected you'll use an enum, and whether that means Option or Result depends on whether your function can fail in one way or in many.
Isn't the IO library going to undergo a major overhaul? If I am not mistaken, the IO library in its present form is pretty old. The language has changed quite a bit since it was originally written; and I think there are efforts to re-write large portions of it. It could be good to ask around on the IRC channel or the dev-rust mailing list.
It needs an overhaul, yes, but nobody's taken the initiative to do it yet. Proper error handling will be an important consideration once the effort is finally underway.
I think the actual drawback is that this adds another tricky piece to the already complex type inference, which is a burden to both the implementation and users. Rust already made some "less inference" changes, for example requiring declarations of lifetime names even if they can be inferred in "most" cases.
If we used the `fastcc` calling convention, we would have a guarantee of sibling call optimization but *not* general tail call optimization. Opting into that requires making a performance sacrifice for *all* function calls.
Yeah, I was looking at an implementation in C and it looked a little... unsafe. :)
I was going to start by leaving most extension methods returning zero values on error, forcing code that needs to know about the error to use conditions. Maybe this will be bad, maybe error handling using conditions will look worse than with `Result`. Even if we do have to put `Option` on all the return values, the condition does keep the details of the error `Result` out of the function signatures. A caller that just wants to propagate the error can do so, one that doesn't care about failure can `option.get()`. Using `Option` over `Result` is a win because it solves the problem that `Result`s with different error `T`s don't interoperate well. Any code, not just code that deals with I/O `IoError`s, `ReadError`s, etc. can trivially propagate errors as `None`. So functions that know and care about `read_error`, or other errors can express interest in that condition, while still correctly unwinding the stack for code that doesn't care about `ReadError` conditions. By turning all `Result`s into `Option`s you lose the distinction between optional values and errors, but the distinction is subtle. Still, maybe you want a special type for this. After writing this out I think `Option` makes more sense than requiring condition handlers everywhere to detect the error. Conditions will also allow us to do interesting things like replace disconnected sockets mid stream. Here's an error handling story: * Error values are represented as `Option`. * If you don't care about error handling you call `.get()`. * If you want to your functions to be error-recoverable you return `Option`, a poor-man's IO monad. * The presumably rare code that actually cares about the *reason* for failure can install condition handlers. 
Is there any way to force the compiler to do tail-call elimination and spit out an error when it's not possible to be done?
Right. I agreed with the less inference decision for lifetimes. But maybe different problems call for different solutions. Apparently with DST the problem is that you have to write `:Sized` in so many places that it's a burden. The question is whether inferring it might be a smaller one. I suppose the other solution discussed in the meeting, annotating where unsized types are *allowed* could also work, but (if you actually want the benefits) that would require you to manually write `dynamic T` or whatever any place where you're only referring to `T` by pointer, and ideally not forget it. If I'm interpreting the gathered [statistics][1] correctly: * 475 decls would require `Sized` if unsized is default * 322 would permit `dynamic` if sized is default It's situations like this where the use cases are close to evenly split that choosing a default is most annoying. No matter which you choose there will be a significant annotation burden. (I would be surprised if there's many instances where an unsized type would be allowed but you don't actually want to take advantage of it, though admittedly I haven't investigated. I suspect it would be a hamster wheel where library authors keep forgetting to add the `dynamic`, users complain, and then it gets added in the next release.) [1]: https://github.com/mozilla/rust/issues/6308#issuecomment-18880575
Oh right. Makes sense!
It does, however things are first pending on the scheduler (which is coming to fruition); when the new scheduler is in place, it'll be clearer what the available options are.
Perhaps it would be possible to infer the specific kind of GC, with @ still used in most places?
maybe
I don't understand what this gives us. Will they be able to model pure better with this? Also: Does anybody else feel that the language is getting more complex by the day? Reading the conversations about all the different sort of closures there is and how they interact makes me feel lost. I'm afraid Rust is gonna end as a sort of a system level Haskell, where everyone acknowledge it as the best available but nobody actually bother to use it...
I feel recent posts such as http://pcwalton.github.io/blog/2013/06/02/removing-garbage-collection-from-the-rust-language/ and http://smallcultfollowing.com/babysteps/blog/2013/05/30/removing-procs/ show that the core developers are very much aware of the danger of becoming too complex.
This proposal is mostly a reaction to the removal of the `pure` concept from Rust earlier in the year. So in this particular case, it's wouldn't so much make the language more complicated as it would simply bring us back to a previous level of complexity. That said, I'm not too keen on this. I've never been sold on the idea of annotating purity, especially when it's so trivial to undermine. But yes, as asb said, the devs have no intention of letting the language swell up to infinite size. There's been a ton of work to simplify lately, and there's more on the way (DST and closure reform, even though they may seem on the surface like they make things more complicated, actually work together to eliminate several concepts and odd corner cases and enable overall simplification).
I missed the boat on this topic. Is there a more introductory discussion of sized vs unsized declarations with which I could get my feet wet?