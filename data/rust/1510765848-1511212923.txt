Parallelism implies concurrency, but concurrency does not imply parallelism. They're not really equivalent, but they are highly related.
i just start to learn and use it in work last week. and finish several command line tools in corporation easily. type system looks really cool, and the compiler give me strong belief that the program is robust. i learned it with improving these cmd tools, its really happy to write something in rust. the code, doc, compiler, type, all makes develop happier. these simple.cmd tools modify yaml and toml files, exec docker command,and notify fs change, feel really productive and awesome.
https://youtu.be/eH5JgMlNE8o?t=36m28s
Can you explain a bit more? Or open an issue on github is there's a problem. Tarpaulin compiles the project itself to ensure the linker settings are correct for instrumentation and applies link-dead-code itself so you shouldn't have to do it yourself. Whereas with kcov you would have had to manage those settings yourself.
According to what I said, _implies_ only in the sense it's necessary (concurrency) when there are resources to share.
According to what I said, _implies_ only in the sense it's what you have to make use of (concurrency) _when_ there are resources to share.
Bools are kind of unnatural for a low level language. AFAIK memory addresses actually refer to bytes not bits, and C and C++ both have 8-bit bools.
No, the DLL file is not modified. Just like any executable code it's loaded into memory, and the memory is modified by the detour.
I have a project where a few modules have only dead code in them. It's dead because there no tests for it yet. Without `link-dead-code` rust will remove it and kcov will report more coverage than there is. For some reason I get the sense result now even though Im enabling linking on dead code. I can give you an example in a bit if you want.
What I read is that concurrency is the right word to use because, fearless is about it, about being able to share resources without risks, attaining correct implementation of concurrency, which is about resource sharing, while parallelism isn't.
I see the app is written in Evil-lang, but maybe with time, that could be fixed :p
well, we have `build.rs`, which is a fit for this purpose
I prefer [this frame of mind](https://www.reddit.com/r/rust/comments/7cwpbq/fearless_concurrency_in_firefox_quantum/dpv22j3/) because anytime I read the word concurrency I can't avoid the meaning related not only to completing tasks simultaneously, but also on agreement or competing with something. And I think this if fine in this context, because the _fearless_ is not about running things in parallel, but about the correct agreement for competing resources when doing so.
&gt; It's an impressive achievement that means I don't laugh at the idea of GC as much anymore. Yes, it's very impressive, and I think it enables a *lot* of real world usecases. Of course, personally I work in a world where 10 Âµs meant something went horribly wrong so it's not quite there yet :p
The problem with panicking is that the compiler can't vectorize it, whereas a saturation is trivial to vectorize (vector min/max).
I perfectly understand the feeling :)
Ah so I've looked at the code and now I realise what it is. So you can only get coverage on executable lines of code, so lines that effect contents of memory in the program or the control flow during execution. In the lib.rs you only have imports which aren't executable lines of code, these are resolved at compile time and kcov falsely reports them as lines based on trying to map from the low level information in the binary to source code. Then in zpool/mod.rs you only have type/trait definitions and imports once again no executable lines of code! This is actually one of the things that motivated me to start tarpaulin because mapping from DWARF tables to source only really works for C, languages which add higher level constructs or templates cause kcov to have false negatives and may instrument "instructions" which are never executed causing the results to look worse than they are.
AFAIK, it is not that WebRender isn't finished, but rather that it is not yet properly integrated into Firefox. The setting you turned on in nightly is (AFAIK) an experimental mode that lets WebRender handle some things (leading to some speed up), but everything still has to go through Gecko to be actually displayed (explaining the slowness). 
Fuzzing is the idea of testing a program by automatically spamming it with randomized inputs. Completely random numbers would not work (as they would likely not be valid data and you would just keep hitting the first error condition in the code, which isn't really meaningful testing), so fuzzers try to be smart and generate data that is actually likely to trigger different code paths in the program and generate interesting results. For example, a fuzzer could test a JPEG decoder by generating many JPEG-ish files that have a valid (or semi-corrupted) header, getting the decoder to actually try to process them, but also containing randomised corruption/values in the internal data, to try to trigger edge cases in the decoder to see if they get handled properly. Many security holes and obscure bugs have been detected this way, as fuzzers can generate all kinds of weird inputs for the program that a human simply wouldn't have thought to test, triggering obscure edge cases in the code.
Oh, I see. You're right, you can't test coverage for trait definition since it's just an interface. My already favorite part of `tarpaulin` is how easy it's install on travis-ci.
Concurrency and parallelism is not about being able to share resources. it's literally just running multiple tasks non-sequentially. You can have a program have two threads that do completely different things and that will still be parallelism. You can have a program have two async functions running in the same thread doing completely different things and that will still be concurrency. [How it works](https://i.imgur.com/Jy0Y3mL.jpg) Both things shown are concurrency, because they're running more than one task at a time. The difference is that concurrency (in this case async programming) is running the tasks in the same thread, while parallelism is running the tasks in different threads.
+1 from me
News like this really makes me want to use Firefox, but after using it on my laptop, I can't justify switching completely from Vivaldi. There are just so many little things that I miss when using Firefox. One of which is tab stacks and the tabs in the bar resizing so they fit on the page. Firefox displays all tabs at the same width and doesn't group them so I have to scroll to find it or open a new tab and hope that it will find the tab I have open. The last one on Vivaldi can just be done with F2 and it will always display tabs, bookmarks, and history that matches.
Thank you :) that was another motivating issue, the number of attempts it took me to get kcov and coveralls working!
Nice, looks good!
I meant it in the logical sense. If parallel then it's concurrent, but concurrent doesn't necessarily mean parallel.
`cargo metadata` does look interesting, thanks :) However, I don't think it provides me with any data I'm not already getting. Accessing github is only something I do when the package on crates.io does not contain a license file (eg LICENSE or COPYING). Many crates are missing this for any of multiple reasons. A common reason is when the source repository has a license file in its _root_, but the crate is distributed from a subdirectory. This is for example the case for [diesel](https://github.com/diesel-rs/diesel/tree/master/diesel). Note that license-hound is not satisfied with finding out _which_ license applies (this is easy to find out from the crate metadata), but tries hard to find the actual LICENSE-file. For example the MIT license is actually a template, and to comply with the license terms you actually need the license file in question.
I had a chance to look at the codebase and it's excellent (and open source!!!). Highly recommended way for Rustaceans to get started in the space. 
Because recompiling everything when you need to update your (for example) SSL library is a good thing? How about your C library? Plugins also aren't a thing without dynamic linking. Deploying single static binaries is easier, but maintaining a collection of static binaries is not as nice as having dynamically linked shared bits in that collection.
&gt; Plugins also aren't a thing without dynamic linking. Sure they are. The way you'd handle plugins in a system like that is that each plugin runs its own process and you communicate via IPC. That seems like a more "micro-kernel-y" way of doing things and IMO has a lot of merit. Dynamic linking leads to a lot of obscure bugs because you're basically linking together code on the customer's machine that no developer has ever seen together before. That's a bit risky. 
Agreed. Assuming you have enough of the basics down, I recommend getting at least an introduction to macro syntax before diving into it. https://danielkeep.github.io/practical-intro-to-macros.html https://doc.rust-lang.org/book/first-edition/macros.html
Isn't this the one that lost like $20M recently?
I don't think there would be many of these. Specific threading bugs once found get fixed, and we don't know how many go undetected.
No, the client was never hacked, but the smart contracts shipped with it (written in a different language, which i'm not an expert on, so I can't really comment) were exploited.
... proving once again that rust rules :D
Delphi is big here in Brazil, it's not present in our universities though, so it's on decline.
Okay, so this error is actually rather humorous. It would be totally fine if you had not use `'a` in the signature of the __*EXCLUSIVE REFERENCE*__. The `'a` is is quantified by the `&lt;'a&gt;` in the impl which quantifies over the entire impl block and it's bound by the `&lt;'a&gt;` after the test which is indeed as seen in the definition the same as the `&amp;'a str` in the struct, Which is a `&amp;'static str` which is valid for the entire duration of the program. So you're acquiring the exclusive reference for the entire duration of the program and you're also saying that `self` is referenced by that. The `fn write(&amp;self)` is actually syntactic sugar here for `fn write&lt;'b&gt;(&amp;'b self)` using lifetime elision to make it more readable. If you do `fn buid(&amp;mut self)` then it becomes syntactic sugar for a similar thing and `'b` is quantified for the function itself only thus it's not the same as &lt;'a&gt;` and that should work fine. Similar errors have bitten me quite often I must say.
You can change `browser.tabs.tabMinWidth` in about:config to have tabs squeeze even more. Firefox tabs squeeze too, but they only squeeze up to a minimum width, because after a while it gets hard to manage. You can also type a string from the URL or title of a tab in the address bar and it will give you an option to switch to it (you can force this to only search tabs using `%`) If you're using a lot of tabs I recommend a vertical tab extension like Tab Center Redux or tree style tabs. 
Thanks! The problem I'm having is from a bigger project and this was a simplified version of the error. Currently, I can't remove the &lt;'a&gt; from build() as that causes another error. But I suppose I have to figured out why that is happening.
[removed]
Well if build does actually take an exclusive reference to the contents of info then you're severely limited in your architecture. To be clear from the template here this in general means that build some-how has to mutate whatever `info` points to which is not owned by the Test struct itself. So essentially you are creating an object X that references object Y some-how and use a method on X to update Y. In this particular case though what it holds is a static compile time string which cannot be mutated.
Yes, this exactly. And thank you for getting the joke! :)
There's also [`depgraph`](https://crates.io/crates/depgraph) for Make-like functionality in your build.rs
The resource being shared is the thread.
If a distribution provides functional package management (as I understand it: good, modern package management where package conflicts aren't an issue, where specific versions of dynamic libraries can be demanded if need be) what problems remain?
Man I really got to learn rust. I'd love to contribute something to redox at some point
Concurrency simply doesn't convey parallelism well, hence "if parallel then it's concurrent" doesn't work for me. And, anyway, I prefer it this way, it's less conflation of terms, more specialized usage, and more clear. [When I use concurrent I also imply other things that are orthogonal to parallelism](https://www.reddit.com/r/rust/comments/7cwpbq/fearless_concurrency_in_firefox_quantum/dpv8h9m/?context=3), not simply "to run at the same time".
Ok, another way to phrase it is "parallel behavior implies concurrent behavior, but concurrent behavior does not imply parallel behavior". I think we agree on the core concept, maybe it's just a matter of using a "mathy" way of explaining it.
There's https://github.com/uutils/coreutils
Calculator icon looking like the one in iOS as well.
Also, when I said "which is about resource sharing", I meant it's also about it.
Also, when I said "which is about resource sharing", I meant it's also about it, if it wasn't, then there wouldn't be anything to fear about.
Compiling for musl and having a fully statically linked binary turned out to be super-easy thanks to rustup. It was my solution when coding something at home for my ancient system at work. 10/10 would compile again.
Thanks for the encouragement! I'm glad you have discovered it and found the merit in it. I, too, come from a C++ background, and I got into Programming Languages because I knew there could be something that offered the safety and succinctness of Haskell with the speed of C++ (or better). Rust (even four years ago when I first discovered it) exceeded my hopes and expectations. Enjoy the ride! :)
Oh. The case where it runs normally then goes into a tight loop almost always indicates that one end has closed the socket (perhaps partially) and you're not handling it correctly. The reactor loop in Mio is literally a loop that blocks on poll(). It waits some timeout length (usually 10-100ms) before running some things and then polling again. At 10 polls per second you'll use almost no CPU. If it receives an event, then it loops around and tries again. If it is polling a socket that is in an error/closed state, poll() will return immediately and then it will loop and poll() again.. which will return immediately then poll() again.. which will... I'm not familiar with the ws-rs API model, but there has to be something that can pass on errors. Maybe you'll have to close and re-open the connection at that point. 
Oh, I remember those. I started a similar project when I started with rust. But I noticed today that a simple rewrite isn't very useful. Both exa and ripgrep break with quite a bit of the syntax, but by doing so they become more useful. Except maybe that "exa" is terrible to type quickly since it's all done with the left hand on my keyboard layout. ;)
Do you plan to add license compatibility check?
I have a side question. What's preventing the compiler from actually presenting this kind of information? It seems like this could be way more descriptive error than is actually presented, even if technically correct. (Reminds me of gcc 3 era C++ errors.)
So Python (I wouldn't call Python where `import` spawns a process with IPC "Python") and similar languages/tools just aren't allowed on such platforms? That seemsâ¦odd.
`alias ls=exa` I do agree on the breaking changes between grep and ripgrep, though I agree with them it's annoying to go back and forth. 
Useful error reporting is always hard; it's always hard to guess people's intention I guess as a compiler. The compiler just sees the lifetimes.
A simple alternative to find that I like so far: https://github.com/sharkdp/fd
You can't really access Android's GUI library from non-JVM languages IIRC.
Well my point is it doesn't mention the lifetimes. That would at least give you an inkling. It could say something like "build borrows lifetime exclusive reference".
No, this is only about obtaining the LICENSE files. License-hound has of course also informed me of which licenses my dependencies fall under, and I have verified compatibility myself. During development I have seen people interested in doing automated license compatibility checking, so there is definitely interest around. Maybe [lichking](https://github.com/Nemo157/cargo-lichking) is what you are looking for?
What does this has to do with clone-on-write?
please do email me at dzj[this the where the little point goes]analytixware[again]com if you are interested in work or PM me.
Relevant : https://www.youtube.com/watch?time_continue=42&amp;v=8fnfeuoh4s8
Well it says exactly where the borrow ends which is what a lifetime here and it calls an exclusive reference a mutable reference which is just _wrong_ but hey some people decided that that was okay and they should get spanked.
I'm hesitant to use that alias, since it might break scripts that expect to call 'ls'.
Ah, yeah, misunderstood the title.
Perhaps: alias ls=exa 
Always check /usr/bin before creating aliases. ;) [xa65 filelist](https://packages.debian.org/stretch/amd64/xa65/filelist) Although I probably won't ever use that binary.
Well, I was thinking beyond threading, since Rust is supposed to help against all kinds of shoot-yourself-in-the-foot problems that C++ has, no?
It's almost impossible to run a program on windows without dynamic linking. It's syscall Abi isn't stable and thus you must link with a DLL in order to be able to do anything. Solaris does too and you need to link libc.
I want to try to explain why we can't just do that, becaus this aspect of trait objects isn't understood. When a trait is "object safe" then it means that `dyn Trait` - the type of the trait object - implements `Trait`. That is `where (dyn Trait): Trait` holds true. Consider this code: trait Foo { fn foo() -&gt; usize; } fn bar&lt;T: ?Sized + Foo&gt;() -&gt; usize { T::foo() } This is perfectly valid Rust code today, and in more complicated something like this could be very useful. But `Foo` is not an object safe trait. If it were object safe, you could do `bar::&lt;(dyn Foo)&gt;()`. But there's no vtable being passed in. We can't *just* say that static methods can be called as regular methods on the trait object, because our system is built on this idea that the trait object is just another type that implements the trait. Certainly there is a way to implement dynamic dispatch where you don't have a data element, but our trait object system is not well suited to be adapted to that use case.
The real sad/funny thing is that most everything in rust has been around for quite a while. Rust isn't really breaking new ground in programming language theory. It's more putting a bunch of really clever and smart ideas about programming languages and safety that have been around for a decade or two into a useable language.
That is C++20 as in, it has already been merged into the C++20 standard draft, and has been supported by gcc for 3 years already.
Ah yes, I'd forgotten about this! Thanks for the reminder :D
Sometimes it takes a critical mass of good ideas to forge something sufficiently new. Plus by the time Rust roles around you have LLVM in a pretty modern state, and the state of OCaml helped with early prototyping. Platforms like github also allow for a large endeavor like designing a language in open source to actually work. I sort of feel like it arrived precisely when it needed to, must be a wizard language or something. I'm not surprised that the systems world evolved as it did, over the time scale that it did. What I am surprised by is how stuff like Java, JavaScript, and Python took off given their early state. I guess it was mostly mass amounts of capital during the dotcom era, but all the web tech languages seemed to sap a lot of more attention away from more interesting PL technologies. Sun's reputation as the good guys certainly made swallowing enterprise driven technologies palatable while they lasted. Which I think get's me to my final thought, now that there are so many solid languages, there is less need to aggressively push an one. Which allows things like Rust to percolate at its own pace, something I'm seriously and honestly thankful for.
What tool do you use for packaging?
`alias l="exa -TlmFL1"` Why would anyone type two whole letters `ls` when you can type just one `l`?! I have `l` `l2` `l3` â¦ aliases to show trees of different depth.
I like cargo-make.
Funny thing is, I already have that alias setup on my private laptop. But during my day job I'm working on many different servers without a personalized setup. So my muscle memory switched again to using 'ls -l' before I can even think about typing 'l'...
Unfortunately, itâs actually not an image but a config file, which is meant to be changed by the user. So I canât use include_bytes! I will have a look at CMake. Thanks for your suggestions.
That's why my setup has [a script to quickly copy itself to remote servers](https://github.com/myfreeweb/dotfiles/blob/master/rinstall.sh) :D
Same comment wrt memory safety issues.
AFAIK that "different language" is Solidity and AFAIK it's very bad for writing smart contracts.
FYI, I put the JSON encoding into a CPU pool and it sped up the rust master significantly when GETting said json - 25%ish faster than Go when no data to encode, to ~2x faster when there is a reasonable chunk (I expect that serde is much faster than Go's JSON encoding, so the speed up improves as there is more JSON to handle). Anyway, thanks for the tip :) 
Nope, scripts don't use aliases
The problem was not even the language. It was the fact they have reused one contract to be shared between all users wallets (separate contract) to save computation time (called "gas"), as computation on shared blockchain are kind-of-expensive for the users. Unfortunately, the have left the "self-delete" function inside it. So one developer/user tried it, the contract deleted itself, and now millions of dollars in contracts/wallets that were using the common contract can't do anything, because they call "code" that doesn't exists anymore on Ethereum blockchain.
Thanks for the info. I'm going to try to verify this. If this works, that solves a problem. :)
&gt; B-)= what does he mean by this???
It is possible to have parallelism without concurrency though; you can have a single "thread" of execution making progress on multiple cores at once, i.e. data-parallelism.
You're right. The thing is it allows people to easily shoot themselves. Just like C. It's true that it's programmer's fault, but if the language is unhelpful, then it's also bad design of the language.
I don't have that installed on mine, but absolutely, check before aliasing anything! I don't use alias lightly! :) 
I've been doing this as well and my solution has been to write a build script that include_str! it into the binary and generate code that dumps that to the filesystem if absent, or load from the filesystem if present.
The more competitors Rust is getting, the more we know that Rust is aiming into the right direction :) It seems to have some pretty cool features (emphasis by me): &gt; Puffs is a C-like language, and its compiler actually transpiles Puffs code to C code, but that code is safe with respect to buffer overflows, integer arithmetic overflows and null pointer dereferences. **The key difference between Puffs and other memory-safe languages is that all such checks are done at compile time, not at run time**. An expression like "a[i]" or a statement like "x += 1" will not compile unless the compiler can prove that i is within bounds or incrementing x will not overflow. At runtime, "a[i]" or "x += 1" in Puffs is as fast as it would be in C. This is highly interesting for me as writing safe but fast parsers is one of the goals of Rust as well! It would be really great if Rust could get this feature some day in the future.
Enforcing that the programmer has met certain constraints at compile time is something I've been seeking to create a language for. I've had that idea for months now, and I've got some decent ideas around it, but writing a compiler is a lot of work.
I've tried Tree Style Tabs. All it seems to do is add a second list of tabs to the left of my screen with the tabs at the top still remaining. Looking at my Vivaldi tabs, they're so small that the title isn't displayed at all; not even a single character. That said, it still displays the full icon so it's still possible to tell what's what with no scrolling. Vivaldi has the option to move the tabs to the side, which I've heard some people like, but I find it suffers from most of the same problems that Tree Style Tabs do. Namely, a significant portion of my screen gets taken by tabs. And if I can't find the tab I'm looking for, I'm always an F2 away. I'll check out Tab Center Redux, but I fear it still won't do much to the main tab bar. One other complaint I have with Tree Style tabs is that it introduces weird dependencies between tabs. With Vivaldi Tab stacks, it's (usually) obvious when a tab is a stack and the stack doesn't depend on any individual tab. In Firefox, I see 2 tabs next to each other, but one mysteriously can't be moved and when I close one, several to the right close as well.
&gt; Looking at my Vivaldi tabs, they're so small that the title isn't displayed at all; not even a single character. That said, it still displays the full icon so it's still possible to tell what's what with no scrolling. Right, setting tabMinWidth to something small like 20 will achieve that same effect. tabMinWidth is the minimum tab width after which they start scrolling. You can set it to 1 or zero if you want them to always squeeze and never scroll (unsure if zero works). With TST you're not supposed to use the top tab bar anymore, there's an API in the works to make it possible for TST to hidethe topbar.
One of their reasons was that indexing operations in a tight loop would incur a penalty every time for bounds checking. But in fact bounds checks are often removed by the optimizer.
The thing is that Rust usually doesn't rely on indexing as a looping primitive, but iteration, which often doesn't need a bounds check.
Also... people still use Google Groups? TIL.
That argument is usually the first reply that you are getting when you are pointing out that bounds checks are expensive. The second argument is "just use iterators". For the first argument, a while ago I have tested the performance my vorbis parser by removing bounds checks in th compiler and compared it to an unpatched compiler. **there is an actual, non-zero residual slowdown caused by the bounds checks** that survives optimisation. I might want to re-run that test though now that we have newer LLVM but I expect similar results. About the second argument: yes, iterators cover the 99% case where you just want to do an operation over all elements in an array. But when writing parsers, access patterns are often unusual e.g. you need to both read one part of the array, and write to another one ("need" in the sense that this is part of performance optimisation), or have two index variables that have different strides into the same array, etc. Iterators have some little support for custom access patterns, but don't allow much generally.
tabMinWidth seems to stop doing anything when too low. At the minimum size, my tab bar on my laptop is just shy of 7 full screens wide and there's no way of grouping, say, everything from TVTropes or Reddit in the space of one tab. Moving the tab menu to the side of the screen is not what I want.
Fuzzing is everywhere, but model checking is still way behind :(
Doubly so in a language explicitly intended to be used in highly correctness-critical systems with large sums of money attached.
Cool work! It's a shame about the fully-imperative nature of the language: https://github.com/google/puffs/blob/master/std/gif/decode_lzw.puffs Currently I'm hard at work on an as-yet-unnamed, compile-to-Rust binary data description language based on the IPADS/DDC work: https://pdfs.semanticscholar.org/2f5f/097261fdb7b0922fb548b487be28613640d8.pdf - hopefully this will provide a basis for standards that are more declarative and less coupled to the 'how' of parsing. Of course the intention is to provide escape hatches to do the trickier parts of formats like GIF, but that doesn't mean that the entire language needs to be imperative. :)
&gt; without needing any additional toolchains. What about the puff's toolchain? Its definitely smaller than rust, but its still a toolchain.
never seen it before, but it looks like someone who has a goatee smiling while wearing glasses
The author probably thinks of committing the generated C code into git, but that obviously doesn't help if you want to edit the functionality :).
That description reminds me of [Niko's avatar] (https://avatars0.githubusercontent.com/u/155238?s=220&amp;v=4). Ok, not technically a goatee, but still I think it fits pretty well.
I actually created https://crates.io/crates/oreutils as a single thing you can cargo install to get all the utils, but I haven't yet added projects to it mostly because you can't easily bundle up binary crates.
[Scripts shouldn't call ls either](http://mywiki.wooledge.org/ParsingLs)
You might be interested in Idris which has dependent types to do things like this
I haven't played with it for more than a few minutes, but I do think Idris is cool! It's just not quite how I would do it... no idea it my way would be better or worse, though.
Just a heads up that Firefox Developer Edition doesn't like your website: "Your connection is not secure The owner of shanavas.org has configured their website improperly. To protect your information from being stolen, Firefox Developer Edition has not connected to this website."
It's often like that, it doens't mean it's not a breakthrough in practical terms.
You might as well commit assembly output from rustc into git.
OP was talking about plugins. For example Vim plugins, or w/e.
Only with the new formula.
People at Google do. Yours truly, A person at Google.
Thank you for the info. The site is served by gitlab pages. I haven't configured SSL certificates yet. May be FF dev edition doesn't like it.
Beyond that, it'd be awesome if this was submitted for F-Droid, especially if it doesn't use any GApps libraries.
brass knuckles with two wolverine claws -- it means "keep killin' it"
You can't just remove all the bounds checks and say "see? it's faster!" if you're comparing with Puff. A true comparison would *replace* all the bounds checks with source-level ones that Puff could check, and that would reduce the delta at least somewhat.
So... dependently typed C?
I'd suggest starting with this blog post: [The Problem With Single-threaded Shared Mutability](https://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/) The gist is that a massive swathe of safety issues boil down to mutating shared state... and the simplest way to provably prevent them at compile time is to either disallow sharing or disallow mutating. Actor-based languages generally solve the problem by disallowing sharing. (You have to send things back and forth across channels) Rust solves the problem by disallowing mutation. (The language only allows multiple references to the same memory if none of them are mutable.) ...of course, locking primitives allow you to side-step that by moving the "shared XOR mutable" enforcement to runtime.
Part of it is threading, but shared mutability has flaws that can even be seen with a completely single-threaded program. Consider the following: let mut data = vec![1, 2, 3, 4, 5]; let x = &amp;data[0]; data.clear(); println!("{}", *x); With the current Rust compiler, this is not valid. But if `.clear()` was allowed to take the mutable reference on `data` while `x` is still active, then you could end up accessing deinitialized memory. I hope this example clarifies it! :)
That does make sense! I see now how Rust's checking prevents this kind of thing. Just a hypothetical, could Rust instead just make sure that the borrow reference doesn't outlive the mutable reference? let mut data = makeBinaryTree() let x = &amp;data.left; data.clear(); println!("{}", *x); Do you think it would be theoretically possible for the compiler to track it and say that we shouldn't have used x there? I'd like a world where we could have borrow references if they're guaranteed within the lifetime of the owning reference. I'm guessing that's not possible, because otherwise Rust would have done it.
*yay for proprietary software!*
That is what they're trying to achieve right now with non-lexical lifetimes! Once those land, removing the println! would make this code valid
It looks like it really doesn't do bounds checks at runtime -- at the cost of requiring [step by step proofs](https://github.com/google/puffs/blob/master/std/gif/decode_lzw.puffs#L53) at compile time. So, it would presumably be faster than even the best case of optimized bounds checking.
The old man paused. The silence was punctuated only by the rhythmic *tick, tock* of the ancient clock across the room. The small boy squirmed in his grandfather's lap. "... like what?" the boy asked timidly. "To this day, no one knows," the old man replied, his eyes staring across at the weathered timepiece as it relentlessly delimited the seconds passing. "We waited. Waited an age, but they never finished the sentence. "Some gave up, moved on with their lives. But some of us, we had *hope.* We waited, certain in the knowledge that one day, *one day soon*, they would return and tell us 'like what'." The old man's eyes glazed over as decades of misery washed up before him. "But they never did," he said, a crack in his voice. "We waited so long to help, but never could. By the time some of us started to die off, it was too late to wonder if we should stop. We'd already invested too much of ourselves. It had become a *quest*, something we couldn't abandon." Tears began rolling down his leathered cheeks. "So much of my life... so much of my life *wasted* because of a single missing word. I never did anything with my time, never found a fulfilling job, never found love, never made a family." The boy looked up, confused. "You must have," he asserted, placing a small, warm hand atop his grandfather's. "I'm here." The man looked down, his tears coming in torrents, his features hardening into a look of anguish and pain. "No," he said. "You're not." The man sat alone, crying.
&gt; because of a single missing word. And therein lay the folly of his youth: he had suffered under the misapprehension that that he was missing but *one* word, and that all might be set right with such a simple addition; when in fact it was a phrase of several wordsâseveral words which he and his companions were *spared*, even several words that would have rent their entire existence asunder, leaving nothing whatsoever but a dismal ruin. Better the wasted years of false hope than the void of insanity.
Refinement types could edge it past the optimizer's best case, but it would still require runtime bounds checks in some cases. The idea is to force the programmer to write them, giving them control over where they happen. Completely eliminating them is impossible.
It should be noted that they are, in fact, synonyms in the English language, which I think adds to the confusion. This distinction in the nomenclature only exists by choice in software engineering.
oops, I think I've been using the completely wrong terms. Facepalming hard over here. I meant to ask, "Why does Rust not allow borrow references and a mutation at the same time?" This might be a better example: fn main() { let mut x = 6; println!("Hello, world! {}", x); let myRef = &amp;x; x = 7; // error, because there's an active borrow println!("Hello, world! {} {}", x, myRef); } This code violates the rule at http://arthurtw.github.io/2014/11/30/rust-borrow-lifetimes.html which says "During a borrow, the owner may NOT (a) mutate the resource" It's that rule that I'm really wondering about. x goes out of scope at the end of the function, and so does myRef, so there's no memory unsafety here... I think Rust guarantees that if you have a borrow ref to something, it cannot change. Why does Rust have this guarantee/restriction? Also, I didn't know non-lexical lifetimes were a thing! I'm quite excited for those!
As others have alluded to, it's a bit hard to tell exactly what you're looking for from your post title. :) You might find joy from this list: https://github.com/rust-unofficial/awesome-rust#web-programming My uninformed guess is that you're most likely to be looking for: - Reqwest for a quick and easy to use HTTP client; or - Hyper for a full-featured HTTP client _and_ server. 
Wow, I've never heard it explained like that, that's pretty elegant. So, this guarantee/restriction isn't about memory safety, it's about being more able to reason about your program. You'll never have a value change out from under you because of this restriction. And that's the source of a lot of bugs, apparently. I now understand the benefits of the system. But now I'm wondering why they sacrificed so much for this kind of safety... If I keep 10 Marine objects which are all targeting (via borrow ref) a Zergling object, and I want to change the zergling's x and y properties, I can't now. I remember that to get around that, I instead gave each Marine a "zergling ID" which it could use to look up the zergling. Seems to me like a rather devastating tradeoff for the Rust language designers to have made. What am I missing?
It's not to do with heap vs stack objects. What you're seeing there is a feature called "deref coercion". The compiler sees that you're using e.g. `&amp;Vec&lt;String&gt;` where `&amp;[String]` is expected, and also notes that it can dereference `Vec&lt;String&gt;` and get `[String]` - so will do so to make the types work. The same thing applies for `&amp;String` to `&amp;str`
&gt;What am I missing? That the rust team know that these are issues and that the system isn't perfect. There are ways to allow internal mutation (you give out references to a struct, and the struct mutates the variable internally) at compile time with something like [cells](https://doc.rust-lang.org/std/cell/). Though for your case i don't know if it would work. The system rust uses isn't perfect, but it is found to be good enough for the majority of situations, but for the rare case where it doesn't work the std library provides types that can help you get what you want done.
There are some memory safety issues that shared XOR mutable handles - imagine if you pushed an element to a Vector while you are holding a reference to one of its items!
What is the timeline/process for unstable feature stabilization? I'm looking forwarded to [repr(align)](https://github.com/rust-lang/rust/issues/33626) but can't quite tell if it's close to being on stable. (afaik on stable the largest alignment you can get is 8)
Clay Shirky said âCommunications tools don't get socially interesting until they get technologically boring.â and I think a sort of similar thing can be said of programming languages. "Clever and safe programming languages don't get interesting until they get widespread." 
&amp; never means stack nor heap - it simply means a reference to some object. When given `fn parse_config(args: &amp;[String]) -&gt; (&amp;str, &amp;str)`, there's a single input with a lifetime, so rust implicitly makes the output lifetimes the same. &amp;args[1] is indeed an `&amp;String` - but `String` implements `Deref&lt;str&gt;`, so it can be coerced into `&amp;str` automatically if the type is known. If you want to be explicit, you can do `let query = args[1].as_ref();`. However, this is rarely done in real rust programs, because `&amp;String` and `&amp;str` aren't that different. `&amp;String` represents a shared reference to a heap-allocated string, and `&amp;str` represents a shared reference to any string.
Okay cool, so these are actual drawbacks and not just figments of my imagination. I look forward to seeing how Rust evolves in the next several years and how it tries to tackle these things!
No proprietary dependencies that I can see, should be perfect.
&gt; However, I don't think it provides me with any data I'm not already getting. Local path dependencies don't work by reading the `Cargo.lock`, local dependency overrides most likley neither. `cargo metadata` just handles all this cases and your code is future proof for additional cargo changes. 
Definitely not figments of your imagination. The compiler is helpful, and i think rust has done a really good job of making the compiler useful for *general* programs. I have a bit of a hard time refuting contrived code examples because most of the time the example wouldn't come up in real code (but i do like finding an example that does come up, and then being able to walk through it) At the bottom line the compiler tries to help by the rules it follows and the things it can *prove*. If it can't prove something then the language gives you the tools to do what you need, but then let you handle anything that goes wrong. This is true of unsafe code, code that has things to skirt around the borrow checker, but for the most part i recommend to people to really look for ways to not just *get around* the problem. Look for a proper solution first, and if its not possible then move on. because... Rust's guarantees make it sooo nice to refactor code, especially in multithreaded environments, and ensure that nothing is going horribly wrong (there can still be logic bugs, obviously, but that's what tests are for!) I think rust in a few years is going to be a really nice language, but possibly complex, especially to people new to it.
Scripts you run in terminal executed in a sub-shell it doesn't know about your aliases. If you want your script to care about your aliases then you run it like `script foo` p.s. `\ls` will look for `ls` in PATH ignoring all aliases. source: I'm shell-wizard.
B-) Â· 
To fight climate change you just need to take all that old inefficient software and ***REWRITE IT IN RUST!*** More seriously, while in the large it would indeed produce a tangible effect on worldwide power consumption if we as a profession began favoring efficient languages (more efficient than your average scripting language, anyway), in your personal life there are many other factors that will have more of an immediate impact. You know, the usual: reduce the amount of waste you produce, drive less, eat less meat, etc. In the meantime, if you truly do want to reduce friction to helping people choose power-efficient languages, then identify a popular piece of software in Python/Ruby/PHP with no Rust equivalent and start porting. Eventually, it might add up to something! (As for climate modeling, I really doubt that Rust is getting used there... AFAIK climate models run exclusively on supercomputers, and are written in dialects of languages explicitly tailored for supercomputing.)
Here's a stupid thing and a bit of a moon shot, but if you can figure out a way to make a blockchain proof of work / crypto currency that doesn't require wasting actual shit tonnes of fossil fuels on generating fictional "money" for anarcho-capitalists by doing work that is *literally not valuable at all* it'd actually do a lot to stop parts of SE Asia from polluting the hell out of the environment for actually no good. Like if instead of churning through sha hashes trying to find the right nonce that results in a hash that starts with the right number of zeros you were using that energy to do climate modeling or looking for particles in LHC data or scanning the skies or figuring out the protein folding required to engineer a virus that would finally eliminate stupidity from our species or what have you ... *anything* of value. Blockchains are going to be really big for a really long time specifically because they enable the kind of people who are the worst offenders to keep track of their hordes in minute detail; make those systems do something good as a side effect, and you change the world.
Hello A shameless ad for my own crate :-). There's an alternative to the async-await macros that *work on stable*. Sure, it has some downsides too. But I think you could have made your code easier by that. https://crates.io/crates/corona
There are times when something is safe but the compiler can't [1] verify that (the most obvious examples are Mutex's, which are [`unsafe`](https://doc.rust-lang.org/1.10.0/src/std/up/src/libstd/sync/mutex.rs.html#117-125) underneath). Rust would rather risk being too strict, and if you need to get around it, you can just stick it into an `unsafe` block. [1]. Halting problem and all.
Another thing you could also do is to implement new optimizations in rustc (at the Rust or the LLVM level). Doing this would make all Rust software more efficient for little to no cost at all. 
Ah, that's valuable. Thanks! :)
I forgot to say before; your unpublished crate sounds great! Range's + cpu pool would both be very useful (especially as I realise how important it is offloading as much as possible to the cpu pool - just moving Json encoding sped my master up by 3-4x). `hyper-file-server` or `hyper-file-service`(in line with the whole Service trait stuff) maybe if you want to leave an opportunity for a more generic name to be claimed, and be really explicit?
You could use conditional compilation to decide which file to load at compile time, or environment variables (which you set on the prod machine, but not on the dev machine) to decide at runtime. Conditional compilation would look something like this: ``` // In rust code let cfg_path = if cfg!(features = "production") { "cfg_prod.toml" } else { "cfg_debug.toml" }; // In cargo.toml [features] default = [] # or `= ["production"]` if you want it enabled by default production = [] // When building cargo build --release --features production ``` You can of course add more features if you want more config files. Using environment variables is a bit simpler setup-wise. You just call `std::env::var("abc")` and match on the result.
NSIS for Windows Flatpak for Linux zip via a Python or Bash script for macOS
Nice work! One suggestion though: the systemd-unit man page mentions looking for units in /usr/lib/systemd/system (https://www.freedesktop.org/software/systemd/man/systemd.unit.html). Maybe could you put your unit in there, allowing users to override it if need be without touching your file?
Were there any significant bugs in Rust compiler discovered during Stylo development? I mean bugs that were manifesting themselves in Stylo.
Thanks. I didn't know about this project.
Trust me, Iâm sure about it.
One thing you might not know is, how subtle and devastating such bugs can be. Your Program could work correctly for weeks, before a rare race condition is triggered and totally invalidates your result. Depending on the complexity of the code involved it could take months to find that error, or worse still, it could go unnoticed and important decisions could be made based on completely wrong data. Enforced safety versus ease of use is a trade off, but because you can work much more fearlessly, it's a tradeoff that quickly pays off in debugging time, safety and correctness.
As long as there's just a single Redox, there's actually no problem. If Redox ever explodes to several distros like Linux has, then we get the situation where a deployment made in distro x will not work on distro y because of library differences.
GNU/Linux practically does, ever since glibc broke static linking.
Linux does this too, so misplaced sarcasm there.
Can't differing update schedules lead to library differences? EG, the version number of `dependency` that you get depends on whether the user is installing you before or after `dependency` was updated.
Yes but your `./configure` scripts would still handle that for you if you end up building from source. Which is already a thousand times easier.
I've gone through the same stages writing `rusty-tags`. :)
I totally agree with your post, but what their text communicates doesn't establish a lot of that context.
 &gt; Certainly there is a way to implement dynamic dispatch where you don't have a data element, but our trait object system is not well suited to be adapted to that use case. Maybe there are edge cases where this won't work, but what I envision is that (suppose `impl Foo for i32`) we'll monomorph downto something like: /* Normal case */ fn bar_i32() -&gt; usize { Foo_foo_i32() } /* Trait object case */ fn bar_dyn(f: &amp;Foo_vtable) -&gt; usize { let foo_ptr = &amp;f.foo; (*foo_ptr)() } So the trait object case would add a vtable ptr as a parameter, either behind the scenes like shown above, or we could make it explicit somehow if we have a good syntax for it. 
Could Parity create its own contract language without forking from Etherium?
Could be a smiley face with legs too !
I don't recall any.
*There is no vtable* when you call `bar::&lt;dyn Trait&gt;()`. What vtable would the pointer point to? There could be infinitely many viable candidates, for each type that implements this trait.
Yes, that's possible. The Ethereum Virtual Machine only understands the binary opcodes anyways, so you can go ahead and wrap your own language around it. [Solidity](https://github.com/ethereum/solidity) is only the most popular choice (JS-like). There is also Viper https://github.com/ethereum/viper (Py-like), [Serpent](https://github.com/ethereum/serpent) (Py-like), [Mutan](https://github.com/ethereum/serpent) (C-like), [LLL](https://github.com/ethereum/cpp-ethereum/wiki/LLL-PoC-6/7a575cf91c4572734a83f95e970e9e7ed64849ce) (Lisp-like), [Bamboo](https://github.com/pirapira/bamboo).
Well, not _easily_. It was straightforward with LuaJava because of Java reflection and the extremely plastic nature of Lua. But then dealing with TWO garbage collectors having interactions isn't such fun! 
Some good example of such piece of software?
I check the PRs in the compiler occasionally and I'm amazed by them and I don't think I can replicate them.
If you want an example, please look at my [telnet example written with mio](https://github.com/antoyo/async-io-rs/tree/master/telnet-mio).
Don't worry about that :) the Rust team is very open about helping out and tutoring anybody who would like to do something with the compiler! So if you're actually interested in trying, I would try to find an easy issue on Github and ask for help on getting started, and then you can move up to more complex things like optimizations when (and if) you feel like it :D
I have structs like this: pub struct Indexer&lt;U: Clone + Hash&gt; { pub values: Vec&lt;U&gt;, } pub struct DataFrameBuilder&lt;I, C&gt; where I: Clone + Hash, C: Clone + Hash { /// 2-dimentional block contains multiple type. /// I: type of indexer /// C: type of columns index: Option&lt;Indexer&lt;I&gt;&gt;, columns: Option&lt;Indexer&lt;C&gt;&gt;, } And this: impl&lt;T: Clone + Eq + Hash&gt; From&lt;Vec&lt;T&gt;&gt; for Indexer&lt;T&gt; { fn from(values: Vec&lt;T&gt;) -&gt; Self { Indexer::new(values) } } As usize is Clone + Hash, I'd imagine this would work but does not: let columns: Vec&lt;usize&gt; = (0..values.len()).collect(); self.columns = Some(columns.into()); ^^^^ the trait `std::convert::From&lt;std::vec::Vec&lt;usize&gt;&gt;` is not implemented for `brassfibre::prelude::Indexer&lt;C&gt;` Any idea?
Oh, good catch, thanks!
/u/enrac If you want to see a real-world example for using `nom`, you can take a look at my [`uncbv` crate](https://github.com/antoyo/uncbv). Be careful tough, as it uses nom 2.0.
Obviously it's a cool walrus wearing sunglasses.
The amount of CPU cycles developers burn when compiling rust code negates any potential wins in end-user applications. I'm half serious.
&gt; The real sad/funny thing is that most everything in rust has been around for quite a while IIRC, that was Graydon's rationale for the name: There's no shiny new ideas in the language, only rusty old ones.
my 2 cents on the Rust VS C performance while parsing: - it's easy to write a fast C parser, just don't check anything (most of existing C parsers) - it's hard to write a correct C parser, which often means making a lot of very conservative choices (like redundant checks, copying data to temp buffers, etc) - it's easier to write a correct Rust parser that's reasonably fast (and often faster than correct C parsers) Puffs has the potential to get the same benefits as Rust because a lot of checks are done at runtime. Compiling to C is not really the biggest benefit, they could compile to LLVM IR and get the same result. I like the focus on handling overflows, though. Whenever I fuzz a nom parser, overflows and underflows are among the most common bugs
Most people seem to think that a C toolchain is free, since you basically already have it everywhere. It's sorta like how C and Rust both have "no runtime", even though they do have a runtime.
Just to put the scales in perspective: with Rust code now being essential to software like Firefox, Dropbox, Visual Studio Code, and others, Rust code is still being executed far more often that it is being compiled. But there's certainly an opportunity to help the environment by making our own tooling more efficient :D
What kind of configuration values are we talking about? It's often easiest to use environment variables (so it's just a `std::env::var("DATABASE_URL")` away). On production, your deploy process can set them, and with tools like dotenv you can easily load them from a `.env` file on your dev machine.
You might run in some issue with rust, in that you'll need to prove that it has been validated by intense usage on the field (if i remember correctly the SIL-2 constraint). You might want to check with the organism that will do the certification first, or run the risk of it being rejected.
I suspect you forgot to add ```Eq``` trait to the implementation of method which contains: let columns: Vec&lt;usize&gt; = (0..values.len()).collect(); self.columns = Some(columns.into()); And because of this ```columns.into()``` will not work because your ```From&lt;Vec&lt;T&gt;&gt; for Indexer&lt;T&gt;``` trait implementation explicitly requires defined ```Eq``` trait. My stab at this: https://play.rust-lang.org/?gist=f92a09a78d0de10b062ba196a7c1a12f&amp;version=stable Check what happens if you remove ```Eq``` from ```DataFrameBuilder``` implementation. 
Obligatory HN post describing the [issues](https://news.ycombinator.com/item?id=14691212) with Solidity
Sure, but the Solidity VM itself is [pretty darn bad](https://news.ycombinator.com/item?id=14691212) - an entirely new approach, like [Simplicity](https://blockstream.com/2017/10/30/simplicity.html), seems like a much better idea IMO.
Thanks to slomo from #gtk-rs: https://coaxion.net/blog/2017/09/exporting-a-gobject-c-api-from-rust-code-and-using-it-from-c-python-javascript-and-others/#rust
It doesn't work when you have a more complicated type like a vector which can move it's memory internally then it becomes unsafe. In your particular example it could be made safe (and you can use atomic types for that) however by not allowing that it also allows rust to optimise the code further for example the compiler doesn't need to immediately update X after you set it.
Maybe we someday will be able to say that Rust really rides the trains. :D I'm sorry, I'll show myself out.
Got a sort question! What is the idiomatic way of way of going from `Vec&lt;Result&lt;T, Error&gt;&gt;` to `Result&lt;Vec&lt;T&gt;, Error&gt;`? Currently I'm doing the following enum Error { ... Many(Vec&lt;Error&gt;) } fn a&lt;T&gt;(xs: Vec&lt;Result&lt;T, Error&gt;&gt;) -&gt; Result&lt;Vec&lt;T&gt;, Error&gt; { let mut v = vec![]; let mut errors = vec![]; for n in xs { match n { Ok(n) =&gt; v.push(n), Err(n) =&gt; errors.push(n), } } if errors.len() &gt; 1 { Err(Error::More(errors)) } else if errors.len() == 1 { Err(errors.remove(0)) } else { Ok(v) } } Not quite happy with this solution and wound be interested in seeing different approaches!
&gt; Whenever I fuzz a nom parser, overflows and underflows are among the most common bugs [Yep!](https://github.com/rust-fuzz/trophy-case)
OMG, I didn't know it's **that** bad. Thanks for the link!
I don't think that the name has anything to do with rust (as in oxidation), but everything to do with rusts (as in [the fungus](https://en.wikipedia.org/wiki/Rust_(fungus)))
I can understand Rust not being an option maybe, but there seems to be no mention of ATS anywhere despite it: * also compiling straight to C * having full dependent types and a proof system (so you can â and can require e.g. proof of the termination of a recursion)
Idris is a "higher-level language" (similar to Haskell) though. An other option if you want the fasts (and strong allocation control) would be ATS.
That would be ATS, Puff [specifically reject dependent typing and functional programming](https://github.com/google/puffs/blob/master/doc/related-work.md) and leverages false dichotomies to motivate it e.g. &gt; Puffs is an imperative language, not a functional language, despite the long history of functional languages and provability. Inner loop performance usage matters &gt; Unlike Puffs, [F\*] is a functional language (with dependent types), not an imperative one (with a simpler type system) 
Thanks so much for the answer! I tried removing Eq from all the DataFrameBuilder stuff. Not good. Then I've tried adding Eq to everywhere, same error. Now the only place without Eq is the Indexer struct itself because adding it there breaks other million things so unsure if that's the path Here I've tried to copy the most relevant pieces: https://play.rust-lang.org/?gist=f09cb74efd483a94f51b132b2489b534&amp;version=stable Thanks so much again for your explanation 
/r/rustjerk
Good point. Parity offers experimental WASM support for contracts. https://github.com/paritytech/parity/wiki/Web-Assembly-(WASM)
Rust on Rails
That does ring a bell, now that you mention it, but I'm fairly certain that Graydon brought up the old and busted thing when he originally announced the language at Mozilla Summit 2010 (I was in the room)
Well, if you like shooting yourself in the foot, then just dont buy a gun ;) C++ is perfectly safe as long as you know what you are doing. Rust is written in c++, and you trust rust, which uses c++, so c++ is perfectly safe. The point is not about language, its about trusting people to choose the right tool for the job and to not be lazy assholes who will try to cut corners. The job is job, somebody must do it, so if you are noob at c++ and you think that you are not smart enough to learn it, you will try to learn easier/higher level language. Its also easy for a new language to drop a ton of legacy support. But with time, rust will become same bloatware as c/c++/any other old language.
Right, because it wasnt even a bug, it was noob developer not knowing what he was doing.
https://github.com/casey/just is a command runner in the spirit of `make`
Let's get this out of the way: I'm not ragging on C++. Especially with the recent updates it is a lot nicer, and the proposed changes for the next versions have me very excited for its future! :) I think my choice of expression was a bit.. let's say "accidentally misleading," since you're focussing another problem than the one I was thinking of. Browsers are complex beasts. This inherent complexity almost _demands_ difficult to follow "hacks" at some point, and even if it doesn't, it makes it easy to miss that, say, one code change here invalidates assumptions made elsewhere, introducing a bug that did not exist before. It's not about the corner cutting (although I agree this is probably the bigger cause of bugs in software at large), it's about the inherent complexity of the problems to solve. The fact that C++ lets you shoot yourself in the foot here isn't the fault of C++ so much as it is the result of this complexity, and the absence of tools to manage that complexity.
LLVM-IR output maybe. I wouldnât agree with committing assembly as equivalent, as one of the goals is to be platform independent.
The solution I'd go for is this: introduce a second lifetime parameter for the signature of `build` that is at most as extensive as `'a`. That way, the reference you pass to it doesn't have to live as long as the struct itself. For your reduced example code, this works: https://play.rust-lang.org/?gist=34446c68c77343d0462ada6f05f3b77a&amp;version=stable
Well, chrome is written in c++, and it is still faster than ff, but ff team doesnt have as much great minds as chrome team, so they had to go with different language, that would do itself more. | the absence of tools to manage that complexity There is no way to manage it. Every big product sucks balls and has shit code and millions of bugs. That is just nature of huge code bases and huge products. It all needs to be divided properly, managed properly, and it must make money, and you must do it all in fixed amount of time. And you can choose only 2 of them max. Bugs dont just appear out of nowhere, they are result of potato programming, when you write code that you dont understand, or you made meme potato reusable code base, and you changed one function, but you forgot that a lot of code depends on that function, not just one line of code that led you to that function.
I hope you're aware you are sliding from "reasonable point of view" into ["real programmers"](https://en.wikipedia.org/wiki/Real_Programmers_Don%27t_Use_Pascal) territory here.
**Real Programmers Don't Use Pascal** "Real Programmers Don't Use Pascal" (a parody of the bestselling 1982 tongue-in-cheek book on stereotypes about masculinity Real Men Don't Eat Quiche) is an essay about computer programming written by Ed Post of Tektronix, Inc., and published in July 1983 as a letter to the editor in Datamation. Widely circulated on Usenet in its day, and well-known in the computer software industry, the article compares and contrasts real programmers, who use punch cards and write programs in FORTRAN or assembly language, with modern-day "quiche eaters" who use programming languages such as Pascal which support structured programming and impose restrictions meant to prevent or minimize common bugs due to inadvertent programming logic errors. Also mentioned are feats such as the inventor of the Cray-1 supercomputer toggling in the first operating system for the CDC 7600 through the front panel without notes when it was first powered on. The next year Ed Natherâs The Story of Mel, also known as The realest programmer of all, extended the theme, as have many subsequent articles, webcomics and in-jokesâwith the alleged defining features of a "Real Programmer" differing with time and place. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Ok, that simple example could work. But what about [this](https://play.rust-lang.org/?gist=88b639f74072dbdcb9588eb990f3a51b&amp;version=stable) example? If you click run, you'll see that the address where the first element is stored can change when you push a value. If you were able to mutate it while still holding a reference, the reference could end up pointing to deallocated memory, or memory now being used by something else.
The problem is - your bounds of impl&lt;I, C&gt; DataFrameBuilder&lt;I, C&gt; where I: Clone + Eq + Hash, C: Clone + Eq + Hash, are too wide for ```with_values``` to work properly. You need to define a different trait with narrowed down types. I took a stab here: https://play.rust-lang.org/?gist=0927c76f1a5be35a24e996fae27b134a&amp;version=stable Now all you need to do is to win a fight with borrow checker =D
But puffs is written in go, and is its own compiler. You either need to bring in the go toolchain so you can build the compiler, then build the puffs code. Or you can distribute the puffs compiler. Finally, you could just vendor the outputs of the puffs compiler but that leaves them uneditable by your users. But you could also just vendor rust dylibs, so I'm not really sure I buy this line of argumentation.
This comes up more often writing function signatures and structs, but there's an extra theme here besides safety that's really important: **Rust wants to support stable and explicit APIs.** Say I have a shared reference to a big `Foo` object, and I want to edit one of its fields. Rust says, "sorry, you have to have an `&amp;mut` reference instead." I say, "*C'mon dude*, that's gonna require a ton of refactoring, and you're going to make me reorder random lines to satisfy the borrow checker, even though we both know that this 'shared' reference isn't actually aliased here and that my mutation is safe." And Rust replies unto me: What you're asking for sounds convenient at first, but it turns into a disaster before you know it. First of all, there are limits to what I can prove. Suppose you refactor a couple functions, or put something in a loop, and suddenly I can't prove that this reference isn't aliased anymore. It will seem like random innocent changes are breaking your build for no reason. But second, there are so many ways it might be aliased _in the future_. You could create simple local variables far away from here, that alias what you're mutating in this line. When you try that, who should I say is at fault? You might even find that upgrading a library adds aliasing here, where before there was none. The problem is that, by taking a shared reference, you've told the rest of the world that it's ok to alias this whole `Foo` object. But not you're saying, it's not actually ok. Even if we can somehow prevent unsafety, now anyone who calls your function, or any of _their callers_ need to be aware of the exact details of what you do and do not mutate. Not to mention the _compiler_ needs to perform all that analysis every time you build. There shall be weeping and gnashing of teeth.
BTW, the itertools crate has some iterator adapters weigh make that much easier.
I suppose what I'm going for is general-purpose (in fact, besides the `http-file` crate I mentioned, it's based on a more abstract `http-entity` which you can use to construct abstract HTTP entities for conditional / range GET/HEAD serving) but not canonical. Maybe I need a clever project name in there. Or maybe I should just be more bold and claim the canonical name. I suppose another caveat is that I probably want to change the interface to be based on the `http` crate's types rather than hyper's ones. Looks like hyper is starting to move over. Or the alternative is that I find a good framework and try to get it integrated into there. I don't know if you'd need a different interface to use it with a framework, or if they (will?) support anything using the http types. I talked a little with Iron folks a while ago, but that framework seems to be dead, or at least they still haven't moved over to async hyper. I think rocket, gotham, and shio are the cool new ones?
The `Result` enum implements `FromIterator`, so you can just create an iterator and [collect](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.collect) into a `Result&lt;Vec&lt;_&gt;,_&gt;` like [this](https://play.rust-lang.org/?gist=558290873b11f6ef479aec4b962de664&amp;version=stablee).
Oh... so we can do Etherium contracts in Rust? (Rust -&gt; WASM). Anything holding Rust folks back from doing this today? 
Just a `into_iter().collect()`, with the right type annotation if inference fails, should be enough.
How about writing software to help influence politics and public opinion, to get people into power who are willing to promote fossil fuel alternatives? It's been done before (Cambridge Analytica). Or maybe you could create the AI that will outsmart the human race and take over the world, only you'd be the one who remembered to program it to support human life and preserve the environment. (Fearless Concurrency will be real important as it spreads across the net taking over all the servers.)
So it's not quite an Ethereum contract because the Ethereum blockchain itself doesn't support WASM contracts. Parity as a blockchain node is very general, and allows spinning up chains with different consensus algorithms and features beyond what the Ethereum mainnet supports. You can start your own chain using Parity that uses WASM for smart contracts.
Maybe solving protein folding problems as proof of work? Or other useful scientific problems? Could work like this: Scientific body supplies code modules to run, and problem data to work on. Miner runs code and returns solutions. Scientific body's server provides a crypto certificate that work was done (they'd need a way to reliably check that work was actually done for that problem, though).
Vim plugins are (usually) just VimL code. There's no system linker involved there. However, Vim can load its Python, Ruby, Perl, etc. support on-demand. That requires a dynamic linker. So does performing `import numpy` in Python. Unless your applications are all going to embed all the compiled Python modules and require a recompile for upgrades or additions?
https://twitter.com/sgrif/status/928275953486323715
It's my understanding that Graydon gave several different reasons for the name, and there's no single canonical one.
Yeah, that's the mental gap part. With Bitcoin, for instance, whenever a node says "Found the nonce!" literally every other node can independently and very cheaply run the same hash with the same transactions and candidate nonce and get the same result. The "experiment" is hard, but the confirming the results is inherently easy. "Yup, shit, that's the nonce ... next block please!" You'd need useful work that nevertheless had some of the key properties of a cryptographic hash function. That *should* be quite a few optimization problems, evolutionary algorithms, ML fitting problems, automated bug searches, etc... it's not an easy problem, which is why I haven't changed the world.
There is actually work being done on allowing the Ethereum Virtul Machine to run WASM (Web assembly). Which can be targeted (hopefully first class) by Rust. So you'll be able to write smart contracts with Rust. But on another note, people are figuring out that you don't just write automated programs (smart contracts) that are responsible for handling assets of value and just ship them... banks know this very well. Any type of automated system goes through years of testing and more audits than you can imagine until it is used. The same will happen in the smart contracts world, until then, only $300 million USD was locked up. Much better than 10 years from now when that number could be $3 trillion and could cause an economic hit. (once blockchains are used to store more value)
Damn, I saw the title had Rust and railway, came here to post that =p But this is cool someone is asking. My mom actually works in dev department of one of the top two railroad companies, I should ask her if they have looked at Rust. 
Is it possible? Yes. Does Rust make it _a lot_ easier? Also yes. Note the quote [in the blog post](https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html): &lt;heycam&gt; one of the best parts about stylo has been how much easier it has been to implement these style system optimizations that we need, because Rust &lt;heycam&gt; can you imagine if we needed to implement this all in C++ in the timeframe we have &lt;heycam&gt; yeah srsly &lt;bholley&gt; heycam: itâs so rare that we get fuzz bugs in rust code &lt;bholley&gt; heycam: considering all the complex stuff weâre doing *heycam remembers getting a bunch of fuzzer bugs from all kinds of style system stuff in gecko &lt;bholley&gt; heycam: think about how much time we could save if each one of those annoying compiler errors today was swapped for a fuzz bug tomorrow :-) &lt;heycam&gt; heh &lt;njn&gt; you guys sound like an ad for Rust 
&gt; The way I understand it is that Rust offers some compile time guarantees that usually were done at runtime. That's not quite right, Rust more specifically makes it so that you must have either compile time checking or run time checking or explicitly write "I know what I'm doing is not safe". C++ can have compile time checks, or runtime checks, or not checks whatsoever without warning you. &gt; Can Firefox's parallel style system be implemented in C++? In theory it could be done. In practice, Mozilla [tried twice in the past and failed](https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html). &gt; This top-down structure is ripe for parallelism; however, since styling is a complex process, itâs hard to get right. Mozilla made two previous attempts to parallelize its style system in C++, and both of them failed.
I think you just want the root of the repo like this :- [patch.crates-io] gfx_device_gl = { path = "../gfx" } Cargo knows how to find all crates in a repo.
I see. So basically you can write it in C++, but Rust being safer will allow you to have some guarantees that will make the code more refactor-friendly. if they are Refactor-friendly then you can do optimizations. Nobody doesn't want to touch complex code so it will be optimized less. Also the way I see it is that Rust makes problems local. If I read a function signature, I have a big picture of what it can do and how long the objects will survive. If I describe a method, I know what stuff people will give me. So basically the speed comes from these guarantees, not from memory management for example.
&gt; It's a shame about the fully-imperative nature of the language though Honestly, why is that a shame? Maybe imperative style is the best for this type of problem? There are many cases where functional style is good, but is there something inherently bad about imperative style? I mean, they've created a language which is (1) safe, (2) easy to compile, (3) fast to execute, what's not to like?
Memory management is absolutely part of those guarantees. Writing any heavily parallel code in C++ is a huge pain, because you have to manually keep track of all the data and all the references to it. It's extremely easy to create a data race, or invalidate a different thread's data, because the language provides no good way of managing that without big performance costs (mutexes/locks). Rust, on the other hand, will simply refuse to compile code that contains a potential data race. That's exactly what people mean when they say Rust gives you "fearless concurrency".
&gt; The way I understand it is that Rust offers some compile time guarantees that usually were done at runtime Rust provides some compile-time guarantees that usually were not provided at all. Ownership rules are not verified in any language I can think of that people use for writing popular web browsers (at least not to the extent Rust does it). They might be considered to be good practice, but were not enforced by compiler. &gt; Can Firefox's parallel style system be implemented in C++? Theoretically? Yes. Practically it would be much more error-prone and therefore expensive. Rust is a product of experience web browser developers had with doing exactly that. &gt; I understand ownership and borrowing, I just think they can be done at runtime also. They perhaps could be done, at the cost of performance. What would be the point though? Compile-time errors are much better at preventing problems comparing to crash reports. 
As a native English speaker...I'm not actually sure that's true. Source? I definitely want to be able to separate engineering jargon from normal English...
Synonyms doesn't necessarily mean the exact same meaning. Even in English the use of concurrent and parallel differs widely, going even to quite opposite meanings, for example, [concurrent lines](https://en.wikipedia.org/wiki/Concurrent_lines) vs [parallel lines](https://en.wikipedia.org/wiki/Parallel_(geometry)).
impl DataFrameBuilder&lt;usize, usize&gt; does not sound good. Indexer and Dataframe can accept many types(integers, floats, strings, etc) and I do not want to need to create a specialized builder for each combination. 
Rust also helps you by naturally making you design your program using as many compile-time abstractions as possible. It's common to resort to dynamic stuff in C++ because it's easier to write, although it's still possible to stay as "static" as rust.
The difference you pointed out is also a nomenclature chosen in the field of mathematics. I'm just talking about common English usage. ð
It also comes with all the compile-time abstractions that rust makes easy to use. Designing your program with traits and composition can help a lot with that, compared to the "object" way of C++ by using dynamic downcasts all the time. Also the fact that move semantics are used by default help you make mundane stuff efficient without having to think about it. You can do all of this in C++ (or in C if you really want to), but then the cognitive overhead becomes much higher as you have to personally keep plenty of stuff in check, and explicitly write less intuitive code to leverage compile-time abstractions.
Sure, take a look at [Merriam-Webster](https://www.merriam-webster.com/dictionary/concurrent); the definition of concurrent just means "operating or occurring at the same time," and even has a second definition of "running parallel." The [definition of "parallel"](https://www.merriam-webster.com/dictionary/parallel) is much more formal than "concurrent," and is mostly expressed in terms of geometry: "everywhere equally distant." I can't claim to have an accurate knowledge the etymology of "parallel"'s usage in Computer Science, but I think it's reasonable to assume it came from the idea of thinking about threads as independent, parallel "timelines" of execution, e.g.: https://juank.io/content/images/2013/Dec/4_01_ThreadDiagram.jpg
Sure. But among those reasons, the one about being the diametric opposite of new-and-shiny is a quick and handy rebuttal to the people who complain that other languages have done $FEATURE before so Rust isn't as innovative as all that (affirmative defence: Of course it's not. It's not meant to be).
The DataFrameBuilder cannot - because of the: let index: Vec&lt;usize&gt; = (0..values[0].len()).collect(); but ```with_values``` can be parametrized differently - this is why with_values&lt;X, Y&gt; 
&gt;Some of the safe languages (Go, Java, JavaScript, Python, Rust, etc.) are faster than others, but generally speaking, they're not as fast as C/C++. I've definitely seen examples where the Rust solution is faster than C (ripgrep is one example but there are more). 
&gt; It would be really great if Rust could get this feature some day in the future. I think ATS does this already as well. Not that it's remotely mainstream but this isn't quite new either. 
"The cars tried to pass the crossing concurrently" vs "The cars tried to pass the crossing in parallel". The latter doesn't feel natural at all, possibly even wrong in its intention. I know what you mean, that in general they are interchangeable, but it's not always.
&gt; There are many cases where functional style is good, but is there something inherently bad about imperative style? I don't like it anywhere near as much as functional (that might be just personal preference) but here's a challenge: write me an imperative program which * Computes the Fibonacci numbers * Is obviously correct &gt;I mean, they've created a language which is (1) safe, (2) easy to compile, (3) fast to execute, what's not to like? ATS supports functional style for one. 
&gt; Inner loop performance usage matters Which is funny since in fact ATS can give you exactly the performance of a `for` loop in C.
I think you have a valid point with the second definition of concurrency. I can see what you mean, and hope I don't sound like a stubborn opinionated man when I say next what I have to offer. :) Outside of the (very valid) observations you present here, I don't consider them to be synonymous **in usage** -- probably because I've rarely heard the terms used in overlapping contexts. "Parallel" outside of the phrase "running in parallel", which, to your point, has a similar meaning to "happening concurrently", seems to be the only place where their intended usages actually overlap to me. Does that make sense? What do you think?
&gt; organism Organization? Or has it grown into a living entity? &gt; you might be able to pull it out IF the team that does the certification is willing to support you This seems unlikely. Rust is so new that I doubt the certification team has any real experience with it. This is the main concern here, and I can't imagine the amount of bureaucracy OP would have to go through to get a new, complex language like Rust approved.
I'd like to too. Seems like an opportunity for better security (whether or not it actually *is* secure is not my area of expertise).
Iâve always thought if ATS has a package system as well as cargo for people to share their proofs then the overhead would be alleviated. I donât think without cargo Rustâs overhead would be much better than ATS. 
Well, it might be that experts are eager to try out new stuff (given that C isn't exactly ideal). The only time in my life that i dealt with some, they indeed were quite willing. . Personnally i would not support rust at this point, too many things are cryptic, and heavy looking. The language is simply too new to put one's signature on it. And i would prefer something more minimalist, with more guarantees. And/or much much more time on production behind it. Also you'd need to froze the version for the whole project, or document any change of version. If you encounter a critical bug somewhere, and need to update your rust compiler, then you do have a problem. The fact that i mention it without thinking that very unlikely doesn't give confidence that rust is a good choice at the moment.
You might say that the guarantees allow you to do more aggressive things that are good for speed. Like you can pass an `&amp;str` into any function without worrying about whether the function might save it or free it. The end result is that every function that can take a simple reference usually does, and you avoid a lot of defensive copying.
&gt; or explicitly write "I know what I'm doing is not safe" (unsafe). I think it's important to note that `unsafe` does **not** mean "this code is unsafe", it means the **opposite**: "This code has been manually reviewed to behave in a safe manner even though the compiler cannot prove it"
The block itself doesn't mean that, but that is the *hope* for what the block means, i agree. At it's most basic unsafe literally just gives you access to things you can't do in safe rust: https://doc.rust-lang.org/nomicon/what-unsafe-does.html#what-unsafe-rust-can-do The block just means the compiler doesn't have the knowledge to follow the code inside as it does the rest of rust.
It's a lot of purpose-built code, more often than not Fortran code. I don't think Rust is faster than Fortran for those particular workloads.
Well the author also said "this is a research project to see if it's even feasible and how much work it would be", so. If you're in an industry with a 3-10 year planning timeline, rather than the typical Tech Company timeline of 1-6 months, then your concerns are probably pretty different.
I think it would be, irrespective of cargo the cognitive overhead of managing your proof-threading in ATS is much higher than managing your lifetimes/borrows in Rust, which is by far the biggest cliff of Rust's learning curve. Not to mention ATS is syntacticallyâ¦ challenging.
does it also need to be very efficient? The following Ruby ought to give it in linear time (modulo the fact that integer addition is actually not constant time) and space, but also be fairly obviously correct: # fib(1) = 1, fib(2) = 1 def fib(n) return unless n &gt;= 1 values = [1,1] while n &gt; values.length values.push(values[-2] + values[-1]) end values[n-1] end 
&gt; does it also need to be very efficient? I'd say so. The functional approaches to this aren't linear in space and they're definitely less verbose. I guess my point was: functional programming isn't *always* necessary, but in some cases it's unequivocally better than what imperative programming can offer. Rust isn't a functional language by any means, but doing the above task is possible thanks to iterators. It at least has a (verbose) answer to what we could already do in other languages.
LLVM-IR interestingly enough isn't platform independent: http://llvm.org/docs/FAQ.html#can-i-compile-c-or-c-code-to-platform-independent-llvm-bitcode
Oh totally! This one is my favorite honestly.
I didn't consider that option to be fair. I think that i would still advise to bet on C even at that time scale. Or maybe write the code twice, once with C, and once with Rust, with two implementations that run together cross-checking their states.
Well, it would be fairly easy to do it in constant space as well, but you'd need to be a bit more careful about keeping track where you are.
More info on [Deref Coercions](https://doc.rust-lang.org/book/first-edition/deref-coercions.html)
You're probably right about usage, at least in the context of programming. It's not always a very clear distinction, though, especially for those programmers who may not have heard of this distinction before. I guess context matters for whether the distinction is important or not, e.g. when discussing asynchronous computation.
Aid in computer modeling of climate patterns to help climate researchers better understand what can be done to reverse it? This is a difficult question to answer since it's so open ended. The energy efficiency of rust is hypothetical based on certain anecdotes about the compiler output. It'd be interesting to see real data in practical scenarios, though.
You *can* write any code you want directly in assembly. You *can* write any C++ code in C instead. You know, assuming either flawless, perfectly disciplined developers, or endless debugging time. But those things are unrealistic. C++ theoretically lets you write Servo the same way Rust does. But it also lets you and maybe even encourages you to write it in a different, buggy, unmaintainable, broken way.
Blockchain energy abuse is really an ethical question more than a technical one. There is nothing we can really do under capitalism to fully disincentivize that kind of waste.
&gt; I mean, they've created a language which is (1) safe, (2) easy to compile, (3) fast to execute, what's not to like? It's hard to write :( Specifically, you have to use a lot of `assert` everywhere to prove that your assertions hold (at compile-time) so that the compiler is able to produce safe code. If instead you use description of the input and turn that into a decoder, then it's hassle free for the user.
In theory, yes. In practice, apparently, no. (Maybe the Chromium developers will be spurred to try to prove that wrong.)
&gt; I've definitely seen examples where the Rust solution is faster than C (ripgrep is one example but there are more). Note that those are, generally, apples-to-oranges comparison. That is, it's not that Rust is faster than C, it's that ripgrep uses a different algorithm than grep. Of course, it may be argued that Rust is an enabler in getting the different algorithm to work correctly, especially in the case of multi-threading, but that's a somewhat different angle. To compare two languages' respective speeds, you need to write code that is as similar as possible and check how it fares.
Stylo could technically be done in C++ if humans were perfect and never made mistakes. The problem is that C++ has far fewer safety guarantees, so it is easy to shoot yourself in the foot if you are not careful, which tends to happen when people work on large and complex software like Stylo. Rust, on the other hand, provides some very good safety guarantees that can be verified at compile time, leading to no performance loss, while allowing you to work on ambitious and complex software without fear. Mozilla devs themselves said that they already attempted to rewrite Firefox CSS styling for parallelism and performance several times in C++. However, all of those attempts failed, due to many bugs that were difficult to track down. The latest attempt (Stylo) was successful, mostly because of Rust's safety guarantees. This is where the "fearless concurrency" mantra comes from. In Rust, you can write ambitious and complex software without fear and anxiety. Stylo and the Rust compiler are two famous examples of that in practice. In C++, you have the constant fear and anxiety that you can accidentally screw up some little detail, leading to hours and hours of debugging pain later.
In addition to the vectorization issue, I think it's in the spirit of IEEE floats to saturate in this scenario. They are approximations of infinitely precise numbers, where integer types are exactly precise, so saturation _approximates_ what the value actually is.
ITT lots of people way over estimating the impact of inefficient code on the climate. Monitors, hard drives, GPUs, cooling, these are your power sucks, but even then computers are hardly the problem. If you really want to help the climate it'd be far more efficient to work on renewable sources of energy.
Concurrent and parallel, even though generally interchangeable, have quite different bases. Check for example, [concur](https://en.wiktionary.org/wiki/concur). The aspect of agreement, meeting at, etc, which even [quite literally is carried to other areas](https://www.reddit.com/r/rust/comments/7cwpbq/fearless_concurrency_in_firefox_quantum/dpwtbkh/), is simply absent of the word "parallel".
Yeah, it's far too immature for something completely mission critical. While not an indicator of stability, the lack of a standardized language spec and competing compiler implementations really speaks to the immaturity of the project. Rust is a fantastic language and I hope it gets into that mission critical roles, but it's going to be an uphill battle.
&gt; Am I correct and / or is passing out a &amp;RefCell&lt;&amp;'static mut Foo&gt; safe under the guarantees of safe Rust code? It does seem safe based on my understanding as well. The one caveat I would note is that *if* 2 callers try to get concurrent `RefMut`s the `RefCell` will panic and crash the entire process.
&gt; To compare two languages' respective speeds, you need to write code that is as similar as possible and check how it fares. Definitely, but that tells you a lot more about the compiler than whether *your* program written in that language is going to be fast or not. And that's an important distinction to make, since the second is going to be what you care about most of the time :)
Plugins and libraries aren't quite the same thing. The idea is that a library is linked in once and lives in that exact version forever, avoiding issues with version mismatch etc. You test what you ship. Plugins are expected to be changed independently, and would run as a separate process. This would possibly include "system level" services like SSL. Python runs an interpreter so could do whatever it wants, as long as all the stuff the interpreter wants is linked into the python executable once. Python programs that want to dynamically load up random third party native code would have to live with the same restrictions as everyone else, in such a system. 
I tried to setup LSP on a new computer. I had already forgotten how involved the setup is. Once you have LSP and RLS installed, you must also enable the autocompletion in Rust-specific syntax settings by adding line ` "auto_complete_triggers": [ {"selector": "source.rust", "characters": ".:"} ]`. Besides that, you must also have "clients": { "rls": { "enabled": true } }, In the LSP settings for it to do anything. After I enabled those two, it works flawlessly.
Yes. The tradeoff is that you can't independently update components to a program without relinking the program. This is not entirely a bad thing. There's a lot of issues with DLL versioning and random crashes due to every user running their own unique combination of dynamic libraries. With static libraries all code that runs has been tested to run together. For actual plugins (not libraries) you would have to design them to run as separate processes. Presumably the system would provide some boilerplate to make this more convenient.
There is a pattern that has developed in Rust bindings where there are 2 layers of API. There is the lowest-level repr(c) struct. Then there is a higher level struct which manages interoperability with the low level struct, including marshalling access to *const and *mut. This usually involves managing Drop semantics and such. The higher level struct impl would be the only thing that would touch the lower level struct. So the user of the higher level API would never have to know that a raw ptr exists. So it might look like : struct FooAPI { inner: *mut Foo } impl FooAPI { updateFooX(&amp;mut self, x : u64) -&gt; u64 { c_func_that_updates_foo(self.inner, x) } } Using this scheme, you decide how to control the lifetime of whatever *mut Foo points to. By default, Rust wont attempt to free *mut ptrs, so you don't have to use 'static or Refcell. If you do need to destroy Foo when you're done with it, you can impl Drop for your FooAPI struct. 
My intention is to only ever access data from within the game when the game's main thread called into me. Not only does that (hopefully) guarantee that no other thread will access that memory region, but also that I'll only access the data single-threadedly. *If* there is another thread trying to use the RefCell I'm done for anyways, because RefCell is not Sync (internally does not use locking to handle the runtime checks). But better the game crashes than undefined behaviour ;)
I should note that using this model, you could actually make updateFooX take &amp;self, and multiple callers could mutate Foo simultaneously if that's what you wanted. 
&gt; Definitely, but that tells you a lot more about the compiler than whether your program written in that language is going to be fast or not. Certainly. I think though that there are two "performance" stories: - the performance of the day-to-day idiomatic code, - the absolute performance you can wring out of the language. The latter is typically what you will find benchmarks for, and this where comparing similar algorithms in various languages help you isolate the *overhead* that some languages cannot shed. In short, it gives you the upper-limit. The former is still interesting, because beyond the "obvious" bottleneck which sticks out like a sore thumb when you profile a slow program, a lot of performance loss is actually akin to death by thousands cuts: not one big juicy piece that you can tackle, just a thousands little inefficiencies scattered everywhere which add up to an extra 5%/10% of overhead. Keyword *scattered*. I think that Rust has a **huge** advantage over C++ on the "idiomatic" front for example, simply by virtue of refusing of having automatic conversions which can allocate memory^1 . Anyway, back to your point: - I was talking about pure raw speed of the language, - You were talking about the speed of the programs the language enables. Both are valid, both matters; I agree with you :D ^1 *One of the last performance issues my team tracked down was a look-up by `const char*` in a hashmap. Why? Because the hashmap key type was a `std::string`, and therefore the look-up would systematically create a `std::string` out of the `const char*`, perform the look-up, and destroy the string. Yep. Perfectly normal.*
Nope, have a look at the gfx repo. It contains multiple source roots.
While I do know this pattern, I didn't think about this abstraction for my use case yet. One question I have with this pattern in my use-case: How would I pass an instance of `FooAPI` to the user? Would I just use a static getter-method, which creates a new intance of `FooAPI` and returns that?
Well, no, except provide them with a rock solid blockchain that happens to also do meaningful work as a side effect. They're not incentivized to waste that computing power, they're not even generally smart enough to know that's what's happening... they're incentivized to use the blockchain. We could give them an alternate proof of work that lets them keep churning away on their fictional money. A lot of mountains can be moved by harnessing greed into incidentally doing something good. Of course eventually they'll try to exploit it, because exploiting is what they do, but that doesn't mean you can't have 
Imagine you are writing a yuge project, somewhere in a very beginning to Rc&lt;Something&gt;, or in case of C++ you just did Something. Then you've introduced threads: rustc will yell at you because Rc isn't thread-safe, but C++ will compile it like nothing happened.
`RefCell&lt;&amp;mut Foo&gt;` is safe. `&amp;'static mut Foo` is safe as long as you only ever access it from one owner. I.e. if you only ever have a _single_ `RefCell&lt;&amp;'static mut Foo&gt;` you should be fine. So make sure that static is only accessed directly the first time you create the refcell.
No. There have been two prior projects attempting a parallel style system in Firefox, and at least one in another browser engine (I forget which). All C++. All failed. [Quoting myself from the quantum fearless concurrency post](https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html), &gt; Personally, my âaha, I now fully understand the power of Rustâ moment was when thread safety issues cropped up on the C++ side. Browsers are complex beings, and despite Stylo being Rust code, it needs to call back into Firefoxâs C++ code a lot. Firefox has a single âmain threadâ per process, and while it does use other threads they are relatively limited in what they do. Stylo, being quite parallel, occasionally calls into C++ code off the main thread. That was usually fine, but would regularly surface thread safety bugs in the C++ code when there was a cache or global mutable state involved, things which basically never were a problem on the Rust side. As someone who worked on stylo and has plenty of experience in both Rust and C++, Stylo would not be possible in C++ unless perhaps we did it as part of a ground-up rewrite of the majority of the browser.
It's different for every feature so you just have to see how the discussion is going in the tracking issue. According to the [most recent comments](https://github.com/rust-lang/rust/issues/33626#issuecomment-326161229) it just seems to need some momentum, but there's never a timetable until the team in charge (lang team in this case) starts the final comment period.
Neat
Ok, so in the `bar::&lt;dyn Foo&gt;()` case it's not obvious which vtable it is, so it needs to be specified at the caller site. AFAIK, we currently don't have a syntax for retreiving a vtable, so I'm just making one up here. Either, we have a trait object we can get one from, like this: let z: Box&lt;Foo&gt; = ...; bar::&lt;z as dyn Foo&gt;(); Or we start with a type: bar::&lt;i32 as dyn Foo&gt;(); But maybe coming up with a good vtable syntax would be the second step here, where the first one would be just enabling the cases where it's obvious what vtable we are referring to, such as `(&amp;x as &amp;Foo).foo()`.
In addition to the other points raised, that would be the first time that an `as` cast could panic. I think the better option than making it panic would be to just remove `as` casts for fallible conversions and make you use TryInto (though I don't like that personally).
Like I said above, this works for deploying single binary things, but desktop environments aren't single binaries (nor do I expect them to ever be!). &gt; There's a lot of issues with DLL versioning and random crashes due to every user running their own unique combination of dynamic libraries. First, this isn't something that I've seen in the real world. Usually the linker says "no" before you get *too* far down that hole. Problems arise when applications or SDKs ship dependencies that aren't properly vendored (mangling symbols and library names). Second, how is this not true for IPC communication as well? Why is running with arbitrary service versions any different than running with arbitrary library versions?
Right, multiple threads would be bad (very bad because `RefCell`'s check might not even run correctly). The other way you can have concurrent callers (without multiple threads being involved) is reentrancy: 1. Environment calls your function. 2. Your code acquires the reference. 3. Your code calls something in the environment. 4. The environment calls your function before returning (for whatever reason). In that case you'd get a panic.
&gt; Python runs an interpreter so could do whatever it wants, as long as all the stuff the interpreter wants is linked into the python executable once. So things not built into the Python library must all be pure Python code. That soundsâ¦unrealistic.
I donât have anything to say other than YESSSSS
TL;DR Rust is fast and using rust for heavy data processing in your Python program is can make it a lot faster.
Don't use references, use indexes.
Looks like gfx_backend_gl has never been published which is why it can't be resolved. Did you want gfx_backend_gl or gfx_device_gl? Edit: Ugh - ignore this - that is the right path.
The simplest solution is [split_at_mut](https://doc.rust-lang.org/std/primitive.slice.html#method.split_at_mut). This method was designed exactly for all general cases where you need to have multiple mutable references to elements of vectors or slices. Simplest method for two references would be something like: fn get_two_mut&lt;T&gt;(slice: &amp;mut [T], index1: usize, index2: usize) -&gt; Option&lt;(&amp;mut T, &amp;mut T)&gt; { if index1 == index2 { None } else if index1 &lt; index2 { let (start, end) = slice.split_at_mut(index2); Some((&amp;mut start[index1], &amp;mut end[0])) } else { let (start, end) = slice.split_at_mut(index1); Some((&amp;mut end[0], &amp;mut start[index2])) } } // or this: fn get_two_mut&lt;T&gt;(slice: &amp;mut [T], index1: usize, index2: usize) -&gt; (&amp;mut T, &amp;mut T) { assert!(index1 != index2 &amp;&amp; index1 &lt; slice.len() &amp;&amp; index2 &lt; slice.len()); if index1 &lt; index2 { let (start, end) = slice.split_at_mut(index2); (&amp;mut start[index1], &amp;mut end[0]) } else { let (start, end) = slice.split_at_mut(index1); (&amp;mut end[0], &amp;mut start[index2]) } } Doing three different indices could be similarly written. You'd use this like `let (a, b) = get_two_mut(vec, 1, 3);`. I'm not aware of any initiatives to make this easier, but there could definitely be some that I just haven't come across. In my experience this pattern is avoided through architectural designs - in my limited amount of working with rust, I haven't really ever had the need to do this (though I can see there being some applications which couldn't avoid this).
... which is a downside, but is still safe.
Looks like some crates have been renamed. It works for me with the path you specify if I check out the v0.14 branch. No idea if that is the appropriate source or not.
Aaa...fair point.... less copying helps....
In theory C++ is equaly fast or slightly faster than Rust, but in practice Rust is faster. For example silversearcher vs rigrep or what's the rust grep called...
That will work, but it is arguably verbose to work only with indexes (You always need to directly access the array). And there will still be cases where you need references, eg: passing the elements to a function.
[This](https://www.youtube.com/watch?v=3CwJ0MH-4MA) video gives a pretty good visualization of using Rust to allow a python program to efficiently use all available CPU cores.
Could it output something like a tokio -proto crate, basically a machine that just needs to be driven by something?
Not really. On any phone today each "app" is a single package that's signed and uploaded to the app store. That's pretty much what a system like this would be. Each Python app would have to be packaged up before a random user could install it (just like all other apps), and that package would include all libraries pre-linked together so there's no dynamic linking. During development you'd have some exceptions of course. Only plugins need separate processes, but plugins need a lot of extra care anyway so it's not so bad IMO.
But that is not how any of this works! I find this conversation very frustrating - I'm trying to explain the rules of the system to you and you seem to be ignoring them. Perhaps the use of `dyn` has been confusing - we've adopted a proposal to move to a system where trait objects have the type `dyn Trait` - e.g. `Box&lt;dyn Foo&gt;`. `bar::&lt;_&gt;` takes a type. The type is bound `?Sized + Foo`. If Foo is an object safe trait, `dyn Foo` (or just `Foo` in the old syntax) is a valid type that meets this bound. Making associated functions object safe would be completely unsound for this reason. We can't just reject `dyn Foo` as a type here _for no reason_ - object safety is how we determine if that type implements `Foo`. Yes you could imagine a syntax where you can get "the vtable for this type" and a whole system built around that. But the trait object system doesn't work that way - we'd need a different system.
&gt; Why is running with arbitrary service versions any different than running with arbitrary library versions? Because in the latter you're sharing an address space! It's *incredibly* common for random crashes on desktop environments, many of which are "caused" by untested combinations of DLLs being used. E.g. maybe a library had a latent memory scribble, and then they made a small change which causes this memory scribble to write to a slightly different location and now an app which used this DLL fine in the past starts crashing.
So what did you do exactly?
I would've thought such a library would be relying on [`ordermap`](https://docs.rs/ordermap), any reason that isn't the case?
Do you have an example of what you're trying to do? There might be a more narrow solution. For example if you're trying to build/modify an array in parallel, then Rayon can help with that. If you trying to avoid allocations with a pool like structure, then typed-arena (which has an into_vec) might fit. Perhaps a different data structure might work better, like the ecs systems for games.
Rust wasn't even at its best yet! https://github.com/rochacbruno/rust-python-example/pull/3 (now I fully expect someone else to do even better...)
ag is written in C, not C++. ripgrep is faster than ag mostly because of algorithms, not because of something intrinsic to the Rust compiler. With that said, I've been on record as saying that I never would have written ripgrep in C or C++ because I wouldn't have had the time to do it.
Hmm. I feel like we're using different definitions of "common". Crashes of applications I'm not developing are, at least for me, rare. Sure, maybe a decent portion of *those* are due to mixed up libraries, but those are also pretty sudden in the cases I've seen and it's not like the application gets very far in those cases. I just don't encounter ABI breakage and the linker *doesn't* catch it often enough to count the occurrences of in the past umpteen years. &gt; E.g. maybe a library had a latent memory scribble, and then they made a small change which causes this memory scribble to write to a slightly different location and now an app which used this DLL fine in the past starts crashing. ABI breaks are bad no matter what. At least with dynamic libraries it's easier to get one application using a fixed library while another uses a different one compared to trying to get two applications to use two different copies of some external service (e.g., D-Bus, `ssh-agent`, etc.) since one has a single environment variable (`LD_LIBRARY_PATH`) to set versus various different environment variables (`DBUS_SESSION_BUS_ADDRESS` and `SSH_AUTH_SOCK`).
This sounds like an Apple-ification of computing. No thank you.
I think your comment, vem_'s comment *and* the OP all have this slightly wrong, or at least don't have the full story. It's true that ripgrep uses different algorithms for common cases, but a lazy DFA is at the heart of both ripgrep and GNU grep. If you test that (and only that), in order to make it apples-to-apples, then the performance of the tools is roughly comparable. So on those grounds (along with others, such as comparisons to the reference Snappy implementation and RE2 itself, among things I personally have experience with), I'd say the OP's claim that Rust isn't as fast as C or C++ isn't accurate. :-)
 git checkout v0.14 in the gfx tree. I initially created a binary project and referenced the crates.io version of gfx_device_gl. The I added your patch line, and checked the gfx repo out next to it. On the master branch I see the message you get. If I check out v0.14 and do a cargo build, the hello world app then builds.
You can make it a bit simpler with iterators: fn get_two_mut&lt;T&gt;(slice: &amp;mut [T], index1: usize, index2: usize) -&gt; (&amp;mut T, &amp;mut T) { assert!(index1 != index2); let mut iter = slice.iter_mut(); if index1 &lt;= index2 { (iter.nth(index1).unwrap(), iter.nth(index2 - index1 - 1).unwrap()) } else { (iter.nth(index2).unwrap(), iter.nth(index1 - index2 - 1).unwrap()) } } **edit:** fixed the ordering.
What an honor! Got an reply from the dev! I use VS Code which uses ripgrep :) Awesome stuff :D
I think that would be quite difficult due to the way that multiple top level syntactic constructs can combine together to produce a single object. To retain formatting, you'd want to store the original document, and have parsing simply produce references to locations within that document - that way it's easy to guarantee that the original formatting is reproduced faithfully.
Let me add: Yeeeehaw!
Will this always give back the items in the right order though? I imagine if one used `get_two_mut(&amp;[3, 5], 1, 0)` the expected result would be `(5, 3)` not `(3, 5)`.
&gt; So, this guarantee/restriction isn't about memory safety It absolutely is about memory safety. The examples in the linked blog post are all cases where shared mutability leads to corrupting memory. The primary way it could be relaxed is by talking about *which* types this can happen to. Enums are one case, since you can change their entire shape in-place. Vectors are another, since they can be reallocated. As far as I've thought through it, the general-case description for this is a type that allows you to change its memory layout or any memory it owns, transitively. For example, this includes `Box` but not `Rc`, other containers like `HashMap`, and any type that *contains* one of the problematic ones. The gap between "shared XOR mutable" and "shared AND mutable but only if you can't invalidate interior pointers" is where `Cell` fits in. `Cell` prohibits interior pointers unless you're guaranteed to hold the only alias (i.e. you own it and it's unborrowed, or you have a `&amp;mut Cell`). The biggest place I could see Rust relaxing these rules is allowing shared, mutable interior pointers to types (or pieces of types) that match this rule- scalars, structs, etc. However, just like `Cell`, this would lead to bugs and inhibit optimization, and so we would likely still want to mark it in some way.
That returns the values in the wrong order half the time though.
Maybe you could open an RFC to implement SliceIndex for Iterator&lt;usize&gt; Then you could use that to get a slice of *just* the positions you want and split_at over each sub-section of that slice to retrieve the references you'd like.
I published the updates here: https://github.com/rochacbruno/rust-python-example#updates And accepting PRs to enhance Rust and Python implementations.
Ok, fine. But even if you don't like the no dynamic linking approach on balance, it's not the same as saying "this is impossible" or even "there are no merits to this approach". 
Err, yeah you'd need to keep the `if` then. &gt;_&lt;
Also, lone comments and whitespace can't have a key associated to them, so you want two data structures anyways; a linear vector of all items including keyless ones, and a hashmap that associates keys to indices into that vector, skipping the keyless items, for indexing.
Pretty sure all the xlsx/docx/pptx/etc. file formats are just zipped xml files, so you should be able to use the zip and xml crates to read/write them and perform simple manipulations.
Building everything from source is not "a thousand times easier" than calling a stable interface that happens to be in a DLL.
So this is what Rustaceans do when they wait for their code to compile
Not quite. It keeps lists of "shared strings" in certain xml files and then uses them in others to create the actual sheets. It's a bit more complicated so it would be nice if this existed as a separate library
I'm not talking about the procedural/functional dichotomy here, I'm referring to the imperative/declarative one. Parser combinators for instance could be classed as imperative, but lie in the functional classification. Mostly just that it defaults to requiring a lot of boilerplate for simple things that obscures the actual shape of the binary data. More boilerplate means more chance of introducing a bug, even if your are being memory safe. It also makes it harder to do different kinds of static analysis and generate different styles of API (streaming, tree-walking, etc) Lots of stuff can be derived from a declarative specification. Granted, the author addresses that when they compare it to [Kaitai Struct](http://kaitai.io/) (another influence of mine), saying: &gt; Kaitai Struct is in a similar space, generating safe parsers for multiple target programming languages from one declarative specification. Again, Puffs differs in that it is a complete (and performant) end to end implementation, not just for the structured parts of a file format. Repeating a point in the previous paragraph, the difficulty in decoding the GIF format isn't in the regularly-expressible part of the format, it's in the LZW compression. Kaitai's GIF parser returns the compressed LZW data as an opaque blob. Personally I'd prefer to allow folks to drop down to a lower level (like parser combinators) at that stage, but I don't know if it's worth giving up declarativeness for the entire specification for that though. It remains to be seen whether I'll be successful though. ;) 
We've talked about it before, but now its on crates.io. :-)
Building from source on Linux is easier than building from source on Windows, at least for C and C++ (and many other languages). Rust makes it easier with Cargo and Rustup, but manually invoking rustc isn't a fun experience on non-trivial projects.
...and just using a binary is easier than building from source on both Linux and Windows. Distributing a binary is even easier than that on Windows, where they actually have a stable userspace ABI.
I like the insinuation that F\*-verified programs are slow. Meanwhile, HACL\* is competitive with the fastest C crypto.
Yeah. The punt we're taking with the declarative approach is that we'll be able to generate safe, consumer-driven APIs, or validators that pass over an entire binary file.
Awesome work! I plan on converting my in-progress library immediately.
I'm so so happy to have a way to use derive to generate error stuff. The error-chain macro is such a pain when you want to use stuff like rustfmt or when you have some obscure error.
Nim's garbage collector is quite promising from the benchmarks I've seen. Supposedly quite competitive with C. The future of GC is probably parallel and/or non-tracing.
I'm pleased with how simple the API is. Not only can you make new errors but is also helps to simplify your code by extending types that return results in the standard library. Great job!
The Suckless ("software that *suck*s *less*") community swears by static linking everything, among other things. I don't support this viewpoint as dynamic linking makes a lot of things more convenient, but trying to follow this on Windows is essentially impossible. That being said, Windows inherently "sucks", so nobody that agrees with the Suckless philosophy in *any* respect would even consider using Windows.
I think you want /r/playrust.
Windows' dynamic linking of things like `kernel32` or `user32` is basically the same as "dynamic linking" of the kernel itself. It has none of the downsides (or even upsides!) of usual dynamic linking, it's just the mechanism to talk to the kernel. And for that matter, Rule 4. Windows does not "inherently suck."
The announcement mentions that an Error always carries a backtrace. Is there any information on possible performance impact of this change?
I'm excited for this! I'm working on a command-line tool for modifying toml files (get, set, list add, list add unique, list remove, list remove all). One of the things holding me back from going anywhere with it was the issue of source format preservation.
The [api docs for failure's Backtrace type](https://boats.gitlab.io/failure/doc/failure/struct.Backtrace.html) talk about what has been done to make Backtraces as cheap as possible, and the docs about Error [here](https://boats.gitlab.io/failure/error.html#implementation-details) talk about the optimizations done in Error to make it cheaper. Also, to be clear, you can use your own types that implement Fail without ever using Error, and they are not required to have a Backtrace at all.
Your syntax highlighter isnât getting the language-rust hint (presuming it supports Rust at all)âthe first block is highlighted as C++ and the second as JavaScript.
Gotta say, this is new. I'm much more accustomed to announcements of success :P
I noticed it was off. :-\ Probably there's a plugin I have to install. (The blog uses hugo, if anyone can point me in the right direction I'd appreciate it!)
Is it just me or is the need for `Box&lt;Error&gt;`/`error_chain`/`failure` just not something that comes up very often? I have done a fairly significant amount of both application and library dev in Rust and never used any of them myself. I strongly suspect that any architecture that benefits from them (lots of call stack "layers" with different error types, any significant amount of downcasting, etc.) is probably going in the wrong direction. I can *maybe* see a use for them in applications, but if I see a library trying to hand me a `Box&lt;Error&gt;` or a `failure::Error` that's a big strike against it. I don't want my dependencies building heap allocated stack traces I didn't ask for, even if they leave out symbol resolution.
Ah yes - kind of like [tomlkit](https://github.com/joelself/tomllib)? That one is abandoned sadly.
Author of calamine here. Creating a file is harder than reading in general because you need to implement all the features (formatting etc...) while I could focus on reading the data only. Updating on the other hand should be simpler (possible to reuse existing parts) and xlsx/ods should be workable. If you end up implementing a solution let me know, to see if it can be added to calamine. I am on several projects now and I cannot do it myself in the near future.
If all of your libraries are throwing different error types (as they usually are), how do you handle them? You create an enum that carries all of them as variants? If so, here are my thoughts on why `Error` is better: 1. Your enum is probably huge (in terms of memory layout) unless you are *also* doing the work of hiding it behind a box. Unless errors occur in a really high percentage of your calls, `Error` is almost certainly faster because it doesn't hurt the happy path as much. Even then, there's a good chance its faster because a heap allocation 40% of the time is not necessarily f 2. Maintaining that enum requires is a lot of busy work. If you don't know that what you're doing is faster (and as I've just said, there's a good chance its actually slower), you should be focusing on other things instead of maintaining that enum. Using `Error` frees you up to do that. &gt; I don't want my dependencies building heap allocated stack traces I didn't ask for, even if they leave out symbol resolution. You have to ask for it by turning the environmental variable on. Also you should be ruling out `error-chain` libraries, which also do this by default. But error-chain is quite common in libraries today, suggesting this is not a problem for most use cases.
Are you intending to use the references to mutate the elements? * If not, it's easy: just take multiple const references. * If yes, one alternative to `split_mut` is to wrap the part of each element you intend to mutate in a `Cell`, `RefCell`, or `UnsafeCell` so you can take const references to the element.
All I want to add: If the program is only run on the server, use `target_cpu=xxx` or `target_feature=+avx2`. You can get a significant performance boost, but the program might not work on all CPUs.
Having seen boat's talk on failure, and having talked with him about it on IRC later on, I think that failure has solved many issues I had with `error_chain`. Really great improvement! Now that it is on crates.io, I'll try it out. Maybe my position is skewed by the use cases I'm targetting, but the only danger that failure seems to be posing is that it promotes usage of the `Error` struct too much. Sure `Error` is great for prototyping and still better than unwrapping but when using libraries/internal APIs of large applications it is really bad if you don't know *anything* about which kinds of errors a function can return, because it tells you about the failible parts of the function. I'll probably use the `ErrorKind` and the `MyError and ErrorKind` patterns for most of the code I am writing, for the places where I'm okay with a big (compile time big, not runtime big!) dependency like failure in the first place.
&gt; for the places where I'm okay with a big (compile time big, not runtime big!) dependency like failure in the first place. failure only depends on backtrace, compile times should not be too large. (I guess the custom derive crate depends on syn, but so do any other custom derive crates you have). I did think about this aspect of it. For example, I removed skeptic tests (tests of the code in the book and readme) because the way skeptic works it becomes a dependency of everyone else who depends on this crate. I wouldn't add another dependency to fail without an extremely compelling reason.
This is a very odd response to me. When I said I never use these tools, it's because I never feel the need to- I never have an interface that joins up more than one or *maybe* two error types. Neither #1 nor #2 apply here. That's also what I meant by "probably going in the wrong direction." In my experience, interfaces that return *that* many error types are either doing way too much in one place, or they're not providing any useful information on error. &gt; You have to ask for it by turning the environmental variable on. Also you should be ruling out error-chain libraries, which also do this by default. That's still not enough, and `error_chain` *is* a mark against libraries for me. When my dependency fails, I just want it to just tell me it failed with a `mov` and a `ret`. I don't want it reading environment variables, I don't want it accessing atomic globals, I don't want it allocating, and I *definitely* don't want it calling `DbgHelp.dll`. Not all errors are off the fast path. One of the biggest benefits of `Result` over exceptions is that returning a `Result::Err` is so much cheaper than anything exceptions do. The choice of doing something expensive should my mine, not my dependencies'.
I don't adhere to Suckless, I was using it as an example. I was referring to it (not by name) when I brought up whole-system static linking. It sucks by that defintion. I will admit that it's nice that the whole OS doesn't crash when the graphics drivers do, but that's a result of micro/hybrid kernels and isn't unique to NT. And Rule 4 only mentions programming languages. ;)
I love the improvements to the code and docs since this was last posted, great work! Random thoughts - How come `Context` is hard coded to `Error` rather than being generic over any `Fail`? For how much the new documentation focuses on "there are many ways to do it", it seems weird that `Context` only augments one of them. - `The Error and ErrorKind` feels like a lot of work with little value because of the stateless limitation. Am I missing something in it for why this has value? Is it just lacking clarification of how to handle adding state with this pattern, maybe by pointing to the others and using `Context` directly? - I got in the habit of `Type` in `Result&lt;_, Type&gt;` always implementing the error trait. It looks like `failure::Error` doesn't implement `Fail` which feels odd but I can't see a problem with it. This does impact some less common cases of using generics over error types (see my `Context` comment above). 
If errors are in your fast path the `Error` type is not appropriate for your use case. The kinds of libraries you're talking about should just implement Fail for a custom type. I don't know what kind of applications you write, but many people write network services for example. These likely talk to other network services, which very often return an HTTP JSON response. If they depend on a library - or just write an isolated subsystem (as they should) - for communicating with that other service, its going to have an error type which could return: * IO errors * HTTP errors * JSON parsing errors * Semantic errors of its own For a library like this, the `Error` type is totally appropriate in my opinion (though the Error + ErrorKind pattern may be more robust, especially if this is a public library on crates.io). That's not the code you're writing, and that's fine. But I still think you should implement Fail for your error types so that if your code is pulled upward into a larger system, it can integrate smoothly.
&gt; failure only depends on backtrace, compile times should not be too large. If someone never wants it, should `backtrace` be optional, like I think error-chain does? &gt; For example, I removed skeptic tests (tests of the code in the book and readme) because the way skeptic works it becomes a dependency of everyone else who depends on this crate. This similarly bothers me. I'm tempted to follow a suggestion of killercup &gt; Apropos skeptic: Do we still have problems with that? I recently rediscovered `cargo build &amp;&amp; rustdoc --test ./README.md -L ./target/debug/ -L ./target/debug/deps/` but totally forgot to tell you about it, @epage. https://github.com/killercup/assert_cli/issues/51#issuecomment-341970446
At the moment, I will be using Go as the language for the project as it needs to be finished in a short amount of time and the only real requirements are a compiled executable, parsing a JSON config file, parsing a bunch of CSVs, and updating an xlsx spreadsheet with the data retrieved (and manipulated) from the CSVs. I may look at adding updating capabilities to calamine during my winter break though (I'm currently a student and I work for my university's IT department).
The backtrace dependency is not present in the no_std mode (turning off the default `std` feature).
Those things shouldn't all go in a single error type. A pipeline approach lets each stage deal only with its own type of errors, and also makes things more testable without obscuring what the code is doing. This is why I could maybe see this kind of thing in applications- they're the place that constructs the pipeline. But they're also the place that *handles* the errors, so that makes it less necessary to wrap them up. Another reason I would prefer not to use libraries that depend on `error_chain`/`failure` is that, even with Cargo, I prefer to keep dependencies to a minimum. Every shared dependency is a place that can split the ecosystem, every dependency increases build time with stuff I don't use, and every dependency complicates reading the code. I am all for dependencies that do appreciable work (e.g. `rayon`, `regex`, `hyper`, `winapi`, `cc`) or are required for interop (e.g. `serde`, `http`, `futures`, `std`), but `error_chain` and `failure` aren't worth it for me.
&gt; Those things shouldn't all go in a single error type. A pipeline approach lets each stage deal only with its own type of errors, and also makes things more testable without obscuring what the code is doing. And yet `hyper::Server` does not take a `R: Read` and then parse a Request out of it, but instead takes a SocketAddr and listens on it for you. A pipeline is an excellent way to structure the internal components of some subsystem, but once it becomes a publicly exposed component encapsulating that pipeline is often a very good. Like many comments I see in the Rust community, you have taken one good design idea and pushed it to the extreme, so that it is a maxim that must be applied everywhere. Anyway, `failure::Fail` is intended to be for interop, and is perfectly useful without `failure::Error`.
I remember some issues with some software/programming languages when progress bar was updated too often and sometimes would take a significant amount of CPU (battery, slowdowns). Just a random thought, I am guessing it was taken care of here. :)
&gt; I got in the habit of Type in Result&lt;_, Type&gt; always implementing the error trait. It looks like failure::Error doesn't implement Fail which feels odd but I can't see a problem with it. It wouldn't be coherent because it would make the `From` impl for all Fail types conflict with the reflexive impl of `From` in libcore.
&gt; Those things shouldn't all go in a single error type. What should the error type of [`reqwest::get`](https://docs.rs/reqwest/0.8.1/reqwest/fn.get.html) be? Currently, it includes an enum of [many possible failure types](https://docs.rs/reqwest/0.8.1/src/reqwest/error.rs.html#247), any of which can occur when making an HTTP GET and parsing the result into a [`Response`](https://docs.rs/reqwest/0.8.1/reqwest/struct.Response.html).
This is a bit more complicated than 2 files but as /u/Diggsey said, it is still much simpler than working with binary (xlsb / xls) files. The hard part is more in my opinion all the things you don't really care but must be there for the files to be compatible (it is not just plain data).
You're looking for /r/playrust. This is the subreddit for the Rust programming language.
I see. You can use rust for the core part and use others language for excel updating (python, c#). Would be nice if you find that time for updating calamine though!
&gt; So, this guarantee/restriction isn't about memory safety, it's about being more able to reason about your program. It's both! The post starts off with cases where it impacts memory safety (enums, vectors, etc; i.e. any case where you have a container that may contain a variable type or number of things, and the "type/length" can be changed independently of the contents), and goes on to say it's also just about correctness. &gt; But now I'm wondering why they sacrificed so much for this kind of safety... It's not. You rarely need stuff like this. When you do, the cell types exist. `Cell&lt;T&gt;` enforces the "stuff can be changed independently of the contents" restriction for types not involving indirection by operating by copies only. `RefCell&lt;T&gt;` has a slight runtime cost but can take in any type. Both give you the ability to mutate through immutable references.
What's the plan for getting this into std? I can see this creating a big ecosystem headache if every crate out there starts depending on failure and failure_derive and then both (or a slightly modified API of both) get moved into std. What are the plans to update learning materials like the book and the std docs. There's a risk of "use failure instead of std::error::Error" becoming tribal knowledge that makes Rust harder to learn for beginners.
Assuming that there are no breaking changes to the API, a non-breaking change could be added which just re-exports the types in std, making that transition painless.
&gt; Like many comments I see in the Rust community, you have taken one good design idea and pushed it to the extreme, so that it is a maxim that must be applied everywhere. That's quite the unfair characterization. I never claimed everything should work that way, or even that `hyper` should work that way. My criteria for picking a library is not that it never encapsulates anything or never makes tradeoffs. The examples I gave all encapsulate a significant amount of work, and they make tradeoffs in how they solve the problem. The key here is that those tradeoffs are in the problem space- `rayon` is explicitly about work-stealing, `regex` is explicitly about DFAs, `hyper` is explicitly about managing HTTP connections, etc. What they don't do is sacrifice their clients' choices around peripheral concerns for convenience.
no way, not at all, they banned me, i need a key. re: Rust key from AussieHootie sent 1 day ago I have a spare ban, would you like that?
That enum looks great, honestly. `reqwest` is a library that could reasonably be used in a wide variety of situations. There's no need for it to start returning `failure::Error`.
&gt;Those things shouldn't all go in a single error type. &gt;That enum looks great, honestly. But the latter is an example of the former, isn't it? At what point *does* an enum have too many variants, then? You also seem to think that "depending on Failure" automatically means "all functions will be rewritten to return "Failure::Error", which doesn't necessarily have to be the case. A library author could simply imp `Fail` for their errors while continuing to return the concrete types in function signatures if that's what makes the most sense.
Most of the errors in that enum contain boxes. The error itself is 136 bytes. I doubt that using `failure::Error` would negatively impact the performance of reqwest.
And why do you think posting in an unrelated sub will help you? You should be banned.
I would be very interested to read a blog-post about your approach to application architecture.
I'm working on a little library and while I'm concentrating on getting the actual functionality working, I've punted on error-handling by just returning `Result&lt;_, Box&lt;Error&gt;&gt;` everywhere. I had planned to go back and create my own carefully-thought-out error enums and update all my return types... but perhaps I should just switch to returning `failure::Error` everywhere instead?
Its up to you! `failure::Error` is essentially the same as returning `Box&lt;Error&gt;`. If you'd rather return something more precise, you just create an enum and derive Fail for it. It depends on how you expect your library to be used to determine if returning `failure::Error` or writing your enum is better. I can't say in the abstract which is more appropriate for your situation.
&gt; Most of the errors in that enum contain boxes. No, only the Serde ones do, and `serde_json`'s has a comment noting that they actually investigated the performance and found the `Box` to be faster because they return so many `Result`s.
&gt; You also seem to think that "depending on Failure" automatically means "all functions will be rewritten to return `Failure::Error`" No, I specifically called out libraries returning `Box&lt;Error&gt;` or `failure::Error` as the problem, and gave *other* reasons I don't want to depend on `error_chain` or `failure` as a whole.
&gt; No, only the Serde ones do, and serde_json's has a comment noting that they actually investigated the performance and found the Box to be faster because they return so many Results. Did the authors of the other libraries investigate the performance trade offs between enums and boxing or did they just do what is easy to do in Rust?
Probably the latter, which is good, because thus far we've kept "what is easy to do in Rust" the generally-fast option. If we make the default into "allocate stack traces and errors here and there," that will no longer be the case.
Rust makes it extremely easy to create values that are thousands of bytes large and then `memcpy` them around implicitly whenever you move them. Cargo culting the idea that "heap allocations are expensive" is extremely bad, if performance matters you should be profiling to actually understand the trade off involved.
&gt; Rust makes it extremely easy to create values that are thousands of bytes large and then `memcpy` them around implicitly whenever you move them. Absolutely, no disagreement here. Can we maybe not devolve into bickering?
This! Doesn't make it less great, though!
&gt; but if I see a library trying to hand me a Box&lt;Error&gt; or a failure::Error that's a big strike against it I don't think you can make a blanket statement like this. Such things aren't necessarily a cop out. Opaque errors are useful for avoiding public dependencies for example, without sacrificing user visible messages. The regex crate does this for example (although it uses a string, the principle is the same).
Hadn't seen tomlkit before, but yeah, kind of like that.
Sure, I even listed `regex` down-thread as a crate I think handles this sort of thing well. I didn't mean it as a deal-breaker, just something I weigh strongly.
Then take out elements when you need them.
I think I got it. The compiler when looking at the I, C, X bounds will in fact compile a version for each type which is in use, as in that function I'm using usize, I'm kind of pegging that down to be usize as any other type would not work. Ok, I'll need to rethink this then, maybe not a builder but just more functions on the dataframe type to create on each specific case. the idea is that index and columns can be any type but if missing, then default to just integers.
This is awesome! A sorely needed crate.
&gt; Most are fairly accessible, and ideal for beginners. Be sure to submit these to the [TWIR call for participation](https://users.rust-lang.org/t/twir-call-for-participation/4821).
Stickying this because [#10184](https://github.com/rust-lang/rust/issues/10184) is not only the oldest soundness bug in Rust's existence, but also one of the few stable soundness bugs that aren't contrived! Help us fix it! :)
Thinking of that sort of thing, common buffering arrangements can be a *nightmare* over SSH: I was helping to diagnose something on a coworkerâs VM, and while weâre both in Australia it was simplest for both of us to hook it through a machine in the US. Installing things with Yarn without &gt;/dev/null took *minutes* instead of *seconds*, presumably because of stdout buffering having ~560ms latency.
The links to the subpages on the [Patterns &amp; Guidance](https://boats.gitlab.io/failure/guidance.html) page are all broken (.md should be .html).
Lot's of terminals support Unicode already. You can optionally use Unicode bar symbols to make progressbar look much nicer.
You could completely break type safety with aliasing, at least one mutable reference, and a `enum`. Consider: enum NumOrPointer { Num(u32), Pointer(&amp;mut u32) } let external : &amp;mut NumOrPointer; match external { Pointer(internal) =&gt; { *external = Num(0xdeadbeef); // let's see what's at address 0xdeadbeef println!("{}", *internal); }, } Because we're able to change the actual type of the value through `external` without `internal` "knowing" about it, we successfully managed to effectively reference arbitrary memory.
The cuprite here is what's called "existential types" (like an `enum`) that let you have one block of memory take on different types at different times. That's very useful for conserving memory (`Result` would be pretty expensive to have everywhere without it), but this is the fundamental tradeoff. And it helps that choosing this side of the tradeoff also helps make a bunch of programming easier. A good way to work around this can be to use wrapper types with "internal mutability" like `Cell`, `RefCell` and `Mutex`.
I've been following this discussion. Here's my take: If a library uses a generic error type, like `Box&lt;Error&gt;` or `failure:Error` in it's public functions, there's a few scenarios as to why this is: # the library maintainer(s) decided that this was the more efficient to use a generic, boxed error than custom errors # the library has thoughtful error handling, but hasn't created a custom error type (but this is likely planned for a future version) # the library isn't mature yet, and minimal consideration has gone to the types of errors that can happen and is using a one-size-fits-most approach to errors 
You may not realize this, but you've personally sailed this ship around the world already.
Thanks.
##1 Early-return can only be triggered through `if` or `match` or `?`. If the early-return behavior is needed I would probably spell it this way: ^() let map_id = match renderer.current_active_map { None =&gt; { return None }, Some(id) =&gt; id }; or if renderer.current_active map.is_none() { return None } let map_id = renderer.current_active_map.unwrap(); (if you use rustfmt, see which one you think looks prettier; it also depends on the style you like to write in) ##2 Not as a standard feature, sorry, but this could be done with a syntax extension and maybe possibly macros. Rust still has boilerplate. (The `match` function compiles to actual instructions, so this might be a language feature? since the enum is not zero-cost.) ##3 `error_chain` is specifically designed for this use-case, but another possible solution (for the general "I need many traits to enable automagic") is to `pub use` the commonly needed traits in a "prelude" module which you import everywhere.
&gt; (now I fully expect someone else to do even better...) Challenge accepted! I can't get their benchmark to run, but I bet you could speed it up by removing the (presumably frequently mispredicted) branch inside the loop: diff --git a/pyext-myrustlib/src/lib.rs b/pyext-myrustlib/src/lib.rs index f62aa94..ffb4ca6 100644 --- a/pyext-myrustlib/src/lib.rs +++ b/pyext-myrustlib/src/lib.rs @@ -22,9 +22,7 @@ fn count_doubles_once(_py: Python, val: &amp;str) -&gt; PyResult&lt;u64&gt; { let mut chars = val.chars(); if let Some(mut c1) = chars.next() { for c2 in chars { - if c1 == c2 { - total += 1; - } + total += (c1 == c2) as u64; c1 = c2; } } My other idea would be to unroll it, but that's really hard when iterating over UTF-8 characters rather than bytes.
Thanks:)
Ehh, never mind, I got it to run but there was no difference. Maybe the compiler optimized them to the same thing. Using bytes rather than characters is twice as fast but that might be considered cheating. Unrolling with bytes (4 or 8 per iteration) seems to be slower than the simple bytes approach, so there's no hope for a char-based unrolling to be faster.
I can see the motivation for using rust for everything, but surely in 2017 this must be solved well by something already? Nix? Blaze?
Hi, Puffs author here. This is my first post or comment to the /r/rust sub-reddit, and I am a little late to the conversation. I'm also not very fluent in Rust. I'm happy to learn where I've mischaracterized Rust. But I'll jump in anyway... First, Rust is great tech, written by great engineers. As I tried to say in https://github.com/google/puffs/blob/master/doc/related-work.md, I'm not claiming that "write it in Rust" is a bad idea. It's just a different approach, with different trade-offs. For example, some C/C++ projects are hesitant to fold in new languages and new toolchains into their build process. Yes, y'all here on /r/rust obviously think that it's worth doing so, and Mozilla is leading the charge, but not everyone is so enlightened. Yes, Puffs is also a new language, but it generates C code that works with existing C toolchains. Yes, I'm checking generated C code into the git repository, so that people can use the Puffs libraries without installing the Puffs compiler. Yes, it makes it harder (but not impossible) for end users to edit the Puffs codecs. But I don't expect end users to edit the Puffs codecs that often. The project is focused on libraries, not programs, and specifically, libraries for well established file formats (like JPEG and PNG) that are very stable. Yes, there are other ways to design a programming language and its libraries, and other ways have their benefits. Again, I'm not claiming that Puffs' approach is always best, just that different approaches have different trade-offs. On performance, as I said on the Hacker News post (https://news.ycombinator.com/item?id=15711767): --- Yes, in general, Rust performs comparably to C/C++. As I noted elsewhere, Rust with runtime arithmetic overflow checks currently performs worse than Rust without such checks. So, yes, Rust without those checks is as fast as C, and in general, arithmetic overflow isn't the biggest concern. steveklabnik, a Rust expert, commented on this [Hacker News] page that, in the future, "if the runtime checks get cheap enough, we can do them in release mode as well". If so, that's great, I'm happy to be proven wrong. Cheap still isn't zero, though, and see "nanoseconds become milliseconds" at https://groups.google.com/forum/#!topic/puffslang/2z61mNTAMns/discussion In contrast, Puffs today performs as fast as C, *with* arithmetic overflow checks. They just happen to be compile time checks. And sometimes overflow is indeed a concern (search for "underflow" in https://blog.chromium.org/2012/05/tale-of-two-pwnies-part-1.html). --- Also on performance, it's not Rust per se, but the numbers at https://github.com/google/puffs/blob/master/doc/benchmarks.md shows that, on Puffs' benchmarks, gcc 7.2 noticably outperforms clang/llvm 5.0. I'm sure this is a solvable problem, and not a fundamental flaw with llvm, but fixing that's beyond my llvm knowledge. So maybe a future version of Rust will be able to do everything that Puffs aims to do, with respect to safety and performance. If so, that's great. Programming is not a zero sum game. People also mentioned ATS on this page. Again copy/pasting another of my HN comments: http://ats-lang.sourceforge.net/DOCUMENT/INT2PROGINATS/HTML/... says that "By default, the constraint-solver implemented for ATS/Postiats makes use of the standard arithmetic of infinite precision. For the sake of efficiency, one may also choose to use machine-level arithmetic for solving integer constraints. Due to potential arithmetic overflow, results returned by the constraint-solver that uses machine-level arithmetic can be incorrect". Puffs does not use infinite precision integers (aka "big ints"), for performance. But Puffs still checks for arithmetic overflow.
...or, if you want to go the "ugly but works" route, you do as I once did in a Python-based reader/writer for DOSBox conf files, and use Python's ordermap equivalent (`OrderedDict`) but synthesize a temporary key for each comment or whitespace line, then recognize and discard it when re-serializing. (In my case, an initial identifying character, either `#` or `\n`, followed by a value from a monotonic counter to make it unique.) ...now that I've been reminded of it, I should probably make a TODO note to write a test suite for it and then rewrite it properly.
Well, that sentence goes on to say: &gt; Inner loop performance usage matters, and it is easier to match the optimization techniques of incumbent C libraries with a C-like language than with a functional language.
That's basically the approach I take in Python too. Being able to hand someone a single binary which will write out a default config file if necessary is very convenient for deployment.
In RustCrypto we have two main sources of `unsafe`'s: handling endians and low-level optimizations (e.g. SIMD + assembly) I think there is two approaches to `unsafe` problem: 1) extend `libcore` so it will cover some use-cases of `unsafe` in third-party crates (handling endians and SIMD should definitely go here) 2) review most used `unsafe` crates and allow reviewed versions to be used as dependencies (so if we have reviewed `foo v0.7.1` and `foo v0.7.2` is published, tock will use the first version until the second gets reviewed) I don't think you'll be able to comfortably work without implementing the second approach. Also, relevant [issue](https://github.com/rust-lang/rust/issues/38509), which I think should be fixed as well, otherwise even if crate is marked as `no_std` it can have required `std` dependencies unbeknown to the crate author.
&gt; Blaze *cough* Bazel *cough*
Maybe there could be some demand for a crate that does some adaptive buffering before output. Maybe you could even print stuff with "priority markers" and depending on the sink speed, it would drop low-priority stuff.
Clarifying question: the README says serialization is a non-goal, but for it to be able to edit a TOML file, it must have the ability to write back out somehow, no?
&gt; Also, relevant issue, which I think should be fixed as well, otherwise even if crate is marked as no_std it can have required std dependencies unbeknown to the crate author. Wouldn't this become a little easier once cargo sees std and core as crates? In that case, you could tell cargo to fail if something resolves to std. 
From what I understand, a violation of "unwind-safety" means that the type might have incoherent state after an unwind and therefore might not behave according to its logical invariants anymore. But what's *not* permitted is for a violation of unwind-safety to create a violation of *memory-safety.* https://doc.rust-lang.org/std/panic/trait.UnwindSafe.html and https://doc.rust-lang.org/nightly/nomicon/exception-safety.html have some more info about the distinction.
How's this? // Uses 0-based indexing, and starts the Fibonacci series at 0 fn fib(n: u32) -&gt; u32 { let mut current = 0; let mut next = 1; for _ in 0..n { let temp = current; current = next; next = temp + current; } return current; }
I am not sure. I think you misunderstood me a bit, I meant that crate can have [dependency](https://github.com/RustCrypto/hashes/issues/36) (maybe somewhere deep in dependency tree) which didn't enabled `!#[no_std]`. Cargo will silently compile such crate, even though it will no work in `no_std` environment. As I see it, if root crate enabled `no_std` cargo should run a lint which will recursively check if all dependencies enable it as well, and if not it will rise a warning, which in future can become an error.
Still not great. If you write next = temp + current; current = next; it stops working. And either way relying on variable names is not great.
There's one thing that puzzles me about these stats. My understanding is that a non-negligible source of `unsafe` comes from the `*-sys` crates: bindings to system libraries. If you are surveying for usage in Tock, I guess any crate depending on a system library is, in any case, out of your scope of interest. But these crates would likely be big users of `unsafe` (FFI calls) and `std` (there are system libs, so there is an userspace and a libc). Doesn't including them in this survey artificially makes things worse than they are?
If unicode or colors are to be used, it needs to be done conditionally (if supported). Let's not repeat the mistakes of web browsers, please. And even if supported by the terminal, not all users want colors and unicode in all situations. Think piping to a log, etc.
Are there any details to be found on the analysis? For example, as the linked tweet mentions, extern functions are unsafe, but Josh didn't filter them out. Things like implementing Send and Sync is unsafe and arguably not the problem they describe. Also, I don't quite see why they double down on `unsafe` so much. It's a fundamental part of the language and I would argue that it plays the role of _establishing_ type safety around parts where it couldn't otherwise be. Yes, reviewing unsafe dependencies is a chore, especially on upgrades, if this is the level of security you need to work on, but saying that the ecosystem is not ready for embedded Rust because of that is stretching it. Also, what's "the crates.io ecosystem"? crates.io is perfectly capable of hosting multiple ecosystems and the most popular crates are arguably _not written for embedded_, which means this criticism doesn't quite apply. It's an interesting analysis, pointing to problems, but the way I read it is: there's no sizable embedded ecosystem on crates.io, especially around popular crates. Still, I'd be very happy to see solutions to make all these chores better manageable.
If std were a standard crate though, it would show up easily recognisable in the dependency tree and as a second step, cargo could be made aware that you _don't_ want to accept trees that include libstd.
&gt; Because there is no way to use cargo or the compiler to enforce that dependencies are safe (although we floated the idea), we checked to see if any of those explicitly-safe crates have unsafe dependencies. We found that all 21 crates have dependencies that use unsafe, making the #![forbid(unsafe_code)] flag a bit misleading. The same is true about no_std, which I don't really understand. If I want to make a no_std, there is absolutely no self-contained support for stopping myself from accidentally introducing a non-no_std dependency. Both of these feel like an oversight to me.
Doesn't Bazel pull in the JVM? Until the new Graal and friends tools can produce proper AOT Java binaries, I would personally stay away from adding JVM as a dependency for building Rust projects.
Ah, yes, this could work too.
So, please correct me if I am wrong: 1) Since now on for all libraries I should prefer Failure over std::Error? 2) Eventually, Failure will be in std library?
For no_std: https://github.com/rust-lang/rust/issues/38509
Don't leave us hanging, what are the 13 crates that qualify??
&gt; Also, I don't quite see why they double down on unsafe so much. It's a fundamental part of the language and I would argue that it plays the role of establishing type safety around parts where it couldn't otherwise be. I agree. When you find that you exclude most existing libraries if you refuse on principle to use any that has `unsafe` code or depends on something that does, maybe itâs a sign that that principle is too restrictive more than a sign that something is wrong with the ecosystem. And libcore uses `unsafe` too. I assume Tockâs capsules are not forbidden to use it. Sure, libcoreâs unsafe code probably has got more review than a crate that I made last weekend and that nobody else has used, but itâs a matter of degree more than a fundamental difference.
Yes, I find that changing the order code is executed in often breaks it. If we wrote current = next; next = temp + current; let temp = current; it wouldn't even compile! Your challenge was to write an imperative Fibonacci function that is obviously correct, and I think I've done that. You can't change the challenge after the fact. &gt; And either way relying on variable names is not great. Why not? Variable names are a fantastic resource. Which would you rather encounter, the function I wrote above, or this one: fn kfjakfdjdsfkj(kjfdsfdsj87: u32) -&gt; u32 { (0..kjfdsfdsj87).fold((0, 1), |yyyyyyyyyyyyyyyyyy, _| (yyyyyyyyyyyyyyyyyy.1, yyyyyyyyyyyyyyyyyy.0 + yyyyyyyyyyyyyyyyyy.1)).0 }
How does PyPy compare?
By the way, consider casting with `as *mut Foo` and then dereferencing that, instead of using `transmute`. If your transmute code was correct it ends up doing the same thing, but transmute will happily do *anything* so itâs very easy to get wrong. If you do still use transmute, consider fully qualifying it: for example `transmute::&lt;Foo, Bar&gt;(x)`
&gt; I can see this creating a big ecosystem headache if every crate out there starts depending on failure and failure_derive This is my concern too. The same story goes for `futures`. Being a part of `std` is probably the only way forward for a crate like this.
The placeholder ones.
I actually started that way, but couldn't resist using the type system to match the problem better. Less points of failure with guaranteeing uniqueness and all.
Yeah, missed it this week but it will be in the next!
I am a fan of swapping out, and swapping back in.
Oh sure, we'll have string representations and the appropriate impls. I just mean I'm not going into Serde.
&gt; I don't quite see why they double down on `unsafe` so much. It's a fundamental part of the language and I would argue that it plays the role of establishing type safety around parts where it couldn't otherwise be. Yes, it seems strange to put such a strong focus on `unsafe` while not mentioning any of the [known soundness problems](https://github.com/rust-lang/rust/issues?q=is%3Aopen+is%3Aissue+label%3AI-unsound), many of which can affect safe code.
&gt; Of course, it is entirely possible that all uses of unsafe in crates.io are valid, necessary, and actually safe. The moment you want to do anything barely low-level, like using an intrinsic, you need unsafe code. If you build high-level abstractions on top of low-level ones, as you should, what are the chances that the lowest level of the stack uses an intrinsic when available at compile-time, or run-time? I, at least, would be very angry if they wouldn't do that, making my programs slow on purpose, "just to be safe". This article should be titled "tock developers with wrong expectations are not ready for the real world".
Looks great! Would it make sense to have a version of `context()` that takes a closure? In case I want to `format!(...)` the context but only want to do that allocation in the error case? ps: When reading "Implementors of this trait are called 'failures'." I did a double take, until I realized a second later that "Implementors" means "Types that implement Fail", not "People that implement Fail for their Types." ;)
You left out the reason I stated? Out are you just adding reasons?
I totally agree on a second issue, using no_std is a pain. Just look at what ugly hacks people have to do, to make their crate compatible with no_std: https://github.com/serde-rs/serde/blob/master/serde/src/lib.rs#L140-L163
I don't agree. Soundness problems are bugs in the implementation, `unsafe` explicitly opts out. These soundness problems don't make Rust less ready then other languages and are often niche problems.
My lewton crate has one dependency on `byteorder` and one optional dependency on `ogg` which itself depends on byteorder and optionally depends on a couple of futures related crates (you won't need them unless you want to do async). For lewton, if you don't use futures, starting to depend on syn would be a big change. On the other hand, I have other projects where I have a ton of dependencies and it doesn't really matter to add a few more :)
There is a variant of context on the `ResultExt` trait that takes a closure.
I think you should prefer failure over std::error::Error, but it'll only end up in the standard library if people agree it is an improvement after using it.
This might not be rust-specific at all, but how can I work out the right value for `--target-cpu`? I can't use `native` since I need to cross compile.
I just finished the submit: https://gitlab.com/fdroid/rfp/issues/420
/u/desiringmachines While you're here fielding questions, I figure it'd be convenient to just check my understanding here. What I'd like to do is move the `csv` crate to using `failure` (it's still in 1.0.0-beta so I can :P). The `csv` crate exposes an [`Error`](https://docs.rs/csv/1.0.0-beta.5/csv/struct.Error.html) type and a [`ErrorKind`](https://docs.rs/csv/1.0.0-beta.5/csv/enum.ErrorKind.html) type. On a purely surface level, it seems like I want to therefore use your [Error/ErrorKind pattern](https://boats.gitlab.io/failure/error-errorkind.html). How would you go about this given the definitions of `Error`/`ErrorKind` in the `csv` crate currently? My general philosophy here has been to report as fine grained errors as possible, without introducing any new public dependencies. Note though that this isn't well motivated by strong use cases for that level of granularity. Here is the path I think I should be taking based on your docs: * Drop the `is_io_error` and `Io` variant from `Error` and `ErrorKind`, respectively. Higher-level code that needs to inspect the actual I/O error if one occurs should migrate to using `Fail::root_cause`. For an example of said higher-level code, see [here](https://github.com/BurntSushi/xsv/blob/b7f0e51ebd91639bd68ff084c789523a611972fe/src/main.rs#L120) and [here](https://github.com/BurntSushi/xsv/blob/b7f0e51ebd91639bd68ff084c789523a611972fe/src/main.rs#L217-L227). * For the `ErrorKind::Serialize` variant, drop the `String` data and instead embed that message in `failure::Context`. * There is also a `ErrorKind::Deserialize` variant with another level of nested errors corresponding to [`DeserializeError`](https://docs.rs/csv/1.0.0-beta.5/csv/struct.DeserializeError.html) and [`DeserializeErrorKind`](https://docs.rs/csv/1.0.0-beta.5/csv/enum.DeserializeErrorKind.html). I feel like this should mostly stay the same (sans perhaps removing the `String` data from the `DeserializeErrorKind::Message` variant, similar to what I suggested above for `ErrorKind::Serialize`). One thing that isn't completely clear to me is what to do with other nested errors such as `Utf8Error`, `ParseBoolError`, `ParseIntError` or `ParseFloatError`. Should I be endeavoring to hide those from my error types and demand that callers use casts to try to discover them? If so, I think I'd probably put up a stink about that. :-) But I'll stop here for now. Thanks!
In the end there's no difference whether bugs in your program are caused by faulty `unsafe` code, or unsoundness of the compiler, either may be fixed at some unknown point in the future (when the code or the compiler is fixed), but either also has the potential to cause problems *today*. The blog post mentions: &gt; This divide \[between the kernel and untrusted capsules\] allows us to maximize the safety guarantees of Rust while still making a kernel possible. So they're not just avoiding `unsafe` for no reason, they're doing it to maximize the safety guarantees. And yet there exists various soundness holes that break these safety guarantees, even in the absence of `unsafe`. And as long as these problems exist, the effectiveness of their method (avoiding all `unsafe` in capsules) to achieve their goal (maximize the safety guarantees of Rust) is reduced. I would expect fixing these soundness holes would have high priority for the Tock developers, or, if that isn't feasible, to develop tooling that attempts to detect code that (knowingly or unknowingly) is affected by these soundness holes.
&gt; We found that only 0.2% (21/12360) of crates explicitly disallow unsafe code. [...] We found that all 21 crates have dependencies that use unsafe, making the #![forbid(unsafe_code)] flag a bit misleading. I'm author of one (or even two, depending on the way you count) of those crates: `ogg` and `lewton` (the lewton safety story is complicated, you can turn on a flag and use the same function from libstd, but it got stabilized only recently). Now yes, ogg depends on `byteorder`, and byteorder uses unsafe. Does that mean I want to mislead people? No! I think byteorder is a very robust crate and the APIs it exposes should all be safe to use. This is in fact the model for how unsafety should be used in good Rust code: create a small self contained library with a clearly defined API, allow that to use unsafe, and make that library expose a safe API. Then use that safe API throughout your program. So I don't really think this "misleading" claim is justified in any way.
Oh, stupid me. I didn't think to check there, while it wouldn't even make sense on `Fail` because there you already are in the error case.
I agree. To say that the stats quoted in the OP are misleading would be an understatement. If Rust's safety story works, then it follows that there should absolutely be foundational crates that use `unsafe` internally but expose safe APIs. Foundational crates tend to become dependencies of a large number of other crates, which means it is going to be very difficult to transitively depend on crates that don't use `unsafe` at all. What's more, **that's expected**. I think the "people are using unsafe too much" story is incredibly important and requires more thorough investigation, but stopping short with the statistics quoted in the OP is... well... not nice. And on top of all of that, tying the prevalence of `unsafe` to embedded programming specifically? That doesn't make any sense to me. (The second half of the blog post---using `std` everywhere---is certainly related to embedded though.) I'm not at all impressed by this blog post.
Someone did make an error chain derive crate.
I started working on something like this, but it's still in the very early stages. My attempt is to have the whole file into an in-memory representation, either for creation or for manipulation. There will be extra libraries for reading and writing the different file formats. I am still exploring the design and learning Rust, so the whole thing is currently rather in a proof-of-concept state for generating, not yet reading, an ODS file with very basic capabilities: - Add cells with text, numeric or boolean values and an unchecked formula - Formatting the font weight - Formatting the cell borders - Formatting the cell background color The code can be found [here](https://gitlab.com/silwol/rheets), a short example for generating a file is [here](https://gitlab.com/silwol/rheets_opendocument/blob/master/examples/demo.rs). Because of the early stage it is not yet in a good state in terms of documentation and structure. Of course I am happy to take suggestions about how to improve the whole thing, bug reports or pull requests. Once it has reached a basic usability, I will gladly publish it to crates.io, but as it's a spare-time pet project of mine, I don't know how fast it will progress.
It compiles now, but does not use the local version...
&gt; In the end there's no difference whether bugs in your program are caused by faulty unsafe code, or unsoundness of the compiler, either may be fixed at some unknown point in the future (when the code or the compiler is fixed), but either also has the potential to cause problems today. Still, I maintain to get unsafe code wrong is clearly easier then hitting a soundness whole. Unsafe code has unquestionable unsafe semantics, while a soundness hole is an error in correctly implementing safe code semantics in the compiler. That's _fundamentally_ different. &gt; So they're not just avoiding unsafe for no reason, they're doing it to maximize the safety guarantees. Yes, that's the whole raison d'etre of Rust, but as you say: maximising, not make them perfect. The soundness of Rust is - for example - also highly reliant on LLVM being bug-free. It isn't. It's usually trustable. What you are doing is throwing all these issues into one bucket, while the steps that need to be taken to avoid all of these are very different.
I, uh , missed it. Thanks!. 
That's... honestly kind of amazingly elegant to my eyes, especially considering the difference in features the two environments provide? That's barely 20 lines of facade to hide the difference between embedded and std environments! I haven't worked with no_std at all so I'm not clear on why they have to export either from core *or* from std, instead of just from core. (I thought re-exports were transparent.) Maybe that's what you're talking about?
:)
We're specifically looking to replace a single tool that lots of engineers at Mozilla use: [icecream](https://github.com/icecc/icecream), which is a distcc replacement. The C++ part of the Firefox build will build with lots of parallelism nowadays, so engineers have setup compile clusters in various Mozilla offices. Unfortunately our security folks are not wild about this as icecream has no support for authentication or encryption for client/server communication, and it also supports uploading arbitrary toolchains to run, so we'd like to have a tool that provides distributed caching with those features to keep our security folks happy and so we can roll this out as a company-sanctioned thing. We're already using sccache as a ccache replacement in all Firefox CI, and sccache already has support for running C compilers and a client/server model, so extending it to add this functionality seemed like the best path forward. FWIW, we are also actively working on transitioning the Firefox build away from make, and we're going to be looking into using Bazel for builds at some point, but I don't think you get things like distributed compilation and caching for free with it--I think you have to do some work to enable them.
I think these are good suggestions! Speaking (I think) on behalf of Tock's perspective at least, it's probably fine if the SIMD et al stuff is in a non-libcore crate as long as it's all in _one_ crate that we can decide to trust rather than implemented in every crate that needs it separately (or a bunch of "competing" SIMD crates). If we can enumerate the crates we need to audit and trust to be able to let people use a _lot_ of crates.io, and that number is, say, a couple dozen, that's a really good start.
What's the state of openGL in Rust? I tried it about a year ago and found that it was pretty clunky and awkward to use, especially while trying to avoid unsafe blocks.
This was somewhat intentional, as bazel doesn't have out of the box distribution and caching right now, though it's on the 1.0 roadmap.
In my libraries, Iâve taken to clearly documenting the purpose and impact of any unsafe code extant in the README, such as with [anymap](https://github.com/chris-morgan/anymap/blob/0850f5ec36b14904ae452ffdfa0a2ae0ba05c854/README.md#unsafe-code-in-this-library); I began this with anymap, actually intending to put in an `unsafe` feature enabled by default, thinking that the unsafe code was mostly just about doing things faster because of known invariants, which would allow you to choose to omit all unsafe code at a certain performance and potentially functionality penalty, like `#![no_std]` is sometimes tied to a `std` feature which is enabled by default, allowing the user to opt out of std functionality; but I came to realise with anymap that with the Clone, Send and Sync functionality added over time it had reached the point where the safe API would need to be structurally a little different, and so I gave up and just documented it instead.
`stdsimd` looks like a convergence point, which hopefully will be included into `core` one day. But it will take some time for projects to migrate on it. It definitely will be interesting to see such review of crates ecosystem.
Depends on the system you run on. On Linux, you can use `lscpu`, which will tell you a lot of stuff about the CPU(s) of your systems, including the arch.
They are both Turing complete so yes you could... But you'd have a lot more problems with bugs and crashes due to manual memory management without compiler help.
We should probably have a clippy lint for non-`#[no_std]` crates that only use core features.
&gt; lscpu But `--target-cpu` isn't the architecture, right? That would be `x86_64` here, but I don't expect to be able to plug the model name in `--target-cpu`.
Ripgrep is amazing.
I mentioned this on Twitter, but I'll mention it here too: I once did an audit of all my unsafe crates in my .cargo dir https://gist.github.com/Manishearth/6a9367a7d8772e095629e82184f05ad4 It's mostly -sys crates, which you mostly don't end up using typically. I think it's rather reductive to look at the ecosystem that way. You could say the same thing about allocation, or other things. You could say the same thing about crates that do direct I/O for the "tokio ecosystem". Or crates that use the GPL. The embedded ecosystem is going to be a subset of the ecosystem, looking at the overall numbers isn't that helpful. That said, I do think we need a trust checking tool that helps with this. But it needs a lot more thought put into it; it needs to handle: - Different axes: "Do I trust this crate at all?" "Do I trust this crate to write unsafe code?" "Do I trust this crate to not do bad things in its build script?" (the last one can tie in with a build script/plugin sandboxing proposal I have to write) - Different kinds of trust: "I have verified this version and I trust it" "I trust this crate to never mess up its unsafe code in all versions" - Different kinds of unsafe usage: Used for FFI, perf, abstractions, etc - Better control over transitive trust 
I don't think nix/bazel handle distributed builds like icecream does
Solidity is the reliability of JavaScript combined with the modern tooling of COBOL. I review other people's code but I don't touch the stuff.
One annoyance that has put in roadblocks for no std for me is that slices don't have inherent methods without std. It causes problems due to the method resolution difference with trait methods. It's something I'd like to solve sooner or later.
Not to mention that `unsafe` is not a crime. It's a core language feature.
Finally! Congrats!
- There was a [big debate](https://github.com/rust-lang/rfcs/pull/1236#issuecomment-127771504) over whether to make `panic::catch_unwind` safe at all, and this was the compromise - It [turns out](https://github.com/rust-lang/rfcs/pull/1323#issuecomment-156410396) you can sometimes observe broken invariants during a panic anyway (for example, poisoned `Mutex`es can be [unpoisoned in safe code](https://doc.rust-lang.org/beta/std/sync/struct.PoisonError.html#method.into_inner)) Given this messy situation, `AssertUnwindSafe` basically adds some friction and is intended to make you think about what you're doing and make sure you handle things carefully. But (as far as I understand) there was no good way to reliably prevent the kind of problems that can happen.
Waaaaaaaaaat?!
The more intuitive version is to break out common types (that don't change) to their own crate. Of course that means more small crates.
100% that's a really good idea
&gt; Let's not repeat the mistakes of web browsers, please. What mistakes have web browsers made here?
&gt; I think the "people are using unsafe too much" story is incredibly important and requires more thorough investigation, Even ATS requires unsafe code at times. Partly for low-level stuff but also just because static analysis is a pain in the butt and it can't catch everything (and indeed there are theorems asserting that you cannot). 
&gt; Yes, reviewing unsafe dependencies is a chore, especially on upgrades, if this is the level of security you need to work on Not to mention the fact that there aren't exactly a lot of competitors when it comes to writing safe low-level code. 
PyPy's JIT is great for this kind of use case as well. The same algorithm ends up being about 4x faster than regex just by running in PyPy, but still about 8x slower than the Rust version (I tested without doing any kind of JIT warmup).
Title seems a bit sensationalist. Embedded programming often requires use of `unsafe` to do things like DMA. "Crates.io ecosystem not ready for TockOS" seems like a more accurate statement.
The problem with metrics is when the metric becomes the goal, instead of measuring part of the goal. Especially when you choose an arbitrary metric and treat it superficially without trying to understand what it's actually telling you.
In the case with a progress bar it's easily fixed by just deciding on a reasonable granularity and only do printing updates on changes in underlying data. I think too many programs just redraw every iteration, which may be "as fast as possible". It's also a good idea to print the progress bar on stderr and have a flag to disable progress bars if a user ever has a reason to pipe the output into a text file or another program.
I hope I'm not being too much of a buzzkill here, but while I think `failure_derive` is great, I strongly think returning `Result&lt;T, failure::Error&gt;` should be avoided in the return types of libraries. **Why?** The problem is that returning `Result&lt;T, failure::Error&gt;` essentially turns all errors into dynamically typed objects, and I don't think I should preach to /r/rust the advantages of strong static typing. But just to clarify a little bit: If a function returns a `Result&lt;T, failure::Error&gt;`, then you have no idea what that `Error` could be. If you want to do anything other than to log or display it (without localization), you have to rely on the documentation of the function/library to see which error types it can return. If a library decides to add another error type, then the compiler cannot help you update your code to handle it everywhere you might encounter it. And if the error type is just a string, which I fear will happen more than it should because that function `err_msg()` is oh so convenient, then you're completely out of luck. To clarify a bit further, there are 2 kinds of errors: 1. Those that should never happen if your program is bug-free 2. Those that can happen even in a bug-free program: invalid user input, network is down, file not found, etc. `failure::Error`s are a great container for that first kind of error: it has a backtrace, is easy to log or print (raw, but who needs localization in log files), etc. But it's not a good container for the second kind of error, since it's hard to do anything non-trivial with it. Doing localization, converting the "technical error XYZ caused by blablabla" reason-string into something more user-readable, knowing which HTTP error code to return... all of these require branching on the type of the error. But you can't do an exhaustive match on a `failure::Error`, you can only try downcasting to what is in the documentation of the libraries you're using, with the need for a fallback case. Additionally, as the original post says, changing a function's return type to `Result&lt;T, failure::Error&gt;` is a breaking change, which is another significant downside. **But what instead?** So if libraries shouldn't return a `Result&lt;T, failure::Error&gt;`, what should they return? I think they should return an enum, with each variant containing a box with details about the error (possibly a further cause of the error), for which they can derive `fail` using `failure_derive`. That enum won't be more than two words big, because all the data is boxed up. Importantly, if this is what they're already doing then it's also not a breaking change, just derive `Fail` for the error enum. Now you say that writing and maintaining this error enum is much more work than simply returning `Result&lt;T, failure::Error&gt;`s. It is true that it's much more work than just using `?` to pass the `failure::Error`s from dependencies straight to your users, but doing that just shifts the inconvenience from the library writer to the user. Should the user of a library really be burdened with trying to downcast a `failure::Error` that it got from a library into all possible error types of the library and its dependencies and its transitive dependencies? No, the library should do the work of converting exotic transitive error types into some set of meaningful error reasons for the user. Let me explain with an example: **reqwest** Reqwest was used as an example in this thread, and it's a great example. The error enum for `reqwest::get` should have only these variants and nothing more: * invalid URL * server returned invalid HTTP * server returned HTTP error code X * server returned invalid JSON (because reqwest can parse JSON) * connection failure Reqwest should do the work of converting errors like TCP errors, socket errors, too-many-redirects, etc. all into that one variant "connection failure", because a user of reqwest rarely cares about the technical reason why a connection failed. And if they do, well, details may be found in the box, which might contain for example a further enum or `Box&lt;Error&gt;` or `Box&lt;Fail&gt;` with the possible causes of the connection failure. But users certainly shouldn't be expected to downcast a `failure::Error` they get from `reqwest::get` to all of `::hyper::Error`, `::native_tls::Error` and `io::Error` to find out if the reason for getting an error is that the connection failed, which they would have to do if hyper, native_tls and io all returned `Result&lt;T, failure::Error&gt;` and reqwest just passed those on with `?`. So regardless of whether a library is returning `Result&lt;T, failure::Error&gt;` or `Result&lt;T, CustomLibraryErrorEnum&gt;`, the library should do the work of converting transitive error types into library-specific error types, and that's just as much work with `failure::Error`s as with custom enums. But it's less maintainable with `failure::Error`s as you can't do exhaustive matches and need a fallback case. 
&gt; Using bytes rather than characters is twice as fast but that might be considered cheating. Indeed! PR 7 has a C+SWIG version using bytes, which was only a little faster than Rust using chars, so I've suggested [Rust using bytes](https://github.com/rochacbruno/rust-python-example/pull/7#issuecomment-345307389) for comparison. I'll bet this could be parallelized too... something like `.par_chars().fold().reduce()`, where the fold keeps track of the edge characters for the reduce to compare while it's summing the totals. That extra bookkeeping might be expensive, but it may still be a win for large inputs.
Enforcing ownership and borrowing just makes a compile time error a runtime error. I'd rather it barf at compile time rather than during some possibly obscure set of circumstances I'd need to recreate to debug at runtime.
Is a recording of the talk available?
I think you've missed some of the other ways this library can be used (which is understandable since the blog doesn't mention them). I think that [A Custom Fail Type](https://boats.gitlab.io/failure/custom-fail.html#a-custom-fail-type) or [An Error and ErrorKind pair](https://boats.gitlab.io/failure/error-errorkind.html) address your points more or less. I agree that standalone libraries should avoid returning `Box&lt;Error&gt;` or equivalent.
Unwind safety is not a memory safety concern. It can cause memory safety concerns with poorly designed unsafe abstractions, but Rust demands stuff be safe in the presence of unwinding. AssertUnwindSafe is there for acknowledging when unwind safety is lost, since it can still lead to logic bugs.
For a simple crate, merely "extern crate core as std" is enough. Here simple means, it only supports either just core or the full std, no alloc crate in betweens etc.
&gt;By default your binary will dynamically link at least libc Even if you aren't using the `libc` crate?
The elegant solution would be not having this code at all. Even if some crate doesn't use anything from `std`, you still can't use it without this completely unnecessary modifications. Suppose some library has a function that reads data from file. And you want to use this library in no_std context. The sane way to do it, would be just commenting out this one function, or putting it behind a feature flag. But instead you have do this dance with namespaces at the top of each crate.
And right below std would be the compiler version. If Rust had an ABI, then crates could be compiled with the version of the toolchain they expect. This would allow codebases developed over 10s of years to use new features while maintaining robustness via older toolchains.
&gt; What mistakes have web browsers made here? I was referring to the HTML/CSS/JS feature disparity and all the frameworks to deal with it. Applied to cargo progress bars, I mention it to stress that "advanced" terminal features ought to be used if one is certain about their availability. Terminal emulators come in many forms, from one of the builtin Emacs terminals to fancy multimediate ones like Terminology. And people also use terminals that don't support updating a line inplace but demand printing to a new line, but it's a niche for sure. &gt; Cargo already strips color when piping (and other things), so this is going to work anyway. Good. I suppose it makes sense to enable frills by default while providing flags to disable them per user/project.
https://air.mozilla.org/bay-area-rust-meetup-november-2017/
Except disabling`std` to get rid of backtrace severely limits the crate from my quick glance. Maybe I missed something.
Less copying, less heap allocation, more concurrency. All backed by the guarantees the compiler provides so that anyone can write or edit this code without spending hours manually checking that all invariants are checked and race conditions avoided.
This is amusing but generally placeholder crates don't go through the effort of being `no_std` ;)
&gt; It wouldn't be coherent because it would make the From impl for all Fail types conflict with the reflexive impl of From in libcore. *grumble* This is the second time I've run into the reflexive impl of `From`. Has anyone developed patterns for dealing with this? I worry this will be a continual problem, requiring ad-hoc work arounds that will make APIs uglier.
You're probably thinking of npm. It was a bug that was fixed. Bugs happen.
Nix doesn't work on Windows.
This is a completely different discussion.
Nothing is stable till you can use it in stable. They're still very much experimental. But, that means you should try them out so that we can get feedback about the design!
Is there a reason why conditional `use std as core;` in the `lib.rs` is not sufficient?
Thanks for posting this! I did an exchange in Germany two years ago, but all of my classes were (unfortunately) in English, so it's nice to finally see what a CS course taught in German is like. And /u/DebuggingPanda your lectures are *really* good. Makes me wish I went to OsnabrÃ¼ck.
&gt; it is still much simpler than working with binary (xlsb / xls) files. It is also documented (likely incompletely) as part of ISO/IEC 29500:2008
Is there any way to take out and put back elements which can be as fast as using references? Most of the time I need to do this kind of things thousands of times per second.
OK, on my box the 'family' field would print 'Skylake', which is Intel's 6th-gen family.
My question was more general, but I have a few situations where this bothered me: finding the neighbors and changing cells in a cellular automaton like simulation; updating joints in a physics simulation, where I need all the data from two bodies at the same time while updating their velocities. After a while I eventually found workarounds which involved not using references, but still, it was a useless task to avoid a problem that didn't really exist. (There may have been arguably better designs, but I think there was nothing so fundamentally wrong with mines to be scraped, at least on the memory-safe side of things).
I guess it depends on how big of a copy it is. Are vector elements just pointers/references to something or are the elements larger. If larger then it won't be as fast.
FASTBuild supports and is FAST. Won't have the security requirements I imagine, but it is worth taking a look at it.
I don't want to offend anyone but this looks more like obsession with safe code to me. There's nothing wrong with correct usage of `unsafe`. The language would be useless without it. TockOS wants to have "untrusted" capsules. While the idea looks appealing, I think it's fundamentally broken design.
And that would tell you to use `--target-cpu=skylake`? Sorry I feel really really dense here... maybe there's a list of possible values for`--target-cpu`? I tried looking at the docs, but I did not find something like this.
While not specifically about Rust, this response to flawed StackOverflow answers provides a great exploration of exactly how and why you can shoot yourself in the foot with optimizers like LLVM's if you don't think about the entire context of the code you're benchmarking.
Thanks for the solution, but as far as I understand this only lets you get two references at a time, while I would have liked to get a general N references solution.
Sorry for making you feel frustrated. You're right that I did not pay enough attention about your explanation about trait object safety. Partially because I don't expected that to change much, at least not initially. Right now we have the two variants `fn foo() -&gt; usize`, which disqualifies the entire trait from becoming a trait object, and `fn foo() -&gt; usize where Self: Sized`, which does not do that, but instead you can't call the method from a trait object. I'm kind of looking for a third variant: a method that can be called either if it is Sized or if there is a vtable. A method that I can call either like `i32::foo()` or, if I have a trait object, by a virtual dispatch, like `(x as &amp;Foo).foo()`. So let's skip my last comment and assume trait object safety is unaffected, in the sense that you still can't call the method from `fn bar&lt;T: ?Sized + Foo&gt;`, because here there is no Sized bound, nor is there a vtable. Does that help? Adding another entry to the vtable should be straight forward, but maybe finding a syntax for expressing "either if it is Sized or if there is a vtable" is not as easy. 
The announcement of generators being available in nightly was on 30th of August and after an inital flurry of demo projects it's gotten very quiet. It would be interesting to hear if anyone has used them more extensively and can give some info on their experiences.
There is a disconnect here between trusted code as a language pattern and trusted code as a contractual agreement for system maintainers and developers. I don't see why a trusted capsule couldn't itself use unsafe code and still maintain the trust agreements desired here. `unsafe` or not, code can still be untrusted and use no unsafe blocks. There is a lot to be said about crates.io having crates support the `no_std` pattern at all. The rise of the 'std' feature pattern means that the ecosystem is consciously thinking about use cases outside the Tier 1 platforms in the compiler. If the bar of "ready for embedded" is that _most_ of the ecosystem is designed around this, I'm not sure how you could accomplish this whilst having the "no unsafe" requirement. "Not ready" is a rather strong claim considering there are many practical examples of Rust being used to target highly specific embedded systems. It's never going to be perfect because they don't fit the happy path for support, but it's quite a bit better than most languages already.
I'd love to work with you Ted
IMHO the best approach is to have a separate thread do this: while running.read() { print!("{}%\r", percent.read()); sleep(interval); } `interval` certainly should be &gt;= 1/60s. (No point in redrawing faster than max FPS.)
This is my first Cargo subcommand crate! I wrote it mostly because I wanted to see how [this rustdoc hack](http://docs.rs/pwnies) has been done (warning: ponies). With a broken repository link there is no obvious way to download the crate's source code -- not without making a dummy Rust project and spelunking through Cargo caches. Incidentally, the project also seems to address [this Cargo issue](https://github.com/rust-lang/cargo/issues/1861) :) It's a pretty early version and there are still obvious features missing, most importantly extracting the crate's gzip archive. I'm of course open to any other suggestion!
Can it be used for crates.io offline usage during an extended period without internet?
Yeah, sorry, you are right. I did a cargo update and got the "did not resolve any crates" message also. I managed to get it working. Looks like you can get the source for 0.14.4 from `git checkout 9dcca7` if that's the version you are trying to use. You should then be able to `cargo update` and have it switch over to the patched version but it did not work for me - somehow it was wedged. I deleted `Cargo.lock` and reran `cargo update` and it picked up the local checkout. I verified that it was building the local copy with `cargo build -v`.
It could be used to pre-download any crate you might use in the future, of course. It only gives you the archive though, so there would be some additional work to put it in the right place for `cargo` to find. If you're asking if it works by itself when offline, then well... it does talk to the crates.io API so... :)
Depending on your LLVM version, `rustc --print target-cpus` should give you a list.
I can understand not wanting to trust other people's `unsafe` code willy-nilly, but at some point you have to trust something (or vet the unsafe code yourself). The whole point of `unsafe` in Rust is so you can wrap otherwise unsafe operations inside and abstraction that is completely safe to use from safe Rust. Without this encapsulation, most of the Rust standard library would not be possible. Every crate depends on libcore, so according to their reasoning, 100% of crates are unsafe, including TockOS. It sounds like they're trying to make TockOS hyper-safe, and they are realizing that the `#![forbid(unsafe_code)]` attribute doesn't suit their needs. What they need is a way to figure out which crates they depend on use `unsafe` blocks, vet them and mark them as trusted.
You don't have to disable `std` for your project, you can just set the features for `failure` to an empty array. Everything else can still use std.
I didn't realize generators were usable! For anyone interested in trying them out, here is the documentation for the unstable feature: https://doc.rust-lang.org/nightly/unstable-book/language-features/generators.html
You can also never turn it on with the env var, making the cost a very small null struct. Compiling backtrace from clean takes less than 6 seconds on my machine, and only has to happen once. I don't think it would be wise to add a bunch of fine grained cargo features to avoid things like a single small dependency and a couple words of heap space.
It did thanks! Looks like it's `x86-64`.
Is this different from [`cargo clone`](https://github.com/JanLikar/cargo-clone)?
Then you're spamming the connection for no real gain. Having a time based peak refresh makes sense, but the primary basis for re-rendering should be that there's been a unit of progress. That way you can predict exactly how much data the progress bar sends over the socket, for a worst case scenario.
I think you meant "sum type". Existentials are something different.
I didn't know about Rayon's par_chars. Looks like it takes advantage of UTF-8's self-synchronization to find a valid chunk boundary without having to scan through the whole string first. Neat! &gt; where the fold keeps track of the edge characters for the reduce to compare while it's summing the totals. That's similar to what my unrolled attempt did. It compared in blocks of 4 or 8; the first byte was compared against the last of the previous iteration. To not have to special-case the first iteration, I widened it and started with a value outside the range of a byte. Same technique would work for parallelism (and hopefully would actually be worthwhile there).
You're probably right, but I'm using "existential types" as it's used to mean the general form of these types (for which "sum type" is an instance) in the literature that points out this issue (https://homes.cs.washington.edu/~djg/papers/exists_imp.pdf).
Ah, that's true. Updated code: let mut last_percent = 0; while running.read() { let current_percent = percent.read(); if current_percent != last_percent { print!("{}%\r", percent.read()); last_percent = current_percent; } sleep(interval); }
Looks good to me, only thing i really miss is print being on stderr rather than stdout. (Or is it? I'm not good enough in rustacean).
Almost certainly. It would be good to exclude FFI calls that don't do any non-trivial code. More generally, I think it's about being able to track which of your dependencies might be able to break type-safety if they happen to be buggy. That extends to `*-sys` crates (FFI calls can absolutely break type safety if not wrapped correctly, which can be hard), it's just in those cases you may be stuck (which is fine, just still useful to be able to know).
The two general solutions are to use indices, or to have a vector of cells, (or ref cells, mutex, rwlock ie things with interior mutability). For a classical cellular automation (on a grid), indices would probably be faster than than references due to reduced cache usage, since neighbor indices can be calculated from the current location. If it's a graph like structure, which is challenging for the borrow checker (see [Too many linked lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/)), then maybe a graph crate like [petgraph](https://crates.io/crates/petgraph) would work. I'm not familiar enough with physics simulations to guess how to handle them. But one trick is too use mem::swap, mem::replace to take an object out of out but put in a dummy (like None), then swap it back when you're done, this fundamentally isn't all that different from using Cell. The hard part of getting multiple mutable references to the vector is convincing the compiler that they are in fact to different elements. using split_at_mut obviously returns things that are disjoint. It should be safe to grab N different references in one call, and I suspect a macro could be made that makes sure the N indices are all different then produces the split_at_mut calls to do it safely, or just acts as the safe interface the unsafe code to get the indices. But the confirming all the indices you want references to are independent could get expensive as N increases. The more I think about the more just using a vector of cell's seems the easiest solution. Is there a reason that wasn't suitable?
I miss Ticki :'(
Tock's capsules _are_ forbidden from using it. That's the point.
Yes. $ cargo new --bin no-libc Created binary (application) `no-libc` project $ cd no-libc $ cargo build --release Compiling no-libc v0.1.0 (file:///home/jgulotta/Projects/sandbox/rust/no-libc) Finished release [optimized] target(s) in 0.86 secs $ ldd target/release/no-libc linux-vdso.so.1 (0x00007fff17dbb000) libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6da1154000) librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6da0f4c000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6da0d2f000) libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6da0b18000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6da077b000) /lib64/ld-linux-x86-64.so.2 (0x00007f6da15bc000) 
(original reply minsunderstood a key senstence, sorry). You're exactly right, I think: Clearly _some_ crates will need to use `unsafe` (or maybe will deem it worthwhile even if not strictly necessary). Anyone using those crates (either directly or not) will have to trust those crates, as they have to trust `libcore`. Nothing wrong with that. _But_ there should also be a way to _know_ that only a set of trusted crates (that the user specifies they trust) can explicitly get around compiler-enforced type safety. It's exactly a matter of degree, and there ought to be a way to tune that degree. That's all.
I know they're being used for C#-like await / async syntax in [futures-await](https://github.com/alexcrichton/futures-await), which is amazing. 
Just noticed his GitHub page. Is he all right?
I think the current behavior is best for the unsafe_code case. I found [est31's comment](https://www.reddit.com/r/rust/comments/7dis52/cratesio_ecosystem_not_ready_for_embedded_rust/dpyeute/) convincing.
seems like you threw together this impressively fast! Lol @ this avant garde Rust formatting style: https://github.com/Xion/cargo-download/blob/master/src/main.rs#L5
His twitter is also suspended. We're out of the loop, what happened?
I think specialization fixes it.
Yeah, that's definitely a downside. Similar solutions could be made to get any N elements using split_at_mut N-1 times, but I'm not sure which strategy would work best for what you're doing.
/r/playrust
Can you actually do evil things with it? Or is it just "technically" UB?
IIUC that's the generic baseline target, i.e. the default that you're trying to avoid. What does `native` report your host as? (Again e.g. `skylake`, `ivybridge`, `core2`, something along those lines.)
Let's.. not be too noisy about it. Maybe he just wants some time off.
I like it. I've always felt `#[macro_use]` was a bit clunky, because imports are usually a single line. This at least makes imports line up.
I'm doing my degree right now, which unfortunately leaves me with virtually no time for open source for at least a year. I plan to spend vacations on it, but there haven't been any long enough vacations yet for it. I'm not dead nor have am I quitting open-source, I just have very little time right now. As for my Twitter account, that is a whole other story, and a quite funny one as well. Maybe you recall the [4th november hoax](https://www.snopes.com/is-antifa-planning-a-civil-war/), where right-wingers thought antifa was planning civil war and murder of all white people. Me jokingly playing along with this and telling a white supremacist that I was going to break into his home and take his toothbrush made Twitter suspend my account for "making violent threats".
Thanks for the link, just gave it a read-through. I'm struck by how similar `Generator` and `Iterator` are. The only difference seems to be that `Generator` has a return value separate form the yielded value, whereas `Iterator` only returns `Item`s. As one of my prime use cases for generators is as an easier way of implementing iterators, it would be great to see something like: impl &lt;Y,R&gt; Iterator&lt;Item=Y&gt; for Generator&lt;Yield=Y, Return=R&gt; { fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { match self.resume() { GeneratorState::Yielded(y) =&gt; Some(y), GeneratorState::Returned(r) =&gt; None } } It just gives you `yield`ed values until it hits a `return`ed value, at which point the iteration halts.
I've also found cases where I couldn't translate correct C++ code directly into Rust because of the borrow checking rules. But I think it's worth it to have actual proof your code is memory-safe. In many cases the result is be better in other ways, too. For example, I have a big array in one program that represents "slices" of a file to construct on the fly. There are several types of slices. In my C++ code, Slice was an abstract class. I had a vector of (64-bit size, 64-bit pointer to the Slice) for each. The overall class held the vector and (sometimes indirectly) the Slice objects. Those each had a vtable pointer (64-bit) and usually another pointer or two. That's 24 bytes per entry or more. In my Rust code, I gave up on having the pointer because it'd mean I had a struct which owned a vector which contained references back to the struct, not possible in safe Rust. I decided I'd pass in the pointer to the overall struct to everything and have a vector of enums, most of which took an integer parameter that was an index into another array. Then I realized with the valid ranges of things (files up to 1 TiB, various reasoning about valid indices), I could represent an enum with a packed representation: a single 64-bit number with the size in the low bits and the type+parameter in the high bits. I reduced the memory footprint by at least 24 bytes per entry in total, which gave me a warm fuzzy feeling cache-friendly feeling. Presumably more stuff can be inlined, too because dispatches uses a match class rather than a virtual function call.
I suppose if your project is "no_std" then it has no dependencies.
I'm pretty sure `chalk-parse = { git = "https://github.com/rust-lang-nursery/chalk" }` will just work
What level of safety and security does Tock target? Embedded systems have a wide range of requirements. Without setting the goal the metrics will be arbitrary. For example, the kernel seL4 written in C has a formal verification which proves that the OS matches the spec and is compiled correctly. Rust compiler is not ready for that either, but that doesn't mean that the ecosystem is not ready for embedded. Rust memory safety fares better than C, but not as good as verified C. It is safe enough for quite a few applications. To develop a verifiable safe system Rust would first need the tools for the theorem provers like Isabelle or Coq.
That works if you have a known set of common types that aren't likely to change, but other parts of your interface that are likely. The semver trick is good if you have to make something that is technically a breaking change, but is unlikely to break many people in practice, or which is just mostly a reorganization. The nice part about this is that you don't have to predict in advance which that is, so it can be used when doing some kind of reorganization after the fact. In fact, the semver trick would be a good way to do a breaking change which factors out the common types into a separate crate, so it can work together with your suggestion.
It's an interesting trick. Splitting types out to a separate crate can be done without the semver trick, and without a breaking version upgrade.
Glad to hear you're doing well!
No problem dude, just remember that we'll miss you :)
Yep, that worked, thanks!
That also moves away from the "by default" qualification :D But, yes, it's totally possible to remove the C dependency entirely. And many projects do, particularly ones targeting embedded devices.
I was surprised too to see that generators don't implement `Iterator`. I imagine they will one day. I was playing around with generators and logging their sizes, and found something that seemed kind of strange. Can anyone explain why generator `d` takes up 80 bytes? https://play.rust-lang.org/?gist=887e7d4f43819e2b9ff56637e60d1bd5&amp;version=nightly
Oh, I see. 8 + 24 * 3 = 80. `v1` keeps its slot in the generator struct, even though it is moved during the second resumption. I thought they would be implemented like enums, but I guess not
I suspect that in order to avoid expensive copying between yields, rather than representing generators as enum { State1(live at state 1), State2(live at state 2), ... } they are represented as struct { tag: u32, live variable 1, live variable 2, ... } Since `v1`, `v2` and `v3` are all live at some `yield`, they are all stored in this struct. That struct (`u32, Vec, Vec, Vec`) is 80 bytes. Since the optimizer is not built for this kind of code, I suspect liveness analysis simply isn't performed, so `v1` and `v3` never share their spaces.
What I've noticed about LLVM is that it is bad at reordering statements. For example: let s = (-s1_y * (p0.x - p2.x) + s1_x * (p0.y - p2.x)) / coef_div; if s &lt; 0.0 &amp;&amp; s &gt; 1.0 { return None; } let t = ( s2_x * (p0.y - p2.y) - s2_y * (p0.x - p2.y)) / coef_div; if t &gt;= 0.0 &amp;&amp; t &lt;= 1.0 { // do something } else { return None; ] can result in more efficient assembly than: let s = (-s1_y * (p0.x - p2.x) + s1_x * (p0.y - p2.x)) / coef_div; let t = ( s2_x * (p0.y - p2.y) - s2_y * (p0.x - p2.y)) / coef_div; if t &gt;= 0.0 &amp;&amp; t &lt;= 1.0 &amp;&amp; s &gt; 0.0 &amp;&amp; s &lt; 1.0 { // do something } else { return None; ] [Context](https://github.com/fschutt/polyclip/blob/master/src/point.rs#L7). The second method spills "s" onto the stack, while the first one keeps it in a register, which is faster. LLVM could reorder the statements here, but it doesn't (maybe because of floating-point comparisons?). LLVM can also often not "see" duplicated code (small values such as `coef_div`) - if you copy-paste code, even small amounts, you almost always calculate the value twice, because LLVM doesn't search your code for identical tokens / patterns. So that would be the first line of micro-optimizations. LLVM is also very bad at vectorizing loops (stepping over elements 4 or 8 or 16 elements at a time) or reasoning about the access bounds of loops. I usually use unsafe code in loops, because I often have to step across an array in a certain pattern where iterators are not sufficient. Rust does also not have FMV so you have to do it manually which is a real PITA. Sure there are a lot of factors at play, branch prediction, pipelining, instruction cache size, certain operations can take different time in different situations, depending on how the registers are set up. But LLVM isn't the holy grail of optimizers. My guide to micro-optimization is: - Algorithm first. Always. - Don't optimize cold code, it's useless. - Be nice to LLVM. Precalculate duplicated code so LLVM can see that it is reused. - Reorder statements so that values can fit in the registers. A load from the L1 cache is said to be 3-5 cycles for access time, while registers are instant (1 cycle). On the other hand, more code = more time needed for the CPU to decode the statements, maybe worse branch prediction. Benchmark. - Vectorize manually where possible (esp. loops). Compile with `+avx2` or `+sse3` first to see what the compiler would do (if anything). - Use flat arrays instead of structs for performance-critical code. I once made a SIMD-accelerated polygon-rotation function and wondered why it performs worse than non-SIMD code. Well, if you have a struct, you can only load one value at a time, so `load(arr[i].x, arr[i].y, arr[i+1].x, arr[i+1].y)`. Rust does not realize that `struct` only consists of an x and a y field and if you have them in an array, that they are contigouus. If you have an array with flat XY-values, you can say `load(array[i], 4)` which is faster - but only if you have the array set up in an X, Y pattern. - In for-loops, use unsafe if you are sure that there can't be any access violation. Be careful.
I think this is the wrong way to look at it. Rust's reason for existence is safe code. It's true that, yes, unsafe is built in to the compiler, but it's a compromise relative to that reason for existence. Using it isn't a crime, but it's definitely something to take seriously. Did I dream it, or was everyone saying for a while that most people should never have to write unsafe code? Reading this thread, I'm worried that people are complacent about the risks. It only takes one mistake to write a RCE bug.
I believe that's only true if the lattice rule gets added to specialization
You'll probably want to use the pkcs11 API to interact with the HSM. Looks like this crate has at least the building blocks implemented. https://docs.rs/pkcs11/0.2.0/pkcs11/index.html I've done some work with pkcs11 from golang and it can be a little daunting at first figuring out how things work but it's not bad after you become more familiar with it.
https://crates.io/crates/gen-iter has a wrapper for turning generators into iterators. See also [some previous discussion](https://internals.rust-lang.org/t/help-test-async-await-generators-coroutines/5835/16)
Unsafe is not bad per se, but relying on under-reviewed unsafe code certainly is. We need better tooling to tackle this problem. I'd love if crates with unsafe blocks and impls were labelled as such in crates.io, not because they are bad, but because they are worthy of extra scrutiny. And then be able to whitelist in cargo a small number of foundational crates; all other unsafe crates would be forbidden.
That said, inefficient or wrong code has a huge impact on the economy. Strengthening it could make it more resilient to taxation and remain needed to prevent further climate damage.
Here is a guide for running it in a browser: https://developer.mozilla.org/en-US/docs/WebAssembly/Loading_and_running You need a host to run your wasm, I'm not familiar with any, but there are probably hosts outside of browsers for wasm out there.
Well, panicing might be exactly what you want in that case :). For that reason, I generally recommend to use RefCell this way: cell.borrow_mut().immediately_call_method() If you want to call multiple methods, always in the same pattern, add it to Foo. I'd also like to note that Servo has a fork of RefCell that _is_ sync, for places where they are reasonably sure that no concurrent access ever happens and that would be a bug. Note that this has correct semantics: a racing access will lead to a panic, not to a data race, which is the only thing that Sync guarantees.
I know this isn't the focus of the post, but &gt; a negative value means that the first value is smaller than the second, a positive value means that the first value is greater than the second, and zero means that they are equal. Negative, zero, and positive return values. Not -1, 0, and 1. The next time I see branching in a real comparison function when `return a - b;` would suffice, someone's getting shanked.
I'm happy to be proven wrong, but I believe that HACL*, and crypto algorithms generally, consume a constant number of bytes per iteration. Puffs' zlib decoder has to consume a variable number of bytes per iteration, because Huffman codes use a variable number of bits per code. Eliminating bounds checks in this case is a harder problem. You can't use a Rust style "for val in bytes_iter" loop and avoid referring to the iterator inside the loop body. Perhaps dependent types could solve this problem (eliminating bounds checks, with a variable number of bytes consumed per iteration). Even if HACL* isn't in this category, perhaps F* can verify these types of byte stream consumers too. Once again, I'm happy to be proven wrong. Still, dependent type systems are a more complicated type system. More powerful, yes, but also more complicated. As I said on https://github.com/google/puffs/blob/master/doc/related-work.md, I'm not saying that dependent types are bad, it's just a different approach, with different trade-offs.
&gt; I'd say the OP's claim that Rust isn't as fast as C or C++ isn't accurate. Sure, in general, Rust performs as fast as C/C++. Still, to repeat what I've said earlier, Rust can check for arithmetic overflow (at run time), but after some debate, the Rust team decided to disable such checks when building in release mode, for performance reasons. So, Rust out of the box (without arithmetic overflow checks) is as fast as C/C++. But the implication is that Rust (as it is today) *with* arithmetic overflow checks is not as fast as C/C++. In comparison, Puffs (as it is today) has mandatory arithmetic overflow checks, yet performs as fast as C/C++ (without any checks). If you want to write a blazingly fast regexp engine, and you're not concerned about arithmetic overflow checks, then that's great. Use Rust. Don't use Puffs. Everyone is happy! But for some classes of software, I want arithmetic overflow checks. P.S. Hi, burntsushi! I still use your x11 code every day, in my day-to-day window manager. :-)
That almost works, but you can overflow.
Sounds like an opportunity to start building an ecosystem of Crates that specifically target `#![no_std]` and `#![forbid(unsafe_code)]`. Small stuff, UNIX style (simple purpose). Only then can bring order to the (embedded) galaxy.
That was already mentioned in the comments. The [response](https://blogs.msdn.microsoft.com/oldnewthing/20171117-00/?p=97416#comment-1316165) then pointed out that it gives the wrong result for `compare0(INT_MIN, INT_MAX)`
Okay, fucking love this formatting. I'd be happy to see this be an option for RustFmt.
Certifying Rust for signaling would request certification of a specific version of the compiler. That is why main signaling companies use C or ADA with appropriate SIL level coding guidelines. To afford complex code (driverless metro are partially SIL4) the industry developed several methods to handle huge code base: - B methods for mathematical proof of code properties (see ADA Core) - code generation from Model Based Design (matlab simulink for POC, SCADE for production) None of these methods have been proved perfect. In this industry, the architecture &amp; specification teams are at least as big as code writing team. Due to SIL requirements, it is better/recommended that the person who specifies is different from the person who codes , different from the person who tests and different from the person who asses safety. All of this to say that, according to me, Rust provides guarantees for performance and safety of shared states in multi-threaded architecture. The amount of engineering for these projects makes rust unappealing since coding activity/ performance is only a single part of the success of a product. Generally, industrial avoid coding activities in favor of engineering optimization due to the global cost of each line of code to ensure that codebase is not fragmented/full of dead code which runs against safety. See [Singapore accident press release ](http://www.straitstimes.com/singapore/transport/inadvertent-removal-of-software-fix-led-to-collision) for example 
Emscripten provides a runtime which emulates a lot of features from traditional environment that a browser doesn't have (a file system, ...). Emscripten was designed to compile large code bases to the web without the need to change a lot of code. In order to emulate all those features, Emscripten has to ship a lot of JS code which does most of the emulating. So when you compile with Emscripten as a runtime, you *cannot* get rid of the big JS file. So can you compile without Emscripten as a runtime? Yes and no: you can, but it's very annoying to do and/or unstable. I wrote [some instructions on how to produce minimal WASM files without Emscripten](https://gist.github.com/LukasKalbertodt/821ab8b85a25f4c54544cc43bed2c39f). This is quite some effort, especially compiling your own LLVM version and so on. It's more a proof of concept than a "this is how you can do it in production". Luckily, work to make this easier, is on its way! [This awesome PR](https://github.com/rust-lang/rust/pull/45725) was recently accepted. There is also [this internals thread](https://internals.rust-lang.org/t/state-of-webassembly-and-rust/6077) which discusses the future of Rust &amp; WASM. I hope this helps for now, even if there is no good solution yet.
The feature flag would have to be called `"unsafe"`, since it would be adding the `unsafe` code.
I'm almost afraid to ask... how to I get to see what `native` gets translated to? I tried a lot of printing and logging options, but everywhere `--target-cpu=native` would keep appearing.
I'm currently working on better documentation and tutorials explaining all of this. It will be online on http://www.hellorust.com/ shortly.
Can you show us the assembly code? Unless I am blind or something, I don't see the spill of s. Also, actually it seems that LLVM is able to see duplicated code (This is GVN's job, I assume). I've tried to replicate the assembly code by copy pasting coef_div and it the optimizer was successful in removing the same expression. Rust: https://godbolt.org/g/6EPgpU C++: https://godbolt.org/g/1XA2MJ (Produces similar code) Am I missing here something?
From `rustc --print target-cpus`. ;-]
AFAIK feature flags should be additive, but here I don't know whether the feature is to use `unsafe` code, or not to use any of it.
That's where I got `x86-64` from, but that was just my guess from comparing `lscpu` output with this. Unfortunately, nothing's in there that says "this is your native cpu".
On a microcontroller, like where TockOS is used, unsafe is **necessary**. You have to read and write to arbitrary memory addresses, since those addresses correspond to peripherals in the real world. Someone has to build the safe abstractions on top of the unsafety. I also don't think anyone claims you should never need to write unsafe even on desktop applications. If you interface with a C library, you're automatically required to use unsafe, since C code does not promise to uphold any of Rust's invariants. "unsafe" simply means that the programmer is taking on additional responsibility for ensuring the safety of that code. Usually, if you're tempted to use unsafe for a data structure, then you're probably doing something wrong. The standard library has a great selection of data structures, and crates.io has a selection of more niche ones too. Reinventing the wheel is frowned upon in most languages, since your wheel is unlikely to be as good as the ones that already exist. It remains an option, of course.
Not sure what you're seeing, but I get output like [this](https://gist.github.com/anonymous/375b68cb2a9306c04d736f53051fa5f2). :-/
Ah thanks for clarifying! I agree. It seems I took your statement out of context. :) &gt; P.S. Hi, burntsushi! I still use your x11 code every day, in my day-to-day window manager. :-) Yay! My own WM is still holding up strong for me too. I don't look forward to the day that I'm forced to switch to Wayland. :-(
Are you actually reading the docs? :) Here: https://ws-rs.org/api_docs/ws/struct.Sender.html Go through the methods and I promise, you'll find the answer.
According to [this](https://ark.intel.com/products/92979) you'll want `--target-cpu=broadwell`.
First of all, thank you for your answer! And yes I did go through the docs, but the on_message method from the handler only has a Message parameter, and no Sender, so I can't use it to retrieve the token. Unless I'm missing something? 
Oh, I guess I missed something. I overlooked the `Handler`. Can't you just store the `Sender` in the `Handler`? ``` public struct Client { pub sender: Sender } impl Handler for Client { fn on_open(&amp;mut self, _: Handshake) -&gt; Result&lt;()&gt; { println!("New client connected!"); Ok(()) } fn on_message(&amp;mut self, msg: Message) -&gt; Result&lt;()&gt; { // ... You can access self.sender now Ok(()) } } ```
Well that's my problem! I am storing the senders on the handler, but since I'm storing multiple, I can't tell which one sent the message! I guess the API is just not designed to store multiple senders for one handler? Thanks a lot for your time!
Using Cells was the solution I most of the time ended up using, but the performance loss still bothered me, and I was wondering if there was a way to do this with pure vectors. The fact that I'm always switching between C++ and Rust also doesn't help with this things, which are very specific with Rust, I'm sorry if I'm making this a more complex problem than it is for most Rust programmers.
Most people writing application software don't need unsafe. Most people writing embedded software do. Saying "most people" unqualified doesn't mean a whole lot here.
This all could very well change in a future compiler version.
Please remove JS from gnome and perhaps replace it with rust?
Yes, you are right and my original statement (about the spill) was false. I misinterpreted the assembly, my bad. I'll remove the thing. However: - You have to check for 0 (ex. if all points are the same point, this will be 0), to avoid a divide by 0. - It makes the code more readable. - If you add the null check, the only difference between the code is the branching. In that case: - In the case that there is no intersection, my code seems to have less jumps and will execute slightly faster (about 1 nanosecond faster, that is). - In the case that there is an intersection, your code seems to be better (about 10 - 15 nanoseconds faster). [Benchmark here](https://gist.github.com/anonymous/e4ebaee5e324f2f1bce39bed53af255b) Results (for me): test tests::bench_do_intersect_default ... bench: 1,265 ns/iter (+/- 16) test tests::bench_do_intersect_early_return ... bench: 1,282 ns/iter (+/- 84) test tests::bench_no_intersect_default ... bench: 1,277 ns/iter (+/- 34) test tests::bench_no_intersect_early_return ... bench: 1,276 ns/iter (+/- 10) So I guess as a whole, the original solution is better overall and I was wrong. However, my points about SIMD still stands, so I won't remove the whole comment, just the part that was wrong. Thanks. I should learn to benchmark more :)
&gt; I think this is the wrong way to look at it. Rust's reason for existence is safe code. It's true that, yes, unsafe is built in to the compiler, but it's a compromise relative to that reason for existence. Using it isn't a crime, but it's definitely something to take seriously. No, Rusts reason for existence is pushing unsafe as far back as possible and wrapping it safely. Yes, it has to be taken seriously, but there are areas where it's just plain necessary. For example, Rusts type system doesn't make implementing circular data structures in memory possible (such a structure would always borrow itself and cannot be moved). _Any implementation of such a data structure is unsafe or transitively relies on unsafety_. Hardware interfaces are always unsafe. But in general, we don't write data structures or interact with hardware interfaces all day, we share data around, manipulate it or use the interfaces of those data structures. This should happen through safe code. The use of unsafe is to be minimised, but it is there for a reason. &gt; Did I dream it, or was everyone saying for a while that most people should never have to write unsafe code? Reading this thread, I'm worried that people are complacent about the risks. It only takes one mistake to write a RCE bug. No, we never said that. `unsafe` is a conscious language feature. Making `unsafe` code _explicit_ is also a conscious language feature. What's necessary is more education on when you should reach for it and when not. Saying that any code that uses unsafe internally is evil is not the solution. 
I agree with you, broadly speaking, but I find this imperative implementation to be fairly obvious, if a bit unusual: fn fib(n:usize) -&gt; i32 { let mut nums = [1;3]; for i in 0..n { nums[i%3] = nums[(i+1)%3] + nums[(i+2)%3]; } nums[n%3] }
&gt; On a microcontroller, like where TockOS is used, unsafe is necessary. You have to read and write to arbitrary memory addresses, since those addresses correspond to peripherals in the real world. Someone has to build the safe abstractions on top of the unsafety. I should mention that the greatest selling point of Tock is that they try to reduce exactly that through the capsules they mention in the post. https://github.com/helena-project/tock/tree/master/capsules
&gt; It's exactly a matter of degree, and there ought to be a way to tune that degree. That's all. I'm not sure if that degree can be meaningfully established. I think a better path is proper auditing tools that do - for example - warn if unsafe code is modified transitively within the project.
The source for the chat example on the WS-RS website is here, sounds like it could give you a good starting point: https://github.com/housleyjk/ws-site-chat By the looks of it, the part that's tripping you up is that you're expecting there to be one `Handler` instance for the entire server - looking at the example code, it seems like there's actually one `Handler` *per connection*. Therefore, if you want to share state between multiple connections, you'd need to store it outside of your handler and use an `Rc` to share it around. This conceptual shift also solves your issue of "which sender sent me the message" - it's whichever one's stored in the `Handler`!
I agree with you broadly speaking, but I find this implementation fairly obviously correct: fn fib(n:usize) -&gt; i32 { let mut nums = [1,1,2]; for i in 3..n+1 { nums[i%3] = nums[(i+1)%3] + nums[(i+2)%3]; } nums[n%3] }
Tock _is_ an OS spearheading and researching safe programming on embedded. The meme that "unsafe is necessary for embedded" is precisely what they want to disprove.
[Related StackOverflow question](https://stackoverflow.com/questions/46786956/can-i-add-a-dependent-crate-that-is-a-subdirectory-in-a-git-repository/).
I have got a feeling that the bottleneck of the existing pandas approach is the reader from sas, as it is pure python. The writer to feather is c already and from the pandas doc the throughput is good. I think an easier alternative to ârust everythingâ is to cythonize the sas reader used by pandas. Btw I recognise your name from a recent Julia vs data.table post. Didnât know you are into rust as well!
Oh, it sounds "simple" then. I would pay USD$800 for the fastest Cython implentation and USD$200 for the second fastest. Just PM me.
correct, I'm familiar with TockOS, and my point is 100% accurate. TockOS is on a microcontroller, and it must use unsafe in some form to access the peripherals. I'm not claiming that applications in the "userspace" of TockOS would need to use unsafe to access those peripherals. I said someone would have to build the safe abstraction on top of the unsafety, which in this case would normally fall to TockOS. But, if we take this blog post's statistics to their logical extreme, we can easily asset that *no* crates on crates.io are suitable for TockOS, since they all depend on `libcore` at a minimum, and libcore is certainly intent on safely abstracting unsafe code. Why should third party crates like `byteorder` be treated with such disdain? Obviously, if safety is your primary goal, then you should carefully review any unsafe code you depend on. They need to create a whitelist of acceptable third party crates -- ones who have been vetted, such as `byteorder` and `libcore`.
Yeah I think you're right, I was too stubborn in trying to make multiple clients for one handler work. Thanks for your help and if you do end up using this library and doing a server / multiple clients implementation I would love to see how you did it! Have a nice day. :)
Have been thinking about creating some GTK Rust tutorials, myself, as one of those who have never used GNOME libraries, and found the C documentation to largely be worthless.
This feature "approximates" the finite value 179769313486231570814527423731704356798070567525844996598917476803157260780028538760589558632766878171540458953514382464234321326889464182768467546703537516986049910576551282076245490090389328944075868508455133942304583236903222948165808559332123348274797826204144723168738177180919299881250404026184124858368 as the finite value 2147483647 which is pretty drastic and not really within the spirit of IEEE 754. But as others point out, other `as` conversions don't panic, so the expectation is that it has to silently return *something*, and 2147483647 works for at least some use cases. Converting NaN to 0 doesn't make sense numerically, but there's no obvious i32 value which makes more sense.
That'd be great! I intend to start it when I have time and after next release. Maybe even add a new page on gtk-rs website to explain each example a bit more.
Or at least make it optional. I want to build extensions in GTK, not JavaScript, so I should be able to use any language with GTK bindings.
Let me back up. The point of Rust is safe programs, yes? Memory safety and all that? In the ideal world, all safety would be statically, automatically proven. &gt; No, Rusts reason for existence is pushing unsafe as far back as possible and wrapping it safely. This doesn't actually have different consequences from my original thesis, except that it starts to lose sight of the goal, which is safe programs. This is about proving program behavior, not, fundamentally, a particular piece of syntax called `unsafe`. The reason `unsafe` is necessary in all those cases is just because it's hard to prove their behavior. If it was practical to prove the safety of cyclic data structures and hardware interfaces in a mainstream language, we would do that. Instead, we compromised and punted to the programmer. This is the underlying reason for the inclusion of unsafe. None of this makes me less worried that people are overusing it. Unless all of that code has been rigorously checked by people who know all the invariants it has to satisfy (it hasn't), there's a good chance of including *actually* dangerous code, `unsafe` that really is unsafe, in my supposedly safe project, which may then have a vulnerability as a result. &gt; Saying that any code that uses unsafe internally is evil is not the solution. Good thing no one is actually saying that, then. We're just saying it's unsafe.
- itertools has no `unsafe` code and is optionally `no_std` since 0.7.0. Does it work for you?
Can C language be used at the SIL4 level of criticity ? What C compilers are certified for the SIL2 level ? Is a certification of the compiler mandatory at this level ?
At least Thales has a C compiler certified up to SIL4 Note that SIL4 certification removes the ability to use pointers, loops,... therefore the amount of compiler to be certified is pretty limited.
Ok, that was the final straw, NOW i got it... it's nightly. Thanks!
Alright, that looks like it, thanks!
Does this effectively mean that there will be absolutely no progress on TFS for the next year? Or would you perhaps still review and accept contributions / pull requests, even though you don't have the time to do any actual development yourself? I might consider writing some code for it ... I was really hoping to see TFS move forward; it is an awesome project.
`_truncated_record_marker: c_void`, how big is this field? Is that even defined behavior in the ffi?
It's a bit complicated to explain and I'm not the one that'll provide the best explanation. cc /u/sdroege_
Is it possible to use futures-await with hyper? 
I haven't been able to try it out myself but there *is* an example of it on the repo. 
There is? Where? All I see is an example of using TcpStream's for echo, not a third party crate.
Instead of this (or the box variant) fn bar(baz: Box&lt;Foo&gt;) -&gt; Foo { // process *baz } I would suggest fn bar(baz: &amp;mut Foo) [ // process } If you have a choice between boxing something inside a function to return `Box&lt;Foo&gt;`, or not to return `Foo`, I would elect to not. But if a function get's something in a preboxed form to return I would dereference it to return the unboxed form.
Short answer: Don't even box things if you don't need too. It's better to use and return a struct if you don't need it in a smart pointer Longer answer: So your example program works [without Box](https://play.rust-lang.org/?gist=65fb4c691c7af855a3f3681c60a6fd48&amp;version=stable). In fact it's probably best to not use Box here unless you need it to be a smart pointer for something. Usually you'll use a Boxed struct for something like a recursive data structure like a [linked list](http://cglab.ca/~abeinges/blah/too-many-lists/book/first-layout.html). Box really shines though with Traits. So you could do something like return a Box&lt;Trait&gt; and that means that whatever gets returned implements the trait. At that point it loses Struct specific functions and things but it means you can use anything that here that implements that type that's generic for them all. Of course this has it's own costs associated with it. It's known as dynamic dispatch. Use it sparingly and try to use generic types with where clause constraints if you can. If that's not clear let me know!
I meant it mostly as a pseudocode.
This is the very first snippet of Rust code in the README; sorry if I wasn't clear enough. 
So, am I correct in understanding that the essence of the material I read was correct, in that returning structures directly is superior to boxing them, provided that there is no reason to box them? It isn't a performance issue to pass and return a struct rather than a box or even a reference to a box? I'm not necessarily saying that I'm going to do anything in any particular pattern, I'm just trying to get a better understanding of Rust. Intuitively I would think that passing structs would be a performance issue, but from what I've read it's actually suggested to pass and return structures over boxing things in a function and then returning the box. Of course references are another option as well, as RustDragon mentioned. 
Ah right, forgot about that... the only problem is I can't figure out how to actually compile an example like that. It seems like it might be only theoretical without changing hyper (and any hyper dependencies that use futures) to using this crate.
I was also wondering about this, but I think Rust only identifies symbols using their full name, without checking their origin crate. Additionally, there are plenty of options on Cargo, and I'm pretty sure you can override dependencies of dependencies with your own. 
Only box if you need to. If the caller wants it in a box, they can box it themselves. If they don't want it in a box, they can just use the return value. 
But the crate has a different name (`futures-await` vs `futures`) so the `extern crate futures` will fail if I just tell Cargo to replace them... though maybe I can rename the crate globally somehow with Cargo.
[Actually, it does look very easy to do.](http://doc.crates.io/specifying-dependencies.html#testing-a-bugfix)
Even if futures-await didn't it's not clear to me how to deal with the crate having the wrong name (short of just changing it's name by cloning it and editing its Cargo.toml)
When you pass and return a struct by value, the generated code is not going to end up copying the struct into the function and then copying it back out (probably). See this discussion: https://users.rust-lang.org/t/can-i-trust-rust-to-optimize-move-semantics/1137/3
The consensus in the community seems to be to just rely on the optimizer dealing with performance issues related to argument passing unless they start actually causing issues. The optimizer is supposed to be able to do a pretty good job at it. The choice between reference/structure just has to do with flexibility. Taking an argument by reference lets you call the function in more places (since you only need a reference to call it). Taking an argument by value lets you do more things to the value (since you own it). So always prefer to take a reference over a value when the interior of the function allows you to. (Exception: Immutable references to `Copy` types have no advantages, so I usually take them by value when given the choice)
Okay, thanks to everyone who responded to this thread you cleared things up for me. 
Hi! I'm think my [program](https://github.com/sighol/simplewiki) is leaking memory somehow. When I look at the memory usage in htop on linux or in the task manager on windows, I can see that the used memory increases with every request. How do you find such leaks? I don't use any unsafe code, and everything is rocket.rs behind the scenes, so I'm not sure where to look. I've tried `valgrind --leak-check=yes` but it crashes on startup. 
I've been here a while and I have to say the same thing: thank you community!
Sure, unsafe code is never fundamentally necessary, but it is arguably necessary under some common design constraints.
It's dangerous to write something that's intended as psuedo as compiling or almost compiling software. Considering the current state of software industry it'll end up as enterprise and then later legacy software running on an uncountable number of devices. ;)
Just a random note because i didn't knew this was possible as of a few weeks ago you could write this fn bar(baz: Box&lt;Foo&gt;) -&gt; Foo { // process *baz }
&gt; Does this effectively mean that there will be absolutely no progress on TFS for the next year? Or would you perhaps still review and accept contributions / pull requests, even though you don't have the time to do any actual development yourself? If yes, I might consider writing some code for it ... So chances are I won't actively look through my Github notifications, but if you PM me here on Reddit with a link, I will review them, and be very happy to see contributions.
Just a random note, you could write this fn bar(baz: Box&lt;Foo&gt;) -&gt; Foo { // process *baz } as fn bar(**box** baz: Box&lt;Foo&gt;) -&gt; Foo { // process baz } in case you have the experimental box syntax enabled I've found this one as of a few weeks ago, was surprised that this worked!
This is the Rust programming language subreddit. Try r/playrust .
I just tried out the given example (minus the bugs related to the outdated `Hyper` docs), and it compiles fine with `extern crate futures_await as futures;`.
You can't use those structs by value, only as a raw pointer and passed to C functions. It's the fallback if the code generator doesn't know how to fill the strict with proper typed fields, e.g. because of C unions or bitfields As such it doesn't matter how big the field is, or if accessing it is defined behaviour. You just don't do this as it's simply wrong (as dereferencing any pointer with a wrong/incompatible type) and will cause memory safety issues because this is not the actual representation of the struct in C (from that field onwards, up to that field it is). Hope that helps :)
Someone will have to do the work, and it's going to be quite some effort. It's not magically going to happen :) Rewrite gnome-shell in Rust, make it feature equivalent, better and ideally possible to load most JS extensions, then this could happen IIRC gnome-shell is the only major GNOME part that actually is JS
&gt; I'm not sure if that degree can be meaningfully established. Even by an end-user? I don't think anyone is suggesting there be universal agreement about which crates are trustworthy and which are not (though libcore will probably have to be trusted by everyone). E.g., I think I can meaningfully establish that i trust libcore and a handful of other crates (e.g. i've audited them, or maybe I know something about their development process and rigor) but I don't trust dozens of others I know nothing about but should have no problem letting extensions to my system/app use.
&gt; though libcore will probably have to be trusted by everyone `libstd` as well, btw. the agrument to try to avoid libstd _when possible_ is orthogonal.
Oh btw, I worked on making the class struct available from the bindings. To expose class methods and be able to later hook into thstvfor overriding virtual methods when doing subclassing from Rust code. Inheritance is already supported and someone else did this :) that might be misunderstood in the blog post Also I worked quite a bit on improving the GStreamer bindings: https://github.com/sdroege/gstreamer-rs Bug fixing, API cleanup, example cleanup and especially made the usage of integers in the API more type-safe
Oh... cool! How? Doesn't client.get() return a different type of future than the futures_await future? 
Looks like Rust doesn't care about that, unlike say, C#, which tracks the origin of every symbol and ensures they come from a specific assembly at a specific version.
At least Thales has a C compiler certified up to SIL4 Note that SIL4 certification removes the ability to use pointers, loops,... therefore the amount of code in the compiler to be certified is pretty limited.
Is there a way to include code samples of other languages than rust in the documentation? I'd want to put a sampl of viml into the module level documentation, but using three backticks means `cargo doc` will try to run it. I could not get the `ignore` directive (as mentioned in the first book) to run... Secondly, "somewhat" related... is there a documentation of cargo doc? I'm having a hard time finding more information about all the features I might want to use.
Why would you be benchmarking integer comparison operations unless you have some profiled code which identified that as a bottleneck? And if you have said code, why bother to write anything else for the perf benchmark?
You're essentially coming at the same point as Raymon Chen from a different angle. Context matters! 
Rust definitely does, it caused lots of trouble when libc changed versions. Maybe what is happening is that `futures-await` re-exports the `futures` type (which would explain the dependency), rust is smart enough to understand that a re-exported type is the same as the original type. But if all it is doing is re-exporting why is it necessary to replace `futures` with it at all?
Rather than packing structures into arrays, how is the performance of Struct of Array or Array of Struct of Array (the sub-arrays being sized to the vectornoperation size)?
Rust is not the language I would choose for desktop extensions. Lua or Nim seems like better choices.
No, that's not completely sound. `RefMut&lt;&amp;'static Foo&gt;` derefs to `&amp;'ref mut &amp;'static Foo`. Safe code may then call `std::mem::replace::&lt;&amp;'static Foo&gt;` which returns the `&amp;'static Foo` without any lifetime bound. 
IIRC the author explains this well somewhere, but I'm unable to look this up right now. 
I'm currently focusing the development on the Facebook Messenger. But I want to support multiple platforms in the future like Slack, ...
So, to be clear, you used a doc comment like this: ///```ignore ///derp derp derp ///``` And still had an error? If so, I have no idea why that's not working. Bug in rustdoc perhaps? &gt;`cargo doc` will try to run it `cargo doc` does not run code in docs, as far as I know. `cargo test` *does* run code in docs, and when I have the above doc comment, `cargo test` outputs this: Doc-tests elasticlunr running 1 test test src\lib.rs - (line 11) ... ignored
When I try to match with a statically declared str static MEEP : &amp;'static str = "MEEP"; let x = String::from("MEEP"); match x.as_ref() { MEEP =&gt; println!("MEEP found"), _ =&gt; println!("NO") } I get compile error: "match bindings cannot shadow statics". So effective I have to use string literals in a match? I canÂ´t use "constants"? That doesnÂ´t make sense, does it? Is it a wart of is there a deeper reason for this behaviour? 
"desktop extensions" in general are weird. I have no idea why you need a special extension runtime. Most other systems just have a some tool that you can use to interface with by fork/execing it. "extensions" in Fluxbox are written in whichever language you want; you just fork/exec `fluxbox-remote` with arguments to interface with it. Extensions on GNOME honestly reek more of a way to limit what exactly can be modified than a way to allow modifications with all the wars on mailing list going on with people debating endlessly how much exactly they feel should be modifiable by extensions because they feel it's bad for marketing if too much can be modified.
If massif works, it might be helpful: http://valgrind.org/docs/manual/ms-manual.html I've used it on Rust code before.
Another option is to use a profiling memory allocator. jemalloc (the default Rust allocator) has a configurable debug mode, which can be enabled...somehow. Another option is to use https://github.com/redox-os/ralloc with the `log` feature.
Exactly like you said. Maybe it didn't try to run the code, only compile it? By now, I'm using `text` instead of `ignore`, which seem to do it, but I figured maybe there'd be syntax highlighting available.
&gt; It isn't a performance issue to pass and return a struct rather than a box, or even passing a mutable reference to a struct One more thing that nobody else covered, there should be absolutely zero difference between passing a Box&lt;T&gt; and passing a &amp;T, since they're both just pointers, with different compiler-enforced rules on how they an be used
What does a "c program without loop" do ? Does it executes only once ? Or is there a hardware that execute it on an event ? I'm curious. Does it support branches ?
I don't remember the exact reason now, but you can use `const`s instead `static`s when pattern matching (just change `static` to `const` and it will work).
I like the idea of extensions _if_ the DE can sandbox them and limit their resource usage. Turns out gnome can't prevent the extensions from going haywire so that's not happening here but I do like the concept of a central desktop extension distribution, which gnome has. OTOH I never felt the need for a desktop extension on OSX to begin with because their default desktop is good enough. So I'm a bit conflicted.
You can always run whatever script calls a remote tool in a sandbox and run it under `nice` or whatever else to limit its resources. This is probably the part where someone namedrops "Unix philosophy" and how you shouldn't be monolithically re-inventing tools that already exist.
&gt; match Worked like a charm. Thanks!
Oh cool, thanks for the explanation!
It's basically https://github.com/dtolnay/semver-trick , but without any breaking changes at all.
Hi! I won't be able to look at your code in detail at the moment, but for implementing graphs, you should check out these articles: * https://github.com/nrc/r4cppp/blob/master/graphs/README.md * https://rust-leipzig.github.io/architecture/2016/12/20/idiomatic-trees-in-rust/ * http://smallcultfollowing.com/babysteps/blog/2015/04/06/modeling-graphs-in-rust-using-vector-indices/ The unique constraints of Rust's ownership and lifetime rules make these solutions a bit unobvious, but it is definitely possible to model graphs in Rust! Hope these help you at least a little. :)
If you write a blog post about it, I'll link to it! :)
&gt; which just re-exports the types in std Just that is horrible.
They *could* prevent them from going haywire, but they decided to implement extensions via monkey-patching rather than actual well-defined extension points with a coherent model.
Unless you return the `Box&lt;T&gt;` the `Box&lt;T&gt;` will call a destructor when it goes out of scope.... that's a pretty big difference.
Was some weekend hacking after I discovered the [cargo_metadata](https://github.com/oli-obk/cargo_metadata) crate and wanted to play with it. It is terribly inefficient with all the clones everywhere, but the task is generally small enough that it doesn't matter. It is super easy to use the `cargo_metadata` crate so if you are thinking about building tooling and are worried about the build times associated with linking to `cargo` definitely give `cargo_metadata` a shot.
Is there any place to track the Bazel investigation? I've seen bugs related to Tup (which I'd never heard of before), and have been watching with delight as `make` is gradually removed.
Have you tried rewriting your loop into mapping over the iterator? That seems both more idiomatic and like it might help you with the borrow checker. Do you have a link to your code? I can't run the borrow checker in my head yet so I could provide better help if I could actually play around with it.
Worked nicely! I was somewhat irritated by the different sizes of the circles, but that seem to be a graphviz issue?
Well, there are attributes that could be set on the shapes to try to keep the sizes more normal. And I should probably add color to the ones that are part of the workspace since just making them squares doesn't make it easy to notice them. Maybe if I revisit this, just thought it'd be a fun thing to hack on at a cafe :)
FYI, this PR fixes cargo graph support for workspaces, not sure why it's not merged yet: https://github.com/kbknapp/cargo-graph/pull/38
Probably because it doesn't fix the problem. The source of the problem is looking for `root` in `Cargo.lock` which doesn't exist in workspace projects. So while it can get the name of a project correctly, it doesn't handle the missing `root` attribute and still doesn't work.
Certain obvious patterns in other languages don't fit well with safe Rust. Typically there's a safe back door (often Cell + Rc), and hopefully a equally performant alternative design, then there's of course falling back to unsafe. I'm mainly interested just in case I run into a similar problem at some point. From a performance perspective, the cost of Cell is probably the extra size of each object. Of course guesses on performance should be always verified with profiling. A predicated branch is nearly free on modern x86. And the Cell checks should always take the same path. So as long as the cell borrow check can stay monitored by the CPUs branch predictor in your inner loop the the branch should be very cheap. So assuming the cost is mostly memory I can think of two ways to reduce that. If the items in the vector are references/box's (any thing NonZero), then wrapping them in an Option would add no memory overhead and the mem swap/replace trick, should compile just be an easily predicated branch and one or two pointer copies. Alternatively it might be possible to build a structure optimized for a vector of cells (which internally would use unsafe). Where the borrowing of cells is marked in a separate bit mask. That bit mask being so dense is likely to burden the cache less than the overhead of cell. The main exception is if the vector is huge (such that the bit mask is a bit larger than the cache itself) and randomly accessed. Also following that same line of thinking, the cache savings of a 32bit index vs a 64bit reference, may more than compensate for the always passing bounds check (which can be removed with unsafe if it's performance critical and well tested). I think that would be worth experimenting with. I'm not super familiar with ECS systems but I've heard that they solves similar issues while improving performance in games. Last I checked [specs](https://slide-rs.github.io/specs-website/) was considered the fastest for most things, and it provides some easy parallelism. It would likely have some large changes on your code structure but if it fits it might solve some other areas of friction with Rust while boosting performance with easier multicore support.
Adding to that, the rust compiler has [some issues](https://github.com/rust-lang/rust/issues/41160) at the moment with evading extra stack copies of structs have any complexity, especially with `Box::new`, so using a mutable reference as suggested (or if that isn't possible, box::clone()) may be the better choice even if it's not as clean.
As another general comment, you may be able to implement this by adding a function that takes a closure argument which the graph can then apply to itself. Something like fn do_thing_to_node(&amp;mut self, FnMut(Node)) [This is basically what rustc does](https://github.com/rust-lang/rust/blob/master/src/librustc_lint/builtin.rs#L144).
Wrong subreddit, post on /r/playrust for the Rust game. /r/rust is for the Rust programming language.
Thank you 
If you're taking it as a box, you should return it as a box. Other commenters have suggest you should in fact return it unboxed, but I disagree. You shouldn't *either* box or unbox it without a good reason to do that. However, you should also question why you're taking it an argument as a box, and only do *that* if you have a good reason.
&gt; The consensus in the community seems to be to just rely on the optimizer dealing with performance issues related to argument passing unless they start actually causing issues. The optimizer is supposed to be able to do a pretty good job at it. For what its worth, I've talked to several people who were profiling code and had a very different experience - the optimizer is not actually that good at figuring out when it should pass by reference instead of memcpying. You shouldn't box without a good reason, but I'm pretty sure the community consensus is wrong here. (Of course most people don't profile their code because its just Fast Enough, but this isn't because mecpying isn't a problem; most code would be Fast Enough if we heap allocated values over a certain size also.)
Now we got Numba, Cython and Numpy results for comparison https://github.com/rochacbruno/rust-python-example#new-results
You may have misunderstood because "in" is ambiguous, the types are defined in std and re-exported in failure. ("the types in std" are re-exported).
Does anyone know where to find the documentation for the most recent version of [`libtest`](https://github.com/rust-lang/rust/tree/master/src/libtest)? I've found a number of resources but they all seem to be for older versions.
I was just wondering if Mozilla is now or will in the future get direct revenue from their investment in Rust? Coming from the Scala world, the company that funds most of the development in the Scala compiler(s) is able to turn around and make a profitable business out of their expertise by offering paid consultancy services, support contracts, and even small bits of proprietary software like monitoring solutions that fit well with the types of applications that a lot of Scala shops develop. 
/u/bitemyapp did a stream not too long ago about using Rust with GTK, afaik he was starting with little/no prior knowledge about the GTK bindings, so it might be a decent resource to look through if you're interested in that sort of thing. https://www.youtube.com/watch?v=stJOwqiG5Do
I would say do not ever try to "reason" about performance if you can't prove and quantify the difference. If you can't quantify how much something will be faster then it's pretty useless anyway to even think about it. Modern optimizing compilers really destroy a lot of the reasoning you think you can make about performanceâthe only way to find out which is faster is to test both. Empiricism wins over rationalism here.
Everything that panics must already guarantee that it's _memory safe_ to use in the middle of a panic - the only thing that could be broken is expected behavior.
You might be looking for r/playrust. This subreddit is about the rust programming language.
Your problem is that you are trying to return mutable references to multiple possibly objects. Try returning the IDs.
*A relevant comment in this thread was deleted. You can read it below.* ---- correct, I'm familiar with TockOS, and my point is 100% accurate. TockOS is on a microcontroller, and it must use unsafe in some form to access the peripherals. I'm not claiming that applications in the "userspace" of TockOS would need to use unsafe to access those peripherals. [[Continued...]](https://www.resavr.com/comment/crates-io-ecosystem-not-9857466) ---- *^The ^username ^of ^the ^original ^author ^has ^been ^hidden ^for ^their ^own ^privacy. ^If ^you ^are ^the ^original ^author ^of ^this ^comment ^and ^want ^it ^removed, ^please [^[Send ^this ^PM]](http://np.reddit.com/message/compose?to=resavr_bot&amp;subject=remove&amp;message=9857466)*
Technically I'd used the GTK code examples in Haskell a couple times and I once did some GTK+C+GLib 10 years ago.
I would guess that using `Future` through the new crate isn't strictly necessary. But it is a nice way of doing things, because you need to `use` the new crate anyway (for the macros).
&gt; Extensions on GNOME honestly reek more of a way to limit what exactly can be modified I'm guessing you've never written a GNOME Shell extension then? The entire desktop can be hooked into and modified. The flexibility has always been more than what browser extensions allow. This was only possible because the Shell is written in JS which allows runtime monkey-patching.
This is a subreddit for rust, the programming language. The subreddit for rust, the game is /r/playrust
Actually, [Rust](https://www.rust-lang.org/en-US/) was created so as to stop you from shooting yourself in the foot, so your request is unfortunately denied. Maybe you are looking for https://www.reddit.com/r/playrust/
&gt; Even by an end-user? I don't think anyone is suggesting there be universal agreement about which crates are trustworthy and which are not (though libcore will probably have to be trusted by everyone). What I meant is that you cannot force a level of trust on user. Pre-deciding what "high", "mid", "low" or whatever is, is contextual. Yes, users can establish if they trust or not, that's why I recommend improving auditing tools to make it easy for them.
oh im sorry does this happen often
Even if there is no direct revenue, with project Quantum, Rust has already played a vital part in making Firefox relevant again, after a period of being an also-ran with many people switching to Chrome because of its superior performance. This is (not alone, but still) quite literally saving Mozilla's core business, so I don't think any $s coming in from external projects could have been comparably valuable to them.
But even still, seems that they are leaving a lot of money on the table of they don't offer consulting and support contacts don't you think? 
This subreddit is for discussing the Rust programming language. You probably want to post that to /r/playrust instead.
Is Rust like PUBG or something like that?
So before we steamroll this through (and finally get this off my open PRs list `&gt;_&gt;`), i would like to draw attention to the [âshow in docs when a functionâs return value implements Iterator/Read/Writeâ](https://github.com/rust-lang/rust/pull/45039#issuecomment-345099229) one. That link goes to a comment in that thread where i posted a few screenshots of the latest design. This is technically for a feature request, but i would like to gauge community feedback since previous iterations of the design were met with confusion. The newest design from /u/imperioland makes the net impact to the flow of docs rather small, but it still feels like a big change to be making.
cheers mate
no it's like a survival game mate but there are different game modes you could play such as battle royal etc.
Thank you so much for this, It gave me a much clearer picture of how to use this crate in order to accomplish what I set out to do! I really appreciate it!
You should maybe send a PR so it'll be in the lastest news part. I think it'd be better or would draw more people's attention.
[Petgraph](https://crates.io/crates/petgraph) is the best maintained graph library for rust AFAIK. And it has similar methods, so check out their source. I believe the 'rust way' to do this would be to return an iterator 
Author here. Would love to get some feedback and/or issue/bug reports in the github issue tracker! The parser interface is not yet documented, so one has to read the source to understand the syntax... but I wanted to get this out before investing into a nice documentation, so you all can review my stuff and tear my code appart! &lt;3
Uhm, this is awkward :) Seems like I haven't looked hard enough for an existing command, and inadvertently replicated part of the functionality of `cargo clone`. Should've looked at [this page](https://github.com/rust-lang/cargo/wiki/Third-party-cargo-subcommands) first! Oh well. It was a nice exercise either way :)
/r/playrust
Not only is this the wrong subreddit, I'm pretty sure this violates the code of conduct: &gt; We strive to treat others with respect, patience, kindness, and empathy. 
Pro tip: include a description of what this is in title or your comment! &gt; Calculate times with chrono "plain text like" in Rust.
Thanks for that. I'm going to install [SoftHSM](https://www.opendnssec.org/softhsm/) to see if I can play with PKCS#11 while I wait for delivery of the Nitrokey HSM.
Reminds me of python dateutil rrule. Very nice. I would just say that many date libraries fall down on parsing time deltas such as 10Y or 18M which are common in some areas of finance 
C and C++ have set in their standard that the size of bool is implementation defined. Usually the size is one byte, but it doesn't have to be. Also notice that a byte doesn't have to be 8 bit in C, although you'd be hard pressed to find an architecture nowadays where that isn't the case.
Have a look at the tests in timetype.rs! As far as my tests go, these things should work.
I'm so grateful for this because it plays nicely with intellij, unlike error_chain.
Yes, exactly! We can't a-priori know what is trustworthy for everyone, so we need to give people tools to determine and enforce what's trustworthy for them. The original pre-RFC (though flawed in several ways, hence a "pre-" RFC in the first place) suggested exactly that. Maybe slightly different tools than what you're imagining, but basically a way to figure out which dependencies are using `unsafe` and a way to whitelist specific dependencies when compiling your own binary. That's all! There are various ways to achieve similar goals, that was just one suggestion to try to get the ball rolling. Somehow it seems folks are getting the impression we're suggesting to disallow the use of `unsafe` by anyone except Graydon Hoar himself. We're _really_ just suggesting more tooling and, yes, advocating a slight cultural shift that leans more towards avoiding `unsafe` in the ecosystem (but not daemonizing it... we don't think you anyone who uses `unsafe` is a criminal... in fact it truely _is_ sometimes necessary... we use `unsafe`!).
I do like reading these hackfest reports, but I wish all three of you hadn't picked the exact same title. :P
Naming things is one of the most difficult thing in computer science.
I have to agree.
I have tried to use generators together with `future-await`. But I ran into [a bug](https://github.com/rust-lang/rust/issues/45259) in the generators impleentation. Now my hobby project is stalled until generators are fixed :(
The linker can't find libx11. Do you have it installed? Linking to C libs is always a bit rough; but if you're on an Ubuntu system you can `apt install libx11-dev`.
I would have liked to have had this kind of abstraction last week when I was writing some time calculations at work. This is real neat. It reminds me of Moment.js
You are so right about that. Apparently there are three major problems in computer science * Naming Things * off by one errors
I think you are asking the wrong question, or at least asking it in the wrong way. There is basically never a right answer to the question 'is X or Y better for performance?'. The answer nearly always depends on the context and is often surprising. In this case, to answer the question the important missing question is are you righting an application or a library? If it's an application, you should not ask the question at all and use a profiler. If it's a library, then the answer depends on a lot of things, but primarily how the function is used and how big the returned data are. In general, the answer here should be 'it doesn't matter' unless you are writing a perf-sensitive library, or this is in a *very* hot loop (or possibly a large memory-bound or CPU-bound application), one allocation either way won't make much difference. For comparison, languages like Java and C# use boxes for *everything* and they are fast enough for most things. So, the better question to ask is 'what are the trade-offs between moving data, and allocating on the heap and passing a pointer?'
Out of curiosity, has there been any progress on gvariant bindings?
I only managed to be there for one afternoon, but managed to get a couple of fixes into glib-futures. As someone that never touched a gnome codebase before, I must say that it was surprisingly accessible and people were very kind to help!
So I'm currently have [this](https://github.com/mmstick/gtkrs-tutorials) put together. Hosted via GitHub Pages [here](https://mmstick.github.io/gtkrs-tutorials/).
Rust has a protobuf implementation. Also capnproto
There are ofcourse 4 but you probably forgot to forget to include cache invalidation ð¬
Still in progress. I think that /u/sdroege_ is the one working on it?
yes, i always forget about the cache!
Oh damn. That's impressive! O.o That's already an excellent start. However, it'll all be required to be on the gtk-rs repository directly. However, as I told before, it's not a hurry since I prefer to wait for next release before starting (breaking changes incoming).
Primarily I just wanted to determine if it is still considered an anti pattern to return boxes. Early official Rust documentation seemed to take a strong stance that it was in fact an anti pattern, which indicated to me that it should be almost always avoided, but subsequent documentation made no mention of this at all. 
Sorry for the late reply, I wasn't paying attention to my Reddit mailbox. &gt; For me the biggest bug is that the tool output is not unique.... This is surprising (and a bit worrying) we work really hard to make sure rustfmt is idempotent and that we don't depend on the original formatting in the file. In fact we have an entire test run devoted to enforcing this. If you have examples, please file issues - we treat these as bugs. Caveat: we currently do not format comments or string literals (in general there is too much 'semantic' formatting for us to do a good job), so formatting from the source will be left alone there. Formatting should still be idempotent, but different source files might cause different output. &gt; formatting matrices Yeah, this is a big problem and one we hear about a lot. It's been difficult for Rustfmt to detect this kind of thing. It's something we'll try and improve some how post-1.0 (perhaps by allowing the user to hint formatting to rustfmt).
This looks really nice. I don't have a reason to use it right now, but I'm going to follow this. The api looks very convenient. Thanks for sharing!
I'm not, but GVariant bindings are already there? What's missing? :) https://docs.rs/glib/0.3.1/glib/variant/index.html
So much to do, so little time :) I'll write one with everything for the next release of the bindings, which will happen once the gtk release is out.
I'll be continuing to work on this for the next week or so, refining each chapter and attempting to adhere to [UDL](http://www.udlcenter.org/aboutudl/whatisudl) as much as possible. If webkit2gtk gets included in the next stable release, I'll also add a section for demonstrating that alongside the horrorshow HTML templating crate.
As a note, there's a good change round-trip latency will be the bottleneck. As long as your message is small and fits in a TCP packet, its actual size doesn't matter so much. What *may* matter is encoding/decoding speed, for which JSON is not the fastest.
Crate frunk has something similar, it's generic and labelled generic functionality. They also have derives. https://docs.rs/frunk_core/0.0.21/frunk_core/labelled/index.html It sure sounds convenient though.
Yeah, I would say that calling it was an anti-pattern was a mistake and that has been corrected by no longer calling it an anti-pattern. You still shouldn't box stuff 'by default', but I think it is much more subtle than a right way/wrong way.
I am not familiar with all of these APIs but I will give you my best! First, instead of: writeln!(io::stderr(), "Network interface name not supplied!").unwrap(); It may be more concise to use: eprintln!("Network interface name not supplied!"); I also noticed the `tx` variable is unused, and is likely giving a compiler warning which could be silenced with `_`. But maybe you are planning to use it in the future. My last code review comment would be that the code mixes `unwrap` and `expect`, and one of the expectations has an unhelpful `???` message. But this confusion maybe is more due to the community itself being inconsistent in its style. Overall, it looks like you are definitely grasping the language concepts. Good work!
Damn! I should have checked beforehand. Sorry for the useless ping...
I need to ask for the PRs remaining how much time remains before they're complete...
The more humorous way I habe heard it being put was: There are two things difficult in computer science: Caching, Naming and off-by-one errors. 
Thank you! Yeah, some of the ambiguous/unhelpful output (`???`) is more me just being lazy and quickly iterating through the code. I'll definitely take your advice and put something helpful in there though. I like the `eprintln` macro! That is _way_ more concise! My ultimate goal with this little project is to turn it into a Rust version of `Little Snitch`, or the Linux variant, `OpenSnitch`.
Should https://github.com/gtk-rs/glib/issues/113 be closed?
That seems really nice. I would maybe expect the â to be closer to the return type (inside the code block?) or maybe for the hover block to say something about the fact that it's referring to the return type, not the function itself -- "Important traits available on the return type"? I, personally, have to put a lot of things together when looking at the images to realize that the modal is referring to the return type. Something else that might be cooler/harder to implement would be if the modal was directly overlaid over the return type. So where right now [one](https://user-images.githubusercontent.com/5217170/32921794-a0a056b0-caf4-11e7-8043-b88f1be391de.png) says "Important traits for `Once&lt;T&gt;`" it's unclear where it pops up, but if that `Once&lt;T&gt;` didn't move from where it is in the type signature (so that the modal overlays the parameters) it would be more clear that it's referring exactly to that item, and would require less explanation. That said, people will figure it out with the current design, and it seems like a useful addition for the small amount of extra busy-ness it adds to the page.
Wait... what's the fourth?
Use cargo clippy. It automates a lot of code reviews already. Just let clippy run over the thing, it tells you about style issues. I mean I get loads of errors already with the default rust compiler. warning: src/main.rs:45: unused variable: `tx` note: src/main.rs:45: #[warn(unused_variables)] on by default note: src/main.rs:45: to avoid this warning, consider using `_tx` instead warning: src/main.rs:63: unused variable: `e` note: src/main.rs:63: #[warn(unused_variables)] on by default note: src/main.rs:63: to avoid this warning, consider using `_e` instead warning: src/main.rs:45: variable does not need to be mutable note: src/main.rs:45: #[warn(unused_mut)] on by default help: src/main.rs:45: remove this `mut` So the first thing to do is to remove unused variables to see what's wrong. clippy gave me the following errors: warning: This expression evaluates to the Unit type () --&gt; src/main.rs:53:21 | 53 | let iface_arg = match env::args().nth(1) { | _____________________^ 54 | | Some(i) =&gt; i, 55 | | None =&gt; { 56 | | writeln!(io::stderr(), "Network interface name not supplied!").unwrap(); 57 | | process::exit(1); 58 | | }, 59 | | }; | |_____^ | = note: #[warn(unit_expr)] on by default note: Consider removing the trailing semicolon --&gt; src/main.rs:57:13 | 57 | process::exit(1); | ^^^^^^^^^^^^^^^^^ = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.172/index.html#unit_expr warning: unused variable: `tx` --&gt; src/main.rs:78:10 | 78 | let (mut tx, mut rx) = match pnet_datalink::channel(&amp;interface, Default::default()) { | ^^^^^^ | = note: #[warn(unused_variables)] on by default = note: to avoid this warning, consider using `_tx` instead warning: unused variable: `e` --&gt; src/main.rs:96:17 | 96 | Err(e) =&gt; { | ^ | = note: to avoid this warning, consider using `_e` instead warning: variable does not need to be mutable --&gt; src/main.rs:78:10 | 78 | let (mut tx, mut rx) = match pnet_datalink::channel(&amp;interface, Default::default()) { | ---^^^ | | | help: remove this `mut` | = note: #[warn(unused_mut)] on by default warning: missing documentation for crate --&gt; src/main.rs:2:1 | 2 | / #![cfg_attr(feature="clippy", feature(plugin))] 3 | | #![cfg_attr(feature="clippy", plugin(clippy))] 4 | | 5 | | // Enable as many useful Rust and Clippy warnings as we can stand. We'd ... | 168 | | } 169 | | } | |_^ | note: lint level defined here --&gt; src/main.rs:10:9 | 10 | missing_docs, | ^^^^^^^^^^^^ warning: use of `writeln!(stderr(), ...).unwrap()`. Consider using `eprintln!` instead --&gt; src/main.rs:56:13 | 56 | writeln!(io::stderr(), "Network interface name not supplied!").unwrap(); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ | = note: #[warn(explicit_write)] on by default = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.172/index.html#explicit_write warning: called `filter(p).next()` on an `Iterator`. This is more succinctly expressed by calling `.find(p)` instead. --&gt; src/main.rs:62:21 | 62 | let interface = interfaces.into_iter().filter(interface_match).next().unwrap(); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ | = note: #[warn(filter_next)] on by default = note: replace `filter(interface_match).next()` with `find(interface_match)` = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.172/index.html#filter_next warning: you don't need to add `&amp;` to all patterns --&gt; src/main.rs:67:9 | 67 | / match ip { 68 | | &amp;IpNetwork::V4(a) =&gt; println!(" IPv4: {}", a.to_string()), 69 | | &amp;IpNetwork::V6(a) =&gt; println!(" IPv6: {}", a.to_string()), 70 | | } | |_________^ | = note: #[warn(match_ref_pats)] on by default = help: for further information visit https://rust-lang-nursery.github.io/rust-clippy/v0.0.172/index.html#match_ref_pats help: instead of prefixing all patterns with `&amp;`, you can dereference the expression | 67 | match *ip { .. } | I'll make a PR if you want, but please consider fixing the compiler errors first before you ask for a code review next.
I'm still pretty new to Rust myself, so perhaps this is sensible, but I notice let interface = interfaces.into_iter().filter(interface_match).next().unwrap(); and let mac_addr = interface.mac.map(|mac| mac.to_string()).expect("???"); Is there any reason to use map there rather than avoiding it? Perhaps simply trying to .unwrap() the Option&lt;MacAddr&gt; amd calling .to_string() on the result? I think .map() is primarily a functional alternative to loops, but it seems you are only using a single value with it, so I wonder if that is like writing a while loop with a single iteration. Keep in mind that I'm also new to Rust and that this comment is in no way authoritative, that is just the first thing I saw that struck me as odd. 
Are you looking for `extern crate futures_await as futures;`?
Internally, the enum variants are given discriminants numbered from 0. In the no branching case, your mapping looks like this with the types erased: 0 =&gt; 0 1 =&gt; 0 2 =&gt; 1 This is very easy for the optimiser to generate non-branching code for (in this case, `return s &lt;= 1`) In the branching case, the mapping looks like this: 0 =&gt; 0 1 =&gt; 2 2 =&gt; 0 In this case, the non-branching code would have to look something like `return (s &amp; 1) &lt;&lt; 1`. Either the optimiser decided this would be slower than a branch, or it simply wasn't able to deduce that expression. Why are you interested in branching vs non-branching code - is this for performance, or to prevent timing attacks?
I was wrong, but I was acting under the assumption that I would need to edit all my dependencies replacing `extern crate futures;` with that. Apparently that's not necessary though, just using it in the crates where you want to use await features is enough :)
It is already possible. A working example I quickly have at hand is https://github.com/rkusa/web-rs/blob/master/examples/await.rs (this uses a small wrapper around hyper, but there are probably also examples using plain hyper somewhere)
When we were putting it together, /u/imperioland said it would be impractical from a design standpoint to put it next to the return type, in case the length of the function signature makes the circle-i wrap but not the return type itself. (I think that was the reasoning, at least.) The most recent update (after the last screenshot) will make it say "Important traits for `Once&lt;T&gt;`" now, though. It's my hope that by explicitly calling out the name of the return type in the tooltip, the connection is a little more obvious. I'll let imperio comment on the viability of the second suggestion though. I think we can afford to pick it up in a later PR if it's worth putting it in.
Thanks! Just out of curiosity, why is it that I can run this without issue in my machine? I mean, I see warnings, but no _actual_ errors.
Thanks for that. This is for performance. I've just started learning rust since being impressed by the speed of Firefox quantum. Wanted to see what rust does with this kind of high level code. I heared structs can be rearranged by the compiler to alleviate padding, was wondering if something similar might be available for enums, but I guess enums are too low level since you can ask for the ordering. Anyway, pretty impressed so far!
The more humorous way I have heard it being put was: There are two things difficult in computer science ~~distributed systems~~: Caching, Naming and off-by-one errors, only-once delivery. ^(sorry /u/ebrythil)
I didn't it close to the return type for the same reason I move the anchor link to the left: it's nearly impossible to keep something good looking. However, I intend to modify the display of the functions and all so I might be able to move it to the returned type. But not right now. (Just like /u/QuietMisdreavus already said)
You missed one It's supposed to be There are two hard problems in computer science Naming things Cache invalidation Off by one errors
Yeah, warnings. Fix the warnings and listen to the warnings. The crate compiles just fine, but there are warnings. I've done a PR, check github. Otherwise, just add documentation, testing, CI + examples, put the thing into a library for reuse, let [clap](https://github.com/kbknapp/clap-rs) handle the command-line arguments. That would be it.
Saw the PR, thanks! One question about `clippy`: Based on the github repo I'm supposed to add the dependency to my `Cargo.toml` file (done) and then I run the linting with `cargo build --features "clippy"`, but when I do that I only see errors coming from the actual clippy library, not my own. I'll research on my own, but I thought I'd ask you as well.
The original post has this in the Cargo.toml: [features] unstable = ["clippy"] [dependencies] clippy = { version = "0.0.*", optional = true } I haven't put this in your Cargo.toml, you might have to put it there. Rust is probably just complaining because of an "unknown feature clippy". On the other hand, lots of these lints will probably not apply to your crate. 
You don't need to add it as a dependency. Simply install the cargo-clippy binary, and run `cargo clippy` in the terminal.
Oh that seems great!
That seems reasonable! I'm glad you're thinking about linking the tooltip to the return type, which is really what I wanted to call out, I shouldn't have jumped so fast into solutioneering.
Just use whatever is easier at first. Then optimize it when you have your protocols sorted out. Sometimes it's better to go as human readable as possible at first, so you can have an easier time testing your messages than trying to get the optimal solution from the get go.
If you allow users of your struct to access the values of the fields directly it becomes more difficult to design the struct in a robust way. This is because your users could put the values in _any_ state, and your struct must still work correctly in all of them. If instead you control access to the fields your struct only needs to work in the states your API allows, which is easier.
Even though Rust is not strictly an object-oriented language, the *encapsulation* buzzword still applies: to use the struct creation syntax you have to make your fields public and thus expose internal representation to the end-user which might not always be a good idea. 
In this specific instance? There isn't really one. In general though, your struct may have private members (those not marked as `pub`) which won't be accessible to users. You may also want to do some initialization upon creation, like for `File`. You can `open` or `create` a file, but it doesn't really make sense to just create a `File` struct by filling out the fields. &gt; I for one, really like the fact that names of the members are required _Speaking of, it would be nice if we had named arguments_
Thanks that makes sense. 
Thanks man.
Some good points here. Thanks! 
Sometimes, syntax really *is* all there is to it. However, there are three broad reasons one might create a âconstructor methodâ that go *beyond* just syntax: - The `struct` has more than one sensible way to instantiate it; for example, you could have a `Temperature` `struct` that can be directly instantiated with a number in Kelvins, but have constructor methods that convert from Celsius and Fahrenheit as well. - The `struct` has some sensible default values that are derivable by calculation from the non-default ones; for example, you could have a `struct` that implements fuzzy bounding boxes, and you might want to allow the user to provide only the inner bounds and derive the outer bounds as some percentage increase of them. It should be noted that if the default values are constants that *donât* depends on the non-default parameters, then you can just implement `Default` (whether by `derive` or manually) and then use the [`struct` update syntax](https://doc.rust-lang.org/stable/book/first-edition/structs.html#update-syntax) on `Default::default()`. - The `struct` might have additional requirements on its contents, or otherwise not want to make its composition public; then, the *only* way to instantiate the `struct` is to use a âconstructor method.â Of course, any twoâor even all threeâof the above could be simultaneously true of a given `struct`.
Either off-by one errors or exactly-once delivery, depending on the order you count in.
FWIW, I have the same toolchain as you (x64/MSVC/nightly), but I could not reproduce. My compiler version is: `rustc 1.23.0-nightly (5041b3bb3 2017-11-19)`. Which version of MSVC are you using?
Serde supports a lot of formats other than JSON. There is a [list of popular ones in its documentation](https://serde.rs/#data-formats). Many of them are efficient binary representations. However, there are two more issues that you will need to deal with. One is whether there is good JavaScript support for the encoding you are interested in; it does look like there are modules on npm for many of them, like MessagePack and CBOR, though I haven't ever tried it so don't know if it will be as convenient as JSON. Another is picking a protocol, with message framing and identifying types of messages, whether they are RPC or publish-subscribe or the like, on top of one of the above encodings. There are a number of such protocols already; for instance, there's [tarpc](https://github.com/google/tarpc) using bincode. There are also protocols like [cap'n proto](https://github.com/capnproto/capnproto-rust) which include serialization and deserialization though via their own protocol definition compiler and not Serde. There are also existing protocols that are designed for exactly this purpose, such as [WAMP](http://wamp-proto.org/), which supports either JSON or MessagePack over websockets, with both RPC and publish-subscribe. However, it seems to be somewhat over-engineered and complicated, and I don't know of a Rust implementation. So, there are a lot of choices out there, but I don't know of one that is easy to pick out of the box that supports everything you would want.
Funny clip, but sadly this is not the subreddit for the Rust game. r/playrust is the one you are looking for. This one is for the programming language Rust.
Visual Studio 2017. Whatever comes with that. Did you clone the repo I linked or recreate the theoretical offense? As I noted, I've been unable to reproduce the ICE locally with anything other than the project as it exists at the linked commit. That's what's thrown me for a loop.
Nah, just copied the code from the playground link into a fresh project. Will clone the repo when I get a chance.
Oh my god, I'm sorry
Of course this entire thing is sort of destroyed again by pattern matching and all, especially on enums. It's sort of a conundrum that if you want to export a nice matchable syntax you are forced to not encapsulate to the point that Rust just says that all enum variants are public anyway so in order to keep them private you need to use a newtype of some sorts. It'd be nice if you could easily define a nice stable interface to match on while keeping the actual implementation and data layout under the hood private of enums.
&gt; Speaking of, it would be nice if we had named arguments A lot of the "keyword argument" proposals seem to be centred on tackling keyword arguments and anonymous structs in one go. It would be nice is `Point:;new(x = 3, y = 5)` for instance was just syntactic sugar for `Point::new({x : 3, y : 5})` quite possibly even incorporating a way into that to have optional/default arguments.
As a side note, most of the performance gains of Quantum didn't come from Rust. Don't get me wrong, Rust had an impact, and for the parts that were rewritten, Rust caused major gains. But most of the gains came from good old fashioned profiling and optimization of existing code. Most of the code was C++ or JavaScript. Also turning on multiprocess (e10s) for everyone helped in some cases.
Yes, repro'd! 1) Clone repo 2) Checkout 177c5dbb 3) cd tokens 4) cargo test Result: ICE, ICE, baby. 
I have been working on something similar for awhile, specifically because interacting with diesel can require a large number of intermediate structs: https://github.com/quodlibetor/diesel-derive-intermediate I've been using it enough now that, even though I have some ideas for further features (mostly around generic contexts and attr propagation) I am thinking it might be good enough at this point to put up in crates.io. So it doesn't yet actually exist. Re: your questions: * Re: implementing methods, in went the route of generating names that are unlikely to be used because they refer to structs generated by my derive * Default does seem more reasonable for that use case * Why do you need all the fields to implement clone? It's not an overwhelming burden but it suggests that your users won't be able to decide when items are cloned, which is unfortunate
Can you put a struct with private fields inside the enum?
It's ok. We get this just about every day.
[Off by one errors](https://www.youtube.com/watch?v=qLk81XnkGUM)
Good to know I'm not crazy. Now I just wish I could create a manageable reproduction case. The fact that the linked playground which has _almost the exact same code_ (it's only modified to be one file) doesn't repro the ICE is what's driving me crazy.
It's way better than the official style which intersperses `#[macro_use]` with `extern crate` lines so you lose any visual indication of which crate it applies to. My personal style puts all the `#[macro_use]` imports first (alphabetical), and then the non-macro crates. At least the whole issue is going away soonish.
Sure, no need to hurry :) it's ready when it's ready, and e.g. the async methods would be nice to have
Coming from a C++ perspective, the important concept here is *invariants*. An invariant is something that is true for all the time an object is valid. For example the month field of a date class is only ever in [0, 13). This also means that not all values of the underlying struct are valid or there is no invariant. The invariant is closely coupled to *lifetime*. Basically the whole time the object is alive, the invariant holds (including changes to the object). The one responsible for checking changes or defining preconditions to the object is the author of the class. A user cannot be trusted to correctly setup the invariant. So the author provides functions which safely construct the object. In C++ this is modeled by having constructors and a destructor. The object is alive from the end of the constructor to the beginning of the destructor. If the constructor cannot bring the object to a valid state (think vector with failing allocation), it may throw. For Rust, a similar way of reasoning is necessary. The lifetime of an object begins not with all data members being initialized, but with the invariant holding. This delay must be encapsulated by a factory function / constructor which really sets up the object. For example if all vehicles of a vehicle class are to be registered in a global vector, you need a function to do that. This is the reasoning for the Rust factory functions. Destruction is easier so only one function is needed.
Actually not, what is missing right now is support for nested types at least. Only flat/basics types (ints, floats, strings) are currently supported but not putting them into arrays, tuples, dicts, etc.
In my code, constructors sets default value which is comfortable because my structs are big (struct of table of struct)
Rust is going places. I see students learning it in courses at my uni already.
clippy should not be listed as a dependency, just install with "cargo install clippy", then run on your project with "cargo clippy"
I'm writing a Merkle tree library with full support for both inclusion and consistency proofs. Hope to have it on crates.io eventually, just need to come up with a good name... The more obvious ones are taken ;)
Indeed!
The real question is why doesn't Rust provide a uniform initialization syntax that works whether or not the type has a constructor or is just an aggregate.
I'm working on something in Rust for work, which is awesome. Also, I want to get [this little WASM thing](https://github.com/killercup/wasm-experiments/pull/2) into shape.
No, Option type has method called "map", just like iterators. This method comes from functional programming and doesn't involve any loops. Basically it means "use value if it is Some and return None if it is None" 
A nice pattern I've used has been to return a facade enum with references to data in a complex struct or enum that I wish to encapsulate. impl ComplexStruct { fn as_simple_enum(&amp;self) -&gt; SimpleEnum { .. } } This situation permits for context-specific pattern matching without exposing internals.
I'm porting the [ENet](http://enet.bespin.org/) game networking library to Rust because I want to use it in a game.
Because not all types want to give users that ability?
In Haskell, where the same problem exists, people have come up with the idea of a View type alongside the data-type So you have say: -- Type is public, fields are private data DateTime = DateTime { timestamp :: int , timezone :: TZone } deriving Eq, Hash -- Type is public, fields are public data DateView = DateView { year :: Int , month :: Int , day :: Int } deriving Eq, Hash, Show new_date :: Int -&gt; Int -&gt; Int -&gt; DateTime new day month year = ... new_date_time :: Int -&gt; Int -&gt; Int -&gt; Int -&gt; Int -&gt; Int -&gt; Double -&gt; DateTime new_date_time d m y hh mm ss ms = ... year, month, day :: DateTime -&gt; Int ... as_view :: DateTime -&gt; DateView as_view dt = DateView (year dt) (month dt) (day dt) Then in a function you'd write is_leap_year :: DateTime -&gt; Boolean is_leap_year = go . as_view where go (DateView y _ _) = y % 4 == 0 &amp;&amp; y % 100 != 0
What about crates that use std by default, but allow to switch to libcore-only by using features? This is the first one I found by googling, but I know that there are a few more: https://github.com/tomprogrammer/rust-ascii
I've started working on an extremely niche [neovim plugin](https://github.com/KillTheMule/nvimpam).
&gt; The idea of this event is to progressively allow using more Rust for developing **Rust** softwares. GNOME software no ?
Updated to 1.3.2, now added readme markup view.
Do you mean something C++11-like where I can use syntax for a struct literal and it actually calls a constructor? That's too magic for Rust's tastes, in the same way that operator overloading and implicit casts are too magic for Rust's tastes. In particular, in C++11, this code calls malloc: void something(std::vector&lt;int&gt; v) { } int main() { something({3, 4, 5}); } Rust absolutely does not want secret behind-the-scenes mallocs.
&gt; **Future plan** &gt; &gt; [â¦] &gt; &gt; Another interesting feature to implement would be to generate the asynchronous methods to return a **`futures`** so that we can run futures on the **glib event loop with futures-glib**. I see what you did there. 
How would that be beneficial to the language in any way? The constructor is really just a function, there is no point in having special syntax for it, especially since rust is not an object oriented language.
Apart from encapsulation and invariants, which are good points others have made, there's also an argument about public vs. private API. If I expose the members of a struct directly to my users, then if I add another member, even if it's just a cache or something, that's a breaking API change for all my users. OpenSSL is a great real-world example of a library that made this mistake: a lot of the changes in OpenSSL 1.1 are [asking people to use accessor functions instead of directly manipulating struct fields](https://wiki.openssl.org/index.php/OpenSSL_1.1.0_Changes). That way, if a struct field gets added, the old constructor can just give it a reasonable default, and if a field goes away, the old accessor can just compute the value from what it has. Whether you choose to have public members depends a lot on whether you think your structure is long-term stable. For something named `Vector3`, yes, it's probably going to be a 3-element structure with fields named `x`, `y`, and `z` forever. For an SSL context, no, it's probably not interesting to directly construct it and it probably will have a lot of changes to support new SSL versions/features or optimizations.
What about `angela`? (A silly pun on Angela Merkel.)
You can overload operators in Rust: https://doc.rust-lang.org/core/cmp/trait.PartialOrd.html
I'm working on a GameBoy emulator
I'm working on a Discord bot responsible for people being able to claim hunting spots for themselves in a popular MMO game ;). 
Yes, but not a private struct.
Any projects where noobs can actively contribute?
Yes but operators still have interfaces that must be adhered to. C++ operator overloading can do anything with any operator. Look at std::filesystem, the ranges proposal, even stream operators are just overloaded bit shift operators.
Spent a few hours working on fixing tests in [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) in an effort to get integer storage types working. Currently trying to decide how to deal with tests that work with floating point numbers but don't for integers.
&gt; I can clear the cache and everything builds fine... once, after that the problem persists again. This does seem like a travis issue. Very odd.
That was my first thought as well, but since it only happens on beta on osx I thought maybe it's not.
Even if that's not really the spirit, you can do a lot of things with operator overloading in rust: `&lt;&lt;` overload a bit as in c++: https://play.rust-lang.org/?gist=57eb9f133cf4afc2a67523dc0efd0f0b&amp;version=stable
getting sozu to be more usable, and messing with wasm and canvas :)
`angela` is not strong on inclusion, though. But the politics are at least consistent.
And GHC includes a language extension, [ViewPatterns](https://ghc.haskell.org/trac/ghc/wiki/ViewPatterns), to streamline use of view types.
Well, this consumes your `Cout` though. And this works for `stdout` and `stderr` (due to `print!`), but it won't work for arbitrary streams (ergonomically) since you have to either capture the move at the end again or explicitly `.clone()` (and `Copy` won't be available for such things most of the time). And `File` only has `try_clone` since it can fail. Yes, possible, but I can't see anyone thinking that such overloading is remotely a good idea (not that I like C++ stream operators either, overload or notâ¦). The other issue with operator overloading is that you're stuck with the intended use case's precedence. Rust will help you with type errors, but this is valid C++: `cout &lt;&lt; foo &amp;&amp; bar` and is a boolean, not a stream.
Although, to be fair, I've never really seen it used...
Hah. But no, I think ;)
I'm writing a partially lockless concurrent stack implementation.
Yeah, seconded, and to refine it further, the goal was to make Rust "fearlessly concurrent," specifically for this reason, so that the code was safe and easier to write. Theoretically, you could achieve the same performance using C++ by just optimizing the hot spots. After all, we use the same backend as Clang (llvm).
Hm, this is strange. I tried to create a MVCE but I can't reproduce the problem there: https://github.com/saltyrtc/saltyrtc-client-rs/blob/develop/examples/wstest.rs Both versions block as they should...
Specifically, it avoids an if statement by making it implicit.
Is it https://github.com/rust-lang/cargo/issues/4699 ? I had the same issue, adding a `rm -rf target/*/{*,*/*}.dSYM` in the build script made the trick for me.
I'm super noob at programming in general (only competent at writing Solidity smart contracts). I got interested when Parity came out with their Rust implementation of the Ethereum client, and even more so recently with their Parity-Bitcoin client and their pwasm ERC-20 token contract in written in Rust deploying to WebAssembly. Seems really complex, so I'm taking it a turtle-pace approach in figuring out what the codes do
that's cool. I know two library for p2p in rust: * [crust](https://crates.io/crates/crust) exist for p2p but I don't if it suitable for games and its GPLv3/commercial * [libp2p](https://github.com/paritytech/rust-libp2p) I don't really know the specification but it sounds promising.
I have been working on a small collection of terminal utilities that help you move between directories and run commands faster. It's a work in progress, but you can find it [here.](https://github.com/pknight24/sgd)
Iterating on your last sentence: if you're taking something as a box, and want to return it, just take it as a mutable reference to begin with. No need to deal with boxes in this context. A box as an argument only makes sense (unless I'm not seeing another use-case) if you want to actually consume the object. If you're only mutating it and giving it back, that's what borrowing is for. And if you're returning *another* object of the same type, then you shouldn't box it, leave that up to the caller.
From [the docs for `send_all`](https://docs.rs/futures/0.1/futures/sink/trait.Sink.html#method.send_all) (i'm assuming `framed` is a `futures::Sink`): &gt; This future will drive the stream to keep producing items until it is exhausted, sending each item to the sink. It will complete once both the stream is exhausted, the sink has received all items, the sink has been flushed, and the sink has been closed. Note that it specifically says the future will only complete once the sink is closed. where the docs for `send` do not say that the sink will be closed. So presumably closing the sink like this causes the next `send_all` in the loop to error or panic (this behaviour is documented on `Sink::close`)
That very much looks like it, thanks! Also thanks for posting in the issue.
I'm working on a simple os written in rust as a long-term project. I'm currently implementing the global memory allocator. The project located at github.com/euclidOS.
Working on an [RTPS 2.0 server/client implementation](https://github.com/sgodwincs/rtsp-rs)
Can reproduce on `nightly-x86_64-unknown-linux-gnu` (`rustc 1.23.0-nightly (5041b3bb3 2017-11-19)`, `cargo 0.24.0-nightly (abd137ad1 2017-11-12)`): https://pastebin.com/UmhbsL68 Running with `RUST_BACKTRACE=1`: https://pastebin.com/w8FLxjc1 Running with `RUST_BACKTRACE=full`: https://pastebin.com/yuxpQBr7
Sure. There are many projects seeking contributions no matter how many Rust you know. Some even offer mentorship, so that you can have a mediated learning experience while benefitting the community. [This week in Rust](https://this-week-in-rust.org) has a list that you may use as a starting point, or you can state your interests here so that we can find projects that might interest you.
Thanks for not doing this.
In ocaml, there is the `private` keyword for exactly this. In short, you can export an interface module L : sig type list = private Nil | Cons of (int * list) val nil: list val cons: int -&gt; list -&gt; list end let list = cons 1 (cons 2 (cons 3 nil)) (* let list2 = Cons(1,Nil) Compilation error: Error: Cannot create values of the private type list *) let length list = match list with | Cons (_, list2) -&gt; 1 + length list2 | Nil -&gt; 0 which creates an enum with two constructors which can't be called but can be pattern matched on. What makes this work is that the constructor method is defined in the module that defines the enum type, which does not exist in rust. http://caml.inria.fr/pub/docs/manual-ocaml/extn.html#sec236 
`[T]` doesn't have a fixed size, so normally it couldn't be on the stack. The `&amp;` makes it shared. If you need an owned slice, you could use `Box&lt;[T]&gt;` (a `Vec` is nicer though). And `str` is just like `[u8]` with extra steps.
Thanks! I'm actually going for more of a dedicated server model to avoid most of the issues you would run into with P2P games.
str is an unsized type, which is ALMOST [char]. Therefore you can't use it as is, you use it with a &amp;. It ISN'T [char] though, because char is always 4 bytes long, while str allows each character to be 1-4 bytes long (because it's UTF-8 encoding). As an example, ['a', 'b', 'Ð´'] will take 12 bytes (4 for 'a', 4 for 'b', 4 for 'Ð´'), while "abÐ´" will take 4 bytes (1 for 'a', 1 for 'b', 2 for 'Ð´'). You can't get a simulation of this using a slice of a regular type, which is why it has a specialized type.
Doesn't it already work the way you say it should be for consistency?
&gt; The most recent macOS release fixed 32 vulnerabilities caused by memory unsafety. The most recent Google Chrome release fixed 10 vulnerabilities caused by memory unsafety (not including ones found by Google's internal fuzzing and auditing efforts). The most recent Firefox release fixed 38 vulnerabilities caused by memory unsafety. Google's OSS-Fuzz has found more than 750 memory unsafety vulnerabilities in popular open source projects. Google's Project Zero has found more than 1,000 vulnerabilities, many of them memory unsafety (70 of the 86 critical vulnerabilities are memory corruption).
That's awesome! Are you planning on using tokio as a base?
I have been playing around with [robigalia](https://gitlab.com/robigalia/) source code.
`[T]` and `str` are the types for the *actual* data. i.e.: These types are an unknown number of bytes long. You could have a statically known array or String somewhere in your program. That would have `[T]` or `str` as its type. Everywhere you use that data, you use `&amp;[T]` and `&amp;str` (array slice and string slice). These are fat pointers, because they need to save the length of what they point into somewhere.
Thanks for the explanation. Do you know why the `wstest` example linked above works as intended? I can't spot the difference between that and the code that closes the connection. By the way, `framed` has [this](http://cyderize.github.io/rust-websocket/doc/websocket/client/async/struct.Framed.html) type, it's both a stream and a sink.
Any ideas on how to go about reducing the search space for the ICE cause? So far all of my efforts to create a smaller reproduction case (see: linked playground) have not run into the ICE. The only case I've been able to build is the full project so far. I'll be attempting to reduce the size of the reproduction case via removing from the existing one rather than adding to a fresh project later, but progress isolating the issue has been a non-starter so far. Good to know that it repros on another tocharian though as that means it'll be easier to confirm once the minimal repro is created.
You can implement it for `&amp;mut Read` in `std`, but whatever, that's not the spirit of Rust. `cout &lt;&lt; foo &amp;&amp; bar` is more about implicit cast that Rust almost always forbid (except `.` sugar and `Deref` coercion).
Your post reminded of reading that &amp;str was a bit like &amp;[u8]. What would be the difference between the 2 then? Is the difference considerable enough to warrant a specialized type, when one could simply use an instance of &amp;[u8] ? To be pedantic, is it right to say that *&amp;str* is a reference to a slice, and not a slice in and of itself?
I understand [T] and *str* is the actual data. But just as you can [T] a "slice of T" and str "a slice of str", wouldn't you call &amp;[T] a "reference to a slice of T" and &amp;str a "reference to a slice of str" ?
Even though there are so many http clients in various stages of incompleteness written for rust, I still went out and started a new one. Not yet published [mio_httpc](https://github.com/SergejJurecko/mio_httpc). It is entirely async and runs on top of just mio. It is meant for those of us who prefer using just mio for our async networking needs. It uses the http crate for Request/Response types and tls-api so you can pick your own TLS backend. 
`str` must be valid UTF-8; `[u8]` doesn't.
Wow, the [full-time jobs](https://jobs.lever.co/boom/f0ae24b1-6992-4e64-9853-cd331237c821) there have some _serious_ requirements. I meet _most_ of those requirements, but I absolutely do not meet the 5+ years of avionics industry experience or the DO-178 guideline experience. Working for Boom seems like it would be awesome.
I always thought that `&amp;[T]` was the slice type and `[T]` was the unsized array type
Last week i picked [egg-mode](https://github.com/QuietMisdreavus/twitter-rs) (my twitter crate) back up so i could finish up media upload. Itâs coming along pretty well - iâve got chunked upload all wrapped up in a single `Future` type now that just manages the chunks by itself, so all you need to do is hand it the bytes and the MIME type and itâll run the rest. I think i want to tweak how the media IDs are represented in the public API, and i still need to get the metadata/alt text endpoint in there, but i may be able to get a new release with all this polished up and shipped this week. Next stop: Streams!
As others have said it, the Emscripten target requires an extra runtime to accompany the `.wasm` file it generates, and `stdweb` also requires that the code generated in that `.js` file is present. Once the Rust's newly merged pure wasm target (`wasm32-unknown-unknown`) is more usable (be at least available to download with `rustup`) I'll add support for it to `stdweb`.
You seem to be caught up on terminology. Honestly, I've seen them called lots of things in inconsistent ways. This is probably down to most languages just not having a concept equivalent to `[T]` and `str`. First, `str` is exactly the same thing as `[u8]`, except that the bytes must form a valid UTF-8 sequence. Secondly, `[T]` and `str` can't be used by themselves in most contexts, and are almost always used behind pointers to a subset of some *other* thing, so they're often called "slices", although they could also validly be called "array" and "string", since they are the actual data itself. Again, because you almost always have to use them behind a pointer, and `&amp;` is overwhelmingly the most common *kind* of pointer, `&amp;[T]` and `&amp;str` are also often just called "slices" despite, yes, more correctly being "pointer to (array|string) slice". And yes, they *can* be used on their own, both as type parameters (`Box&lt;str&gt;`, `Rc&lt;str&gt;`, *etc.*) and as part of user-defined dynamically sized types (that's a whole other kettle of fish that's complicated and nasty). Finally, "is slice just a fat pointer" depends on which thing commonly called "slice" you're talking about. If you're referring to `&amp;[T]` and `&amp;str`, then yes, those are fat pointers.
Also, to answer the second question you asked: I don't know the internals, but `&amp;str` is probably laid out the same as `&amp;[T]`. It just has different methods, because it is a string (all the unicode shenanigans has to be in there)
str is an [u8] in it's internal representation. However, it comes with the guarantee that it's data represents valid UTF-8 encoded text. All operations on a str will keep this guarantee. In addition, it works with it's data as though it's data is composed of chars. For example, "aÐ´b" is a string of length 3 (even though it takes 4 bytes in memory). Iterating over it will give you the 3 chars. An attempt to split it will not allow you to take only half of the 'Ð´' character. If you where to use [u8] directly, you would need to make sure you are keeping UTF-8 encoding valid manually. The length of the slice would be 4, which isn't the amount of characters you have. You can split the slice into two in the middle of a letter, causing the UTF-8 encoding to become invalid.
https://play.rust-lang.org/?gist=6ea15b21b6537dc235a5708bebdd80ce&amp;version=stable
Great description and it really addresses the question. This should be part of the Rust book to help with beginners understand the terminology.
This is depressing.
Really? I find it empowering and validating. This is our time! We're doing something useful for the larger world! We are the defense against these kinds of problems.
More compiler work. I can now compile ADT constructors and perform simple arithmetic. data Tuple a b = Tuple a b add a b = llvm_add_int64 (Tuple a b) (`llvm_add_int64` is a builtin function that does exactly what it says.) The next step is ADT destructuring and pattern matching. Once that's done, `Bool` can be changed to a user defined ADT rather than a builtin type, as well as being able to support things like `Maybe`/`Option`.
I'm working on my crate called [leechbar](https://crates.io/crates/leechbar) which should allow me (and in theory everyone) to easily write my own bar/panel/dock for X.Org. To give you an idea of what is possible, this is how my current setup looks: https://i.imgur.com/4brleKt.png Clicking through workspaces and dragging around the volume slider are what I got working yesterday. :)
This week I'll working on getting [xi](https://github.com/google/xi-editor) working on Fuchsia again (many things changed while I was on sabbatical). I've also been experimenting with [xi-win](https://github.com/google/xi-win) and plan on using [DirectComposition](https://msdn.microsoft.com/en-us/library/windows/desktop/hh437371(v=vs.85).aspx) to achieve butter-smooth scrolling and minimal latency. Expect to hear more about that soonish, I think I'm on to something.
Your last sentence is gold!!
Thanks for the great explanation!
The `lazy_static` crate might do it. Not sure if you can initialize things into it or modify it after initialization though. I should probably find out!
nah fam
The existence of these vulnerabilities and people still using C/C++ is depressing. It's great that Rust is working to fix these problems, but there's also the fact that every single programming language still used today other than C and C++ is memory safe and nothing has been done.
If you have specific places where this is off, please file some issues; we try to keep terms very consistent.
My take on it is this: I'm going to use Rust and make better software, and because it's better my software will overtake software written in these other languages as a virtue of the free market. Then when the companies using Rust are bigger than the companies not using it demand for Rust programmers will increase and obsolescence will happen for C/C++. Eventually C/C++ will lose their market value and people will stop learning it. Then someday we'll really be forced to stop using it when C/C++ goes full COBOL and the existing C/C++ skillset just dies from old age.
I thought to be quite fluent in rust, but why are the Box sizes different?
They are "sized" references to unsized collections, right?
Meeting most of the requirements is all that's required. If you are interested and you think you can do the job, apply even if you don't meet all of the requirements.
&gt; What does the &amp; in &amp;str, reference? The data (the `str`) is embedded in the binary (for a `&amp; 'static str`). This is why the lifetime is `'static` - as long as the program is running, the pointer to it (the reference) will always be valid. This is why you write: const SOME_DATA: &amp;str = "Hello"; The actual data for "Hello" is embedded in the binary. Whenever you use `SOME_DATA` in your binary, you are not using the "Hello" directly, just a pointer to it. 
Any reason this shouldn't work? (Where NonSendable is a type from an FFI library that contains a pointer field and does not implement Send nor Sync, and I want to access it across threads) struct Proxy { context: Arc&lt;Mutex&lt;NonSendable&gt;&gt;, }; // later... let proxy = Proxy { context: Arc::new(Mutex::new(non_sendable)) } // elsewhere ... let arc = proxy.conext.clone(); thread::spawn(move || { loop { let mut data = arc.lock().unwrap(); // do something to a field within NonSendable } }); I get an error on the spawn line saying that 'a_type_within_non_sendable cannot be shared between threads safely`. I thought that was the point of the Arc and the Mutex? Even though NonSendable does not implement Send or Sync (out of my control, its a library), can't the compiler see I am using a lock? 
&gt; but there's also the fact that every single programming language still used today other than C and C++ is memory safe and nothing has been done. But there are good reasons for that. It's just that Rust removes one more of those reasons (speed, no GC).
I looked (lightly) through the Box source, rustonomicon, reference, and RFCs and I couldn't find anywhere that specified the behavior of Box for ?Sized. But `box foo` is a builtin so it makes sense that it automagically becomes a fat pointer when `foo` is unsized.
I see now that Arc&lt;T&gt; is Send if T is Send, and Mutex&lt;T&gt; is send if T is send, so Arc&lt;Mutex&lt;T&gt;&gt; is Send if T is Send, but I have no Send, Jerry! How can I get Send!
It takes a lot more than just writing software in Rust for it to win in the market, and most of that is completely independent of programming language.
I'm looking to finish up last year's http://adventofcode.com problems. I have day 24 &amp; 25 left. I may try to find some time continuing to work on my game, further implementation of the tech tree. So looking at showing research duration upon researching, as well as the cost to research.
Sure, and I expect a lot of Rust software just won't take off, but some will.
Sometimes it's not that the terminology is off, it's just that there's not a circle drawn around an unstated convergence of facts and what they imply.
I should mention that I'm also disappointed that nothing like Rust has been used before. ATS provides many of the same guarantees and predates Rust but almost nobody knows about it.
&gt; ATS provides many of the same guarantees And much more! Except that ATS syntax isn't accessible, there is almost no documentation and nobody wants to write 10 lines of proofs for 2 lines of code. Pragmatism, practicality, accessibility, aesthetic and ease of use play a gigantic role. EDIT: We would all be writing ATS, Agda, Forth, Lisp or Haskell if these things weren't important.
The history of computing is littered with languages that wallow in obscurity because they don't consider the eco system of writing programs in that language.
 fn sub_bytes(input: &amp;mut [u8; 16]) { for i in 0..16 { input[i] = tables::SBOX[input[i] as usize]; } }
 fn shift_rows(input: &amp;mut [u8; 16]) { let mut temp: u8; for i in 1..4 { for _ in 0..i { temp = input[i]; input[i] = input[i+4]; input[i+4] = input[i+8]; input[i+8] = input[i+12]; input[i+12] = temp; } } }
while you're at it, you COULD use it for something real (with no risk), like [this](https://cryptonote.org/cns/cns008.txt)
 fn mix_columns(input: &amp;mut [u8; 16]) { let mut temp: [u8; 4] = [0,0,0,0]; for i in 0..4 { temp[0] = tables::GMUL2[input[4*i] as usize] ^ tables::GMUL3[input[4*i+1] as usize] ^ input[4*i+2] ^ input[4*i+3]; temp[1] = input[4*i] ^ tables::GMUL2[input[4*i+1] as usize] ^ tables::GMUL3[input[4*i+2] as usize] ^ input[4*i+3]; temp[2] = input[4*i] ^ input[4*i+1] ^ tables::GMUL2[input[4*i+2] as usize] ^ tables::GMUL3[input[4*i+3] as usize]; temp[3] = tables::GMUL3[input[4*i] as usize] ^ input[4*i+1] ^ input[4*i+2] ^ tables::GMUL2[input[4*i+3] as usize]; input[4*i] = temp[0]; input[4*i+1] = temp[1]; input[4*i+2] = temp[2]; input[4*i+3] = temp[3]; } }
 fn add_round_key(input: &amp;mut [u8; 16], round_key: &amp;[u8]) { for i in 0..16 { input[i] ^= round_key[i]; } }
Nifty, thanks for the link.
 fn debug_aes_encrypt_128 (plaintext: &amp;mut [u8; 16], key: &amp;[u8; 16]) { let expanded_key = expand_key_128(key); println!("Round 0:"); print!("\tstate: "); debug_print(plaintext); add_round_key(plaintext, &amp;expanded_key[..16]); print!("\tstate: "); debug_print(plaintext); for i in 1..10 { println!("Round {}: ", i); sub_bytes(plaintext); print!("\tstate: "); debug_print(plaintext); shift_rows(plaintext); print!("\tstate: "); debug_print(plaintext); mix_columns(plaintext); print!("\tstate: "); debug_print(plaintext); add_round_key(plaintext, &amp;expanded_key[16*i..16*(i+1)]); print!("\tstate: "); debug_print(plaintext); } println!("Round 10:"); sub_bytes(plaintext); print!("\tstate: "); debug_print(plaintext); shift_rows(plaintext); print!("\tstate: "); debug_print(plaintext); add_round_key(plaintext, &amp;expanded_key[160..]); print!("\nciphertext: "); debug_print(plaintext); }
 fn debug_print(input: &amp;[u8; 16]) { for i in 0..3 { print!("{:02x}", input[4*i]); print!("{:02x}", input[4*i+1]); print!("{:02x}", input[4*i+2]); print!("{:02x}", input[4*i+3]); } print!("{:02x}", input[12]); print!("{:02x}", input[13]); print!("{:02x}", input[14]); print!("{:02x}\n", input[15]); }
Sorry for the giant wall of functions, here's the last one: fn expand_key_128(key: &amp;[u8; 16]) -&gt; [u8; 176] { let mut expanded_key: [u8; 176] = [0u8; 176]; let mut temp: [u8; 4] = [0u8; 4]; let mut rcon = 1u8; let mut len = 16; for i in 0..16 { expanded_key[i] = key[i]; } while len &lt; 176 { for i in 0..4 { temp[i] = expanded_key[i + len - 4]; } if len%16 == 0 { let t = temp[0]; temp[0] = tables::SBOX[temp[1] as usize]; temp[1] = tables::SBOX[temp[2] as usize]; temp[2] = tables::SBOX[temp[3] as usize]; temp[3] = tables::SBOX[t as usize]; temp[0] ^= tables::RCON[rcon as usize]; rcon += 1; } for i in 0..4 { expanded_key[len] = expanded_key[len - 16] ^ temp[i]; len += 1; } } expanded_key }
It's true that a snappy name can go a long way for awareness. Maybe we should have one for the phenomenon as a whole. I vote for **MUCUS**: Memory Unsafety Compromises Ubiquitous Systems. The visceral reaction it invokes will help prepare people to understand why it's a problem.
Lazy static are "guarded by an atomic check", so not the unmutable simple static ref I expected, but it might do. Thanks !
F# did something similar with [active patterns](https://fsharpforfunandprofit.com/posts/convenience-active-patterns/) which is perhaps a more natural fit for Rust. 
I think you could make a newtype wrapper and impl `Send` for that. But do you know if it really is safe to send the type across threads?
Well I guess I'm not sure. The struct is simply a wrapper around a pointer, which also implements Drop to free it when it goes out of scope. If I only access it from within a locked mutex scope, isn't it safe?
A big issue is money. As it stands, proactive security is more expensive than reactive apologizing and a year of credit monitoring for the millions of people affected. Companies don't want to pay to have their code audited or rewritten, because it's working and making them money and they don't have to think about it. Even people who like C/C++ often hate refactoring their own code, let alone someone else's, because bugfixes are often sources for new bugs. So you're in a Catch-22 where people don't want to fix code unless they're being paid for it (good decision on their part) and the people who can pay for it aren't going to. Writing code in Rust that outperforms C/C++ isn't completely viable as a method of replacing C/C++. I think it would require a fundamental shift of general society. People would have to care about security as a primary goal of software, and as it stands most of them don't. Many never will. On a positive note, I think there's a small but growing group that care. The older edge of the millennials are in a situation where they're old and connected enough to realize that the Equifax hack is probably going to affect them for their entire life. That credit monitoring for a year doesn't mean squat when your data is permanently stolen. Here's hoping for a less dystopian future.
I've been building some simple algorithms to get familiar with rust and just wanted to see how you all might recommend improving/optimizing/rusticizing the following insertion sort implementation: extern crate rand; use rand::Rng; fn main() { let mut my_vec = get_n_length_randomized_vector(20); print_vec(&amp;my_vec); insertion_sort(&amp;mut my_vec); print_vec(&amp;my_vec); } pub fn get_n_length_randomized_vector(n: i32) -&gt; Vec&lt;u8&gt; { let mut my_vec: Vec&lt;u8&gt; = Vec::new(); let mut rng = rand::thread_rng(); for _ in 0..n { my_vec.push(rng.gen()); } my_vec } pub fn print_vec(ref vec_to_print: &amp;Vec&lt;u8&gt;) { print!("["); for index in 0..20 { print!("{},", vec_to_print[index]); } print!("]\n"); } pub fn insertion_sort(my_vec: &amp;mut Vec&lt;u8&gt;) { let length = my_vec.len(); for index in 1..length { let mut i = index; while i &gt; 0 { if my_vec[i-1] &gt; my_vec[i] { let placeholder = my_vec[i]; my_vec[i] = my_vec[i-1]; my_vec[i-1] = placeholder; i = i - 1; } else { break } } } } 
Oh neat! You know what's a stack-based vm..... webassembly...........
And the Java Virtual Machine.
Sorry, no idea on that â I actually haven't written any Rust for quite some time, but still frequent the subreddit and decided to slightly help out by posting my results â¦
Yeah, that these are coding flaws that can be prevented at a language level and there are still people who want to defend C/C++ and villify Rust because "good coders don't need training wheels" or whatever is really depressing. There seem to be an alarming number of coders out there that think they're perfect and always write pristine code, and that it's completely unrelated to them that when a codebase gets too large to mentally track entirely that memory bugs just show up. No! It's not the language! It's *certainly* not me! Those are things that just happen when code gets complicated! Bugs like these are inevitable! I know this because I'm a perfect coder who never makes mistakes or codes while at less than 100%!
Looking at those requirements thinking, if I met these, I'd be very proud of myself.
I haven't gotten around to using serde-json yet, but the key to working with `PyResult&lt;PyObject&gt;` is the [ToPyObject](https://dgrunwald.github.io/rust-cpython/doc/cpython/trait.ToPyObject.html) trait. (If you're using PyO3 rather than rust-cpython, here's [the PyO3 doc page](https://pyo3.github.io/pyo3/pyo3/trait.ToPyObject.html).) That trait functions similarly to traits like `AsRef` and `IntoIterator`, allowing return values like `PyResult&lt;String&gt;` which transparently convert from Rust to Python types for you. Given that `Value` [uses a `BTreeMap`](https://docs.serde.rs/serde_json/value/enum.Value.html) for its `Object` variant by default, you might be able to use this existing `impl` to bridge the gap: impl&lt;K, V&gt; ToPyObject for BTreeMap&lt;K, V&gt; where K: Eq + ToPyObject, V: ToPyObject (Try having your function return `PyResult&lt;BTreeMap&gt;`... you'll probably still need to construct your own `BTreeMap` though, since I doubt the one provided by serde-json will satisfy the `where V: ToPyObject` constraint.)
Have you seen [Rust container cheat sheet](https://www.reddit.com/r/rust/comments/74yrdp/rust_container_cheat_sheet_reposted/)?
I had a look through the source of wstest and tokio and I'm not sure why wstest works. To me it looks like that's not behaviour you should be relying on but I could be wrong.
You're running into some confusion with types. contents -&gt; &amp;str .lines() -&gt; Iterator&lt;Item=&amp;str&gt; .map(F) -&gt; Iterator&lt;Item=String&gt; where F: Fn(&amp;str) -&gt; String filter(F) -&gt; Iterator&lt;Item=String&gt; where F: Fn(&amp;String) -&gt; bool .collect() -&gt; Vec&lt;&amp;str&gt; or Vec&lt;String&gt; (types conflict) I would recommend folding map and filter into just this filter: .filter(|line| line.to_lowercase().contains(&amp;query)) This converts to a lower case String, then runs contains. That ensures that the cases are preserved (a problem with map + filter) and that the result is a Vec of slices into contents. In this case, the 'a lifetime hints at a Vec of slices into contents.
My preferred way of looking at these is ownership, i.e. `String` owns the value, while `&amp;str` is a slice of someone else's `String`. So consider calling `to_lowercase`. Does it return a new `String`, or a reference/slice to another one? The docs show it returns `String`, so it's a new one, but in the general case can't be owned by someone else, because there's nobody else to own it. With this in mind, your function calls `to_lowercase` on each line, which yields a bunch of `Strings`, so you'll have to return a `Vec&lt;String&gt;` instead. This fixes one of the build errors. The other error comes from the fact that `contains` doesn't take ownership of the pattern, so `query` must be lent to it, like in `line.contains(&amp;query)`. I'm not sure how lifetime elision works in general, but you don't need the annotations in this case.
So dereferencing query allows it to be borrowed?