I also use Emacs. But more packages then you. First of all [cargo.el](https://github.com/kwrooijen/cargo.el), it has nice feature run current test - if your cursor inside function marked as `#[test]` you can press some keybinding and it executes this test. Also I use racer-mode, so autocompletiotion and jump to definition works for me. There is also client for `rls` - [lsp-rust](https://github.com/emacs-lsp/lsp-rust) but IMHO it is not mature enough.
Shorter function signatures could also easily be accomplished either by `use task::TaskContext as Context` or declaring a type alias, so it's not like either choice is a huge imposition. I know I can write code that namespaces the overly-generic type name (I mentioned that in my comment), but it's annoying, doesn't play as well with autocomplete and doesn't help with code that I'm forced to read. I get that it's a personal preference thing, but I'm still irked by that name. As with most bike shedding suggestions, it's a ergonomic micro optimization, not an actual problem with the design.
There's also [`text_io`](https://crates.io/crates/text_io), which is notable for being available in [Hackerrank](https://www.hackerrank.com/environment) solutions.
Yeah, I have to step up my game, and actually learn gdb in emacs instead of `println!`-debugging, but I'm an ex-vimmer still in his emacs-reconversion phase :)
Vim + syntax files and a script which launches rustfmt on save. Not ideal, certainly beaten by Sublime, Visual Studio or Intellij, but hey I'm old and hardwired to vim now.
Exactly. I started off believing in GPL, but really when some megacorp in another country rips you off, what are you really going to do? Years of legal grief, or let it go and just get on with living your life?
I don't see it as free labor for anyone in particular (including corporations), but as redundant labor they don't have to go through because you did it first. OSS benefits the whole ecosystem, including big corporations. The fact that a big company depends on some open source libraries also gives this company a reason to spend resources to improve and maintain said libraries. If they don't, well, that doesn't really impact your project anyway, does it?
Thanks. I mentioned your fork on the github issue. /u/llogiq has already had 1 PR merged on this repo, nice of the original author to respond quickly.
What is a “grabber”?
I've been a vim user for ages, but I have always used emacs as a UI for gdb. It's pretty nice.
Let me interject with some OCaml: module type S = sig val f : unit -&gt; unit end module Plic = struct let f () = Format.printf "plic\n" end module Ploc = struct let f () = Format.printf "ploc\n" end let run_f m = let module M = (val m : S) in M.f () let () = ListLabels.iter [(module Plic); (module Ploc)] ~f:run_f This sort of arcanic stuff is way out of view for Rust though. It also wouldn't really fit, since it would make Rust stray really far from C/C++ (which I think isn't the philosophy) and most of the problems the OCaml module system solves are solved using traits in Rust. By the way, you might consider wrapping your `run` function in a `struct` with a `Runnable` or whatever trait, which will let you achieve what you want (if you need it to be dynamic, otherwise a macro will do).
Isn’t there a cargo command for this?
This is a really good article which makes this proposal very accessible! Thanks for the write-up. --- As for the specifics, I'm a little worried by this part: &gt; Note that unlike Cell, Anchor goes around the outside of the pointer type to guarantee a stable address; otherwise you could write code like this: &gt; &gt; // Call resume once at one address. &gt; let x: Box&lt;Anchor&lt;_&gt;&gt; = make_generator(); &gt; x.resume(); &gt; &gt; // Move the anchor out of the box. &gt; let anchor: Anchor&lt;_&gt; = *x; &gt; &gt; // Move the anchor into a new box at a different address. &gt; let evil = Box::new(anchor); &gt; &gt; // Call resume again. &gt; evil.resume(); What's to prevent someone from messing this up? If the answer is "nothing", would it be possible to make `Anchor&lt;T&gt;` a wrapper around a pointer to a heap-allocated `T`, instead of a wrapper around a `T` directly? That way `Anchor`s themselves are safe to move, since you are just moving a pointer, and the underlying heap-allocated `T` stays in a fixed location. --- Another question: if immovability matters only once the `resume` is called, would it be possible to make a generator change type once `resume` is called, losing the `GeneratorMove` trait in the process? Maybe the first call to `resume` could consume the generator and return a new generator of a type that does not implement `GeneratorMove`.
I've used nth-oauth2. Not sure its current status though, haven't touched it in 6 months.
that library won't help here
Because many Rust projects use Bors in CI or a variant thereof (homu).
The Free Software Foundation (FSF) its European daughter FSFE as well as the Software Freedom Conservacy can and do enforce the GPL (in a very nice way!).
I don't see gdb in emacs as an instead-of, but as a complement to `println!`-debugging. If I want to go through one case that I know fails, I step through the program using emac's gdb support, which saves time as I don't need to write the `println!` statements and such. If I'm debugging something complex which generates lots of data and I'm trying to find where things go wrong, I use `println!`-debugging to generate loads of logs that I can somehow analyse. I don't think an interactive debugger would help there.
With criterion being available on stable _and_ a more rich tool I've been scooting all my work to its use as time allows. 
The updated one (serial number 0012) is in [this GDrive folder](https://drive.google.com/drive/folders/1nlHoOYHOO-wMqUN4iHmAtrbfWPk1Tb2n). The `*-print.png` image in that folder was rendered out for ~60cm^2 at 300dpi.
custom test frameworks should make it easier to use Criterion on stable rust
How optimistic!!! I looked at that when I had my problem, and my situation didn't fit their published criteria for helping out. So I was still on my own. I didn't have the cash to consider legal action in another country. The whole thing looked like a totally soul-destroying time and money sink. So, move on ...
Neovim with Racer and RLS plugged in. 
VS Code or IDEA with the Rust plugin. They're both great. 
Anyone else getting compilation errors trying to use 0.2? It apparently depends on `unreachable 1.0`, and that's failing to build for me on rust 1.22.1.
&gt; There are exceptions to this of course - maybe the eirst value of the iterator is special, so you call `.next` to get that, and then `.collect` to get the rest of the values. But in general, you don’t need to move the iterator between calls to `.next`. This approach can be leveraged to allow passing values into the generator. Consider this example in Python: def my_generator(): assert (yield 1) == 2 assert (yield 3) == 4 yield 5 it = my_generator() assert next(it) == 1 assert it.send(2) == 3 assert it.send(4) == 5 On the first call we can only use `next` - we can't pass a value because there is nothing to receive it. Only in the second and third call we can use `.send` to pass a value to the yield. Python has dynamic typing, so it can get away with having two functions - one that doesn't pass anything but can always be used and one that passes a value but can not be used on the first generator resume. With Rust, of course, this is unacceptable - if you could use the non-passing function after the first time you would be passing nothing to an expression that expects something. But - if we already do this separation to solve the movement problem, the design of value passing becomes easy - the first function that doesn't pass is allowed to move the generator and thus can only be called on the first time. After that we need a different function anyways, so just let it accept an argument.
This will work as soon as attribute-like proc macros are stable. :) You can do this today: https://play.rust-lang.org/?gist=7c1cefa582402a0c81213bf8cc6dba11&amp;version=stable
Not at the moment, not with `macro_rules!`. It's possible to do a lot of things, but operations on idents themselves are not possible. There's `concat_idents!()` which is unstable, and can work in some situations, but it's fairly limited. It will be with macros "2.0", but that's still a ways off. You can also do this with a procedural macro, but that requires a separate crate to do all the compile-time stuff. If you need to do this sort of thing a lot, a proc macro for it could work. This is my main resource on `macro_rules!` macros: https://danielkeep.github.io/tlborm/book/ and procedural macros: https://github.com/dtolnay/proc-macro-hack
Yes in this this simple case yes but what I would like to do is something like: #[MatchMany] match EXPR { ... } else { EXPR }
To detect by extension: https://github.com/abonander/mime_guess
Yeah, this is a concrete example of where algebraic effects are useful. I want to be able to tag the poll function as requiring 'task wakeup' effect. That would be implicitly bubbled up the call stack, until the effect is intercepted, and 'handled'. That way you get the best of both worlds: implicit context passing, whilst also having the documentation in the type signature. The challenge however is making it perform well - I believe most systems at the moment require continuation passing, which can lead to lots of allocations...
Note that `std::io::Result` isn't called `std::io::IoResult` nor `std::io::Error` is called `std::io::IoError` (even though they have the same name as the standard Result type and Error trait). So I think `task::Context` fits better Rust conventions than `task::TaskContext`.
Two options, one of which is viewed as business friendly, but still requires source to be released 1. MPL, [Mozilla Public License](https://choosealicense.com/licenses/mpl-2.0/) 2. LGPL, [Lesser, Library GPL](https://choosealicense.com/licenses/lgpl-3.0/)
Note that it's `futures::FutureResult` not `futures::Result` so I don't even have to leave the crate we're talking about to find a counter example. Also, I find that `io::Result` causes confusion with people new to Rust since they don't understand why io code uses a Result that has one generic parameter while other code uses a Result with two. And given the number of crates that export their own Result type, I think you've found an example where the ergonomics of the language would be improved if types were named to avoid conflict. There is a lot of Rust code that does `use std::io::Reuslt as IoResult`.
Some more debugging of my [mechanical keyboard firmware](https://github.com/ah-/anne-key) based on the fantastic cortex-m-rtfm. Managed to talk to the second stm32 that controls the RGB leds, but sometimes it's a bit fussy and doesn't respond after a reset. Once that's figured out it's time to fix up USB again.
&gt; We have ideas (especially Niko does, of course) about how to typecheck the notion of immovability, but its unrealistic to imagine those coming to fruition sooner than 2020, given that NLL, GATs, and const generics are higher priority Is there any way to speed up the implementation of improvements to the compiler? It feels like the biggest problem of Rust right now is that compiler improvements are conceived much faster than they are implemented. In this case, I think there's even a pull request implementing ?Move (maybe not fully, not sure), so why does it need to take 2 years? 
&gt; Is there any way to speed up the implementation of improvements to the compiler? Yes! Contributing &gt; In this case, I think there's even a pull request implementing ?Move (maybe not fully, not sure), so why does it need to take 2 years? I discussed why `?Move` was not an adequate solution in the previous post.
1. Yes. 2. Yes. 3. No (at least, nothing that I see).
Yes, just now
Neovim with RLS and rust.vim
I am using it to, it works really well.
We all benefit from a free source ecosystem. The time saved by others by being able to use your code leaves them more time to write other code, that they will share back with the community. That's my view on it anyhow. I'm being playful here, but what I mean is: free software doesn't go against the core of open source software. MIT is enabling closed-source programs, not GPL. That's an ideological position, and both could be as convenient, it's just a matter of culture. :) The AGPL is the most copyleft license available, I doubt the author would be considering MIT given such a strong intent.
It works, formatting works, debugging works. But only on Linux. On my mac box, it crashed if I made syntax error. 
`use std::io::Result as IoResult` is actually an intended usage, and it allows both `IoResult` and `io::Result` depending on what each project's style is. I personally greatly prefer just importing modules, and referring to names as `io::Result`, and I almost ever directly import names. `use std::io::Result;` is definitely a wart - but I think that's fixed by just `use std::io;`, not by renaming `Result` to `IoResult`.
Ugh...that's unfortunate. But there's still plenty of instances of crates within the ecosystem that export unique type names, so it would be disingenuous to suggest that either preference is the idiomatic way. Honestly, it feels like a design decision for a world where RLS isn't a thing. Having to complete `futures` first and then `Result` is going to be much slower than just completing `FutureResult` which should be the first or second option when you type "fut" then autocomplete. Rust types being as static as they are, it seems reasonable to expect that we'll one day have IDEs that are every bit as capable as the ones in the Java/C# world, so designing for text editors built in the 70s (or any editor where you have to type the whole type name) seems short-sighted. But whatever...I get that it's a preference. I just have to push back when someone suggests that it's anything more than that. If the decision is being made because the people doing the work prefer it that way, that's fair. I'm not doing the work, so I only get to voice my opinion in a forum like this and live with whatever decision gets made.
Continuing to hack on [Rustodon](https://github.com/rustodon/rustodon), a Mastodon and ActivityPub-compatible server for microblogging. Going to be focusing on getting a minimum viable UI working now that we've settled some of the database and web framework stuff (Rocket and Diesel are aweeeeesome). Hopefully we can start implementing minimal federation soon.
nvim + Languageclient-neovim + rls (or other langserver)
Hey Andy, long time (Michael from ex. FullContact)! I think you should stick with that for (almost) everything. Or, make your worker node a crate you statically link into your worker binary. That way, really all you're doing is starting a binary in a container on K8S + using etcd/kubedns to go find your job controller task to establish an RPC connection with. That way, you can also eventually even just write more or less any valid Rust code so long as you adhere to a basic API. Having done the whole Hadoop/Spark thing many a time, I think it's really just asking for trouble trying to send code up to a static pool of workers, though I get the desire for being able to send arbitrary SQL up -- that seems do-able. 
Hi Michael! Nice to hear from you. I think you're right.. having user code deployed as a crate dependency (either compiled locally or pulled from a crate repo) is actually going to turn out to be a strength rather than a limitation.
Could you help me understand why RON? When I first saw it announced, I assumed it was either a joke or a tool to help in debugging serde. While I do agree that json and toml are too limiting, RON feels too feature rich to me. I feel like I'd want something simple for my config format.
Yeah... this is me currently as well...
 Heh when researching my [Async IO post](https://manishearth.github.io/blog/2018/01/10/whats-tokio-and-async-io-all-about/) the lack of this parameter was exactly what confused me. "How does Tokio know which things to epoll for if futures are going to be behind layers of combinators which know nothing about tasks waking up?". The answer was that Tokio basically uses TLS to send messages "across the call stack" without there being explicit parameters. Hooray global state. I'm not sure if this will fix that entirely, but will probably make it easier to reason about. 
There is a Vim plugin for VSCode though. Or does that not work for your work flow for some reason?
How come `std::io::copy` doesn't follow this?
Geany deserves an honorable mention. Not as good as Sublime or VSCode overall, but offers a decent experience right out of the box, with no plugins or configuration required. 
emacs
I don't know about doing it through cargo, but you can make a build.bat or build.sh that just runs the right cargo command. That's what I do.
BTW, what I meant is that I see errors on the bottom (a transcript of `cargo build`) but not inline (underlined).
I remember the last time this came up I asked him a few questions about the actor lib and he said its design was really built around his game. Not a general thing at all, really.
/u/theanzelm is pretty responsive on /r/citybound
&gt; Honestly, it feels like a design decision for a world where RLS isn't a thing. Definitely agree with this! This decision was also made, as you say, before RLS existed - when everyone just wrote types out by hand. This feels like a limitation of RLS though - wouldn't it be completely possible to have `futres&lt;tab&gt;` match multiple levels down and complete to `futures::Result`? I don't think Rust or libraries should make design decisions based off of current tooling limitations. But, yeah, that is my opinion (and I do like the look and ergonomics of `futures::Result` better than `FutureResult` for all other purposes). 
just one nitpick: `fmt::Result` isn't an `io::Result` at all - it's `std::result::Result&lt;(), std::fmt::Error&gt;`. Formatters aren't allowed to have IO errors, only formatting errors (which are completely opaque).
`cargo build` should always build everything if you don't tell it, but I don't have any tips for running. Maybe you could do a shortcut in `.cargo/config` of the project? [alias] x = "run --bin bin1" would make `cargo x` do what you want when in that project.
&gt; the linked posts (assuming they're real) don't seem sexist, just stupid. They are most definitely real. This happened not that long ago. If you don't think some of those comments are sexist, then I think we should hash out exactly what you believe sexism is. There are literal examples of discrimination listed. --- The real crux of that particular situation, was that this was right after Rodd Vagg was accused of allegedly violating the node CoC. https://github.com/nodejs/CTC/issues/165 That whole situation was handled very poorly, and made me skeptical of Node's teams in regards to handling complaints maturely and properly. It was dubious at best, and then Ashley made distasteful comments, that IMO, violated the Node CoC as well. People were certainly frustrated that **CoCs seem to be applied arbitarily**. With Node at least – but that is not the first time a CoC has been kind of weaponized. I am hesitant to trust any CoCs to actually be applied fairly.
How can you have an implementation for Arc and Rc though? get_mut() might return None, and make_mut() might have to clone the underlying generator. And even if you could have clone() fix up the self-references somehow, it would prevent any actual sharing of the generator, since each first use of it would be required to clone it if it was shared, so you might as well just use Box.
You're right actually I realized this after the fact that for generator's use case at least this is no good.
No IDE; text editor and terminal. I've found no need for anything else.
Why is JSON more of a problem than HTML or Javascript? Those two are text formats solely for developer comfort. That's a big part of what made the WWW accessible to people.
This is all you need
In practice, it's not as complicated as people initially believe, and it sure takes all guesswork out of serialization/deserialization.
Vim + RLS works nearly perfectly for me. Only complaint is that adding a dependency requires a restart before it is recognized by the RLS.
Spacemacs+racer with autocomple+rust layers,sometimes sublime+enhenced+rls
I second this. Works nearly perfectly for me, except new dependencies aren't recognized until you restart. Not a big deal though.
With enough plugins (things like language servers, etc) you can make the rest of (neo)vim really usable, imo. 
Read this issue on emacs-racer github page. https://github.com/racer-rust/emacs-racer/issues/91
Making steady progress on [castle-game](https://github.com/tversteeg/castle-game), a lemmings meets tower defense game I've had in my head for 8 years. I've been working on it for a month in the few hours I've got left after work. 
Excellent conclusion! The `Anchor` type is such a beautiful use of `unsafe`. It brings a tear to my eye.
I suggest you to follow mentioned api guideline. The reason is to be able to pass reader by value and by reference depending on your needs.
Hey everyone, thank you for raising this again, I updated the issue [Separating Kay into its own crate](https://github.com/citybound/citybound/issues/209#issuecomment-363347531) with some more clarification regarding my motivations
Let me tell you, the real sweating bullets moment was the multiplayer demo. Wifi latency was through the roof because of the many devices in the room, something that I never had in my tests.
Thanks for the kind words :)
Not sure what exactly you mean with your question. Actors are updated every time they receive a message, in the order that messages arrive on a particular processor core. There are so called broadcast messages though, that are delivered exactly once to every living actor and these are used for things like the physics update.
The solution here might be [implicit params](https://docs.scala-lang.org/tour/implicit-parameters.html). This should help with a few other usecases too, like passing all request scoped context (metrics, User struct).
All Vim and tmux.
So, how is this an actor model? Actor model is more overused terminology that no one ever defines.
&gt; Racer-mode is unusable for me: very slow even for small projects and sometimes it hangs trying to autocomplete the beginning of a line Very strange. I think that you should report this issue to mainstream, because of I never seen such behaviour. ``` (setq racer-cmd "/home/evgeniy/.cargo/bin/racer") (setq racer-rust-src-path "/home/evgeniy/.rustup/toolchains/beta-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/src") (add-hook 'rust-mode-hook #'racer-mode) (add-hook 'racer-mode-hook #'eldoc-mode) (add-hook 'racer-mode-hook #'company-mode) ``` you may be not need `racer-rust-src-path` it should at now calculate this automatically.
FYI - LGPL requires the library to be dynamically linked (or similar) to the final distribution. If it's statically linked (as I think Rust typically would?) then it requires a GPL license instead.
Does VS Code with the rust plugin have in built cargo support?
Note that I am not asking for the author to change the license of the game, but to split out their actor framework into a separate crate and to make that available under MIT. I think it makes sense that someone might agree to use more liberal licenses for code that is used as "building blocks" while keeping their stricter license for the thing they are building itself. Not quite unlike how companies often open source general building blocks while keeping the portions defining their product proprietary. I find it conceivable that the intent of the author was to keep companies from making proprietary additions on top of his game, not so much that he wants no-one to build anything proprietary that utilizes the actor framework. Of course only the author can know his own intent. But even if he does currently want the actor framework to be AGPL also, perhaps he could be convinced by careful and respectful reasoning to MIT license that part only.
Announcing Rust for Undergrads, a community initiative by Rust India. "https://github.com/rustindia/Rust-for-undergrads/" Aim: Encouraging undergrads to re-write C/C++ questions in their academics, job interview and programming practice problems into Rust language. Goal: To make it easy for undergrads to start programming in Rust. Evangelizing universities/schools to replace Rust in place of C/C++. Level: Beginner/Intermediate If you like it, *star* the repo.
Awesome, thanks 🙏🏻
There are some good reasons to use private blockchain, not a database: 1) when you need to make sure that the data wouldn't be changed unnoticeably (and our private blockchain allows you to anchor to Bitcoin network) 2) if there's a lack of trust among participants and with BFT consensus and anchoring you can fix this issue (and that's applicable for a lot of spheres like retail and supply chain, government registries etc) and when it comes to sensitive (healthcare, finance) data 3) if want to have auditability from client-side (all received information is validated on the client side with the help of Exonum light client) Anyway, you're right, not everyone needs blockchain, database is still a good fit for a lot of use-cases. So before implementation of blockchain, you need to have an answer if it's really needed or just because it's a trend and everyone talks about it.
`mime` only declares the `Mime` type and some constants, it won't guess from file extension or magic bytes. If you can trust the extension (you're hosting the file) or there's little reason for the user to spoof it (like if you're just telling them what it is that they sent to you) then guessing based on file extension is usually sufficient. If you're decoding or otherwise doing something with it then you're going to want to handle it a bit more robustly.
You'd still be involved in a heavyhanded legal process with an outcome that might not be worth the effort.
Strictly speaking: LGPL only requires that you can reassemble to program with a replacement version of the library. You _could_ provide rlibs for everything and the possibility to relink those with a different version of the library.
2.0.0 is in alpha until the `mime` crate releases its 1.0, though the API it is using now is already pretty close to what's going to be released. I released mime_guess 1.0 before I really understood semver and people came to depend on it before I realized my mistake, so that's why it already needs another major version bump.
Let's say it's a tool which you can use to extract some data from the particular site for futher processing, like searching etc.
vim with the [vim-racer](https://github.com/racer-rust/vim-racer) plugin. no complaints here
No idea. If you know of one, let me know. :) I am doing it this manual way, because that is the way I am familiar with from C.
Why do people want to replace c/c++ with rust, rust is near impossible to learn / appreciate without c
True but that's a sticky situation if one's distributing a closed source binary; I imagine a lot of companies would want to avoid the licence completely in that case.
Also, some of the rust-lang projects (like crates.io) have already migrated to bors-ng and work is in progress by bors-ng devs to finish developing what is missing there to support rust-lang/rust's workflow.
Well, if you watch the video, he explains how.
How to implement PartialEq trait for a simple Point struct? I want to be able to do a == b I tried something like this but it doesn't work struct Point { x: num::bigint::BigInt, y: num::bigint::BigInt } impl Point { fn eq(&amp;self, other: Point) -&gt; bool { self.x == other.x &amp;&amp; self.y == other.y } fn ne(&amp;self, other: Point) -&gt; bool { self.x != other.x || self.y != other.y } }
how can I use it? simple example?
I think it is learnable, but it is hard to find motivation for learning its concepts, if you never debugged some tricky seg fault/race condition/...
One sentence in the introduction on what eBPF and BPF are really helps outsiders like me understand what's going on. :-)
Here is a method that worked for me. Define a trait named Run, and add a constant named RUNNER (for lack of a better name) to each module, and make sure that the each RUNNER is an instance of a struct that implements Run. Now you can do let runners = &amp;[module1::RUNNER, module2::RUNNER]; for runner in runners { runner.run() } 
There are a few licenses that might cover your need. This is a quick description of the most common copyleft licenses GPL&gt; Every derivative work that include part of your code has to be released under GPL too. AGPL&gt; Same as GPL, but you have to publish the source of derivative work to the user, even if the modified software is not installed on their computer but accessed over network. LGPL&gt; Modifications to the code of your project have to be published, but you can use it as library for proprietary code too MPL&gt; Il your derivative work contain modification of files under MPL licence, these files have to be under MPL too, but your final product can be build from a mix of files under MPL and files from other licenses including proprietary ones. 
I appreciate it as a Java developer. No NPE and exceptions, addition of sum types...
&gt; GPL&gt; Every derivative work that include part of your code has to be released under GPL too. No. Works including GPL code must be distributed in a fashion that complies with the GPL. (Which means: they reserve your full right to modification, usage and access to the source) They can absolutely be under a different license. But for example, forbidding _commercial use_ is a GPL violation. 
BPF is the Berkeley Packet Filter, an in-kernel VM historically used by `tcpdump` and friends for filtering packets on *BSD and Linux systems. eBPF is a Linux extension to use this VM for tracing/profiling actions system-wide. Linux has had a bunch of different mechanisms for this (e.g. SystemTap), eBPF is the newest and most capable. (It's also gained some [notoriety](https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html) recently because it was used in the proof-of-concept Spectre attacks.) [Brendan Gregg's eBPF page](http://www.brendangregg.com/ebpf.html) has a lot more information.
I fundamentally believe this to be untrue. We have quite a lot of people in our learned group that have neither learned C or C++. I also know some universities on the way to switch from C to Rust for their base course. It is absolutely possible to appreciate and learn Rust without C.
No, logical would be for the loop iteration variable to be a fresh variable on each iteration, not to be reassigned. C# 5 fixed this for the foreach loop (one of the few cases of actual breaking changes).
I am 3rd year undergraduate student and I have been learning/writing rust for 6 months. I am sorta interested in the project. Just wanted to say, That your motivation might be good but wording invites criticism and shitposters. 
For people having learned to program with high-level languages, Rust is way less scary than C if you want to dive into low-level stuff.
Obviously my experience is only anecdotal, but I myself have gotten into Rust because of its modern language design (type inference, closures, iterators) and reputation for being fast. But once I started learning it, I also learned to appreciate the safety Rust provides, even though I have never been a c or c++ programmer.
I know but I wanted to keep it simple for a first introduction.
Why? This is basically the "if you never burned your hand on the stove, you can't appreciate that they are hot"-argument. Race-conditions are perfectly possible outside of the C-space, and the luxury of Rust is that you _don't_ need to debug segfaults outside of your unsafe code, which makes the problem vastly easier. Linear types solve _much more things_ then memory-safety.
There are a couple problems: 1. replace `impl Point` with `impl PartialEq for Point` 2. replace `other: Point` with `other: &amp;Point` You don't need to define `ne()`, the default implementation is `!eq()`. You don't need to define any implementation. `PartialEq` can be derived, so you only need this: #[derive(PartialEq)] struct Point { x: num::bigint::BigInt, y: num::bigint::BigInt } 
Yeah, but you spend hours debugging lifetime stuff that doesn't make sense, or not understanding value / reference passing, or why you can't use dynamic dispatch on a value, or why you need 2 different string types (3 with OSString), or why you need 2 types of everything (path / pathbuf for another example) None of this makes any sense coming from just java or python, where everything's a list and it 'just works' - no need to understand why you can't append to an array (although you can't append to an array in java, so maybe java was a bad example here)
I mean, rust isn't magic here - part of the reason rust is still so good is that most of the people using is really like it, you can still write 'bad rust code' In fact, writing 'bad rust code' is probably way easier without learning C, because people will just get frustrated with the borrow checker &amp; go for hacky workarounds without understanding the consequences, potentially using unsafe {} without proper consideration etc
&gt; No, logical would be for the loop iteration variable to be a fresh variable on each iteration, not to be reassigned. 1. That's orthogonal to what I'm talking about, I'm saying the issue is intrinsic to closing over mutable references, you're declaring than one just has to not close over mutable references. 2. That fix is only an option for block-scoped languages which had initially mis-scoped the for loop. Python is not block-scoped.
Yes &amp; with the same argument I can say that in C it doesn't make sense why one needs to call `free` when you come from the land of GC
So it seem easier to you to come from a higher abstracting language (like Java or Python or similar) to go down to C - almost bare metal level - learn creepy things and then proceed to Rust and be happy to climb the ladder again? 😈
Thanks!
Top Resources - [Rust By Example](https://rustbyexample.com/) and [Official Rust Docs](https://doc.rust-lang.org/book/) [Source](https://hackr.io/tutorials/learn-rust)
 extern crate mime_guess; let mime = mime_guess::guess_mime_type("path/to/some/file.js").to_string(); assert_eq!(mime, "application/javascript");
IntelliJ's definitely the best Rust IDE right now and the Vim plugin (IdeaVim) is pretty solid. Supports vimrc (just make an ~/.ideavimrc file and `source ~/.vimrc` in it). It's not *perfect*. Things like Ctrl-O and macros are pretty buggy, or at least don't match up with the behaviour I expect with all the addons I have installed for Vim.
It depends what you mean by "learning C", I'd say. I'd say that a brief introduction to the C *machine model* is a necessary and appropriate part of learning Rust, and most CS101-type courses don't extend much beyond that anyway. I *do not* think that people should be expected to write *non-trivial* C, much less C++ before learning Rust.
I learnt Rust comig from a javscript/php/python background, without knowing any c/c++. I persoanlly this much easier than learning C, which I have tried in the past. Partly because the Rust book explains a lot ofnthe required concepts (e.g. stack vs heap), whereas those resources seem to be harder to find in the C/C++ world (unless you buy a physical book or take an organised course it's hard to know what you don't know). And secondly because if you make a mistake in using these new concepts in Rust, it's likely to be a compile time error, whereas in C it's likely to just silently work incorretly. For me this was a big plus, as it gave me feedback on where my assumptions were wrong, and allowes me to google the error and learn very quickly. The other benefit was that once I got it to compile, my program was super reliable. And I was confident putting my newbie-level Rust program into production at work. Indeed, it's been one of our most reliable pieces of software.
Relying on metrics of the space glyph seems wrong. For what it’s worth, the components of [`FT_Glyph_Metrics`](https://www.freetype.org/freetype2/docs/reference/ft2-base_interface.html#FT_Glyph_Metrics) are documented to be “font units” when using `FT_LOAD_NO_SCALE` (which you do apparently). Font units need to be divided by the value of [`FT_FaceRec::units_per_EM`](https://www.freetype.org/freetype2/docs/reference/ft2-base_interface.html#units_per_EM) (after converting to float or something) to obtain a length in multiples of the size of the EM square. In PDF, the unit of glyph space is one-thousandth of the EM square.
No no, not c++, maybe some non-trivial C, you can't really understand the machine model without writing a little bit. For example: When being taught C, you might learn that passing by pointer is faster because you have to copy less data. So, you might write a function like this: ``` int* add(int* a, int* b) { } ```
Yes, knowing Haskell helped me a lot with the Rust type system for example.
As someone coming from a systems programming background, I find programming in Python very uncomfortable. There is no compiler to tell me that my code is valid and catch common errors. I have to actually run the program and hope for the best. There are too many things that I can get wrong. It is very unproductive. Somewhat ironically, I dislike python for the same reasons why I dislike C. It shares a lot of the same problems regarding having to always be super paranoid and careful not to make mistakes. So much mental effort required. I find writing Rust almost effortless. The language doesn't get in the way, and when it does, it is almost always because *I* am doing something wrong, so I just thank the compiler and fix my code. I got so used to the Rust compiler's guarantees about my code that writing in a language like python feels like being on quicksand.
yeah, definitely Stack / heap allocation doesn't mean anything to java or python programmers who've never touched anything below that, rust compiler errors don't tell you that 'this is a stack alloc so when the function returns the reference is no longer valid' though, it'll just tell you that the reference lifetime isn't valid even though from your model it is C is the best way to learn about the machine model, which is necessary to learn rust - consider these 2 code examples: int* foo(int a, int b) { int c = a + b; return &amp;c } Integer foo(int a, int b) { Integer c = new Integer(a + b); return c } In both examples, we're taking a and b by value, then returning 'by reference' the result (at least, that's how a java programmer would understand it) In java, this is fine, because Integer is actually a heap allocated object which wraps an int. However, returning &amp;c would return a reference to a stack allocated variable, which is obviously invalid.
An 'understanding of low level code' basically is an understanding of c, c is as simple as a language gets 
I agree. I use the same strategy of only importing modules. I also only import traits inside of function bodies, using namespaced trait names outside of them. I find it makes for rather nice reading. I know any `Foo` is local and I can access it's private data (unless I'm in a facade module), every `bar::Foo` is in another module, every `use bar::{ Foo }` refers to an available trait inside the scope. It also provides a good distinction with things like `foo::Error` (general `foo` error) and `foo::IoError` (I/O error of `foo`).
Ok. thank you, that does answer my question. So the system does synchronise certain messages to make sure stuff isn't dropped or duplicated.
There’s no reason autocomplete couldn’t (or shouldn’t) match agains fully qualified type names. Typing eg.`fut::` in a type context should suggest types in the `futures` module. Java is full of redundant prefixes in type names because it doesn’t support package aliases in imports. Also, `FutureResult` is not a `Result` but a `Future` so its name makes sense.
[His response](https://twitter.com/mrfrasha/status/960869427402207237)
So it’s just a scraper?
You can think of it as similar to desctucturing 'regular' datatypes. Say we have a very simple struct called `Wrap`: ``` struct Wrap&lt;T&gt;(T); ``` If we want to get the `T` out of a `Wrap&lt;T&gt;`, we can use pattern matching: ``` let wrapped = Wrap(42); // we create a value with the Wrap constrctor... let Wrap(thing) = wrapped; // So we can also desctruct it! ``` We can also do this in a for loop, if we have an iterator yielding `Wrap&lt;T&gt;`s: ``` for Wrap(thing) in wrappeds { // do something with the thing } ``` `&amp;` just works the same way: `let bar = &amp;foo` creates a reference to `foo`, `let &amp;baz = foo` 'destructs' the reference and puts its contents in `baz`.
Only if there are no references to it, which means you're now tracking that at runtime...
Please no. This ‘feature’ is one of the reasons I stopped using Scala, it practically begs developers to engineer magic, undebuggable libraries.
That's sounds 90% of the way to a GC.
In terms of what?
A simple simple lib to make scrappers fast, yes.
Repent Unbeliever! All programs must be rewritten in Rust!
C is not at all simple. 
jemalloc happens to use optimizing behavior at runtime for hot acquire/release paths, and AIUI, it will allocate large slabs for the program to use so that there are fewer calls to the system allocator. But since allocator behavior at runtime is is uncoupled from static analysis of the code (for instance, if your allocations and deallocations are coupled to environmental events), there's nothing Rust itself can do here. Plus, Rust needs to remain uncoupled from allocator specifics. It's not an allocator, and we need to eventually able to swap allocator choices. As long as the API surface remains usable, the implementation details of how memory is acquired or released is out of Rust's scope.
Project idea: Get someone to rewrite a Linux kernel module in Rust.
&lt;pedant&gt; It's Nic Cage, rather than Nick Cage
Replace javascript with web assembly? Or do something visual with a gui library or opengl?
Why not start contributing to existing Rust projects you might be interested in first?
My first computer science professor once told me, "If you can write a Texas Hold 'Em game in a language, it's safe to say you know it." As for not feeling productive... You're probably not being productive. I have the same problem; there are probably a dozen projects I took on as learning projects that didn't actually solve a problem anyone had, and so I lost interest. If there's some kind of problem that I think I can solve (or solve better), I feel much better about the project and am way more likely to see it through.
this sort of thing is why I've always favoured fine grain control over memory allocation (custom pools, precompiled object graphs etc) .. I dont think there's a one size fits all solution. I think Rust gives you enough tools (like C++), whilst having something 'ok for general use' out of the box.
Any suggestions?
Maybe this project would be of interest? https://github.com/koute/stdweb Try some webgl + wasm experiments?
This is a good idea. I’ve always wanted to play with OpenGL. 
It does look interesting; I will look into it, thank you very much!
I just edit it directly in nano because otherwise I have to scp every time I make a file change. 
bbedit
I published my first crate after winning a prize at a hackathon for a domain specific language I made called Glitter. It's a developer tool for pretty printing git stats in your shell prompt https://github.com/glfmn/glitter
if in doubt it would be nice to contribute to tooling, although thats probably difficult (deeply complex systems like the RLS)
Atom + Racer-plus + DBG + DBG-GDB + language-rust - Vim-mode-plus + ex-mode Works pretty well.
I find Rust to be really useful for writing small (possibly command-line) tools. Maybe look for something in your normal workflow that's awkward or time-consuming, and try to write a tool to improve that.
&gt; I say this as someone still learning Rust &gt; frills For someone who still learning `Rust` I recommend install `rust-playground` package. Installation is pretty simple via `M-x list-package` and should helps you to make language experiments with less efforts. 
Re-writting of my trading orders routing software in Rust. The original code is C#, F#, Java and some C++.
I think this kind of supports the point you were making. :-)
if this interests you I would love new contributors! https://github.com/vitiral/artifact I'm doing a rewrite of the `artifact-data` layer right now using [`ergo`](https://github.com/rust-crates/ergo) as the primary library. I'm about to start work sometime this week.
Embedded was one of the use cases I was thinking of. What do you think about copying long lived data to a safe area (checkpointing) and doing a [microreboot](https://www.usenix.org/legacy/event/osdi04/tech/full_papers/candea/candea.pdf) of the main loop? 
Agreed... especially if it's stuff you want to leave running in the background or run in a cronjob, where Rust's strong compile-time checks, efficient and predictable use of memory, and and fast startup time can shine. (I use it to replace Python because, when you think about the theory, you realize that a strong type system is quite literally worth a million unit tests.)
I like where you are going with this. General case? Who knows. Was mostly a thought experiment on things that *could* be possible in Rust. What do you think about using ML (machine learning) to route allocations to different allocators based on the call stack? 
Isn't an allocation a runtime borrow from the heap? 
rlibs have metadata inside of them similar to a header; you don't need the source, just an rlib.
Probably just an oversight. It's an old API that predates those written guidelines.
Please forgive me
&gt; apm list | grep rust | grep -v disabled I'm also on Atom, but i only have language-rust as a plugin. It works well enough. I keep meaning to try IDEA, but never get round to it. 
Do you have ideas for how this would look from a documentation perspective? How would you indicate that a particular trait, type, etc is available on a particular platform?
That's unrelated to "tracking ownership", borrowing, etc. To move data you either need GC-like tracing, or you need to keep a list of pointers to each allocation (e.g. by making each pointer 3 words wide and using the additional to 2 words for a linked list), or you need to make all pointer accesses go through an indirection table. Also, to defrag Arcs (or Box/Rc owned by another thread), you need to protect against concurrent access, which means you need to keep accurate stack/register information either at all points or at least at a point for each loop/syscall, and you need to stop all threads when moving. It's possible, but a nontrivial amount of work, and it's not clear whether the positives outweigh the negatives (in fact it seems way more harmful than helpful). 
For the unaware, buildroot is a system (like yocto) for building linux images. I've got a few 'embedded' (well, raspberry pi) applications I'm looking forward to porting to rust with this. There is a cargo package as well.
I'm currently working on a small function to return an index with the smallest associated ```f64``` value. This is a subject that has been discussed to death in the context of the fact that ```f64``` doesn't imply ```Ord```, but I'm not quite sure how best to proceed for my particular use case. For the likely-unnecessary background, I'm screwing around with [petgraph](https://crates.io/crates/petgraph) and a GUI. I have an iterator of indices, and I want to return the index of the edge with the minimum pixel distance from a node somewhere else with some cursor position ```[x, y]```. I've sorted out the vector algebra and produced the distance: ```self.graph.edge_indices().map(|i| /* calculate distance */ (i, dist))``` Now I want to return the ```i``` with the minimum distance. Can I use ```fold``` with an accumulator that stores both values? Is there a way to convince ```min_by``` or ```min_by_key``` to respect the tuple? I'm hoping I can avoid creating a new struct and implementing Ord for it.
Will it be doable with attribute proc macros though? `unless COND { EXPR }` is not a valid syntax tree - I thought attribute proc macros only work with valid syntax trees as their input?
&gt; What do you think about using ML (machine learning) to route allocations to different allocators based on the call stack? perhaps that could work, 'profile guided optimizations' .. basically automating empirical trial and error
I wish people would downvote just a bit less on this sub. I think there's a valid argument here - that writing C/C++ before Rust may make sense. I can think of two reasons for this: * Hitting segfaults etc can build an appreciation for languages without them * Rust *is* more abstract than C, regardless of whether C is considerably more abstract than asm. I don't believe the first point is that important. But the second is worth exploring. If the goal for a student is to go 'lower level' and remove abstractions, is Rust the best language? Their goal is likely not going to be "write the best, safest software" but "understand what memory actually is, how it's allocated" etc. In that way learning C first may be worthwhile.
I really appreciate a company taking the time to write a document like this. It goes a long way to show others that rust *is* a production ready language.
Maybe the [`ord_subset`](https://crates.io/crates/ord_subset) or [`float_ord`](https://crates.io/crates/float-ord) crates will be useful for you.
I'm not sure about microrebooting, but I _have_ thought about doing defragmenting in stages. I.e. - start defrag process, copying data - if any process takes a mutable reference to the data, a bit will indicate it - finish the defrag process, but if the data changed start over from the beginning. This could theoretically be used to make defrag-rs be usable from within interrupts (with only a few operations requiring globally disabling interrupts).
Implicits feel like one of those features, like operator overloading, where if it was used prudently, it could make code much, much better, but if it was used without honour or humanity, would make code much, much worse. 
FWIW I also gave a couple of talks about my work on Rust debugging. Those weren't in the Rust devroom though.
do you have a list of 'edge case' platforms to prove this?
about indexes: What about moving pointers too? but then you probably need a mapping table. You still can't encode pointers in a float or with tagged pointers but more introspection could solve that. (or you can take a risk and overestimate moves and use boehms heuristic. Watch out for segfaults though)
This sounds like a great plan. In particular also because it will help with docs. A lot if core docs actually document std. 
Right now, rustdoc has a specific form of the doc attribute, `#[doc(cfg(...))]`, that allows certain platform-specific items to be documented on other platforms. Right now it’s there so that the official docs on https://doc.rust-lang.org can give docs for Windows and macOS, but i imagine that that functionality (or something like it) can be used to give other platforms or portability features a banner as well.
It looks like you wrote a lowercase I instead of an uppercase I. This has happened 5487 times on Reddit since the launch of this bot.
I love how the audience **bursts** into applause.
Sure, but the code ain't ANSI-compatible anyways, so I'd ditch that. The thing is, although bad code is real, if someone attempts to compare two languages they should either write same-quality code on both sides, or don't base the comparison on examples. Also, if something might serve as a learning point, it's important to make it correct -- such a blog can easily have more impact than many books already... . It's critical to be aware of that. That's true of course, although the error is more gracefully handled on the Rust side -- and in general, I do totally admit that Rust is "better". That's what I love it for and prefer it over C in production. But I like arguments to be well-made and the C code as was posted here really was short enough do give it a critical look, but not doing so really lowered the quality of the post in a relevant way (imho).
`use task::TaskContext as Context` literally satisfies _no camp_. No one wants to use `Context` in the scope! One group advocates for `task::Context`, the other for `TaskContext`. If you're looking for an intermediate, the only way to accomplish both is to export `task::Context` and support `use task::Context as TaskContext`. This does favor the `task::Context` camp, but it at least _allows for_ the other. Exporting `task::TaskContext` does not allow for the first camp at all.
Is that practical? What would you do when allocation fails other than crash?
Isn't this how linking against std and friends almost always works?
It's a hard spectrum. The other extreme is to go the C route and have no functions allocate memory, but always either accept a raw buffer or an interface to an object (say, the `Alloc` trait) that provides hooks for how to deal with management. 
Bad bot 
Thank you z\_mitchell for voting on Yasuo\_Spelling\_Bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
I find this proposal pretty unfortunate in the degree to which it would make reasoning about subsets of the standard library a lot more difficult and subtle. A crate dependency graph is pretty easy to reason about - `libstd` depends on `libcore`, so you can have `libcore` without `libstd` - but I feel like this proposal would make reasoning about what features are compatible with what other features a lot more difficult - especially for people contributing to `libstd`. In my experience, it is devilishly difficult to think through all the possible combinations of `cfg` options and come up with a design for which every possible combination works properly and is not surprising to the user. That said, I understand the problems with the existing multi-crate model (such as the fact that atomic operations can't really be considered as part of a core set of always-available functionality). I just wish that there were some way to reorganize the crate hierarchy to make it work. More importantly, if it's not possible, I'd love for somebody to make an argument that it's not possible. aturon clearly knows what he's talking about, so I expect that there's background to that article that I'm missing that would change my mind.
Do you have a minute for the good word of sshfs
There are systems where you may want *speculative* allocations. For example, the C++ `stable_sort` has O(N log N) complexity [if additional memory is available](http://en.cppreference.com/w/cpp/algorithm/stable_sort), and otherwise uses a less efficient O(N log^2 N) algorithm for in-place sorting.
Good bot!
Even then, on many systems, you don't get an error on OOM, so this doesn't help.
Correct. Attributes can only be placed on a successfully tokenized tree that is a valid Item. Generating your own syntax requires an item-like macro, as I understand it.
Didn't know that. Thanks 
Having a linter that runs `cargo check` is very convenient. And `racer` for autocompletion.
crates.io distributes source only.
[removed]
It would still be very annoying. If we truly wanted a way to handle OOM we should instead do it backwards. Instead of having the caller handle OOM, we immediately pause the execution and handle the OOM. This is a bit of a challenge, as you cannot allocate any more memory to handle the error. So you'd simply have to allocate the handler beforehand and have it run on a constant memory size allocation. I guess that at some point you could allocate a new OOM manager to handle it in a separate fashion. OTOH it'd probably be better to simply use fallible allocators manually and handle the whole error. Either way the environments were OOM is a possible error and something you want to recover, are not environments your standard Rust library is written for. Libraries would have to opt-in to this stronger constrained system (they should still work with standard though). Still none of these requires a change in Rust or on all the libraries. It's merely understanding that it's a constraint that few have.
Thanks for the clarification. I believe the position I am arguing from is more generic and abstract, while you are an expert explaining specifics, it is easy to argue past each other. Is there a word for the neophyte/expert impedance mismatch? Would it be possible to model allocations as ranged borrows from a Vec? I am aware that one can't have multiple mutable borrows from a Vec, but I am hand waving that away.
As a student I think some depth into systems programming is worthwhile. I'm not against any of this - more trying to elaborate on the parent's point.
Bad bot (wtf, that's like the most benign "spelling error")
Yes, it's unbelievably slow.
`Generator` only has `unsafe_resume()` method. So if you are writing a function that takes a generator by reference you'd have to type it as `g: &amp;GeneratorMove`.
Currently attempting to (rewrite the C++ Chess Engine Stockfish)[https://github.com/sfleischman105/Pleco] in Rust. For anyone thinking about porting some C/C++ code to Rust, do it! It's a very challenging and rewarding project to undertake. You'll also get *very* familiar with both languages along the way :)
Oh this is true.
Interesting and a good read, thanks for sharing. Not sure if it's correctable at this point, but there's a missing "the" in the first sentence of the "Making Room for New Features" paragraph.
Ill pass it along, thanks!
There are already crates that use unsafe code and the principle that a box allocation can't be relocated. (`owning_ref` is quite popular) And `Mutex` in the standard library depends on this too. So, not in *general*. But yes, the Rust approach can be applied to other kinds of memory management. There's a pretty neat blog post about how Servo interfaces with its JS garbage collector. I'll see if I can find it later.
(Minor, but the PDF's title is slightly peculiar as "Microsoft Word - Rust-Tilde-Whitepaper.docx". This appears as, e.g., the tab title in pdf.js in Firefox.)
cfg flags can be structured as a DAG just like a crate graph; they're only more complicated when you don't maintain that discipline
Yet another interesting proposal with deep consequences for the Rust ecosystem. This is another occasion where we can look at java (and project jigsaw), who are currently making a similar move and, with the ecosystem already quite settled, face many problems, both technical and cultural. Makes one wonder how large our time window to make such a change happen really is.
Thank you! This is flexible and powerful.
The crate hierarchy approach feels to me like it inherently has the same kinds of problems as class-based inheritance hierarchies. There will always be some way that someone wants to use it, but that doesn't match the existing organization. Rather than thinking in terms of possible combinations, I would hope this would enable thinking more in terms like traits or Javascript-like feature testing- there are *some* dependencies between parts of std, but those can be modeled directly and everything else is independent.
Many non-Windows systems can be configured not to use overcommit, and are routinely used that way in embedded systems.
I have... no idea :) I wrote the library as a hobby project. Basically it works by having a table of indexes which can (if they contain data) hold a block offset where the data is located.
Thanks!
I was hoping to trigger the bot with my induction variable.
Is [this](https://github.com/servo/servo/blob/master/components/script/docs/JS-Servos-only-GC.md) the post you are thinking of? Maybe /u/manishearth can weigh in?
Title seems a bit absurd considering the only other language their team was familiar with was ruby. Otherwise, good to see companies writing about production deployment of rust.
Wow, this is really cool! Thanks for posting.
This description is basically Scala as a language :) Tons of really cool features that are waaaay to easy to abuse.
I believe I got it to work using fold and an accumulator, actually. I used a (Option&lt;index&gt;, f64) type for the accumulator and just put a basic if-statement in the body. ```fold((None, f64::INFINITY), |acc, x| if x.1 &lt; acc.1 { x } else { acc }).0```
The title seems fair to me. They were coming from ruby, but needed the performance/memory characteristics of a compiled language. Weighing C/C++ against rust, they were more successful with rust. That success led them to be able to compete well in a space that is very resource constrained and has a low tolerance for service disruption. They felt that rust allowed them to differentiate their product meaningfully on that score.
None, Vim
I'm fairly certain that Linux will work without overcommit, though it will obviously OOM if you run out of memory. I also need to point out that you're misunderstanding /u/Rusky's use of the word embedded. I prefer to use the term "embedded" for any system which is unlikely to have the software updated or altered. It's a fixed subsystem. An oscilloscope might be running full Linux and be a complex system unto itself, but it's unlikely to ever receive an update, and it's treated as an appliance, not a computer. I prefer to use the term "deeply embedded" for microcontroller-based systems that don't run a full OS like Linux and have less than 1 or 2MB of RAM (at most).
&gt; I'm fairly certain that Linux will work without overcommit, It works, but one issue is that you can't fork then exec a new process if you don't have at least enough RAM to store the entire process space of your process free. So if you have 4GB of RAM, and a 3GB process which wants to spawn something tiny, you just can't. However, it will work if you have 4GB of swap just lying there, even if it's never actually touched, as long as Linux knows it *can* go to it if needed.
Isn't that metadata unstable and potentially incompatible for every compiler release (i.e. every six weeks)?
Eh... Some C functions do allocate memory (and it's made your responsibility, sometimes, to clean up after them). Though for the most part it's your job to give them a buffer. I quite like that idea, actually. If they need access to *multiple* allocations, you could pass them a large buffer, of which they could run their own alloc in to divide it up as needed. Or somehow tell an alloc function to go look in that memory chunk. If they run out of memory there, they could either return a OOM error somehow (return value, probably).
Some embedded systems are just Linux, and do have an allocator. Fork+exec absolutely do work without overcommit, they just use more RAM than strictly necessary. So all of those systems work, given the right hardware and system configuration!
yup!
&gt; Notice wc counts = as a word. (From the wc(1) man page) &gt; A word is a non-zero-length sequence of characters delimited by white space. You could also argue that if you're *ignoring* puncuation, you should also ignore it as seperators, so "same_line_if_else = true" is actually 5 words. `same`, `line`, `if`, `else`, `true`. I'm presuming things like \_\_word\_\_ parse as 1 word? 
Any JS/C++ libraries or functions that don't exist in Rust you could try to implement
Speaking strictly as someone who hasn't read any of rustdoc or rustc source in meaningful depth, the vague concept I have for how such a thing might eventually exist would be to make a compiler flow similar to what I *think* `cargo check` does: do just enough work to identify documented items, then pass the item signatures to rustdoc and exit the compiler. I may very well be wrong, but I feel like I have a vague memory of cargo check hitting all my code, even that which would evaporate under cfg, but I'll have to check again. As I understand it, though, rustdoc doesn't care if the code is semantically correct, just that it has enough syntax correctness to register items. I am presuming, from little concrete information, that any of this is more or less dependent on increasing the modularity of the compiler so that we can expand the functionality of various tools dependent on complex source construction like macros, build scripts, etc demand without having to add support for them in the compiler proper. I get the impression that adding a `target=doc` path in rustc proper that will do all the source generation and then not strip cfg attributes is probably not tenable in the long term, so Maybe this is something the pull-based compiler architecture concept might be able to handle more easily? Idk
As someone working in the niche you're proposing to leave out in the cold, I have to object. It should be absolutely possible to build a modular std, or (imo) better yet a replaceable std, that cuts down on the number of potholes when targeting something outside the mainstream. I get where you're coming from, and I think there's definitely only so far the official std should go to cover target-related differences, and a divergent enough target should have its own std rather than something cobbled together out of abstractions that don't fit it. That said, unused and/or unavailable target items (such as CloudABI's absent processes or environment) should be able to be cleanly disabled without requiring a std transplant or unimplemented! trap.
(we fly satellites running full Linux that are code-frozen and sometimes even Harvard architecture machines, for another example)
cargo check does not hit code that evaporates under cfg. rustdoc is built on top of the compiler so you don't need to loop it around anyway; you just need better querying support in the compiler for stuff like this.
Thanks! I know that using the space character was wrong, but I think it worked because usually the space character has the same height as the whole font. Do you know if rusttype exposes this `units_per_EM` in any way? Then I could use rusttype. I'll definitely take a look at the units_per_EM, thanks.
IMHO it would be great to leverage something like the marker traits system for the job. I can imagine an alloc "feature marker" on std::Box, which could be propagated up the composition chain automatically; cfg on all dependant structs/functions just would be a pain to maintain. Just a random thought...
I think most of the examples of where the "X depends on Y" model is insufficient are valid examples. To your question about what I'm arguing for, I suppose my point was simply that I like the clean model of expressing dependencies with crates. E.g., if you had a `core`, `alloc`, `net`, and `std`, where `alloc` and `net` each only depended on `core` and `std` depended on `alloc` and `net`, it'd be obvious what to do if your program only need allocation but not networking or functionality from `std`. But as you point out, there are cases that don't fit nicely into that model, so it makes the cleanliness argument moot.
Nice. Looks like it could be really useful. I'd second the thing on the naming though. Maybe, "CIGAR", "Class Interface Generator Abroad Rust" (meh).
If you're using Hyper 0.10 then you need to use mime_guess 1.8 so that the versions of the `mime` crate match.
Hi, I've started this project https://github.com/ivanceras/diwata and it is now really huge. The front-end is in elm (webassembly wasn't ideal at the time). The backend is in rust.
Oh my, those if actors are giving me lambda calculus flashbacks. Honestly, add a syntax to send and receive multiple values in a sequence in a single line, and I wouldn't mind using it.
*Option 3:* enum OrderKind { Buy, Sell } enum OrderPrice { Market, Limit(::std::ops::Range&lt;u32&gt;), } struct Order { kind: OrderKind, quantity: u32, price: OrderPrice, } Feel free to pick and choose ideas from that. The idea is: - Use an enum to represent buy/sell -- newtypes are great and self-documenting - For the price, instead of encoding whether a low, high, or both bounds were set, use the dedicated type for ranges of values, [`::std::ops::Range`](https://doc.rust-lang.org/std/ops/struct.Range.html) (more often seen in it's sugared syntax `0..5`). If only a higher limit is set, the low limit is `u32::MIN`. If only a lower limit is set, the high limit is `u32::MAX`. (I would suggest [`::std::ops::RangeInclusive`](https://doc.rust-lang.org/std/ops/struct.RangeInclusive.html) but that's still [nightly-only](https://github.com/rust-lang/rust/issues/28237#issuecomment-363374130).)
Did exactly this when I was in college. Attempted to rewrite the core utils in order to learn Rust. Did about a few and then got bored c:
The same error still remains
"you don't know SQL do you myrrlyn" I do not
For all intents and purposes, isn't cargo the compiler? 
The way I see it working in Rust is very explicit. Maybe its only similar in spirit. In Rust it should just be syntax sugar: ``` fn usage()(metrics: &amp;mut Metrics) { something("Hello", metrics); } fn something(param: &amp;str, metrics: &amp;mut Metrics) { // ... } ``` ``` fn usage()(metrics: &amp;mut Metrics) { something("Hello"); } fn something(param: &amp;str)(metrics: &amp;mut Metrics) { // ... } ```
I don't understand why it would. Can you post your Cargo.lock to a pastebin or a gist?
&gt; Use an enum to represent buy/sell -- newtypes are great and self-documenting Thank you, that what I do. But for the purpose of this example I omitted it. &gt; For the price, instead of encoding whether a low, high, or both bounds were set, use the dedicated type for ranges of values There is no range here. sorry I did not clarify enough. Stop price is a threshold when market reaches that threshold, the order becomes active becomes active as either a market order or a limit order. 
Ah, I wasn't familiar with the terminology. Having read a quick [what's-the-difference](https://www.investopedia.com/ask/answers/04/022704.asp) I think I'd go with enum PriceBound { Above(u32), Below(u32), } struct Order { kind: OrderKind, quantity: u32, stop: Option&lt;PriceBound&gt;, limit: Option&lt;PriceBound&gt;, } modulo naming. A stop-loss order would have `stop: Some(Below)`, a market price offer would have `limit: None`, etc. From what I can tell, a stop is essentially a trigger to wake an order up and tell it to start looking, and a limit a required price to buy/sell above/below. There's no reason to include the type when that's encoded in the `Option`-ness of the stop/limit bounds.
No? It's the project build manager, but rustc can certainly be invoked on its own or as part of a different build system. I use a Rust library inside a C project at work, so I have CMake invoking rustc directly, without Cargo's interventions or opinions.
Thank you for your reply. This looks good. there is no need of PriceBound here because depending on OrderKind you will only have one or the other way as valid. Which is similar to option 2 in my example, except that OrderType is derived from stop and limit. 
The linked article I looked at specifically notes that a selling stop order can be to stop loss (sell after hitting low) or to guarantee profit (sell after hitting high).
&gt; while the Java server could use up to 5GB of RAM, the comparable Rust server only used 50MB. I expect Java to use more memory, but how is this possible? Is it something inherent to java?
Yeah, that sounds fishy to me. Like maybe they're measuring VSZ instead of RSS.
&gt; you explicitely place these things in different pools to avoid them fragging eachother, and have ballparks in mind for how much relative space each will take). yea some sort of arena allocator, and if one arena its limited to one thread at a time, allocation would be pointer bump and deallocation making pointer "zero" (start of arena) regarding "ballparks of relative space" this is not really needed for virtual memory systems, just get few gigabytes of "virtual ram" (address space) for each arena, and materialize more of it when its needed (even ARM has memory virtualization these days) 
Not a huge release, but lots of small improvements. Compilation speeds should be much better with this release.
[Ask and ye shall receive!](https://gitlab.com/williamyaoh/shrinkwraprs/tags/v0.1.0) (I had already anticipated needing to split out the mutable traits and structured the code to make it easier, just wanted to wait until someone actually needed it to spend the effort)
Rust replacement for [hexo](https://hexo.io/)
A couple ways to improve performance in Intellij IDEA: * Turning off "Auto-display code completion" (Settings -&gt; Editor -&gt; General -&gt; Code Completion) * Tuning the VM memory settings (Help -&gt; Edit Custom VM Options...) Personally I've been using the 'Red' settings from this article [The One and Only Reason to Customize IntelliJ IDEA Memory Settings](https://dzone.com/articles/the-one-and-only-reason-to-customize-intellij-idea) Namely, -Xms1024m -Xmx4096m -XX:ReservedCodeCacheSize=1024m -XX:+UseCompressedOops The article's a couple years old and there's likely some new features or betterly optizmized settings but it's worth a shot! (Anyone with more knowledge/some settings that work well for them have anything to share?) Those two things sped up what used to be notrious keyboard lag on my machine. (This was a couple years back that it stuck out as an issue for me so ymmv in 2018) The one thing that sticks out today is the lag time for Rust language plugin's red squiggleys to update to reflect changes. They've got a great team working hard on the plugin so those wrinkles might be worked out some time soon? :) If anyone else has some good settings/tweaks/insight, please do share!
Oh yeah, we both agree. cargo by necessity needs to pass a flag to `rustc`. Therefore, adding a value to cargo for rustc to check, means any build system could use the same flag. The only thing that is a bit more tedious for you would be adding these flags to your CMake. But that sorta comes with the responsibility of using a non-cargo vend of crates, no?
I’m not familiar with rusttype but from a quick look at its documentation it appears to only provide "scaled" metrics where that conversion has already been done based on a [`Scale`](https://docs.rs/rusttype/0.3.0/rusttype/struct.Scale.html) that you provide. However, this is based on the size of the bounding box of ascent and descent rather than on the size of the EM square. (std_truetype, which rusttype is based on, provides both.) I think this is a limitation (arguably a bug) of rusttype.
Does [if_chain](https://docs.rs/if_chain/0.1.2/if_chain/) do what you want?
I think what /u/reddit_lmao said is that from a ruby perspective, Rust had a much better chance of winning than C/C++ and it would have been better if the company had a C/C++ perspective and still chose Rust. I do agree that a story by a company which was previously using C/C++ and now are using Rust would be stronger, as it would be a higher bar, but you can hardly blame tilde for having a ruby background. Many companies and people chose ruby, thinking the age of manually allocated memory and compiled languages was over, but they were all wrong, which is why ruby eventually failed and why many rubyists are now switching to Rust. You can't really blame them, one needs to try things out. Some things work out, others don't.
Well worth the switch to CLion (essentially same software) for debugging support. (See my comment below and [1](https://i.imgur.com/C8dCNvD.png) [2](https://i.imgur.com/niDSlzS.png) [3](https://i.imgur.com/cLAgEc5.png) ) :)
They don’t reserve ram, they commit it. If you want to spawn a process on Linux you need to fork first. Without overcommit this means copying a process. So if you have a big process that needs to spawn many little ones, you need to always copy the process first. That often results in absurds amounts of ram requirements since in Linux it’s forking processes all the way down. If you don’t want overcommit, Linux is pretty much the worst system to try to bolt that on because everything is designed to work under the assumption that overcommit is enabled. It’s not just forking processes. Testing whether malloc returns Null is irrelevant on Linux with overcommit (it never returns null even if you ran out of memory). Nothing is impossible, but doing this is just fighting against how the system is designed. On MacOSX you can disable overcommit as well, but then everything just stops working. To fix that you would need o fix Apples proprietary code which you can’t. You might be able to work around this as well but that’s going to be a harder fight than Linux. IMO it’s either Windows or an RTOS designed without overcommit. If you are using Linux on embedded enabling overcommit and trying to be careful is probably a better choice than fighting the system on every step you take.
I agree. But we need to be careful about the definition of “work”. You can disable overcommit on Linux, but you better be in charge of all the code running after pid 0. Most software running on Linux is not tested without overcommit and mistakes that are easy to make in C like not testing the result of malloc don’t show up if you have overcommit enabled. Also just because Linux is used on embedded does not mean that you need to disable overcommit. I think it’s better to have overcommit enabled and be careful, than to disable it and have to supervise all the code running in the system. You would be basically using Linux in the exact opposite way it was designed to work. If you control all the code, RTOSes don’t have overcommit and might be a better choice than Linux in this space. 
Yes exactly.
My perspective on this is different. Rust the language can be appreciated whichever way you want, it's really down to whatever the user is looking for. Where knowledge of C++ and C is useful is the application of Rust to real problems; especially for systems programming, it's kind of hard to integrate with what's already there without the knowledge. You might say Rust is in use in some webservers, but I don't feel that Rust is often the best solution in the webserver space.
As i read, they did a prototype with C++ and avoided it for lack of experience of their dev team. They could have chosen any strongly typed compiled language and probably would have the same benefits as rust. 
[Link to repo](https://github.com/rickyhan/rasta)
&gt; It isn't a failed language Just to be clear, I also think that C and C++ are dying. That thought is I imagine very popular amongst Ruby devs. All of C and C++ and Ruby's prime times are over. Only the death process is faster in Ruby than in C/C++. I'm not stupid, I don't think that Rust will replace both of the languages. But newer languages *like* kotlin, swift, etc will replace all of them, and maybe Rust will get a slice of the new market. It has great position.
Unless they have built identical software in java then it's a bit of a bull shit claim. 
I looked at the code, yes it looks like a spin loop, but I also found that there is a backoff strategy implemented when calling recv [here](https://github.com/crossbeam-rs/crossbeam-channel/blob/defc450b3b50aa89d347ba575d6058d51315a656/src/utils.rs#L22). I can't tell for sure because maybe I missed something looking through tho code rather quickly, but I did not really find where the backoff is actually used to block the current thread? I guess you could also open an issue and you'll probably get a better response.
It might be because it is a data processing program. Gigabytes of data flow through it. If you're relying on the garbage collector to identify single use objects fast enough to keep the memory at 50mb you might be disappointed. I worked on a competitor to skylight, our agent was in c++ for the same reason they built it in rust, but internally we also did processing. One of those processing tools was an industry standard tool written in Python and it frequently gathered over 10gbs of ram after which it would become slow. We ported it to Go, and it would never go over 100mb again. Same fishy improvement.
Very cool projects, that learns me a lot about blockchain and Rust :)
Taking a better look at the API both `FutureResult` and `ResultFuture` (and the associated function `future::result`) are poor names. `future::result` is just a factory function creating an immediately-complete future given a `Result`. The function should be called `from_result` or `immediate` or whatever, and the returned `Future` type something like `ImmediateFuture` (or, in the future [heh], just an opaque `impl Future`).
&gt; But, the point is that "embedded" means different things to different people. I agree, with "embedded" I was referring to bare metal systems, in which you typically don't have a system allocator. For you usage of embedded: &gt; any system which is unlikely to have the software updated or altered Of course you can use Linux there. But you are making two statements "one can disable overcommit on Linux" and "one can use Linux on embedded", which are both correct, but that does not imply that "Linux on embedded is always used with overcommit disabled" (for your definition of embedded). You haven't written that, but your comment reads like you imply it. This is wrong, e.g., Rasperry Pis use Linux, but with overcommit. In fact, Linux has 3 configuration options for overcommit: no overcommit, overcommit just enough so that anything works, and full overcommit (default). I agree that one can use Linux on embedded, but I stand by my previous statement that you cannot use Linux with overcommit disabled. 
You are not incorrect but the generate profit (or take profit) order is slightly different. Lets say you bought at 100 and market moved to 110 and you want to ensure you have at least 8 profit, then you can place a stop order at 108, sell stop order is alway below market buy stop order is always above market price. Generally the other side are very uncommon and called differently. Sell stop limit order at above market price is nothing but a normal limit order. There is also [If-Touched](https://www.investopedia.com/terms/m/marketiftouched.asp) orders which are not very common. 
Currently, the answer is "you don't". Rust does not have a stable ABI, which means that if you switch compiler version, you need the sources to rebuild. The only way around this is to provide a C interface to your library and only access it through this interface. Of course that comes with a lot of drawbacks - unsafe interface, limited to C-compatible types, and no folding of common dependencies.
&gt; I'd also like to contribute to a project implementing the lightning protocol. I thought that too, but couldn't find anything on Github. I started to implement BOLT-11 (locally, not ready to publish yet). Since /u/sellibitze pointed out that /u/thebluematt is already working on it, I'd really like to see the repo published, so everyone interested can contribute.
Thanks for trust-dns! I [just used its DNS protocol serializer/parser](https://fnordig.de/2018/02/07/d-oh-dns-over-https-in-rust/) to write a DNS-over-HTTPS client.
Nice! Now you just need to drop your effects [here](http://nwoeanhinnogaehr.github.io/ladspa.rs/ladspa/) and we can all play with them. :-) [Disclaimer: I haven't tried either your crate or the LADSPA crate linked above. Your mileage may vary. etc.]
You don't need LADSPA for others to play with them. Just a Jack client works for me. [LV2](http://lv2plug.in) was supposed to replace LADSPA anyway back when I was hacking more on this kind of thing. Not sure if that really happened. 
There is a very big difference between asserting that a language is on a downward trend, or asserting that it is "failed". In regards to Ruby, being on a downward trend is debatable, it might be true, but I suspect that it is not. On the other hand calling Ruby a "failed" language is quite offensive. You might not like it, but as a language and as a platform (Rails) it has significantly shaped and influenced the world of software over the last ten years or so. Ruby is a tremendously successful language which has much, much more production users and a much, much bigger ecosystem than Rust does. Without the influence of Ruby, I think it's pretty safe to say that Rust would not be where it is today. Many of the early influential adopters of Rust were Rubyists. Cargo is a major catalyst for Rust and it was first written by two Ruby programmers.
What do you mean by "exact value" here?
I want it to panic like so - `panic!("My error message")` - without all the Rust fluff.
Not possible unless you implement a custom trait with a custom method that does what you want.
What about helping us with https://github.com/martinlindhe/dustbox-rs We can find all kinds of issues you can do, that fits with whatever skill level you have. You can also help us kill our GTK dependency
&gt; And to fully gain from abandoning the facade (i.e., to remove the special magic used in `std` today), we would need to use an epoch boundary to fully remove `libcore`. My understanding of the epoch RFC was that we _weren't_ going to split the crates.io ecosystem into two incompatible parts. This was a long and difficult discussion, but I remember arguing that a full ecosystem split would be a nightmare for commercial Rust users, including my employer. Has this changed? Because if I understand correctly, the plan is to support mixed-epoch code, which means that everything will Just Work. But in order to accomplish this, the compiler may need to provide a `libcore` (even if it's just a fake collection of re-exports) so that crates using original-epoch Rust can still be compiled and can still be used in new-epoch Rust programs. If this proposal forces a full ecosystem split and a change in epoch policy (or if the epoch compatibility policy changed without commercial "friends of Rust" being consulted), then I'm strongly opposed. If this can be handled without an ecosystem split, then I _think_ it looks like a reasonable direction.
Can you make the syntax more similar to Prolog? 
sorry if this is a little off topic but "they're only more complicated when you don't maintain that discipline" is the same argument used with manual memory management in the C/Rust debates. I guess i'm trying to say that it sounds 'non-Rusty' to force discipline when there could be a solution where it could be automatically checked?
Absurd simplicity was intended - I didn't want to spend much time on this, and it's fun for me to figure out whether it's possible to implement anything with only these features. If I wanted this to be usable anywhere there's definitely stuff to improve - both in the actor system and overall ergonomics. But for now there are no such plans.
As mentioned you can use `libc` crate / `winapi` crate and so on to execute OS specific things. Insetad of `secMemClear` you can use [std::ptr::write_volatile](https://doc.rust-lang.org/beta/std/ptr/fn.write_volatile.html)
&gt; Not sure if that really happened. It did. Though most plugins actually use VST instead of LV2. 
Suddenly Terminator: Genisys flashback
&gt; however if I have to use two crates I would rather just learn what is required for me to implement things like this myself This attitude is un-idiomatic in Rust. (Please excuse my flippancy. That phrasing just came to me and it seemed too amusing to throw away.) As for your questions about `mlockall(2)` and forking, I'm not expert but my understanding is that, when it comes to FFI like this, you can use C functions on Rust memory by keeping the following details in mind: 1. Rust follows the same approach to C FFI as C++ does, so treat calling between C and Rust as you would calling between C and C++. (ie. Rust's runtime is as minimal as C's, but you have to annotate your FFI boundaries with things like `extern C` and `#[repr(C)]`.) 2. `rustc`'s default behaviour is to use jemalloc rather than the system allocator, so follow best practices for working with C/C++ code which makes use of custom allocators. 3. While Rust does use LLVM as its backend, it's always possible that it will exercise LLVM's optimizers in different ways, so, if you need certain output properties, apply whatever caution you'd normally use for working with a new and unfamiliar C compiler. 4. Read the [The Rustonomicon](https://doc.rust-lang.org/nightly/nomicon/) for further details on things like what invariants you must restore by the end of `unsafe` blocks and what you're allowed to assume about memory layouts and the like. &gt; Will it be valid to call mlockall from ffi to C from Rust to mlockall of the Rust memory? Given that, by design, a Rust `.so` won't interfere with a parent program that calls `setrlimit(2)` or `mlockall(2)`, a program written entirely in Rust shouldn't cause any problems unless jemalloc would break them in C programs too. (In which case, you can use the nightly compiler and opt for the system allocator while waiting for the APIs to selector allocators to stabilize.) &gt; Would I need to explicitly fork, by some mechanism, in Rust, to lose it? Rust won't fork unless you ask it to. (Rust is all about being predictable and explicit about operations that are expensive or otherwise highly consequential.) **TL;DR:** For FFI purposes, read [The Rustonomicon](https://doc.rust-lang.org/nightly/nomicon/) and treat Rust like C++ plus jemalloc and an unfamiliar set of optimizers.
Also see https://plv.mpi-sws.org/rustbelt/popl18/
The question was not serious, i am sorry. It was refering to Erlang's origins in Prolog.
And unfortunately VST2, not the "newer" (10 years old) VST3 which has a much nicer API.
Is there a rust toolchain for the esp8266?
&gt; Has this changed? It hasn't. &gt; the compiler may need to provide a libcore (even if it's just a fake collection of re-exports) That's the intention.
https://doc.rust-lang.org/stable/std/panic/fn.set_hook.html
I knew what I was signing up for with a CMake management, yeah
I think the parent was getting at you should use a crate if one's available. If an insufficient one is available, and you write your own and publish it that's good too! Since we have cargo, I think the parent was just worried you'd go the C route and write things yourself instead of using our excellent build / dependency system 
Any audio examples?
Awesome! I was thinking of doing this, great that it’s already happened!
What is an example where you could only use `transmute` and not pointer cast?
I would be interested
By the way, what's the point of `set_hook` instead of `catch_unwind`?
I can only speak for myself (/u/martin_lindhe has the last saying) but: We have two use cases the debug window and the "virtual screen rendering "; for the debug window we need something widget based, I'm thinking quickqml. For the screen rendering, I would like something detached from the ui framework such as Egl/gl es. But I'm open for ideas;) 
Strictly speaking, the code samples you provide have nothing to do with C in particular; they have to do with the operating system in which you're executing. You can still make all of these calls from Rust and they'll do exactly what you think (assuming LLVM and jemalloc don't cause interference), because a compiled artifact is a compiled artifact, regardless of source language. You can implement zeroing on drop with a new type wrapper around Vec&lt;u8&gt; or Box&lt;[u8]&gt; that just clobbers with ptr::volatile_write in Drop. This shouldn't, afaik, need an ordering or access barrier but there are mechanisms to talk directly to LLVM to establish fences in, I want to say, std::intrinsics? You can also toss in a ptr::volatile_read to check on it but I'm 95% sure that the volatile accesses have as part of their contract that they can never evaporate unless they're wholly statically unreachable. Other than that, swap integer return codes for Result and you're about set 
They do different things. `set_hook` changes the thing that happens when a panic is invoked, but the panic still gets thrown afterwards. `catch_unwind` actually catches the panic. I suggested `set_hook` because the author said they wanted to change what was printed to only their stuff. Maybe I should have suggested `catch_unwind`, dunno, it's early in the morning :)
What improved compilation speeds? Dropping LARLPOP?
Hooks don't recover from a panic, they just signal that one's about to happen. Catch unwind completely halts the unwinding behavior and resumes control flow. For instance, std wraps user main() in a catch_unwind so that it has time to cleanly tear down the program and do any final signaling before invoking exit().
Yes. I had a few people who were very unhappy about the compilation times after introducing that. I had wanted to experiment with the LALRPOP to potentially go back and replace the hand written parser I wrote at the beginning of the project for zone files. While it works well, the amount of time it added to compilation was significant. It might still be worth it for zone files in the Server, but not worth it in the Resolver for resolv.conf files.
The right thing to do is to use something like [quicli](https://github.com/killercup/quicli), which integrates with `Failure`, which would allow you to do something like: #[derive(Debug, Fail)] #[fail(display = "{}", msg)] struct MsgFail { msg: String } impl From&lt;&amp;str&gt; for MsgFail { fn from(msg: &amp;str) -&gt; MsgFail { MsgFail { msg: msg.into() } } } // main! is from quicli main!(|| Err("oh no!")? ) Which, while I probably didn't get the details right, is probably what you want. That said, while it is a Bad Idea, for quick-and-dirty scripts you can also define a `die!()` macro: /// like eprintln! but also exits with an error code macro_rules! die { ($fmt:expr) =&gt; ({eprint!(concat!($fmt, "\n")); ::std::process::exit(1)}); ($fmt:expr, $($arg:tt)*) =&gt; ({ eprint!(concat!($fmt, "\n"), $($arg)*); ::std::process::exit(1) }); } The downsides to this are: * Drops are not (guaranteed to?) run. This means that open files might not get flushed. The error message *will* always be printed because `eprintln!` self-flushes. * It's not composable, as soon as your scripts grow to be Real Programs you will be annoyed with yourself for not having just handled errors correctly from the beginning. That said, it's convenient.
Saw https://github.com/emosenkis/esp-rs a while ago but haven't tried it
I think you're now describing Arenas. The allocator is completely foreign to Rust proper -- pointers come in, Rust assumes they're clear to use and has at them. If you swapped out jemalloc for a malicious allocator that gave you pointers that overlapped or aimed into other processes or whatever, Ruat would still proceed space because they're just raw pointers about which Rust knows nothing. An Arena is, AIUI, a small heap that is managed entirely within your application, so you COULD do GC-like managing on it if you had the ability to know that your work was transparent to anything using the Arena
To get a better feel for it, there's also the Mozilla Hacks article that walks you through it. https://hacks.mozilla.org/2018/02/how-to-build-your-own-private-smart-home-with-a-raspberry-pi-and-mozillas-things-gateway/
No problem! It looks like it will have basically no maintenance overhead too: https://internals.rust-lang.org/t/a-vision-for-portability-in-rust/6719/3
I *think* the source code is inside the rlib? IIRC it is also used for span generation and error message emission if the span is inside the given library. Even if not, all generic code basically has its source code included. Rust isn't really a language that is good for libraries where you want to hide the source code, and it never will be.
My library [secstr](https://github.com/myfreeweb/secstr) provides a string/vector wrapper that `mlock`s its memory, zeroes on drop and compares without short circuiting (using `sodium_memzero` and `sodium_memcmp` if built with libsodium, otherwise pure Rust implementations).
Exactly. Thanks for clarifying.
That's what the whitepaper says they did.
Seems like you didn't actually read the article? They clearly state other properties which aren't provided by other strongly typed languages, like memory safety and preventing data races.
I would be interested as well! I wasn't even aware of the first meetup.
Spot the person who didn't read the article 
Ah, I see. Thanks!
``` error: linking with `cc` failed: exit code: 1 | = note: "cc" "-Wl,--as-needed" "-Wl,-z,noexecstack" "-m64" "-L" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust0.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust1.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust10.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust11.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust12.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust13.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust14.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust15.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust2.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust3.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust4.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust5.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust6.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust7.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust8.rcgu.o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.test_rust9.rcgu.o" "-o" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/test_rust-c901b969924db4e1.crate.allocator.rcgu.o" "-Wl,--gc-sections" "-pie" "-Wl,-z,relro,-z,now" "-nodefaultlibs" "-L" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps" "-L" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib" "-Wl,-Bstatic" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/libjack-4f40eba63c404624.rlib" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/libjack_sys-61cbea88dec3ece4.rlib" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/liblibloading-73194fc933e4564d.rlib" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/liblazy_static-8f9e0c12e97a2a43.rlib" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/libbitflags-d9077c45affafc32.rlib" "/home/niedzwiedzwo/Documents/petproject/test_rust/target/debug/deps/liblibc-b1ca85687f9f2272.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libstd-58a9e2944951d97f.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libpanic_unwind-167b5e977a2ab35c.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/liballoc_jemalloc-17deae1aa65c9467.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libunwind-b8892ba833aa6677.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/liballoc_system-17235785be0fea01.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/liblibc-2e9b8cf09e3563de.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/liballoc-d7195d5e94bc6586.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libstd_unicode-963f18f265d9b8d3.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libcore-48934815da5d2bfb.rlib" "/home/niedzwiedzwo/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libcompiler_builtins-a6b6fad6cc543169.rlib" "-Wl,-Bdynamic" "-l" "jack" "-l" "dl" "-l" "util" "-l" "util" "-l" "dl" "-l" "rt" "-l" "pthread" "-l" "pthread" "-l" "gcc_s" "-l" "c" "-l" "m" "-l" "rt" "-l" "pthread" "-l" "util" "-l" "util" = note: /usr/bin/ld: cannot find -ljack collect2: error: ld returned 1 exit status error: aborting due to previous error error: Could not compile `test_rust`. ``` couldn't get it to run to be honest...
Awesome :D Be aware that it is very much a work-in-progress
I'm definitely interested!
Not really, https://github.com/rust-lang/rfcs/issues/2003 is what I would like to do
I want to construct arithmetic terms and express comparison relations between them in the most natural way possible. For the terms, I created a box-recursive enum with all the necessary constructors, and implemented the `Add`, `Mul`, `Div`, etc. traits to overload the respective operators. So far, so good. The constraints seem to be somewhat more difficult. Ideally, `model.add_constraint(a == b)` would add the constraint `a == b` to my model. But this cannot work since the method of the `Eq` trait must return a bool. I have several other options (which have the advantage to work): creating a different method for each type of constraint (`model.add_constraint_eq(a, b)`), using a macro (`model.add_constraint(c!(a ;==; b))`), but I don't find any of them as neat as the first. Do you have other ideas to do that?
I guess my question is why do you have 250+ functions? You could easily reduce this to not need a match at all if you use one function called decodeid. Without that context I really can't recommend using a macro in this way. Normally you shouldn't match on numbers directly, but in things like enum variants. Is there someplace to see the rest of the code?
That looks like a code smell. Why not include the `mid` into the decode_id. In fact you did, so you need to deal with the mid in the decode_id function and do appropriate logic code with regards to `mid`. In my experience, there is no way to simplify this code into a macro. THIS ISN'T POSSIBLE: macro_rules! match_mid{ ($n: expr) =&gt; { { decode_id_($n)(); } } } fn main(){ match_mid!(01); match_mid!(02); // .... match_mid!(250); } and you might think that simplifying the 250 cases to match like these macro_rules! match_mid{ ($n: expr) =&gt; { $n =&gt; { decode_id_($n)(); } } } fn main(){ match { match_mid!(01), match_mid!(02), } } THAT ALSO WON'T WORK. I also wished this usage of macro is possible.
I'd rather try to contribute to the existing crates before writing my own from scratch. If you want a single crate that contains everything, make it depend on other crates and re-export what you want.
mid is the message id, and is basically the same thing as the number on the end of the decode method. i.e. decode_id_02 decodes mid 2 
1) I agree that it's totally code smell, which sucks and is why I'm trying to replace it 2) Hopefully I can find a soln to this using some kind of macro, because it DOES suck
[strum](https://docs.rs/strum/0.8.0/strum/) looks like it does what you want.
I added what the method signatures look like above
Thank you, this is precisely what I need.
How do you hope the TRust-DNS will be used? Every time I look at it I get the feeling that it's way too good to go unused. The market for DNS servers [is crowded](https://en.wikipedia.org/wiki/Comparison_of_DNS_server_software#Yaku-NS), and sysadmins may not have as much faith in Rust as we do here. But I'd love for Trust-DNS 1.0 to be a Debian package, and maybe some will decide that the de facto standard is not always the best option.
&gt; They don’t reserve ram, they commit it. Apologies for my imprecise terminology, this is what I meant. You don't have to actually do the copy when you fork, you just have to make sure you have the RAM to do so marked for your use. And with swap, it may not even be physical RAM anyway. Regardless, my point was about *embedded* use of Linux where you control everything and can just do all your forking from a small process to begin with. Again, this is done *routinely* and *must be supported*. &gt; Testing whether malloc returns Null is irrelevant on Linux with overcommit (it never returns null even if you ran out of memory) This is also false. You can also get NULL if you've fragmented or exhausted the address space. This does happen- for example, imagine a 32-bit browser allocating space for a very large image, or a server being DOSed via a very large request. Even with overcommit, this would be a reasonable place to use a fallible allocation API because you can handle it on the spot with a fallback rather than crashing. --- My point is that, while panic is a good default response to OOM *regardless of overcommit*, you also need a way to attempt a fallible allocation, *also regardless of overcommit.*
I think it's an excellent idea to try and list edge cases. I hope that diversity on the working group will naturally lead to consider many different platforms, and it would nice if it could produce such a document to help anyone understand how it works.
To expand: Cadence has an LLVM backend for their newest architecture, LX-7. That's one generation past what's in the ESP32. I doubt they'd do the work to support the previous generations unless some big-money customer insisted. It looks like they haven't upstreamed the LLVM backend, but if they did some motivated person could probably get it to work for the earlier architectures (I'm assuming they are similar). Thankfully, the recent multiple backend work in rustc would make it possible to add support for a target using a proprietary LLVM.
Really? you think rust is the only strongly typed language that provides memory safety and provides some guarantee to prevent data race?
Well, there is a reason it's idiomatic approach. :)
You said "any other strongly typed language". I know it's not the only one, but there are definitely not many languages which would fit their needs. 
You may be able to turn the functions into a function pointer array. Example: const FUNC: [fn(X) -&gt; Y; 250] = [ |a| foo(a), // id 1 |a| bar(a), // id 2 |a| { // id 3 baz(a) }, /* ... */ ]; Then, you can simply index into the array and call the function from there. Keeping track of which ID is which can get a bit wonky, but comments will help you.
Not a bad idea, but it almost feels like just moving the problem from point A to point B, doesn't really simplify the code :\ Thanks for the suggestion 
&gt; Arenas That seems somewhat like the pattern in petgraph and the defrag-rs allocator. Seems somewhat analogous to a user accessible MMU. * Dune * http://www.scs.stanford.edu/~dm/home/papers/belay:dune.pdf * http://dune.scs.stanford.edu/belay:dune_slides.pdf * https://github.com/project-dune/dune * https://arxiv.org/abs/1105.1815 
I mean, instead of writing out decode_id_N every time you can stuff the body of every single function inside this big array. Why not simplify what you have already instead of piling up even more code on top of it? Also, the closures will simplify to function pointers if they don't capture any data, which they can't by definition because they are in a constant.
They all return different types. So I can't have fn(x) -&gt; Y foo(a) and bar(a) return different structs because they're different message types
❤ 
The `stdsimd` release on crates.io right now is using the [platform independent types](https://github.com/rust-lang-nursery/stdsimd/blob/0.0.4/coresimd/src/x86/i586/sse2.rs) where as [master uses the platform dependent types, as outlined in the RFC](https://github.com/rust-lang-nursery/stdsimd/blob/master/coresimd/src/x86/i586/sse2.rs). I'm not sure there's any specific timeline for putting out another release. `stdsimd` has always been and has always intended to be a staging ground for experimentation. Eventually, as per the RFC, it will be exposed through `std` behind a feature gate. I suspect _that's_ the point at which you could start using this stuff in a reliable way (in the normal nightly sense of "reliable", anyway).
Will record it tonight. 
Did you install libjack-dev?
What experience I have with `stdsimd`/`coresimd` suggests that it's the right way to do things. I'm fully on board with this RFC.
I think this is a silly question, but: assuming I'm not interested in binary distributions, and the auto-vectorizer always works for me, is there any good reason to not enable avx (or whatever widest width registers) across the entire program? (That is, with my C++ code I usually just put -mtune=native -march=native in my Makefile to eek out all the performance I can.) I think it's pretty awesome to selectively write various-width optimized codes and runtime dispatch, but from the scientific domain, will I ever need to/should I ever think about that?
here's one suggestion for an extreme 'edge case' platform... C64 (this chap seems to have used a round-about route to use C++ on it) https://www.youtube.com/watch?v=zBkNBP00wJE&amp;t=761s :)
If you're just running locally, there's no real downside to `target-cpu=native`.
The changes proposed by this RFC will allow code to be written which *explicitly* uses SIMD. The SIMD autovectorizer will never capture *all* of the opportunities for vectorization that a program might offer. If you're just running the binary on your own computer, then there are no disadvantages to targeting "native", but this RFC still encompasses the SIMD intrinsics necessary to force the compiler to use SIMD, either in cases that you see it missed the chance, or in performance-critical sections of code where you need to *know* it will be vectorized every time you compile it.
That's what I thought :-) Thanks! 
The LADSPA crate should be working - if it doesn't please open an issue! (I am the author) It's true that LADSPA is rarely used anymore though. It's quite simplistic, so newer systems like LV2 are preferable most of the time. However it's still the most widely supported audio plugin system on Linux as far as I can tell.
Ohhh, I see, hm. It might be possible to make the functions return an enum variant, but that's a bit too verbose as well.
Well yes, I wrote that but I don't think you can do things like some range checking with procedural macros that I think you might be able to do with compiler plugins
That sounds like a great thing for someone to experiment with as a higher level abstraction, but the focus of this RFC is to provide the minimal possible thing to get access to vendor defined intrinsics. Any layer that you try to put between yourself and the vendor intrinsics is likely to make certain use cases harder or impossible. For example, if you have an AVX handle, then that handle must also provide SSE because there's a hierarchy. But how much SSE? For example, I have a CPU that has SSE4a but lacks SSSE3, SSE4, SSE4.1 and SSE4.2. I'm also vaguely aware of use cases where you specifically want to _disable_ use of certain target features, although I admittedly don't understand those use cases as well.
This is specious. First, we're talking about the implementation of std, the foundational library of the language - std contains significant amounts of unsafe code in which discipline is required already. Second, using separate crates carries real &amp; nontrivial costs because of orphan rules and public/private APIs that using a single crate would solve.
I wonder if this library to detect cpu features could be useful: https://github.com/google/cpu_features
As far as I can tell, it's the same stuff underneath.
I would be very interested!
&gt; I wrote that ...I should really read the usernames before I post.
Keep it going!!!
If the docs on docs.rs does not solve my problem I do some quick googling and on failure start reading the crate source. There are some projects / crates that has gitter channels too.
I don't think there is a tried-and-true way. In order, I would try: * Example shown in the README * Examples in the examples folder * Non-API documentation (website etc) * API docs - click around looking for the most promising sounding objects, look particularly at the constructors * Test suite (look for integration-style tests first) Reading source code is a last resort. Usually a signature (name + types) will tell you everything you need to know about a function. I clicked through the API docs for `select.rs` just to see what I could make of it. First I went to `Document` as it sounded promising. See that the constructor takes anything that implements `Read`, so to use it I guess you download some HTML or open a local file and feed it in. Then to use it, I checked out the `find` method, which takes anything implementing `Predicate`, which clicking through I see is implemented for various other types in the crate such as `Name`, `Element` etc. All these types can be constructed by the user. `document.find` gives us back a `Selection`, whose methods will give us a `Node`. And finally from a `Node` we can get the text (if any), amongst other things. Carrying on like this I can build up a reasonable idea of what the crate does and how to use it.
Umm... where's all those "disposition: merge" RFCs come from? Looking at TWIR 219 and comparing it to TWIR 220 it seems a lot of RFCs came out of nowhere and suddenly were approved. Smells fishy.
&gt; I mean, you couldn't really safely assume "I have AVX therefor I have SSSE3" [Is that true, though?](http://www.hpc.co.jp/images/Skylake-SP-AVX-512.png) It's not universally true, but there is a distinct hierarchy for a lot of the extensions. If you're going to say there *isn't* a hierarchy, then you're suggesting that `match` statement would execute **all** matching branches, rather than just executing one branch?
Thank you very much for the detailed response!
Totally newb question: I've got a large `Vec&lt;String&gt;`, and I need to parse the first 8-22 bytes of each string. Should I try to understand SIMD, will this be helping me speed this up?
Before the impl period, the RFC repo was pretty much ignored. The teams are coming back to them now, and many of them are good, so they're being worked on again. It's not just merged, it's also closed/postponed.
If the documentation isn't clear and there aren't examples one thing I've done before instead of diving into the source code is finding the crate on crates.io and clicking the `dependent crates` link. Then I'll check out the source code of other projects with that crate as a dependency and get a feel for how other people are using it. Otherwise just read source code and tests.
You can have something like Scala's implicit parameters solving the issue here without any extra allocations - it's just sugar around passing the function params.
Bonus points for loading CBZ archives (or even CBR).
Any ideas how to make tokio/futures code compile? [Playground link](http://play.integer32.com/?gist=c99ed9c25bbc839efa0ae542e31811f9)
Dumb question, do I use `RUSTFLAGS` to pass that to cargo? Can I set it in Cargo.toml?
I use `RUSTFLAGS`. There might be a way to set `RUSTFLAGS` via `Cargo.toml`, but I'm not great at keeping up with Cargo's new features. e.g., This is how I compile ripgrep for use on my own machines: https://github.com/BurntSushi/ripgrep/blob/master/compile (The `--features` flag is required to enable explicit SIMD support in parts of the code, but isn't generally required for the compiler to use all available features of your CPU during codegen.)
You're looking for /r/playrust
You're looking for /r/playrust
Congratulations! With all-new docs, even! :) I was a little confused about the version number, for some reason I expected this to be 0.2…
Short Answer: No, it's currently not beginner friendly and you'll probably gain more from parallelization with something like (rayon)[https://github.com/rayon-rs/rayon], or doing the parsing as you construct the Vec. Long answer: Other than autovectorization, SIMD in rust isn't stable yet and requires a nightly compiler and unsafe code. So it's not at all beginner friendly. UTF-8 compatible SIMD is particularly complicated. Though (faster)[https://github.com/AdamNiederer/faster] can help if the solution can be expressed in it's API. If your data is too large for cache then you'll have a lot of cache misses touching the start of each String, which will likely negate any gains from SIMD. Ideally you'd touch the memory for each string as little as possible, so doing the parsing when building the Vec, or some other process that touches everything could also help. When it comes to optimization, you need to benchmark constantly to see what really helps. It's usually better to start simple and use a profiler to figure where the real bottlenecks are.
Yes, that is why I'm calling it "Tokio reform" and not 0.1! But 0.2 is coming soon.
Sure, for things relating to security, I would agree that it's best to use a good crate if available. But I have a bad case of 'not invented here' syndrome. I try to avoid using other people's code as much as possible. I program for fun, so it's a learning experience to me. I think telling people to just use a crate if it's available is lazy.
Yep, makes sense. Btw, tiny little typo in the [docs](https://tokio.rs/docs/getting-started/hello-world/): The `tokio_io` in Cargo.toml should probably be `tokio-io`.
Thanks! Fix coming: https://github.com/tokio-rs/website/pull/127
You've got your link syntax switched around. It's [link label](url)
I really don't want to bikeshed but why is it called "cfg_feature_enabled" and not "cfg_target_feature_enabled"?
afaik you can't set it with Cargo.toml, but you *can* set it with cargo.config. And you can have both a global cargo.config in your home directory and project-specific cargo.configs.
Not in Cargo.toml, but it can be set in .cargo/config see the [docs](https://doc.rust-lang.org/cargo/reference/config.html)
Super curious how the ecosystem will move over to this. Default mainloops is generally also a move that happened on Python 3 recently.
Thanks
In the post, there’s a stable deref for Vec. Is it valid? When you push onto vector, elements are moved...
Its copied from the rental ecosystem which define stability only in terms of dereferencing to the target - if you only move the vector or dereference it to an `[T]`, the address will never move. I think the rules around StableDeref and Vec and String need to be thought about carefully, but fortunately this isn't relevant to generators.
The code is already written, I'm just trying to clean it up now
Yes, that makes sense! Am I corrrect that get_mut method on anchor from the previous post would still be unsafe? Otherwise, it seems to me tha one would be able to construct two anchors and than swap their contents?
your software isn't actually compatible with all versions that have ever been or ever will be, which is what the wildcard claims. the Cargo.lock file will generally mitigate this issue for binary projects, but wildcards are completely invalid for library projects.
This sounds really exciting! Any chance you could provide a short example of how this will affect how small async applications are written?
That's what I'm saying: abstract the stuff that's different between each arm (which is apparently just the number and the function name), and have the macro expand to the rest.
Yes, that method being unsafe is how Anchor can guarantee anything at all.
I just read through all the tokio docs and they look great! After reading, I found myself with a question though. Let's say I'm working on a tokio powered web server. In order to service some of the requests, I need to be able to read/write to files on disk. Is there an idiom or pattern I should be using for this? A similar question: if a response requires some intense computation (let's say, capped at 5 seconds), is there an idiom or pattern for that as well?
If anyone wants to play around with trying to break this system: https://play.rust-lang.org/?gist=5d3cd55fd84626f480633983cf408509&amp;version=stable. I had a bit of a hard time believing pin, so I stuck all the code in play.rust-lang.org, and it definitely functions! pretty amazing. Small things in the post, if you're interested: - I don't believe the definition for `Anchor::pinned` can be `T: ?Sized` since `Pin&lt;T&gt;` requires `T: Sized` - `Anchor::new` is defined as `pub fn new(ptr: ptr) -&gt; Anchor&lt;Ptr&gt; {` rather than `pub fn new(ptr: Ptr) -&gt; Anchor&lt;Ptr&gt; {` This topic is definitely interesting!
Similarly, searching on github for “external crate &lt;name&gt;” will often throw up something useful.
For the record, you can find my relevant comment [in the discussion of the previous post](https://www.reddit.com/r/rust/comments/7vbluj/asyncawait_iii_moving_forward_with_something/dtrihre/). I have to say though, that the credit goes to /u/desiringmachines for pushing for `unsafe_resume` and `Anchor`, which were *already* "I want to this function to get access to some data with a guarantee it won't ever be moved again" (just without a general borrowed variant). This is also as good a time as any to apologize for [introducing the idea of `?Move`](https://internals.rust-lang.org/t/potential-solution-for-types-that-shouldnt-be-moved-like-staticmutex/1663) (for a purpose that has since been obsoleted by /u/Amanieu's [`parking_lot`](https://docs.rs/parking_lot), no less), which isn't orthogonal enough (between types and operations with them) to "play nice" with other parts of the language/libraries, on top of all the problems `?Trait` brings. See also [this github comment](https://github.com/rust-lang/rfcs/pull/1858#issuecomment-363865041). I'm sorry and I'm excited we have a chance to finally put it behind us!
Am I missing something? I came here looking for Graydon Hoare's views of how software development practices terrify him, and all I found was what appears to be YouTube comments a la programmers.
Here's a hypothetical example of what a single request with hyper might look like: https://gist.github.com/seanmonstar/bb7162533e0fdeb37654ed7e204fac1b
You get that error because the `result` doesn't have a valid `Debug` impl: note: the method `unwrap` exists but the following trait bounds were not satisfied: futures::future::Either&lt;...&gt; : std::fmt::Debug`
Count me in.
Here's a hypothetical example of what a single request with hyper might look like: https://gist.github.com/seanmonstar/bb7162533e0fdeb37654ed7e204fac1b
The APIs in the post / library are probably not right, but you should be able to coerce an `Anchor&lt;&amp;mut Type&gt;` to an `Anchor&lt;&amp;mut Trait&gt;`, even if its pinned to the stack.
What prevents having all intrinsics implemented, having them polyfilled on non native platforms? So on ARM std::arch::x86::somessething will use a software implementation, while std::arch::neon::someneonthing will use LLVM intrinsics, &amp; vice versa on x86
Putting the decode traits on the struct is a really good idea! Also, thank you for taking the time to answer, I'll try it out!
Person-years of work? Also, the point of most of these intrinsics is to map directly to a particular CPU instruction. If we started doing software implementations, then instead of "hey I got a SIGILL because I tried running the wrong intrinsic" you'll just get a subtly slower software implementation, and you won't really know what's what until you inspect the Assembly.
duplicate reply
duplicate reply
&gt; Obviously, there is an upcoming breaking change with the futures 0.2 release, but after that, fundamental building blocks will aim to remain stable for at least a year. I hate to be *that guy*, but if it's going to remain relatively API stable for at least a year... why not use v1.0. Clearly, - This release is going to be maintained for an extended time frame - It's intended to be used by people for real projects, which is not the purpose of v0.x - `futures` *is already* widely used as is. If you need to release 2.0 in a year, release 2.0... but what's wrong with 1.0? It's just a number, after all, but it's nice to have numbers that seem more consistent with SemVer.
If the offending combinator is `Map` how come that using `future_a` directly compiles just fine? Eg. `let result = core.run(future_a);` Thanks!
Tokio has a public dependency on futures, so unfortunately, Tokio can't hit 1.0 until futures does.
I'm specifically talking about `futures`, not `tokio`, since `futures` was the one I interpreted as "remaining stable for at least a year" and having an upcoming breaking release.
&gt; if a response requires some intense computation (let's say, capped at 5 seconds), is there an idiom or pattern for that as well? Perhaps you could use [futures::select](https://docs.rs/futures/0.1.18/futures/future/trait.Future.html#method.select) with a [timer](https://docs.rs/tokio-timer/0.1.2/tokio_timer/) to limit how long the other future runs. I hope this helps.
Would you do something like `current_thread::run` to handle the incoming http request(s), and then something like `cool_future_thread_pool::run` to handle the intensive capped future?
It's the combination with `.select2()`, whose future returns Result&lt; Either&lt; (ItemA, FutureB), (FutureA, ResultB)&gt; &gt;, Either&lt; (ErrorA, FutureB), (FutureA, ErrorB) &gt; &gt; ( the result from the future that resolved first, along with the future that's not resolved yet) and `Result::unwrap()` requires the error type to be `Debug`, which means `FutureA` and `FutureB` both need to be `Debug`, and `Map` can't be if it contains a closure. However, with `future_a` alone, it just returns `Result&lt;ItemA, ErrorA&gt;`, there's no requirement on the future type itself.
That's the current goal of [faster](https://github.com/AdamNiederer/faster), which defines a set of operations which I can reasonably implement for all SIMD architectures. The main problem is SIMD algorithms are usually really slow if you can't do the operations in one or two cycles, because they exhibit less locality and often do extra work to map well to SIMD. It's better than crashing, of course, but I've seen some operations be 10-100x slower when compiled without SIMD.
My comment does not really claim otherwise, apart from suggesting that the developer needs to have access to all available SIMD intrinsics that apply to a given situation.
The answer in those questions is definitely a thread pool. Adding a guide covering this in more depth is a high priority additional to the guides though. As mentioned, currently the futures-cpupool crate provides thread pool needs for these sorts of CPU intensive / FS operations.
I should have mentioned in the blog post that the docs have been rewritten too :) Here is a hello world example: [code](https://github.com/tokio-rs/tokio/blob/master/examples/hello_world.rs) | [guide](https://tokio.rs/docs/getting-started/hello-world/) Here is a chat server example: [code](https://github.com/tokio-rs/tokio/blob/master/examples/chat.rs) | [guide](https://tokio.rs/docs/getting-started/chat/)
Wow. Elegant. I like it!
I organized the first one (rather badly) but haven't had time to set up a second one. I'd definitely be interest, though. I'm sure we can get some people together.
Look into using nom? Since you already HAVE the requisite parsing functions, have them all try to match mid and fail if t doesn't work, then use alt!() to run down the list Or, if possible, use generics to collapse how many variants you have, cuz, damn
I think there is definitely an opportunity for an LTS release at some point. Tokio just has to reach a point that feels mature to make that call.
&gt; (I'm assuming that std would use `#![deny(portability)]` or whatever) I know what you meant, but the idea of *denying* that a piece of code be portable makes me chuckle.
I use this in `.cargo/config`: [build] rustflags = "-C target-cpu=native" 
I believe that your questions are answered by the following RFC: https://github.com/rust-lang-nursery/futures-rfcs/blob/master/futures-02.md . TL;DR: futures intends to have a 1.0 release in 2018, the 0.2 release simply lays the groundwork for what they expect the 1.0 API will be.
Why not use `serde_json::to_string()` and `from_str()`?
**Nice whitepaper, Rust is gonna MOON!** **I'm bullish on Rust futures, now is a good entry point!** 
Nice work! Btw, I wish there was a PureScript-to-Rust compiler.. It would benefit both communities by cross-pollination, Rust could be to PureScript what C is to Python, and PureScript projects could make use of Rust's ecosystem of typesafe crates..
I'm curious, why did you choose lalrpop over pest?
Wrong subreddit. This one is about programming. You're looking for r/playrust
This series is escalating into awesomeness pretty fast... 😉👍❤🦀
Honestly, coming from javascript and frontend stuff, it might be best to go for Java or similar before trying rust. Rust is best for command line programs (for now), and there's no comprehensive out-of-the-box graphics setup. You could look at [Piston](https://github.com/PistonDevelopers/piston), a game engine for rust. I personally find it difficult, YMMV. I don't mean to discourage you from learning Rust. You could look at some of the projects trying to make GUIs more accessible like Piston, [Glutin](https://github.com/tomaka/glutin), or [Vulkano](https://github.com/vulkano-rs/vulkano). Rust [SDL2](https://github.com/Rust-SDL2/rust-sdl2/) also, but it's been quiet.
Dealing with PulseAudio + JACK is beyond my energy level. I also like to directly plug effects into Flowblade and the like.
Does it support arbitrary enums?
Being past 1.0 means that it is a stable language with a stable standard library, but its surrounding ecosystem still isn't mature and 1.0 in many areas (for example Diesel is the most popular library for using databases and only recently went 1.0). So that is what I'd say they mean by Rust not being ready; it is quite a different language from any that have come before, and it will take a while for people to work out the best ways of writing different kinds of programs that work with its paradigm.
I am literally squeeing; this is exciting! (all hail u/eddyb) Just a few notes: - impl&lt;Ptr: Deref&gt; Anchor&lt;Ptr&gt; { .. } impl&lt;Ptr: DerefMut&gt; Anchor&lt;Ptr&gt; { .. } Shouldn't these be `impl&lt;Ptr: StableDeref`/`StableDerefMut&gt;`? It looks like this is the case in the example crate. - Taking `self: Anchor&lt;Self&gt;` is taking the anchor by move, right? So by what method are we supposed to call `resume` the second time? - Why can't we just implement `Deref`/`DerefMut`/`StableDeref`/`StableDerefMut` for `&lt;'a, T&gt; Pin&lt;'a, T&gt;`? Then we could just always use `Anchor::new` instead of having `Anchor::pinned`.
Thanks for writing this up! &gt; lack of a decent IDE Have you tried IntelliJ with the Rust plugin? It's fantastic! Otherwise VSCode or even (neo)vim with RLS mostly works well enough &gt; memory management isn't a problem I really have at all That's a common sentiment with C++ programmers. And yes, if you are OK with potential crashes or worse lurking in your games, this may work out for you quite well. I hear a lot of AAA games are developed in this fashion... &gt; but that's easy to debug when the mistake even happens at all The problem is that it may not happen at all unless under specific circumstances and now you have an open security hole in your game. If you're lucky, the community will [use it to create awesome maps](http://lmgtfy.com/?q=StarCraft+Order+Buffer+Hack) instead of malware and cheats. &gt; most game libs had dependency errors or just didn't compile I sure hope you've filed bugs. ;-) &gt; it's too early. A statement I commonly hear in the Rust community which is strange being that it's already past 1.0. Java was "too early" for many users until Version 5, which was *eight years* after the 1.0 release. In most languages, ecosystems mature much slower than the core language and standards library. So, why would the statement be strange? Best of luck to you, no matter what language you end up using!
Rust's safety is not free... You just don't pay nearly as much as a traditional memory-safe language at runtime. I also agree that the language is young, and I'm not yet ready to wholeheartedly recommend it; the libraries are mostly unstable and there isn't a good editor story. However, I think you've made a common mistake in supposing that memory safety is Rust's only offer. Don't forget that ownership and borrowing, the same system that produces memory safety, **also prevents iterator invalidation and data races**. I'm not sure how good the thread sanitizers are, but I don't think they'll beat a compile-time check for completeness. Blow's language looks really interesting and I too am excited for the release. As it's purpose-built for games by a games programmer, I don't think anyone should be surprised if it's a better fit for games than Rust. Rust wasn't designed to build the next generation of games.
&gt; Shouldn't these be impl&lt;Ptr: StableDeref/StableDerefMut&gt;? Maybe. &gt; Taking self: Anchor&lt;Self&gt; is taking the anchor by move, right? So by what method are we supposed to call resume the second time? That's why it takes `self: Anchor&lt;&amp;mut Self&gt;`. &gt; Why can't we just implement Deref/DerefMut/StableDeref/StableDerefMut for &lt;'a, T&gt; Pin&lt;'a, T&gt;? Because it doesn't meet the requirements. If you move the pin you move the data it points to.
Looking at [this code](https://github.com/rickyhan/rasta/blob/master/src/effects/overdrive.rs#L24) * It seems to be using allocations (collect into Vec), does this not cause occasional underruns? * It seems - for weak signals - to not take `(2 * x)` but `(2 * abs(x))`, is this really expected? 
Would be much cooler if you linked to the thing you want to link to instead of an lmgtfy :)
in rust you exchange some down-the-road complexity for some upfront complexity. if your app crashing is not costly then it reduces rust's perceived value :p if you want to debug your app though you will have to face that complexity by yourself and there will be no borrow checker to assist you on the way. i'd say for sensitive projects rust is much less complex than c++. you want c++ for some very high performance projects, legacy/maintenance, some libs, or if you have a small team of very good c++ programmers. in all over cases i would use rust (for low footprint sensitive code, when crashes are costly).
Ah, explains more why Firefox needs it!
Thanks for responding. This an other replies really pointed out Rust's use cases to me. 
I've encountered a similar problem yesterday. The problem is that it leaves useless quotes around the string. I fixed it by using to_value().as_str()
&gt; I'm considering just skipping Rust [...] This is absolutely fine. There are a lot of languages out there that fill difference niches, and they're never all going to work for everyone. Just because a lot of people (including myself) are really excited about what Rust has to offer us, there's no rule that says _you_ have to value the same things. :) Perhaps one day more of the friction that bothers you in Rust will be mitigated. But there will never be a day when it's _all_ gone, because a lot of it (e.g. borrow-checking) is _"purposeful friction"_. Or perhaps one day you might come to value what Rust could offer some of your projects enough for you to put up with that friction, especially if you end up working on things like: - Apps/games that operate over a network or accept other user input, and you _really_ want to avoid having that user input allow for remote code execution or crash exploits. - Projects with multiple members/contributors, where you want it to be easier for everyone to understand the structure of the project and contribute code without accidentally violating assumptions made elsewhere. - Apps/tools that benefit from using a lot of third party code (painful in C++ unless your particular OS has the exact version you want, packaged with the exact options you want, etc.), but that also need to be fast (ruling out Ruby, etc. that do have decent package management). - High-performance apps with concurrency, where it's inconvenient to have it crash every day or so. (Concurrency is hard; unfortunately pretty much everyone doing it either builds something way-slower-than-optimal, or just plain wrong that will coincidentally work _most_ of the time.) - Anything that requires powerful metaprogramming where you'd like legible error messages. - ??? Other use cases I haven't yet personally come across ??? It sounds like you've thought about it a fair bit, and you're not really interested in the main selling points of Rust. It's only worth prioritising what works for other people if your work is actually likely to _affect_ other people. So if C++ works well for you, then especially if you're developing stuff solo that isn't really high-stakes, then you should absolutely go with what works for you. At the end of the day, that's all that really matters. 
Replace `.unwrap()` with e g `.ok().expect("Oh no!")`
Shouldn't filesystem handling be included out of box in tokio? I mean, libuv does filesystem I/O too (using a thread pool)...
Check out [this section](https://hacks.mozilla.org/2018/01/oxidizing-source-maps-with-rust-and-webassembly/#code-size) of Nick Fitzgerald's article about writing a performance-sensitive wasm application in rust.
Thanks for your answer :) Beginner-friendly isn't important, I'd not be averse to digging in if gains are to be had. I have ideas about parallelizing my code, but I though SIMD is kind of orthogonal to that? So, the scenario is something like several millions of `String`s in a `Vec`, pretty much all of them smaller than 82 bytes (I could better that estimate, too), and I only need to look at the first some bytes (often it's only the first, also often its 8 of them, sometimes up to 22 though). UTF-8 is not involved. I'll check out faster, thanks!
I think the way to make a stack generator callable is.. let gen = fn_returning_gen(); let mut pinned = pin(gen); let mut anchor = Anchor::pinned(&amp;mut pinned); StaticGenerator::static_resume(anchor.as_mut()); StaticGenerator::static_resume(anchor.as_mut()); then, right? This doesn't work with the crate as published (0.0.0), because `Anchor::as_mut`/`Anchor::as_ref` both require the `Stable` version of `Deref`. In the blog post version these just require `Deref`, and it works with that. I guess that's what was throwing me, is the assumption that `as_ref`/`as_mut` should be bounded on the `Stable` deref traits. Now I think I understand that though `Anchor.ptr` being stable to dereference is part of the struct's invariants, that's not part of the requirement of the `Ptr` type; rather, that invariant is upheld by only allowing construction from `Stable` dereferencing or from a pinned `Pin`. But doesn't this create a hole on `Anchor::as_mut`? `StableDerefMut` is in no way required; we require `StableDeref` when building from `Anchor::new` but `Anchor::as_mut` only requires `DerefMut` and not `StableDerefMut`. I think `StableDeref` will have to imply that both `Deref` and `DerefMut` are stable if `as_mut` is bound by `Ptr: DerefMut`. --- My question about implementing the traits on `&lt;'a, T&gt; Pin&lt;'a, T&gt;` was ill-phrased, I intended it to be more "why can't the `Pin` contain the data pre-pinned (i.e. &amp;'a mut T)" in which case `StableDeref`/`Mut` should be sound to implement for it?
I would like to do something like that. Hopefully soon.
Tower is the `Service` trait and above. tokio-proto was to take a socket and impl `Service`. Hopefully that clarifies a little bit.
♥
To overly simplify my current understanding, in order to help me organize my own thoughts: - `&lt;Ptr&gt; Anchor&lt;Ptr&gt;` takes a pointer-like struct and guarantees that you cannot move out of that pointer - `&lt;T&gt; &amp;'a Pin&lt;'a, T&gt;` is a pointer to data owned over the lifetime `'a` - `&lt;'a, T&gt; Pin&lt;'a, T&gt;` is an opaque owned data type (so nobody but `anchor_experiment::mem` can mess with it) and is effectively meaningless except to enable the above formulation
Thanks! This seems like an informative article to read on quick glance. Seems I should try snip and opt before drawing conclusions.
&gt; This doesn't work with the crate as published (0.0.0), because Anchor::as_mut/Anchor::as_ref both require the Stable version of Deref. In the blog post version these just require Deref, and it works with that. You shouldn't need to call as_mut because `Anchor::pinned` returns an `Anchor&lt;&amp;mut T&gt;`.
What is the 'auto' keyword? The new update of Intelij Rust sais "Support auto keyword in trait declaration". But I cannot find anything on google about the 'auto' keyword on Rust, the [Rust Book](https://doc.rust-lang.org/book/second-edition/appendix-01-keywords.html#keywords-currently-in-use) does not mention the 'auto' keyword. There are some blog posts about 'auto traits' and discussions about how to change the syntax from 'impl trait for...{}' to 'auto trait Foo{}' but I couldn't find an explanation of what 'auto traits' actually are or what the 'auto' keyword does.
So I have a `Anchor&lt;&amp;mut T&gt;`. At least currently, with `this` rather than `self` glue, `StaticGenerator::static_resume` takes that `Anchor&lt;&amp;mut T&gt;` by value, and therefor I no longer have it. So what do I do now? [[play](https://play.rust-lang.org/?gist=9362c6841df0315e37e31f8123673945&amp;version=stable)]
There have been a bunch of good responses. I'll add that since you don't seem to have an immediate need for using the language, you should check back this Fall when the epoch release (Rust 2018) comes out. There's a bunch of plans and mostly completed features designed to make the language easier to use and it looks like the release of most of them will be coordinated for that release. I expect a lot of library work/polish/documentation will land around then as well since there's going to be a marketing/outreach push around it.
Just to clarify: iterator invalidation and data races are a violation of memory safety.
FWIW you can do something similar with `concat_idents!` (nightly only): https://play.rust-lang.org/?gist=30c8d17bd49cc650790a1dee465b8d5a&amp;version=nightly
SIMD is orthogonal, but it only really shines when you're compute bound and the description of the data structure sounds like it's going to be memory bound (it really depends on what exactly you're trying to do). So you'll likely get more performance out of developer time focusing on cache/memory efficiency. With most higher end CPUs, you need multiple cores to tap into all the memory bandwidth, so that should be an easier path to more performance. The main issue is as that as you work through the vec of strings each string will point to a different place in memory and the hardware prefetcher can't predict that, so you'll have a cache (and probably tlb) miss for every single string, each of those is 10's to 100's of cycles of CPU time (and a modern cpu can do multiple instructions per cycle). Now if a good majority of strings only need the first 8 bytes, it would likely be better to just make a Vec of n * 8 bytes (or Vec&lt;[u8;8]&gt;) with the first 8 bytes of each string (padded up if needed), one after each other in contiguous memory. With that you'll get eight strings per cache line (instead of one), and the prefetcher will quickly figure out that you're scanning memory. So it should go much faster. And now SIMD optimizations could show huge gains, since it is likely now compute bound. Of course it's more complicated data structure, and if/how effective this kind of strategy is depend on what you're doing. The cost to build the Vec of the first 8 bytes, may be more than the problem you're trying to solve to begin with. Also if you're not messing with UTF-8 then using Vec&lt;u8&gt; will likely be better than String from a performance standpoint, since Strings have some overhead in some functions from dealing with UTF-8. If you want a ton of homework I consider [What every programmer should know about memory](https://lwn.net/Articles/250967/) to pretty much be required reading for anyone doing performance work. Since memory is most often the bottleneck on modern hardware. It's over ten years old, but the majority of it is still valid today.
Thanks for you explanation :) I'll note this down for investigation, but for now my assumption is that building up that `Vec&lt;[u8;8]&gt;` is too expensive for what I want (I have a similar thing in mind to try some time down the road though, and then it'll be easy to give this a shot). Thanks for that link!
Pretty much this. Most of the time, the "getting started" examples only show the very most basic usage of the crate, and even if the API documentation is complete it's often difficult to find anything in it regardless unless you're already familiar with the crate's API terminology. Poking around in the source code (of the crate itself, or others using it), you can get a clearer idea what exists in the crate and how it's tied together.
I was at the first one, I'd come to any other too.
This isn't affected by [the `josephine` problem](https://internals.rust-lang.org/t/rust-1-20-caused-pinning-to-become-incorrect/6695), right? Josephine was relying on a relationship between memory being freed and destructors having been run for pinned objects (I don't fully understand it), whereas this is only relying on pinned objects not being movable, which *does* hold, IINM? (To pre-empt anyone else who might jump to assuming that others are laboring under an obvious misconception: no, this isn't just about `mem::forget` being a safe function. It's a bit more subtle than that.)
I'd be interested in seeing a lot of C come over to Rust, but it's a journey I think only a large corp or incredibly dedicated group or individual could undertake.
Same here, missed the first Meetup and was wondering if there would be another. 
That's not so simple, on the Linux side at least. Regular file operations will happily block even if you ask them not to, and even AIO operations aren't really asynchronous at all times. More info on [LWN](https://lwn.net/Articles/724198/). That's why using threadpools for IO is not always such a bad idea.
But that's what I mean. A tokio-fs crate that exposes a nice futures-based API but behind the scenes uses a threadpool.
Are you compiling in `--release` mode?
No, how would I do that? The 8 seconds is only counting the execution btw.
`cargo run --release`. The default build mode is debug, which doesn't enable most optimizations and adds a whole lot of debugging machinery to numeric operations.
Ok now I am down to 430ms, still quite a bit slower.
You're probably measuring cargo too. Do `cargo build --release` followed by running the binary in `target/release/` directly.
How do we get the default reactor? How to cope with libraries still expecting a Handle?
Down to 275ms now, we're getting there ;p
I don't know if the "num" crate is efficient for big number computation. You may want to try "rust-gmp" crate since it is based on the gmp library : the reference for speed on big numbers.
Since Linux 4.14, preadv2 takes a flag RWF_NOWAIT which makes the call succeed if it won't block, and fail with EAGAIN otherwise. So you can at least opportunistically try to read in the main loop, and only schedule to the thread pool if it would block. I think this should be possible the way `Future::poll` is arranged.
The `BigInt` interface is unfortunately quite clumsy to use, and tends to produce inefficient code unless you're careful. For instance, doing result = (result * &amp;base) % modulus will allocate two new `BigInt`s and drop two `BigInt`s, with all the associated overhead. Doing: result *= &amp;base; result %= modulus; should reuse existing memory instead. I say "should", because last time I used `BigInt` I noticed that many of these operations didn't actually exist yet. 
I really dislike the `std::vendor` name. "Vendor" is such a generic English word. Vendor of what? Your CPU? Your OS? Your programming environment? Your car? It's not obvious from the name. I mean, we have `std::os` for OS-specific low-level things. For the same reason that it is called `std::os` and not `std::vendor` or something else similarly generic, I think the name of the new intrinsics module should be changed. It would make sense for this to be named something related to the CPU ISA: - `std::cpu` - `std::isa` - `std::arch` - `std::intrin` all seem like good names to me. But please not `std::vendor`.
If you are ok with nightly, you can also try [ramp](https://github.com/Aatch/ramp).
I notice that in the Rust version of point_mul, you're generating a binary string representation of d and iterating through each character doing string-comparisons instead of using bit-shifting. I imagine that would be much slower.
I'd fix these first at least, in addition what others mentioned: * Formatting `d` as a string to get powers of two doesn't feel the best solution and feels like it introduces some unnecessary overhead. Couldn't you use shifting here like in python implementation? * Parsing P from a string on each call to `point_add`. Consider using lazy_static or compute it at main and pass it as reference. 
Looking at the code, nothing obviously wrong - so was pretty sure that the problem was going to be doing an allocation inside a hot loop. The only hot loop is the `while` loop inside `powm`, so that seems a possible candidate. But then I ran Valgrind (`valgrind ./target/release/sep256k1`) to check out the allocations and... it threw up a bunch of illegal reads inside `num::bigint` then segfaulted. Suspicious.
&gt; Calling something that's NULL springs a seg fault but that's easy to debug when the mistake even happens at all. It is not easy when it happens on someone else computer, happens rarely or happens in a library that someone else wrote. Or when where there are many places that could set it to null. Or when the logic for setting null is complicated. Or when null does not segfault but generates wrong result. Or in a million other conditions. There is a reason many new languages use Option type - it solves a big problem. &gt; memory management isn't a problem I really have at all In that case you simply don't need Rust. &gt; I much prefer C++'s way of allocating memory and destroying it when needed I much prefer if programs I am using don't crash randomly, destroy my data or expose them to anyone. &gt; if I should try Rust again due to all the hype in the programming community Hype ... is not a very good reason to use anything at all. 
Handle::default() gives you access to the global handle
I'm still completely lost with Tokio, even more after the reform. Just trying to accept clients connections using Hyper and Tokio 0.1 , bu all I get is "cannot call `execute` unless the thread is already in the context of a call to `run`". https://github.com/jedisct1/rust-doh/blob/5a2529f4af605996c1880d050e6b56f1fd714e50/src/main.rs#L187-L195 Why? There is a call to `run` at the line right below. I guess I still don't grasp the concepts of reactors, executors and tasks and threads. I would love to have just a `spawn` function that accepts something that doesn't have to be `Send` that schedules on the current thread, a `spawn_send` function that requires `Send` and balances across available CPU cores. And nothing else. Not having to worry about reactors and stuff would be way more accessible.
Naive question here, but can't you just use u64?
That should really be called Handle::current()
[This](https://doc.rust-lang.org/unstable-book/language-features/optin-builtin-traits.html) i think
Currently, many game libs have a OpenGL 3.2 dependency, while due to a bug in Mesa my OS only has OpenGL 3.0 support. It sucks. 
Ah, ok, this makes sense. Thank you.
&gt; can't you just use u64 I'll need to shift up to 256bits so 64 aren't enough &gt; Also UBigInt has left shift implemented I am going to give that a try. &gt; Also there is UBigInt::modpow that looks like it does what powm does. I tried that and it doesn't work because there are negative numbers involved. For example q.x - p.x
&gt; Calling something that's NULL springs a seg fault but that's easy to debug when the mistake even happens at all. The problem Rust tries to solve is not a segfault taking down your mockup on your local computer, it is a segfault taking down half Google's datacenters, your rocket's fuel controller, or the safety functions of a nuclear reactor. Even if it is a game someone else is playing on his PC, a segfault doesn't result in him saying "Oh, let's debug this game". It result in a very angry rant online and a 1-star review. If you're someone who prefers writing quick mockups for games over tedious work to prove the compiler that your code will crash in no circumstance, then sorry, but Rust is not made for you.
You'd think so, but it actually forwards to the implementation of `&amp;result * &amp;base`, and then drops `result`. [Source is here](https://docs.rs/num-bigint/0.1.42/src/num_bigint/macros.rs.html#66-74). 
Thanks for answer. Another question - If you are only doing light simple processing on the incoming requests, would it make sense to just do everything on the current thread? Or does it make sense to dispatch all route handling to a cpu pool regardless. (If everything is done on the current thread, how would we leverage multiple cores? By running an instance per core of the server?) Thanks!
If you actually care about getting a fast implementation, and not about comparing two languages then you should switch to some form of projective coordinates so you only divide once per point-multiplication instead of once per point-addition.
Doing a lot of string parsing vs. just doing math. A big difference.
Looks like shifting is much more efficient in Rust. Changing the loop in `point_mul` to: for i in 0..32 { if d &amp; (1&lt;&lt;i) != 0 { makes it run in 38ms for me (original was 250ms) 
&gt; I know that this is bad but I couldn't figure out how to make a BigInt constant so I did the same with Python for fair comparison. I am going to look into lazy_static To clarify, I don't think you are doing the same thing in the Python code you posted. Whether the bigint is a global constant or not is somewhat orthogonal. The key is that in one program (Rust) you are parsing a string to extract a bigint while in the other program you are not. Even if you can't figure out how to use a global constant in Rust here, you could pass that bigint in as a parameter to the function.
Hype is why i know Rust exists. If so many people find it useful for them, I'd want to see if why and if it suits me.
But this is not the same than the old code, there is the point_add call after the if: for (i, digit) in binary.chars().rev().enumerate() { if digit == '1' { q = match q { None =&gt; Some(n.clone()), Some(i) =&gt; Some(point_add(&amp;i, &amp;n)) } } ---&gt;n = point_add(&amp;n, &amp;n); } your for loop should go to 256, but it will overflow the 1&lt;&lt;i so you should do something like : for i in 0..256 { if d &amp; (1_u32.checked_shl(i).unwrap_or(0)) != 0 { q = match q { None =&gt; Some(n.clone()), Some(i) =&gt; Some(point_add(&amp;i, &amp;n)) } } n = point_add(&amp;n, &amp;n); }
Done
Thanks for the thoughtful response. Rust seems very well made to make fast and secure systems where the friction would be needed. For games that's not really what I need, but I'll likely keep coming back to Rust and might use it when such a case comes up!
Literally the first search result for “rust gmp”: https://crates.io/crates/rust-gmp. Why would you state something so authoritatively without doing even the most basic research?
Yeah, my bad. Quick google search reveals lots of complaints on `num` performance so that is probably the bottleneck. Looks like your best bet could be to look at crates such as `rust-gmp`.
Dumb question, why `pinned()` and not `Pin::new()`?
I just tried that and the performance is about the same. Code is [here](https://gist.github.com/mcdallas/d623779fa72c12823bf6fc1d2166dadd)
Wow. Sounds fun ... What is your PhD on, if I may ask?
The program is essentially two nested loops, with 256 iterations of each. So the inner loop (the `while`) is executed 65k times, and that is where the slowdown is occurring. It is the outer loop is where a new `P` is parsed each time. So this is unlikely to be the bottleneck. (I know you probably know this).
I'm not quite sure what is happening here, but I checked hyper 0.11 still depends on the old tokio-core, so there's probably a mismatch of the 2 tokio's involved here.
Changing 'if &amp;exp % &amp;two == one' to 'if exp.is_odd()' gives you 40-50ms
(I'm super excited about this! But...) Is this working because of (currently?) undocumented Rust behavior? I get that the lifetime ensures the struct will always exist in memory, but do we know 100% that no move could occur - or might occur via some future change to Rust or perhaps underlying LLVM behavior/optimization? I'm hoping this behavior is etched in stone somewhere (as much as it can be?) and I just didn't find it. Thanks! 
It might be a good reason to learn about existence and purpose of things, but hype alone is never a good reason to spend time on using them. I've learned about Rust through hype, but I am using it because it solves problems I have better than anything else. There are many other tools hyped far more that do not, so I have no reason to use them.
Ouch. So we now have a fragmented ecosystem where crates using tokio are incompatible with each other? My understanding is that @carllerche tried to avoid this :(
You are frustrated because you don't know the language yet. It is harder to learn than most languages but once you do, Rust is very much a joy to use.
Ramp maintainer here. Glad to see someone using ramp! FYI we already have a [modular exponentiation function](https://docs.rs/ramp/0.3.10/ramp/int/struct.Int.html#method.pow_mod), although it's pretty much the same as the one you have in your code.
Awesome to hear that!
Non-Mobile link: https://en.wikipedia.org/wiki/Photosynthetic_reaction_centre *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^146552
Good bot
Yup good point!
Yes, I have the same doubt. At this moment, it seems like `Pin`'s behaviour depends heavily on it's internal field `&amp;'a mut &amp;'a ()`. However, this would break encapsulation, since inner implementation details can determine whether an outside use of this struct compiles or not, while that should not be the case.
Since you're looking at using Rust for games, I wonder if you've seen [this thread](https://www.reddit.com/r/rust/comments/78bowa/hey_this_is_kyren_from_chucklefish_we_make_and/?st=JDEMXV7V&amp;sh=2d466131) before? It's an AMA with one of the developers from Chucklefish, the makers of Stardew Valley, who are creating their next game in Rust. The short version is that they are quite happy with Rust, but the thread has lots of cool tidbits on using Rust for games :)
Hey, thank you for replying. Just to be sure, let me say that I'm not the OP. &gt; FYI we already have a modular exponentiation function, although it's pretty much the same as the one you have in your code. I'm actually using your `pow_mod` function, I just forgot to delete OP's implementation. ;) &gt;Also you are doing a from_str_radix operation in your point addition code. This does a conversion every time you call point_add. Instead, you should have the constant as a lazy_static so that the conversion only runs once at the beginning. I had a version with lazy_static which (maybe) surprisingly didn't impact runtime performance that significantly. I left it out to be closer to OP's original code. 
If you're looking for a python alternative that is really fast but allows you to write high level looking code I would take a look at [Julia](https://julialang.org/) Here is the equivalent code int_add(p, q) P = big"0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F" px, py = p qx, qy = q if p == q lam = (3 * px * px) * pow(2 * py % P, P - 2, P) else lam = pow(qx - px, P - 2, P) * (qy - py) % P end rx = lam^2 - px -qx ry = lam * (px - rx) - py rx % P, ry % P end function point_mul(p, d) n = p q = Nullable{Tuple{BigInt, BigInt}}() for i ∈ 0:255 if (d &amp; (1 &lt;&lt; i)) == 1 if isnull(q) q = Nullable(n) else q = point_add(q, n) end end end get(q) end function main() G = big"0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798", big"0x483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8" x, y = point_mul(G, 125) println(x) println(y) end @time main() Time produced the following output: 0.000153 seconds (57 allocations: 214.875 KiB) That is 1.53milliseconds and beats them both while still looking as clean as the Python in my opinion 
This will never end...
So having spent the day moving tokio-imap to new tokio, here's some thoughts: * Really want some client docs; not least, what's the new version of the old `core.run(foo)` idiom? * How to tie new tokio into old tokio, in particular, how to get an old-style `Handle` from the new default event loop From limited testing, it mostly seems to work well.
Well, the question is how much fragmentation :). Also, there is an opportunity to update tokio-core to use tokio. If that is done then it should help smooth the transition. The question is if it will be worth the effort. 
Could you file an issue with your thoughts?
I've added `lazy_static` and the version using ramp is much faster compared to the python version (benchmark on Debian 9 using `hyperfine` with 50 warmup runs): - python2: `Time (mean ± σ): 43.6 ms ± 0.4 ms [User: 39.8 ms, System: 0.4 ms]` - python3: `Time (mean ± σ): 54.1 ms ± 0.3 ms [User: 50.8 ms, System: 1.2 ms]` - rust (newest nightly): `Time (mean ± σ): 29.7 ms ± 0.9 ms [User: 28.1 ms, System: 0.0 ms]` Source: https://gist.github.com/anonymous/31c91a6441ae8e87f508bedff4a0d624 
In my experience, people saying they don't need Rust because they don't have memory management problems are people who just don't realize the issues they actually have. Maybe their scripts only run for a short time and they rely on the OS to clean up the mess their program left behind. Maybe the race conditions in their programs are just not visible on the machines they run it on. Even just saying "accessing NULL is not an issue cause there will be a segfault" indicates that you apparently do not realize how serious these issues can be. Yes, when it's NULL, it will be a segfault, but that's actually the best case you could hope for. And if you were writing a program that is supposed to run for a while, segfaulting now and then is the last thing you'd want. But I realize how nice it was to be so blissfully unaware of all the time-bombs in my code before I learned some lessons the hard way. So please, by all means, do your thing in C/C++ and when it finally blew up in your face a couple of times, you may actually realize that Rust takes a lot of worries off your mind and allows you to concentrate on other things. Or maybe you are the one in a million programmer that just naturally navigates his way around memory and parallelism issues. Whatever it turns out to be, I hope the project where you stumble over these issues is not an important one.
The current implementations of multiplication and division need to keep access to the original digits while computing the result. Thus we operate by reference, since we have to allocate a new result anyway. If you know how to do more in place, I'd welcome PRs!
&gt; There's only one game that's been released in Rust and the developer still thinks it's too early. A statement I commonly hear in the Rust community which is strange being that it's already past 1.0. IIRC, this was a comment more on the library ecosystem than the language itself. It's gotten a lot better in the last year in my opinion, and I intend to continue making it better. :D
I think we could still make it a lot faster because it's currently not type stable, I ran code_warntype on it and the type inference system thinks some values are of type Any which means there is boxing going on. This is reflected when we time it because we have 57 allocations. I just tried doing a faithful port of the python code, but if I have a little time later I might post an updated version
You can look here for a discussion about it: https://github.com/rust-lang-nursery/futures-rs/issues/697 &gt; By spawning daemon tasks and then intermittently calling core.run while mixing in other potentially blocking logic is a hazard. When the core is not running, the daemon task cannot make progress. I think basically it make it seem like you just run a specific future at a time, but that can be bad. 
You can do something like this: expect!(bar(expect!(foo(a)))); Is that what you were asking? 
Clarification: they haven't been approved yet. :)
&gt; 0.000153 seconds &gt; 1.53milliseconds Isn't that actually 0.153ms? I.e. 153us
No I'd call that nesting. I am talking about this syntax: fn main() { let x: Result&lt;i32, String&gt; = Err("foo".into()); let y = x.expect("for shame").to_string(); } Which, afaict, this macro (or all?) doesn't allow which means we have to nest like you suggested or create locals.
This is strangely hostile when the GP was being reserved in their statements. Why?
I am making a crate that defines Q0.X fixed-point integers (this case requires special handling that doesn't exist for other types and thus isn't implemented in any crate I can find). Should I make the crate #[no_std]? I have done some searching but I am still confused about the purpose of it and what I have to do.
https://github.com/dalek-cryptography/curve25519-dalek This repository seems to be well written, you could probably take inspiration from it to improve performance in your script.
This gives the wrong result. `point_mul` is calling `point_add` six times when the if condition is true, but it is not calling `point_add` for the other 250 cases.
I'd be interested in an output from two consecutive runs of your `sw` tool. It should tell us something about the measurement resolution with your tool.
Will do. Something like the old core.run is planned soon! You can always call wait on your future but that doesn’t multiplex tasks. 
If you're doing simple processing, for simplicity's sake you can just dispatch to the current thread's executor. Once you have this configured you can benchmark this approach versus a thread pool and go from there. As with most things in development, one size doesn't fit all.
I think that's a problem with valgrind vs. jemalloc. I just ran with the upstream rustc-stable, and I got a segfault as you say. Then I tried with Fedora's rustc (configured to always use the system allocator), and the program completed without any errors.
Ya, you can't make a "macro method", i.e. `x.expect!("for shame")`
FWIW I was talking about the `expect(..).to_string()` part but that doesn't change anything. :)
Some users suggested watching out for Cygwin, Wine, and other "mixed mode" targets :)
There is a `BigUint::modpow`, but we don't yet have that for `BigInt` to replace your `powm`. I'll see about adding that. However, even replacing your code with a naive conversion is significantly faster: pub fn powm(base: &amp;BigInt, exp: &amp;BigInt, modulus: &amp;BigInt) -&gt; BigInt { let base = if base.is_negative() { (base % modulus + modulus).to_biguint().unwrap() } else { base.to_biguint().unwrap() }; let exp = exp.to_biguint().unwrap(); let modulus = modulus.to_biguint().unwrap(); base.modpow(&amp;exp, &amp;modulus).into() } 
Good point. It's a pain when libraries do this, especially when the dependency they pull is an old version. I'll push the update today :)
See my comment [here](https://www.reddit.com/r/rust/comments/7w3v77/why_is_my_rust_code_100x_slower_than_python/dtxuo9f/) -- it's faster even when you have to convert to `BigUint`. I'll work on making this directly available for `BigInt` too.
The startup time of the interpreter is a fundamental part of running a Python program, though. cargo is in no way required to run a Rust executable.
Submitted a [PR](https://github.com/Peternator7/strum/pull/14).
I don't know what gmp does, but I hesitate to look at GPL code for inspiration for the MIT/Apache-2.0 num. (I have no problem with GPL itself though.)
Hey guys! Is the return keyword optional, or are there any cases where I have to use it. ``` fn first_word(s: &amp;String) -&gt; usize { let bytes = s.as_bytes(); for (i, &amp;item) in bytes.iter().enumerate() { if item == b' ' { return i; } } s.len() } ``` Can I ommit the return statement here?
You mean `sw; sw; sw`?
This result *= &amp;base; result %= modulus; is essentially mirroring the C interface, which is actually closer to mpz_mul(result, base, result); mpz_mod(result, modulus, result); Note that this function invocation is impossible in safe Rust due to aliasing rules.
&gt; Just to clarify: iterator invalidation and data races are a violation of memory safety. In Rust, not necessarily in other languages. For example, you have iterator invalidation and data races in Java and they are perfectly memory and type safe, you'll just get an exception or non-sensical results :(
Oh yes I read you a bit too quickly. You're right with libuv.
You're missing `n = point_add(n, n)` from the loop.
And, at least for me, the overhead is mostly during the learning phase. I don't feel slow at all when writing small programs.
Personally it's more the "blank page" syndrome when thinking about a new projects' architecture, as all the dots have to align from the get go. Once launched, it's a free ride :)
Would it make sense to define the parsed hex bigint within `P` as static via lazy_static within `point_add()`, rather than parsing a string each time? 
There is also [rug](https://gitlab.com/tspiteri/rug) (another GMP binding) and [ramp](https://crates.io/crates/ramp) (implemented in Rust).
I was thinking about C++, but you are making a very good point that this can happen in memory safe languages as well, and that my statement was misleading.
&gt; Python has an excellent implementation of big integers I wouldn't say *excellent*. It has an OK implementation of bigints, but tends to fall behind as the number of digits increases because the algorithms &amp; pickings are more limited than in e.g. GMP. For instance see https://www.reddit.com/r/Python/comments/7h62ul/python_largeinteger_computation_speed_a_comparison/. 
But IRL the interpreter launches once, not every time you call the function. Unless the use case is specifically to be launched as a short-lived process, then the interpreter warm-up should not be counted. I believe the point here is to time the implementation, not the infrastructure.
An example is from the h2 library. When you open a connection, it needs to be executing always, even if your sending or receiving is waiting on some other application action, to be able to process pings, flow control, and other meta frames. You do this by spawning the connection into an executor, and then using the client handle to make requests as needed. If you were to only have the executor running while processing a specific request, then the connection would behave very badly in between. However, dropping all client handles does tell the connection task to start a shutdown. Even then, you want it to be able to tell the server about it, so it knows the client properly received all data, etc.
I benchmarked and it did not make any difference. I tried `lazy_static`, hex parsing, decimal parsing (should be slower), and setting `P` as `(Integer::from(1) &lt;&lt; 256 - 0x3d1)` (I think it should be faster), but all cases where pretty much the same. I guess the parsing or shift+add is negligible compared to the other operations in the function. Maybe in a different situation with almost no other operations it would make sense to bench again and pick the fastest solution, but here it just doesn't seem to matter.
You should have a list of parsers written with `combine` in your README. I've seen https://github.com/tailhook/graphql-parser and it looks quite good!
But you wouldn't want to measure the startup time of the interpreter if you're just testing some functions that are part of a larger program. You really want the execution time of _each call to the function_.
Even if you just got network access working that'd be huge. I hope someone continues your work in any case.
Pushed the update. You can now find it in `ramp::traits::Integer`. The docs aren't working because docs.rs is having some issues, but the 3.10 docs are pretty much identical.
What if the new result is allocated on the stack, that's used for calculations, and then mem::transmute over one of the moved-in args? Then, no allocations are made - it just uses the stack for scratch work and re-uses whatever was moved in (which may be on the stack, or it may be in memory somewhere). If that sounds reasonable, I'd be willing to make a PR for that.
Wouldn't it be possible to have resume take a plain &amp;mut T param, and implement Generator for Anchor&lt;&amp;'a mut GeneratorStruct&gt; instead of GeneratorStruct directly? The advantage of this is that it allows any trait to be implemented for a struct that is self-referencing, instead of special-casing the Generator trait just because some implementations happen to be self-refencing. Also, I think there should be AnchoredDeref&lt;T&gt; trait and AnchoredDerefMut&lt;T&gt; traits implemented by Anchor, so that it's possible to write code generic over Anchor&lt;Box&lt;T&gt;&gt;, Anchor&lt;Rc&lt;T&gt;&gt;, etc. 
Isn't there some crate that takes care of safe ffi between rust and python? It seems like that would be much better than rolling it yourself. I found `rust-cpython` and `pyo3`. 
There's a `getppid()` in the `libc` crate if you'd prefer. It's fun to figure out how to call C code from Rust, though.
The existence of a lifetime parameter is a part of the type's public API (its there in the API). The variance of that lifetime parameter is implicit, but changing that is also a breaking change. This is how Rust has worked since beforre 1.0.
Just to be shorter.
Ah right, we don't get the nice properties that `&amp;mut T` has. There are several ways to solve this problem and its a bit in the weeds &amp; raises trade offs between the complexity of this API and the guarantees we enable with it.
For smaller projects I think you're probably right about how difficult it is to debug these kinds of issues. But by way of contrast, I've shipped a relatively large game using an in-house engine that ran on several platforms, and memory errors were the bane of our existence. Here's one example: It started with a few seemingly-unrelated symptoms, which we didn't put together until later: a) a particular visual effect was rendered a frame or two out of sync with the rest of the game, b) the renderer started pushing garbage data to the GPU as geometry, and c) the game started crashing all the time. (It always crashed all the time, but these were new call stacks...) It turned out that visual effect, which had worked fine for years across several games, was the culprit. It was generating some dynamic geometry in the main game thread and bypassed the normal, safe route to the render thread for... reasons. After a couple days of tracking this down (amid several other crashes that for all we could tell may or may not have been related), and several days of trying to quickly patch it up (we had a deadline, and the fixes all seemed to work... at first), the solution wound up requiring rearchitecting the whole thing. Now, Rust wouldn't have magically made this problem go away. What it would have done is introduce the "friction" to point out to the effect's original author, and subsequent authors, all of whom had different areas of expertise and reasons for touching it, that *this design was a terrible idea*. It would have reduced the eternal "why the hell is this happening" phase from days to minutes. We would have wound up with basically the same solution either way, but *much more quickly* because of that "friction."
I think this comment is similar? https://www.reddit.com/r/rust/comments/7vxk1d/rfc_stable_simd/dtw53kz/
The Python version works for any d though not only for u32. How fast is it if you change d to BigInt?
Thought that might be the case. Tried to use the system allocator, it didn't work first time and I gave up. Curious. [Exisitng rustc issue](https://github.com/rust-lang/rust/issues/41465)
&gt; The game libraries and engines I've tried were poorly documented This is definitely one of the Rust ecosystem's weakest areas, in my experience. There's a lot of experimentation and very few solid, works-everywhere tools at this point.
maybe we could make a list of c libraries which are commonly used to give people ideas?
Oh true, I forgot this was unknown-size. We will just have to wait for const-generics to solve this one...
&gt; https://www.reddit.com/r/rust/comments/7vxk1d/rfc_stable_simd/dtw53kz/ I might have been misreading the thread, but based on it being a "match" I assumed that it was talking about a capability object that's not zero-sized, so subject to runtime checks. My idea is specifically zero-sized, so no runtime cost once the capability is obtained. It would be possible to implement my idea on top of a layer where the operations are all unsafe (this is analogous to the "-sys" pattern). But if the cost is low, it might make sense to encode information into the intrinsics. I can also see some challenge of keeping a higher abstraction layer in sync. Do you think it's worth writing this up as a more detailed proposal?
That assumes there's no need to support PyPy. Both of those get much of their convenience from linking against CPython's `libpython`. That said, if limiting it to CPython *is* acceptable, those are great... especially when combined with `setuptools-rust`. 
That's a good idea! Opened a PR with the parsers I know about that has been created in the last year or so https://github.com/Marwes/combine/pull/137. (Also pasted below) ### Formats and protocols * GraphQL https://github.com/tailhook/graphql-parser (Uses a custom tokenizer as input) * DiffX https://github.com/brennie/diffx-rs * Redis https://github.com/mitsuhiko/redis-rs/pull/141 (Uses partial parsing) * Toml https://github.com/ordian/toml_edit ### Miscellaneous * https://github.com/dgel/adventOfCode2017 * https://github.com/MaikKlein/spire-lang 
Alright, but isn't it a bit inconsistent with the rest of std?
Thanks for correcting me! I definitely should have added an "IIRC" somewhere in there, and I definitely haven't intended to sound authoritative: quite the opposite, I have only limited experience with big integers. And I've also messed the sentence structure, because "nobody has done this yet" was intended to refer to "pure rust fast bigint", not the gmp bindings. However, I still think (based on what I've read elsewhere, not on my own benchmarks) that it is true that `num-bigint` is not super fast, and that there's no stable and fast pure rust implementation of big integers? I know about ramp, but looks like it is extremely far from working on stable because it uses inline assembly? 
Definitely. But I was told this year Rust would be doing some syntax changes for readability, so maybe after that, thing will be a little more set in stone 
Arbitrary Self Type: WithContext, now that is an amazingly cool design pattern.
&gt; the makers of Stardew Valley The publishers of Stardew Valley who were brought in on the development for translation and multiplayer work. ConcernedApe created Stardew Valley 1.0 all by himself. (IIRC, he said it was because he didn't want others modifying his artistic vision.)
&gt; projects that could possibly become larger scale or in production, ... all the good ones are ...
I have actually and think it's pretty awesome that it's being used for a game of a pretty big scale compared to a solo project.
That was a while ago :) I think I started by just looking at existing recursive descent parsers and went from there. Recursive descent parsers are pretty straightforward on their own since they are literally just functions calling functions recursively, peeling of the input stream one step at a time. So I don't have any specific resources I can think of but I would recommend starting at recursive descent parsers (or [LL](https://en.wikipedia.org/wiki/LL_parser) parsers, recursive descent parsers are LL). The more advanced types of parsing follow largely from there.
Regex' are fine for simple stuff, but they don't really scale to more complex things, like recursive grammars which turn up pretty quickly when parsing programming languages. I personally have had good success with the [peg](https://github.com/kevinmehall/rust-peg) crate, but it's certainly not the only good parser library in rust.
I use a Focusrite Solo but 2i2 is also good.
I think this is because `Pin` will not need any method to be applied to itself. It doesn’t have any importance.
Thx for the feedback! :-)
The equivalent in [pest](https://github.com/pest-parser/pest) (only one I know well enough in Rust): `{"fn" ~ fn_name ~ "(" ~ fn_args ~ ")" ~ "-&gt;" ~ ident}`. You will still need to define the `fn_name`, `fn_args` and `ident` rules though. Not as short as regex but much easier to read. I don't really want to imagine how a regex for parsing a full language would look.
You will lose your sanity far earlier than you lose your honor. You won't go wrong with Combine (parsing a programming language was its original purpose) but I must also recommend [LALRPOP](https://github.com/nikomatsakis/lalrpop) which is what I currently use. As long as you get over the initial speed bump of learning the syntax of a parser generator such as LALRPOP they tend to be fast early on when syntax is still iterated on.
What if you give a big 256bit int input instead of 125 in main()?
You have a bunch of unwraps() in the interface modules, it would probably be better to propagate those possible errors out to let the library user handle them instead of panicking.
Parser combinators are no rocket science so you may take their source code and decipher their internals yourself. I'm not quite sure where to point you, perhaps this tutorial is a good start http://www.lihaoyi.com/post/EasyParsingwithParserCombinators.html Further, I recommend "Earley Parsing Explained" about Earley parsing: http://loup-vaillant.fr/tutorials/earley-parsing/
:+1: Parser combinators are really just a way to write recursive descent parsers in a structured manner so they tend to be pretty simple as well. Due to the changes written about in this post combine has taken on a bit extra complexity in the parser implementations so it might not be the best thing to study when starting of though. (Version 2 or even version 1 of combine might be good to study however). Earley parsers are super powerful (though they took a bit of effort to wrap my head around, despite actually being pretty simple). I definitely second the recommendation ofthat link. 
unwrap will panic if the condition fails, taking the whole program down. It’s much better to propagate the error so the consumer of your API can decide what to do. See the Rust API guidelines for more info. https://rust-lang-nursery.github.io/api-guidelines/documentation.html#c-question-mark
Window functions? Just use modern postgres with lateral for 90% of cases
If I pass `&amp;((Integer::from(1) &lt;&lt; 256) - 1)` instead of `&amp;Integer::from(125)`, the program takes about twice as long. Which does make sense as `point_add` will be called 511 times instead of 261 times (256 + 256 significant bits - 1 instead of 256 + 6 significant bits - 1).
It seems like both `pyo3` and `rust-cpython` allows at least one way communication so that Python can call Rust. In my case, I also need Rust to be able to call a Python function. So a lifetime order would be: 1. Define a function in Python 2. From Python, call Rust where one of the argument is simply the "name" of the function defined in 1 (i.e. a reference to that function) 3. In that above Rust function, call the Python function a certain number of times with different parameters as needed, read the result from that function, and continue based on that result 4. Read the final result of the Rust function executed in 3. I'll dive deeper into `pyo3` though. Since there's a global access to Python's GIL, I _guess_ there should be a way to make this happen.
There are many cases where bare functions are used as constructors. Here are some examples: * https://doc.rust-lang.org/std/env/fn.var.html * https://doc.rust-lang.org/std/env/fn.var_os.html * https://doc.rust-lang.org/std/fs/fn.metadata.html * https://doc.rust-lang.org/std/iter/fn.once.html * https://doc.rust-lang.org/std/iter/fn.empty.html * https://doc.rust-lang.org/std/iter/fn.repeat.html * https://doc.rust-lang.org/std/thread/fn.spawn.html * https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html Anyway I don't want to bikeshed the exact paths used in these APIs right now, I'm still trying to work out their semantics.
neat! you should have a look at https://github.com/rust-embedded/rfcs/issues/39 would be awesome to integrate with embedded-hal
You got answers for rust already, but I just wanted to note: if you want to make the python code faster, you can annotate the int types and compile the code with Cython. See http://cython.readthedocs.io/en/latest/src/quickstart/cythonize.html On pure-math function, I'd expect a 10x improvement at least.
GMP does allocate temporarily inside these functions. Inside `mpz_mul(w, u, v)`, sometimes it does something like `if (PTR(w) == PTR(u) || PTR(w) == PTR(v)) { /* allocate temp memory */ }`, and `PTR(w) == PTR(v)` (pointer to `result`) in this example. Inside `mpz_mod(rem, dividend, divisor)`, it needs to keep a copy of `divisor` so it has something like `if (rem == divisor) { /* create copy of divisor */ }`, and again `rem == divisor` (pointer to `result`) in this example. Of course, GMP frees the temporaries before returning.
/u/imatwork2017, see the parent comment here
Here's the source https://github.com/bootandy/window_funcs
* `Box&lt;T&gt;` -&gt; `std::uniq_ptr&lt;T&gt;` * `Arc&lt;T&gt;` -&gt; `std::shared_ptr&lt;T&gt;` etc
I guess the question I have is why were those RFCs not all reflected in TWIR 219 but were in 220? For most RFCs I can watch them filter up through the different stages as reflected in TWIR. So when RFCs seem to be nowhere one week and then merging the next week, as an armchair RFC watcher at home it feels like they've skipped ahead without the proper process.
Use [cffi](https://cffi.readthedocs.io/en/latest/using.html#extern-python-new-style-callbacks). It supports pypy and allows you not to link to libpython which makes deployment so much easier.
It requires Deserialize/Serialize traits (which is as easy as #[derive(Deserialize, Serialize)]), but yes. You can also control how the enums should serialize using the #[serde] macro, without having to write your own serializer/deserializer.
I tried to optimize further by keeping a pool of `Integer` values to avoid reallocation, and to avoid the parsing of `P`, and the gains were very small, only about 7%: test bench_orig ... bench: 4,174,821 ns/iter (+/- 61,543) test bench_reuse ... bench: 3,920,121 ns/iter (+/- 113,708) The new code: extern crate rug; use rug::{Assign, Integer}; use rug::ops::RemRounding; #[derive(PartialEq, Clone)] struct Point { x: Integer, y: Integer, } fn hex(s: &amp;str) -&gt; Integer { Integer::from_str_radix(s, 16).unwrap() } struct PointAdd { p: Integer, pm2: Integer, base: Integer, pow: Integer, temp1: Integer, temp2: Integer, lam: Integer, rx: Integer, ry: Integer, } impl PointAdd { fn new() -&gt; PointAdd { let p = hex("FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F"); let pm2 = p.clone() - 2; PointAdd { p, pm2, base: Integer::new(), pow: Integer::new(), temp1: Integer::new(), temp2: Integer::new(), lam: Integer::new(), rx: Integer::new(), ry: Integer::new(), } } fn add_part1(&amp;mut self, p: &amp;Point, q: &amp;Point) { if p == q { self.temp1.assign(&amp;p.y * 2); self.base.assign(&amp;self.temp1 % &amp;self.p); Ok(&amp;mut self.pow).assign(self.base.pow_mod_ref(&amp;self.pm2, &amp;self.p)); self.temp1.assign(&amp;p.x * &amp;p.x); self.temp2.assign(&amp;self.temp1 * 3); self.lam.assign(&amp;self.pow * &amp;self.temp2); } else { self.base.assign(&amp;q.x - &amp;p.x); Ok(&amp;mut self.pow).assign(self.base.pow_mod_ref(&amp;self.pm2, &amp;self.p)); self.temp1.assign(&amp;q.y - &amp;p.y); self.temp2.assign(&amp;self.pow * &amp;self.temp1); self.lam.assign(&amp;self.temp2 % &amp;self.p); } self.temp1.assign(&amp;self.lam * &amp;self.lam); self.temp2.assign(&amp;self.temp1 - &amp;p.x); self.rx.assign(&amp;self.temp2 - &amp;q.x); self.temp1.assign(&amp;p.x - &amp;self.rx); self.temp2.assign(&amp;self.lam * &amp;self.temp1); self.ry.assign(&amp;self.temp2 - &amp;p.y); } fn add_part2(&amp;self, dst: &amp;mut Point) { dst.x.assign((&amp;self.rx).rem_floor(&amp;self.p)); dst.y.assign((&amp;self.ry).rem_floor(&amp;self.p)); } } fn point_mul(p: &amp;Point, d: &amp;Integer) -&gt; Point { let mut n = p.clone(); let mut q = None; let mut point_add = PointAdd::new(); for i in 0..256 { if d.get_bit(i) { match q { None =&gt; { q = Some(n.clone()); } Some(ref mut q) =&gt; { point_add.add_part1(q, &amp;n); point_add.add_part2(q); } } } point_add.add_part1(&amp;n, &amp;n); point_add.add_part2(&amp;mut n); } q.unwrap() } fn main() { let g = Point { x: hex("79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798"), y: hex("483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8"), }; let res = point_mul(&amp;g, &amp;Integer::from(125)); println!("{}", res.x); println!("{}", res.y); } 
serde_json outputs *json*, where strings must be quoted. Please don't use serde_json as an enum stringifier. That's just wrong.
Rust is lifetime-parametric (and this can't change without breaking some chunks of the ecosystem), which means that a "phantom borrow" is as valid as a real reference. And you can already self-borrowing data on the stack in safe code, with the borrow-checker understanding it - the trick is a few years old by now, but all you need is `Cell&lt;Option&lt;&amp;'a Foo&lt;'a&gt;&gt;&gt;` in a `Foo&lt;'a&gt;` type. Note the `&amp;'a Foo&lt;'a&gt;` - the lifetime `'a` showing up twice means `Foo&lt;'a&gt;` may point to a `Foo&lt;'a&gt;` of lifetime `'a`, which itself may point to another `Foo&lt;'a&gt;`, etc. with the lifetime being the same across all these references. This lets you do cyclic data structures, like an entire graph, if you wanted to. If `'a` is `'static`, it's pretty clearly correct (the data can't move because references to it last forever). Otherwise, the borrow-checker will (with no special rules for this case, just the general ones) make the `'a` borrows last as long as the longest-living `Foo` in the list/graph (which may be just one), and because a reference has to always point to live data, that means all the `Foo`'s which are connected through `'a` borrows must be declared at the same time (and/or be part of a larger type which has more than one of them) - this is what we call a "perma-borrow" - if a borrow of variable can be used to make the variable point to itself, it can't end before the variable itself. **TL;DR**: Rust borrow-checking rules allow self-referential data on the stack without any special rules, it just falls out of use-after-free rules giving a conservative but correct approximation of borrow scopes. Combining that with [guaranteed](https://docs.rs/indexing) lifetime parametrism lets you perma-borrow stack variables without any actual runtime references ("phantom borrows").
Thanks a lot, that does explain the segmentation fault. I'm kinda still just starting out in Rust and naively thought it was a function pointer. I'll dive into the libc crate and see how to handle my use case. Thanks again
Welcome! The water's warm. After you learn the basics, I enjoyed [this](http://cglab.ca/~abeinges/blah/too-many-lists/book/) tutorial as a way to understand how memory management in rust actually works. It of course helps if you're already familiar with how linked lists work in something like C.
That doesn't work with coherence (in a crate that didn't define `Generator` nor `Anchor`) and also it doesn't work with `-&gt; impl Trait` the same way the two-trait solution does.
Ohhh I love this concern of fair scheduling thank you!!!
TWIR lists RFCs at two points in their lifecycle: Once when it is first submitted, and then again when it enters its 10-day Final Comment Period. So RFC 2145, for example, appeared in the "New RFCs" section of [TWIR 199](https://this-week-in-rust.org/blog/2017/09/12/this-week-in-rust-199/), and in the FCP section of [TWIR 206](https://this-week-in-rust.org/blog/2017/10/31/this-week-in-rust-206/). But then its merge ended up delayed until this month (because of the impl period, I guess?). It looks like TWIR 220 ended up re-listing a few of these long-delayed RFCs that got back on the radar in past week or so.
I'd point out that the borrowing and ownership does more than just manage memory. It also cleanly handles much more complex resources and allows you to manage them with a greater degree of control than C++ RAII allows on its own. Files and sockets, for example. In addition, it's much easier to reduce the memory footprint of your Rust software than it is with C++.
Of the nine "disposition: merge" RFCs in TWIR 220, six of them *were* listed in [TWIR 219](https://this-week-in-rust.org/blog/2018/01/30/this-week-in-rust-219/#final-comment-period).
Nice! I learned a bit about modern SQL. For those who haven't done a lot with aggregates and window functions, here are some relevant docs: * [Aggregates tutorial](https://www.postgresql.org/docs/current/static/tutorial-agg.html) * [Window functions tutorial](https://www.postgresql.org/docs/current/static/tutorial-window.html) * [Window function syntax](https://www.postgresql.org/docs/current/static/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS) * [Window functions reference](https://www.postgresql.org/docs/current/static/functions-window.html)
If you enjoy watching programming meetup/conference type videos here's the Rust YouTube channel (they just put up the FOSDEM talks the other day actually!) [here](https://www.youtube.com/channel/UCaYhcUwRBNscFNUKTjgPFiA). Some of it is sort of advanced, but many try to explain in a way beginners can sort of understand what's going on. Other than that there's the [second edition of the official rust-lang.org book](https://doc.rust-lang.org/book/second-edition/) which is considered very good by many people including myself. It'll give you a couple example problems to look at that you could try doing on your own. If you want problems to do to learn the language and aren't afraid of some algorithm type stuff, I'd check out [this past holiday's AdventOfCode problems](https://adventofcode.com/2017), and if you get truly stuck or want to know if there's a better way to do something you can always look up solutions for it that people have done in Rust :) If you're one of the people who uses [Discord](https://discord.gg), here's an invite link to the [Rust Programming Language server](https://discordapp.com/invite/NRdKRU) which I'm an active member of (you'll find me under the name `uninitialized::&lt;repnop&gt;()` right now!) with many others :D There's a #beginners channel that's specifically meant for people who are learning the language to ask questions and get answers :) Good luck on your dive into this amazing language :D
aw, shucks... thanks!
OP i had the same opinion as you until i started writing an opengl project and a math library after a week of frustration i started seeing the way i should actually use rust. There are still a TON of stuff to learn and i really wish the books/guides/videos were more and better geared towards beginners, and geared towards getting a newbie equipped with enough rust so he can start hacking away. Understanding Option/Result and explicit lifetimes is no joke you can easily lose 50%+ of new rustaceans in those topics, altough they are so simple they are poorly explained. Its pointless to tell a new rustacean that Option can be Some or None and not event telling him how to get his values out of that bloody construction safely, but by far lifetimes are the worst explained concept from rust and a key one that im still trying to wrap my head around. I understand the syntax but not how/when to use it, when dealing with references i have to give them explicit lifetimes? but why and how is never explained, still looking for that simple properly worded phrase that will make me understand whats the point of defining lifetimes, why cant the compiler see where they come from?
`sw&amp;; sw&amp;; sw&amp;`?
I'm so happy I can appropriately use this form of appreciation: Thank you sir. Hah.
It's possible with live objects, but then you have to scan memory for pointers and update them, while making sure they don't get dereferenced between the object being moved and the pointers being updated to reflect the object's new position. This isn't just hypothetical; this is how compacting garbage collectors work nowadays. 
Doesn't this mean that all combinators now need to be implemented with unsafe code? Or is there a safe way to get an `Anchor&lt;U&gt;` from an `Anchor&lt;T&gt;` when `U` is a field of `T`?
I'm build an API wrapper and the API has a large database of statuses. For instance: 1000 - Error 2000 - Username not found 2001 - Wrong password etc. When the server replies to a request, it only sends the status and not the text. What would be the best way to store these values? A hash map? Would it be better to store these in a text file and only search when an error is encountered? All status and error texts are about 12KB written out. 
Ping /u/slsteele
Unfortunately, there is no preemption. I have not figured out a way to do preemption reliably without language support (i.e. a language designed to preempt).
[That works](https://play.rust-lang.org/?gist=f0a90f92fe535e5a66658780d741df00&amp;version=stable). I use it all the time.
Yep. [It Does!](https://www.jetbrains.com/clion/buy/#edition=personal) so screw that. 
Okay so you *can* do it. Sweet!
Nope, I was asking about chaining onto the expect!(), not calling the macro as an associated function.
Just expect!("...").other_thing() I thought I was pretty clear with "function chaining" but I suppose not.
This blog post actually explains much more about milksnake than their readme does. Very interesting stuff, I'll give it a shot in a few hours! Thanks
If you're making an API wrapper for others to use, I'd avoid using a text file since it's a pain for users to have to copy it around with their executable. Otherwise it's probably fine, unless you need high performance and the API returns lots of errors. One solution would be to use lazy_static + (optionally) maplit, which would look something like this: lazy_static! { static ref ERRORS: HashMap&lt;u32, &amp;'static str&gt; = hashmap! { 1000 =&gt; "Error", 2000 =&gt; "Username not found", }; } You could also use a sorted static array and then binary search on the error codes, which has the minor benefit of avoiding a heap allocation: static ERRORS: &amp;'static [(i32, &amp;'static str)] = &amp;[ (1000, "Error"), (2000, "Username not found"), ]; fn lookup_error(code: i32) -&gt; &amp;'static str { ERRORS.binary_search_by_key(&amp;code, |&amp;(c, _s)| c) .map(|idx| ERRORS[idx].1) .unwrap_or("Error code not found") }
If they are of suitable utility you should publish them. Binary crates can easily be installed with `cargo install cratename`, which makes it super easy for people to try out your tool. This is especially handy when said tool hasn't been added to a system package repository like [AUR](https://aur.archlinux.org/), [Homebrew](https://brew.sh/) or FreeBSD [ports](https://github.com/freebsd/freebsd-ports).
I personally like it very much, that i can just type cargo install ripgrep and there it is. I wish all rust applications were that userfriendly. What i really hate are programs/libraries that need obscure c/c++ libraries, you need to install/compile before cargo can do its magic.
&gt; I much prefer if programs I am using don't crash randomly, destroy my data or expose them to anyone. Rust programs still crash randomly, just for much different reasons than C++ programs. Panic-safety is something the Rust team seems to not really care about at all.
`BigInt::modpow` is now in num-bigint 0.1.43.
I work in games professionally and while I've been pushing Rust heavily on the PC-side tool front, I'm still pretty skeptical of its utility on game consoles and for performance-critical code. From what I've seen Rust's standard library and borrow checker reward haphazard dynamic memory allocation and I'm not convinced this is really an appropriate thing to bring into games.
dlmalloc overhead is understandable and actually welcome. Knowing its constant size is a good thing to keep in mind. I take it even programs with only stack allocated memory contain it as well? Would be fun if it could be eliminated as well, but the practical impact of 10KiB extra per wasm module is neglible. Shouldn't setting panic="abort" in Cargo.toml release profile get rid of the panic stuff? If not, I'll have to try and locate the panic and format code to snip it. As far as I'm aware, I'm not using any text formatting. I tried using the tool based on the blog post, but am not getting *any* output with the "top" mode, which is curious. Grepping through the `wasm-objdump -d wasm_pong.wasm`, I don't see any lines matching the START_FUNCTION regex. The function definition lines (if I'm interpreting the dump correctly) look like this: ```000380 func[26]: 000387: 41 00 | i32.const 0 000389: 41 00 | i32.const 0 00038b: 28 02 04 | i32.load 2 4 00038e: 41 80 04 | i32.const 512 ...```
Awesome seried! Just needs a s/FutureStable/StableFuture
Welcome! I always tell new Rustaceans to go join an existing project, because a) you can work in small byte-, err, issue-like chunks, b) help improving the ecosystem (how awesome is that?) and c) often get great mentoring out of the deal. [This week in Rust](https://this-week-in-rust.org) has a weekly "help wanted" list and [the findwork site](https://rustaceans.org/findwork) has a tab with issues for new contributors. Good luck and have fun!
Yes, you will usually find willing testers among the Rust-enabled, and if there's interest you can then provide binaries for popular systems. `docs.rs` will even render your readme nicely these days.
I still try to understand that argument. When your program crashes with a panic either you wrote it or a library you use wrote it. It's there in the code, clear to see. How is this "randomly"?
An anchor still allows you to get a reference, with which you can access fields.
If the intended audience is rust programmers, definitely publish on crates.io. If not, I wouldn't. I mean, you still _can_, but you should also provide binary releases (via github releases and https://github.com/japaric/trust/) if your audience is not primarily rust programmers.
Oooh, another animation/vfx person! Welcome! Over-all this looks good to me. Some notes below. ---- In chunk.rs: The underscore prefixes on Chunk's field names aren't necessary. A couple of changes I would make to `from_buffer`: - Take a `&amp;[u8]` instead of a `&amp;Vec&lt;u8&gt;`. Since you're not modifying the passed Vec anyway, the former is more general. - In Rust it's idomatic in cases like this to return a final value by simply putting it at the end of the function without a closing semi-colon. - Split the Chunk construction into multiple lines, and inline some more things in it. Taken together, that looks something like this: ``` pub fn from_buffer(buffer: &amp;[u8]) -&gt; Chunk { let mut inner_buffer = buffer.to_vec(); inner_buffer.resize(MAX_CHUNK_SIZE, 0); let digest = calculate_digest(&amp;inner_buffer); Chunk { id: HEXLOWER.encode(digest.as_ref()), size: buffer.len(), buffer: inner_buffer, } } ``` In `from_cache` you can create the zero-initialized buffer in one line using the `vec!` macro: ``` // ... let mut buffer = vec![0u8; MAX_CHUNK_SIZE]; reader.read(&amp;mut buffer)?; // ... ``` You have multiple `impl Chunk` blocks, which feels a little weird to me. But that might just be me! If it were me, though, I'd throw everything into a single `impl Chunk` block. ---- In bundle.rs, many of the same comments apply. Other than that, I'd say try removing some of your type annotations in your `let` statements, because in most cases the compiler can infer it for you (including generic parameters). For example, in `get_chunks` at the bottom of bundle.rs, you should be able to change `let mut chunks: Vec&lt;Chunk&gt; = Vec::new()` to just `let mut chunks = Vec::new()`. Since you're eventually returning `chunks`, the compiler can infer that is has to be the same type as in the return signature. I'm sure there are other things, but I hope this helps!
&gt; What I'd really like is to use a Rust Generator to store Combine AnyPartialState on the stack (optimally sized auto-generated struct/closure). Using generators was actually how I first intended to implement this but after sketching it out I realized that it wouldn't work. This is because the lifetime of the input gets tied into the generator, making it impossible to use in a decoder (consider that the decoder needs to add or remove input and we can't do this with any outstanding borrows to the input). On the other hand, with the implementation in combine now, the partial state is separated from the input which makes it possible to update the input buffer! (and should also let generators be used on top of combine). &gt; Is there an easy way that I could get a non-boxed PartialState so it could be moved into a Generator on the stack? If you use `impl Trait` you can avoid naming the state type while still be able to use it. let mut state = Default::default(); decode(parser, input, &amp;mut state) The only problem you will run into is that you may need to name this type somewhere else instead as it needs to be stored *somewhere* between decode calls. Actually, if you use generators and `impl Trait` you could probably hide it within the `impl Generator` but (I haven't tested this). &gt; May I assume the partialstate will always fit in a usize? No, afraid not. The state division is actually extremely granular meaning that it takes a lot more memory than a usize. For instance, [Many](https://docs.rs/combine/3.0.0-beta.1/combine/combinator/struct.Many.html#impl-Parser) stores the collection that it accumulates values into (often a Vec, making it 3*usize) + the state of its wrapped parser. If the state becomes to large/the states become to granalur it is possible to opt-out of partial parsing with [no_partial](https://docs.rs/combine/3.0.0-beta.1/combine/combinator/fn.no_partial.html) which makes the state a `()` (forcing parsing to resume outside of the `no_partial` parser).
&gt;What i really hate are programs/libraries that need obscure c/c++ libraries, you need to install/compile before cargo can do its magic. That is quite annoying though understandable since Rust is still relatively young and doesn't have a lot of libraries written in the language.
Yes of cause, that was in no way meant as a criticism. Just as incouragement to the original poster!
I write a simple timeout program. But can't catch the TimeoutError. What's the problem. [gist](https://gist.github.com/anonymous/5a56cc32d42a63fb2af621a050ce76d5) 
&gt; From what I've seen Rust's standard library and borrow checker reward haphazard dynamic memory allocation Funny, I thought the opposite compared to C++. For example, `substr`in C++ returns a new string instead of a view/slice.
Sure, all lifetimes that are not static are shorter than static. You could do it like [this](https://play.rust-lang.org/?gist=18f221c815f98fb42d6155893211ed31&amp;version=stable). See also https://doc.rust-lang.org/book/second-edition/ch19-01-unsafe-rust.html#accessing-or-modifying-a-mutable-static-variable.
I think you could write a custom-derive impl to convert from `Anchor&lt;&amp;mut MyStruct&gt;` to `(Anchor&lt;&amp;mut MyField1&gt;, Anchor&lt;&amp;mut MyField2&gt;, ...)`. it wouldn't be the prettiest thing in the world, but it would be safe.
&gt; Rust programs still crash randomly They can crash, but safety mechanisms take a lot of randomness out of it, and they definitely crash a lot less.
So no code review here, I'm still Rust-curious more than Rust-comfortable, but *long*-time VFX pipeline guy here... What's the proposed value of dedupe on chunks? I mean lord knows I've seen terabytes used up with entirely black EXRs, but unless you were splitting on internal knowledge of the file format's memory layout you'd end up with different metadata content preventing real deduplication of the larger file content along fixed width byte boundaries. I've been working on asset management systems for longer than I care to remember, and gone down all sorts of avenues of trying to reduce duplication, but never found way of doing pan-facility compression with it on arbitrary chunks.
I think you could write a custom-derive impl to convert from Anchor&lt;&amp;mut MyStruct&gt; to (Anchor&lt;&amp;mut MyField1&gt;, Anchor&lt;&amp;mut MyField2&gt;, ...). it wouldn't be the prettiest thing in the world...
Doesn't support Windows I think or at least it's not in the [windows.rs file](https://github.com/rust-lang/libc/blob/master/src/windows.rs). Also, happy cake day!
I ported the Python code listed above to C++ GMP and I measured the runtime with `perf` at 20ms. While this is a substantial difference, it's worth noting that nearly 0 time is spent running `mpz_powm` which approximately quintuples if broken down into it's constituent exponentiation + modulo operation which isn't done in the Rust code listed above.
Very cool! Would it be possible to have a guaranteed return value optimization to 'return' stack allocated generators?
(forwarded this question to Mozilla Reps, they might know :) )
Same here, in vfx at the moment, but my experience is more on the storage side. dedup always sounds nice, but I never saw anyone really pulling it off. I know every single pipeline is wildly different, but if you really want to save on space, store diffs, with full saves every X diffs. That'd need a few custom importers, but nothing extraordinary complex. Also, in your code, every `&amp;Path` should be a `P where P: AsRef&lt;Path&gt;` as it makes for a nicer API to interact with.
Have you tried using a profiler?
Im happy to help test! 
I believe you could also give ownership of the buffer to the Transfer struct and it would be protected from mem::forget-ing in that case as well, the buffer does not have to be static. In that situation, either the destructor is executed and DMA is stopped/buffer is deallocated, or it is not and DMA can continue safely on the now inaccessible and leaked buffer.
Even if your audience is not primarily Rust users, users of Rust who know the tool is written in Rust may presume it is there and try to install it that way. And then if it's not there, a malicious actor could put their own binary there that does malicious things.
But combinators would just be regular futures and implement StableFuture through a blanket impl. So they would still get a `&amp;mut self` to access their fields through.
Your punnyshment. Thank you for your suggestions, I would love to see what's needed, so I can learn accordingly. I'm especially surprised to know how fresh rust is, and how it really stands to explode in popularity. See you in the deep end.
You mean storing a `[T; N]` in the `Transfer` struct? That wouldn't be memory safe. The DMA operates on addresses; if you store the buffer in `Transfer` then moving the `Transfer` value around (e.g. passing it to a function or sending it to another task) will reallocate the buffer in memory and that will invalidate the pointer the DMA was working on. Apart from the memory unsafety it would make sending a `Transfer` value to another task both expensive and incorrect. A send operation is a move, which boils down to `memcpy`-ing the `Transfer` value along with the whole buffer (array). In this case the `Transfer` value received on the other end would be a copy of the original and won't be modified / updated by the DMA.
I don't quite like relation between `Generator` and `StableGenerator` (same for futures), what about `Generator` use-cases outside of async code? What user should write if he just wants to use generators for hypothetical [ntegration](https://internals.rust-lang.org/t/pre-rfc-generator-integration-with-for-loops/6625) with for loops? I have a lingering feeling that we have a risk of rushing stabilization of a hacky solution with which we'll have to live from now on. Do we have a deprecation strategy for the discussed solution in case if Rust gets language level support of immovable and/or self-referential types?
The problem is that their fields would not implement `Future` (only `StableFuture`) and so the combinator cannot poll them without using an unsafe block.
Yeah I think a custom derive would have to be the way to go, but you could do better than that: it could simply provide separate getter methods for each field.
I'm at Luzon, sadly.
This achieves the goal, but ?Move seems way better, since it doesn't require to have two variants of all traits and is much simpler and less verbose to use. I'd also consider making ?Move (and maybe ?Sized) default in the new Rust epoch, so that it's not possible to "forget" it. The epoch change also solves the bounds-on-associated-types problem, since traits like Fn can actually be changed, and all the old-epoch code can be translated to automatically have a Move bound on all trait associated types. 
&gt; Creating a Homebrew tap that provides those binaries may be a faux pas That's a curious stance. Does anyone actually hold this belief?
What makes the `is_target_feature_detected!` macro unavailable in core? It looks like the cpu_features works without allocating, so I thought the macro would be elligible for core.
I'm wondering why modern operating systems don't provide syscalls to run checked DMA operations (e.g. within the mapped memory of a single process). 
&gt;whats the point of defining lifetimes, why cant the compiler see where they come from? I think this chapter in the first edition of the Rust book provides a really nice explanation of lifetimes in Rust: https://doc.rust-lang.org/book/first-edition/lifetimes.html Feel free to ask here or in another place if you have further questions.
rust-cpython and PyO3 support both extending (writing libraries that can be `import`ed by Python programs) and embedding (executing Python code within your Rust binary). Each README has two examples in it and, in both cases, the first one is an example of embedding. (Note the call to `py.eval` in the Rust code.) 
I don't think epochs are allowed to change trait definitions in std.
I think you should, as using "cargo install" is a convenient way to install them.
Brew, last I checked, philosophically prefers that not-official packages all build from source.
That's just what I've heard. Official Homebrew packages, for example, are all compiled from source. But they use caching ("bottles" I think?) to avoid users needing to re-compile everything on their own machines. But the packages themselves are written to compile from source: https://github.com/Homebrew/homebrew-core/blob/50083a23d65cf94532bac5469e0cc710f441dd4a/Formula/ripgrep.rb#L17 --- Where as my brew tap just downloads the binary release and installs that: https://github.com/BurntSushi/ripgrep/blob/8e93fa0e7fecf375e03d398660eb0e9ba7cf3473/pkg/brew/ripgrep-bin.rb#L7
You can as long as you interpret older epoch code with additional where bounds on trait associated types that result in trait definitions being the same when used from the old code (implementation in older epoch code also still works since the bounds are more permissive). In Fn/Output case Fn, will now have ?Move on Output (or nothing if ?Move is made the default), but all old-epoch code will have an implied "where &lt;T as Fn&gt;::Output: Move" for each T: Fn so it's the same for old-epoch code. Obviously the source code upgrade tool will need to add the bounds upon conversion where needed. 
Hi! There used be a Rustacean based in Cebu City, but since had moved elsewhere. Checking with our local Rust community via https://www.facebook.com/groups/rustph (please feel free to join therein). Maraming salamat!
That seems an unusable solution for high-performance OSes, since obviously you need to be able to dynamically allocate and free DMA buffers and also need to be able to DMA directly into user-mode memory. The correct solution is simply to use an IOMMU and remove the mapping before the buffer is freed, so the hardware cannot DMA into freed memory. If you don't have an IOMMU, you have to instead trust the hardware to cancel DMA when told (since it can DMA anywhere anytime anyway), and tell it to cancel the transfer instead of removing the IOMMU entry. To ensure the buffer is not freed before the mapping is removed, pass a Box/Rc/Arc/"reference to a sequence of physical pages that increases their reference count" to the DMA function which will return a wrapper that will unmap from the IOMMU entry before giving back the Box/Rc/Arc/etc.. 
`?Move` is worse because it doesn't distinguish between a "pinning" borrow (e.g. calling `.resume()`) and a non-pinning one (e.g. calling `.clone()`). See also [my previous comment](https://www.reddit.com/r/rust/comments/7w05xj/asyncawait_iv_an_even_better_proposal/dtwjl9p/).
Btw /u/ralfj, I'm curious whether/how your model would would be able to cope with this kind of thing (-:
&gt; will the `loop`s never terminate yeah it should be let val = await!(future); let val = loop { match future.poll() { Ok(Async::NotReady) =&gt; yield, Ok(Async::Ready(data)) =&gt; break Ok(data), Err(error) =&gt; break Err(error), } }; or similar.
`&amp;mut &amp;'a T` is invariant over `&amp;'a T`, because you can write in a different `&amp;'a T`, so if it weren't (invariant), you could make `'a` shorter, write a reference with a shorter lifetime and then read it from the original place with the original `'a`. In fact, the same applies to `Cell`, `RefCell`, etc. - any kind of explicit or interior mutability requires invariance for it to be a safe abstraction. Only the `&amp;mut &amp;'a ()` part is needed here, if we had an `Invariant&lt;T&gt;` marker I'd just use `Invariant&lt;&amp;'a ()&gt;` - the first `'a` in `&amp;'a mut &amp;'a ()` is there so I can use `&amp;mut` for invariance and have it compile - something like `&amp;'static mut &amp;'a ()` wouldn't.
It works thanks!
Thanks, I removed `unwrap()` and it works.
The buffer should be heap allocated (or something equivalent), so that it is not touched if the Transfer object is moved. But my original idea still works, Transfer needs full ownership.
&gt; why cant the compiler see where they come from? Imagine this type signature: fn foo(x: &amp;i32, y: &amp;i32) -&gt; &amp;i32 { What lifetime should the compiler pick here? Why? This is the crux of the problem. Remember, in Rust, we don't infer the types of signatures from reading the body, so you can't do that.
I'd be up starting September. You just caught me leaving for half a year for an internship :(. I also saw a couple of students with Rust stickers on their laptops around the EPFL, so there is probably some userbase.
DMA is (mostly?) an embedded systems programming thing
Indeed the Anchor solution can be seen as putting ?Move on the reference, while ?Move puts it on the type, with the effect you state. This could be fixed by making ?Move a typestate-like modifier on types, so that you can have GeneratorStruct which implements Clone, and GeneratorStruct - Move which implements Generator, and the "-Move" (or "+Move") would be automatically added to the type where needed. This seems to introduce a lot of language complexity though, so maybe Anchor is indeed best at least right now. 
I usually think of subtyping as bounded existential types so that part made sense: pub struct Pin&lt;'a, T&gt; { data: T, _marker: PhantomData&lt;exists ('b: 'a). &amp;'b mut &amp;'a ()&gt;, } but I completely missed that rust does variance inference and implicitly propagates that upwards. Couldn't this lead to really easy to miss breaking changes since it effectively leaks implementation details all over the place? Anyway, if i understood this correctly without the invariance trick we would end up with the equivalent of this? pub fn pinned(pin: &amp;'a mut Pin&lt;? extends 'a, T&gt;) -&gt; Anchor&lt;&amp;'a mut T&gt; { Anchor { ptr: &amp;mut pin.data, } } Is there a semantics/reference for rust's type system somewhere? I tried to check the language reference but [it doesn't really seem complete](https://doc.rust-lang.org/beta/reference/subtyping.html).
Oh, right. Debug build. Now I'm seeing results.
Thanks! I'll make the changes once I'm able to sit down and make them. The online thing I'm hesitant on changing is the return statements because it makes the intent more clear (a word vs a character that I usually filter out in my brain). Also, the reason for the multiple impl blocks is to organize the code into functionality. For example, initializers, getters/setters, and other methods. But if this makes things less clear, or you think it is unneeded, then ready fix. As for the underscores for the field names, that's the Python programmer in me showing.
I'll check out clippy! As for the returns, they make the intent clearer to me.
Your comment reminded me of one of my early experiences writing rust: I created an IRC bot based on one of the older libraries and started it up one day, and then I got busy at work for the next week or three. I came back and the bot was still chugging along, doing its job. Blew my mind.
Wouldn't you get similar by using an appropriate filesystem or compression format? Eg BTRFS(FileSystem, compression(can use zstd), dedup, CoW(Copy-on-Write), etc) or [ZSTD/Z-Standard](http://facebook.github.io/zstd/)(Compression by FaceBook) If that interests you, perhaps look into Rockstor which is a Linux OS focused on providing NAS OS, you supply it the drives, it uses BTRFS. Probably not as cool/fun as writing your own thing that doesn't need a FS layer I guess :)
One thing we did at my first studio was create a package tool that bundles up all of the dependencies for a shot and once that is signed off, then all of the publishes or renders that aren't part of that bundle could get cleaned up. I like the idea of storing a diff of the file, and I think that could still apply to my design as something placed on top of this. Also, I'm planning on writing other systems to do filesystem management for temp files generated for farm jobs, workspace files, and published versions/ reviews. The main thing I want to do is have a simple and generic way of storing and transmitting files while handling a ton of potentially duplicated data. The case where I think this will really shine is with virtual studios where you have to download a lot of data, and there may be enough duplicates to make this system really save space. One thing I'm thinking about doing this weekend is downloading the sources to all of the Blender open movies and run them through the tool and see what kind of space saving I get. If it is next to nothing, then I'll consider the tool as education, and move onto another part of my project.
This is a matter of taste, of course.
DMA is also used for large data transfers across PCIe, commonly CPU to GPU.
Homebrew, apparently. When I submitted the tokei tap they wanted it from source, not binary releases
 aura -A ripgrep is also nice though ;)
Yes but put in your project info that you welcome contributions of packaging to various managers like brew, choco, the aur, etc If it's part of a library, always yes to Cargo (for example, diesel has a cli)
That's true, the STL is this way as well, and Rust is able to do a lot of things better than C or C++ due to its non-awful string types, but manipulating strings is a small part of some types of applications. Maybe there are crates for it, but it seems like using something like a pooled or a stack allocator would cause you to run into a lot of issues in Rust. I haven't tried it but offhand I think you'd have to have a lifetime that hits every function you write and that gets cumbersome really quickly.
The equivalent is, at present, an &amp;mut to non-stack memory. Raw pointers don't have ownership semantics and there's not yet (to my knowledge) an owning pointer that can operate without an allocator.
Yes, so the Transfer will need to take ownership of the channel for the duration to prevent such a race.
&gt; When your program crashes with a panic either you wrote it or a library you use wrote it. It's there in the code, clear to see. How is this "randomly"? Sure, when a C++ program crashes due to a memory issue it's also something that someone either wrote or a library wrote, but we call them random crashes because they happen at seemingly random times during program execution and because reproducing them can be incredibly difficult due to the complexity of program state. 
This is an unusable solution for any OS, in that it's built for a freestanding microcontroller program that lacks IOMMU or dynamic allocation
I really wish all these dead code tools would be done in the rust intermediary. It seems like people care about small webasm but not small binaries. When making shared libraries with no external dependencies, small binaries are important. 
Cool, if nothing else we can keep that in mind and start something in September
Question: What is more respectful? Giving a person your honest opinion, including of them, or white-washing everything through a massive "No Offense Filter"? I personally believe the former is more respectful. The latter is just a lying echo chamber. That's just my opinion though.
Yupp! I really love typestate, but my interpretation of it is that it's *exactly* type refinement for stateful types (e.g. the data behind mutable borrows), which means there must be a version without typestate (for refining `fn(T) -&gt; T` instead of `fn(&amp;mut T)`), and I don't think *a trait* works for that. With language-level self-referential structures, we might have a way to specify whether any of the existential borrows are live or not, which would work orthogonal of "linear" vs "stateful" refinement. And *even then* I expect something like `Anchor` to remain relevant.
You're right on basically everything. The one thing that I think I can add with this is transferring data to filesystems I don't have control over. Such as an artist's workstation in a virtual studio. But, if everyone is connecting to the same filesystem, them that should probably use something like zfs, btrfs, etc.
I'm in Fayetteville. I've gone through the Rust tutorials, but I'm more interested in writing code+proofs in Idris or Agda right now. A Rust meetup would be cool, but not something I'm going to drive 2.5 hours to.
In the article, you mention that you use mmap on Unix (I found the relevant source [here](https://github.com/fitzgen/wee_alloc/blob/master/wee_alloc/src/imp_unix.rs)). Might I interest you in our [mmap-alloc](https://crates.io/crates/mmap-alloc) crate? It currently supports Linux, Mac, and Windows, and it'd be good to have more consumers so we can give it more testing :)
Then `Serial::read_exact` etc need to be unsafe, right?
&gt; The variance of that lifetime parameter is implicit, but changing that is also a breaking change. This is perhaps a very surprising source of breaking changes because the crate author might be unaware of what's happening. I feel like the precise rules around variance are pretty badly taught at the moment, but at least [semverver](https://github.com/rust-lang-nursery/rust-semverver/) might be able to automate checking for breaking changes due to this.
&gt; I am not sure how far you want to go here. Removing panics would require very drastic changes to a language, perhaps even designing it from the ground for that purpose - see Erlang as an example. Adding more support for not panicking might be possible, but ensuring your program is correct will take more time than not doing so, so most likely you would still wait just as long for RLS to work. No I don't want a panic-free language, I think that's impossible for Rust especially as it relies heavily on dynamic allocation. I think it starts with the attitude. So far the attitude has been that "panics are safe" and this is oft-repeated, but while technically true it's also wholly unhelpful as merely writing safe software is not the goal of software development. Once the attitude is fixed then technical solutions can flow. Maybe that means adding alternate panic-free versions of some functions. Maybe it means more development towards things like slice patterns that can help avoid a lot of panic-laden code. Maybe it's effort towards linting common programmer laziness issues like unwrap that lead to panics. I suspect all of that will happen at some point during Rust's development. It's just about priorities, and after people start to take memory safety as a minimum standard for a compiled language rather than a novelty the focus is going to shift towards things like panic safety.
You may want to look at [C4](http://www.cccc.io/downloads/) , which is basically an attempt to create a universal and fast content fingerprint so facilities can share content and avoid duplication of media and effort. I still think he chose the wrong hashing algorithm, and that the crypto properties are not as important as speed, but...
That code probably needs an assertion about `mem::size_of&lt;T&gt;() == 1` in `HashSet::&lt;T&gt;::new`.
That sort of transition would also be really nice to have for `Cell`, alongside the `&amp;Cell&lt;[T]&gt;` -&gt; `&amp;[Cell&lt;T&gt;]` made available in [RFC 1789](https://github.com/rust-lang/rfcs/pull/1789).
I think once we use LLD for webasm (LLVM 6?), that will handle dead code elimination itself.
That sounds pretty great -- would you like to make a pull request? I'd love to work together on it!
nice, I had not heard of ramp before! It is exciting to see a more liberally licensed bignum library that actually seems to have good performance.
My understanding is that the current linker already supports `--gc-sections` and rustc emits one function per section. It is only `.wasm` that wasn't supported by the current linker, and so once `lld` is used, the toolchain will be *extending* that support to `.wasm` as well. All of these tools already exist for ELF/Mach-O/etc... and it is just `.wasm` that is still relatively immature.
Sure thing! I'm about to release 0.2.0, which adds a bunch of features, so I'll make a PR once 0.2.0 is out.
I used to like Java too. It was standard for me to implement high-performance code in C/C++, then generate ffi bindings (I had some nice tooling at my old job specifically for auto-generating everything with gradle) and write some larger abstraction/ concurrency/ network model in Java. Since I learned Rust I haven't touched a single line of Java. While I still interact with some C code, concurrency, networking, abstraction and general ease of use has all been replaced with Rust. And tbh, Rust does it a lot better than Java ever did. This is coming from Java being the first language I ever learned, having written several games and game frameworks in Java, using it over 5 years professionally in industrial grade software: I don't miss it
Sure, a heap allocated `Box&lt;[T]&gt;` instead of `&amp;'static mut T` would also work. As I've said [before](http://blog.japaric.io/rtfm-v3/#why-static-mut) `&amp;'static mut T` and `Box&lt;T&gt;` are similar as they are both owning pointers. A memory allocator likely may not be present on the systems where this API will be used so `&amp;'static mut T` is used in the API as it's more (always) accessible. I think the API could be made generic to accept both `Box&lt;T&gt;` and `&amp;'static mut T` in the buffer argument though.
Awesome! 😊
A lot of rust code uses the implicit return, it does seem weird at first, but I got used to it. I think I started to trust the compiler to tell when there is/isn't a semicolon in the right place. I don't think there's a way to mess it up in practice. (But in the end it's just code style which doesn't matter very much) Also, if you don't know, a preceding underscore on a field is used to silence unused code warnings for it. I'd avoid using them for anything else because it can communicate the wrong thing to readers and the compiler.
&gt; an owning pointer that can operate without an allocator. I consider `&amp;'static mut T` to be an owning pointer and you can create one without an allocator :-). `&amp;'static mut` effectively means "exclusive access to T for the duration of the whole program". That reads as ownership to me but I guess you could argue that it's not real/traditional ownership because `T`'s destructor will never be called nor its memory will be deallocated.
I ***love*** it when CLI tools are actually internally split like * a library crate, with `library::Options` and a `pub fn run(library::Options) -&gt; library::Result` * a very small `src/bin/library.rs` that just does argument parsing into `library::Options`, calling `run` with those options, and printing any error messages to stderr and exiting 1 You never know when someone might want programmatic control, and splitting your code like this lets people get it. So, **yes** you should publish such a crate on crates.io, because then people can access both the actual CLI tool and the library it thinly wraps. Examples of my CLIs that I've structured in this way: * https://github.com/fitzgen/wasm-nm * https://github.com/fitzgen/wasm-snip * https://github.com/fitzgen/dwprod 
I was actually wondering why `mem::transmute` is not used to get from `T` to `u8` in the hasher.
Not the version of it in the "Improving the guarantees" section and onwards since only the owner of `dma1::Channel5`, of which only a single instance exists, can access the registers related to that DMA channel. In the versions before that section, for simplicity, it's omitted where `dma1_channel5` even comes from. You can assume it's a "global" variable only visible to the `Serial` abstraction and that there can only ever be a single instance of `Serial` at most.
That was a very humorous take on learning programming, I certainly enjoyed the tale!
Woah! How does one hotload rust code?
Interesting question. What I would do in that particular case is start the DMA transfer with the maximum expected size of the packet. Wait a bit and `peek` into the DMA buffer. `peek` lets you see into the part of DMA buffer that the DMA has already written to; since this part will no longer be modified `peek` gives you a slice view (e.g. `&amp;[u8]`) into that part of the buffer (this doesn't break Rust aliasing rules because the DMA no longer has `&amp;mut-` access to that part of the buffer). You could then parse the header and modify, on the fly, the length of the DMA transfer to the expected length (never tried this but should be possible and safe as long as you don't do something silly like change the length to less to what has already been transferred or to something bigger than the size of the buffer). Then you just wait (either busy wait or let an interrupt call you back) until the transfer is done. Alternatively, you could do it in two DMA transfers. The first one receives the part of the packet that corresponds to the header. Then you parse the header and start the second DMA transfer to get the rest of the packet. Depending on what you are dealing with (e.g. serial without hardware control flow) this approach may be more prone to buffer overruns. &gt; ... or would that nullify the advantage of DMA in the first place? No, seems OK as the DMA will still do most of the job.
That is also true for when the code is just on GitHub.
I'm using [`libloading`](https://crates.io/crates/libloading) and [`notify`](https://crates.io/crates/notify), plus [`cargo-watch`](https://crates.io/crates/cargo-watch) running in a separate terminal. Googling for "rust hotloading" brings up more info.
Something looks a bit off... https://play.rust-lang.org/?gist=f705ec6298677b861aad531a6b93e7b4&amp;version=stable
Is peek something that already exists (that I'm not finding the docs for) or something I'd have to write?
This is amazing! So funny and so true.
It should be (1 &lt;&lt; s) and not (s &lt;&lt; 1) everywhere.
That's cool, I didn't know that!
Neat! So are you just re-linking one single render function, or your whole application?
&gt; Maybe that means adding alternate panic-free versions of some functions. Maybe it means more development towards things like slice patterns that can help avoid a lot of panic-laden code. Maybe it's effort towards linting common programmer laziness issues like unwrap that lead to panics. I'm not sure what more you want, since all of these are already happening.
Ahah, we chew through hundreds of Terabytes every day. And I don't think we're a small player. I guess it's more on how much of a voice does the IT has. 
Oh? I wasn't aware of this, could you say more?
For this exact reason. :)
Yeah, see /u/Cocalus for why, I’m mistaken!
`peek` doesn't exist in the reference implementation of `Transfer` (stm32f103xx-hal). It can be implemented by reading the CNDTR register, which indicates how many more bytes the DMA needs to transfer, and then handing out a slice of the buffer whose length depends on that value.
Gotcha, thanks!
Ah, so RFCs only show up in TWIR when they change states? I guess I was misunderstanding that somehow the whole RFC pipeline was being represented, but I suppose there are too many pending RFCs to list so only new/FCP RFCs are shown. I think I got that impression because a fair number of RFCs have jumped directly from "New" to FCP, but really there's an invisible middle phase.
Sounds like you want `mem::size_of&lt;T&gt;() &gt;= 1` at least? I'm 90% sure this will break for zero sized types.
Essentially one function (and a couple of supporting ones). The whole point is to keep the dynamic part as small as possible so it compiles quickly.
The context thing will help me in a lot of places. Ill need to adopt it. Something worth getting on the Rust idioms and patterns page, for however much that is still maintaned.
Hoo man this is exciting. If we can nail these features and really make the web and embedded domains tight in Rust, then I'm super excited to see how much mindshare Rust can grab if people just get curious and give it a chance. Thanks for the info, Aaron!
It would be really great to have "monadic" smart pointer support, i.e. the ability, given an x: Anchor&lt;&amp;mut S&gt; to be able to say &amp;mut x.field1 and get Anchor&lt;&amp;mut T&gt; where S has a field of type T. This needs associated type constructors, plus a "DerefField" trait that can take a field type and offset and gives a pointer to the field. Also support for matching in the same way would be great (this would allow to get fields of enums as well). This would also allow to write other smart pointers, such as smart pointers that can point to data in a file or on the network, and still work mostly transparently. 
A cleaner way of getting the `u8` value of the input than what you're doing in `hasher` would be to make it so that `T` must implement some trait: trait Unsigned8Bit { fn as_u8(&amp;self) -&gt; u8; } which you can then call instead of doing weird unsafe pointer stuff. You could then also add a `from_u8` method which would allow you to make a method that gives all the values the hashset contains.
&gt; Unfortunately, learning C is the equivalent of getting near the bottom of the turtles C only gets you to within visibility of the surface of the water As a computer engineering major, I regret to inform you that the stack continues below the surface for some distance. I dove to its bottom and climbed my way back up and it is *wild*.
C is the streets and sidewalks of Manhattan. The surfance underneath them is assembly, and most people stop there. Some unlucky souls start digging, and learn about the extra, and quite large, layer or landfill it's actually all built upon. Then some garbage laden SPECTRE from below rears it's ugly head, and the illusion is shattered.
Any links for #2? ''' Breakthrough #2: @nikomatsakis had a eureka moment and figured out a path to make specialization sound, while still supporting its most important use cases (blog post forthcoming!). Again, this suddenly puts specialization on the map for Rust Epoch 2018. '''
Yeah, I was pretty resistant to implicit returns at first too. In the end, it's a matter of taste, but I'll point out a couple of things: 1. The compiler prevents you from accidentally doing anything wrong here. If you don't return something (implicitly or otherwise), you get compilation error. So even though it's only a one character difference, it's not a dangerous one. 2. What really started to win me over is when I realized it's not specific to returns. This works for any code block, which has some useful applications. For example, avoiding polluting your code with variable names that are only used for initializing one thing: let thing = { let a = calc_1(); let b = calc_2(); let c = calc_3(); a + b.foo(c) }; This is a pattern I use quite a bit, when code isn't quite reusable enough to be factored out into a separate function. It keeps it obvious that all of those calculations are just for `thing`, and keeps them out of the larger namespace. Once I started using the feature for things like that, I became a lot more comfortable using it for implicit returns as well (which is really just a special case of this broader feature).
just posted http://smallcultfollowing.com/babysteps/blog/2018/02/09/maximally-minimal-specialization-always-applicable-impls/
Good catch. I didn’t take into account that kind of features.
Not sure it’s the right way to go, since you can mess up the implementation, while a raw bit re-interpretation won’t fail. However, something like this would be valuable: unsafe trait Encoded8 unsafe impl Encoded8 for u8 unsafe impl Encoded8 for … 
I'm a Computer Engineering major as well, I just label it as CS, as it's usually a little more clear what my focus is, programming :) You're 100% correct though. I'm on the dive down currently, and the bottom-most turtle seems to be standing on a pool of murky water. I can barely make out the outlines of some monolithic, *Lovecraftian-esq* beast somewhere down there. Ugh, and I thought assembly was the end. 
I have just implemented `Transfer.peek` in [this commit](https://github.com/japaric/stm32f103xx-hal/commit/4e9dae9b3758b780d396b2d512653ecf23bdb507#diff-1144e93dc5fdc4cfac0b119204aebb50R287).
Ahahaha nope! Have fun building a CPU from bare silicon\*; it's a magical experience! \* on paper or simulation; my college didn't let us run a fabber that would be INSANE
Hello it's me quantum electrodynamics and I'm here to fuck up your entire early 20s
Could there be a "Leakable" marker trait? And thus at in limited context we could have drop guarantees?
I really wish contexts were just fully hidden arguments that are constantly passed. (either really as an argument or some sort of behind the scenes TLS). Python gained that recently in 3.7 ([PEP 550](https://www.python.org/dev/peps/pep-0550/)) and .NET has something similar with logical call contexts. It permits systems that are completely independent to be able to always discover what's happening. This is especially useful if you have things like auditing or security systems where accidentally losing context can have dangerous consequences.
“This is the only time in my entire programming life that I've debugged a problem caused by quantum mechanics.” https://www.gamasutra.com/blogs/DaveBaggett/20131031/203788/My_Hardest_Bug_Ever.php
Not sure what you mean with data corruption, but all the other things you listed are memory safe.
In general I'd say don't use unsafe if you don't have to, and this is a case where you really don't have to. A raw bit Interpretation definitely can fail, for example rust doesn't guarantee the memory layout of types like C does, and also can reserve states for different purposes, and using pointers completely bypasses the type system that would prevent you making a mistake like that. Messing up the implementation of a function that just returns the integer inside a type is plainly not going to happen. 
Take a look at https://github.com/rust-lang/rfcs/issues/2327 I'd love to get more inputs from people who want contexts in their call stacks.
A `Leak` trait was brought up in the context of the [leakpocalyspe](https://github.com/nox/rust-rfcs/blob/master/text/1066-safe-mem-forget.md#alternatives) (also see the [discussion](https://github.com/rust-lang/rfcs/pull/1066) of that RFC). `mem::forget` is already safe though so we can't have drop guarantees in Rust 1.0. Making `mem::forget` unsafe in *any* context would be a breaking change.
For example, if the type has padding bytes, the byte reinterpretation can be UB.
Doesn't LLVM 6 also fix most of the blockers for the AVR target?
I don't know about "most", but it's definitely needed, yes.
Specialization is something that I really wait for!
Regarding repeated uses of a type parameter: Why does it have to generate separate for all variables? i.e., instead of forall&lt;A, B&gt; { if (WellFormed((A, B))) { exists&lt;T&gt; { (T, T) = (A, B) // &lt;-- cannot be proven } } } generate forall&lt;A&gt; { if (WellFormed((A, A))) { exists&lt;T&gt; { (T, T) = (A, A) } } } 
Repeated type variables allow you to sneak in lifetime-dependence, e.g. `(&amp;'a T, &amp;'a T)`
I can't respond at length right now, but my [earlier post](http://aturon.github.io/2017/07/08/lifetime-dispatch/) has a lot of detail about the soundness issues.
I used to be the co-organizer of that meetup. The main organizer left the rust meetup to free for another organization as he wasnt that interested in rust. I was about to step up as the organizer but meetup ask for a subscription fee which was impractical for me. I had setup a meetup once with 4 people, 1 of which was from mexico having a vacation.
Do note that `cargo install` does not require the crate be published to crates.io. It also works with git repos.
This makes me want to see how small+fast a slab allocator could be made, along the lines of the one the Linux kernel uses for kmalloc.
That same author is working on an allocator to improve binary size: http://fitzgeraldnick.com/2018/02/09/wee-alloc.html
I wish the Go designers had thought more about the implications of introducing an explicit Context type. It pollutes *everything*. For example, since there's no function overloading, in order to preserve API compatibility when adding a context, you have to introduce a parallel function/method. Everything that gets a context has to pass it on. Python's solution is a lot more elegant.
I'm in LR, but I'm not past the subreddit stalking tourist phase of learning Rust... if this ends up happening though, please dm me; I'd love to come!
Yup! It even fits my use case quite well! I have a couple other ideas to try first, though. Thanks for noting!
DMA in some form is used for virtually all data transfer in and out of RAM on desktop systems. This includes HDD/SSDs, GPUs, network interfaces, USB devices, etc.
Thanks to everyone again. I'll take a look at your suggestions tomorrow when I wake up and improve the code.
I know it's just bikeshedding, but am I the only one who hates the overloading of the term "stable" that this introduces? Rust uses "stable" vs "unstable" to talk about language features, but now this is using it to talk about whether a memory object can be moved. Even in this post, there is the sentence "Once again, we have two generator types, both of which have stable APIs" and I had to stare at it a bit to decide in what sense of "stable" that was referring to. Suggestion: instead of "stable", use "anchored"--the `Anchor` struct seems well-named to me, and all the introduced "stable" traits are about using them with `Anchor`s. So `AnchoredDeref` (or maybe `Anchorable`) instead of `StableDeref`, `AnchoredGenerator`, `AnchoredFuture`, etc.
I posted this below, but I'll post again. Check out this proposal I've made about context params: https://github.com/rust-lang/rfcs/issues/2327 Let me know your thoughts, like it, hate it, how to improve it. Anything!
I have a similar feeling. I'm not sure how many file formats would lend themselves to this. Very little of our dataset divided up nicely into meaningful chunks. But still an interesting idea none the less. I'd be interested to see an analysis of the results over some typical datasets: Exr files, alembic caches and tiffs or the like. I'm also curious how it works with the inbuilt compression in those formats. 
Left a comment there.
Go's solution is also completely useless for code that needs a context even if an immediate caller does not pass it down. For instance in Sentry with trap into runtime environments to collect breadcrumbs of execution (think "show me all the logged messages relevant to the current http request). You can't do that with go's context and that's why our go client is a lot less fun to use than what we provide for instance for Python.
&gt; For example, since there's no function overloading, in order to preserve API compatibility when adding a context, you have to introduce a parallel function/method. There are some tricks that can help. For example, I've seen an implementation of `io.ReadWriteSeeker` (or some such) that supports deadlines via `context.Context`. It provides a single method for this, `WithContext(ctx ctx.Context)`, which returns an `io.ReadWriteSeeker` that uses the supplied context. * Pro: Very little API surface. * Pro: It's convenient for the caller. you can call anything that uses that interface and have the deadline be respected. And if you're making a bunch of calls, you can keep hold of that object rather than passing in the context repeatedly via `f.WithContext(ctx).Read(...)`. * Con: It must be a pain to implement on the callee side; you need to write forwarding methods for everything. But I think this is minor; there are tons more callers than callees. In any case, does this problem apply to Rust futures situation? Is there a time when a given method would sometimes take a context and sometimes not? &gt; Everything that gets a context has to pass it on. I only think this is a significant pain when you first decide to plumb it through 10 layers of code and have to touch everything. Once it's already there, seeing the extra argument doesn't bother me at all. So my question is: after the initial ecosystem change, is it likely people will be adding these contexts to existing code?
So from the actual uses of specialization in the `std` library, would all of them be allowed under this proposal? I never thought we would get intersection impls in some way, so I am very excited about this.
I do see the benefit for deduplicated *transfer* of files from one facility to another, but that's kind of what BitTorrent and bencode already exist to solve... so then if you just store the bencode of any given file in a DB you can arbitrarily check if the chunks of that file, or the file itself, exist in some other physical or virtual facility. And I see the benefit of a deduplicating file system, like ZFS, where you're doing essentially what the OP is talking about, but -- critically -- *not* in a full, transactional SQL database, and not normally across any sort of networking layer. And I also see the benefit of what [C4](http://www.cccc.io/) is trying to do, with a universal fingerprint for any given file ... if the other facility already has the C4ID, then they already have the file. And it's proposed use in Merkle Tree's is interesting, because now you can have the C4ID of an entire sequence or dependency tree, and use that to determine (from only the C4ID of the candidate) if the candidate belongs to the larger grouping. But the problem -- as I see it -- with C4 is that you're essentially calculating the SHA512 of the entire file, which -- at least at Weta -- would have meant spending *centuries* of compute time on just the back catalog, without any obvious actual benefit from the cryptographic properties of SHA512. I keep wondering if it should be using something like murmurhash3, which is many orders of magnitude faster at file fingerprinting (fast enough to use in deduplicating file systems), but isn't cryptographically secure. Joshua -- the originator of C4 -- seems to pretty adamantly believe that SHA512 is the right choice, but also doesn't seem to think that it's prohibitively slow ... but I tried to sha512sum a goodly portion of some comp renders once in an experiment down the same lines, and found that you just can't afford to wait for 10,000 frames of uncompressed 8K EXRs to be cryptographically hashed and still get the movie on screens.
&gt; The repeated type restriction may have simply been 'harder' due to the class of proofs allowed under Chalks algorithm as he mentioned later. I'm not sure if it implies those repeated uses are inherently unsound or just not currently provably sound. It's never sound. Repeated types are bad because repeated lifetimes are (and types can contain lifetimes). Repeated lifetimes are bad because they introduce relationships, such as "there two places where lifetimes can go in, happen to have the exact same lifetime". E.g. `(&amp;'a (), &amp;'a ())` will match any `(&amp;'x (), &amp;'y ())` with an extra `'x == 'y` condition. Half of the problem with those relationships is that specialization requires *a decision* based on whether `==` (use specialized impl) or `!=` (use base impl) holds between `'x` and `'y`, while type-checking can produce an user error when it can't prove `==`. The other half of the problem is that while you could *in theory* "match on" lifetimes to determine whether the specialization applies, you can't do this without violating lifetime parametricity, which Rust relies on in the form of lifetime erasure in trait objects and function pointers, the [`indexing`](https://docs.rs/indexing) crate, etc. People mention the codegen problem, and while it's pragmatically the worst part about it, as *every execution* of a function produces unique lifetimes for a myriad of points within that execution, including every iteration of a loop, and *all of those* have to be tracked *somehow* (Rust's local reasoning when checking lifetimes and borrows is still a significant workload, but orders of magnitude cheaper than global reasoning and hence superior over most bolt-on C++ attempts), you *sort of* could monomorphize based on lifetimes. You could even pass information about lifetime parameters at runtime - the `indexing` crate would presumably break, but `fn` pointers would presumably still work, with runtime specialization decisions instead of compile-time ones. **Except** `for&lt;'a&gt; extern "C" fn(&amp;'a T) -&gt; &amp;'a U` is a valid type compatible with `extern "C" fn(*const T) -&gt; *const U`. You can't pass any runtime lifetime information through that. That's, IMO, the death of non-parametric lifetime proposals. You can allow bypassing the soundness of generativity (i.e. the `indexing` crate) without changing whether *existing* code compiles/runs, but to pass lifetime information at runtime, you'd have to either: 1. ban lifetimes in `fn` pointers with non-Rust ABIs (existing codebases - including, I believe, parts of Servo/Firefox - would stop compiling) 2. change the ABI (same codebases would crash at runtime when C calls them) * if you think about it, this is like asking C to track lifetimes for us (which it clearly can't) - might be fun with a custom C compiler though
I've been coding go for years and still haven't found a good reason to use context. I'm not a fan, so I avoid it
Wouldn't outright dynamically scoped variables make sense then? They're common in Lisps.
Nice! A small issue: question 8 expects the results to be sorted by weight and name, but the statement only mentions weight.
I know little about lisp but i thought they basically require the calls to be on a stack of sorts?
Semantically it reaches up the callstack to find the top-most value for the slot (I'm sure it can be implemented more efficiently than literally walking the call stack but I'll confess I never checked the implementation details). That seems similar to my understanding of PEP 567, except built-in and with the token dance being implicit?
The point is that there is no stack. 
I just wish that it would not end up as Scala implicits. That's one big mess.
I prefer `pinned` to `anchored` (personally). A boat at anchor is not fully stable, it still bobs up and down, whereas something that is pinned is much less mobile :)
Are you looking for /r/playrust?
Hmm I don't know but I haven't gone looking either.
How do you plan to handle the layout of components that need to wrap around, like text boxes? Is it necessary to fix the length of one side?
On return from my Rust meetup, I get to read this! Woot 😎😊🦀! 2018 is going to be an awesome year Rust-wise. Now if we only had impl trait and macro attributes on stable...
Seems to me PEP 567 context variables have stacks, just stacks you manage by hand using the `Token` returned by `set`, which you can pass to `reset` to "pop off" values up to (and including) the one whose setting generated the token.
I've been on nightly rust since I've started learning. Clippy has been fantastic, and certainly has helped me in my first few months of dabbling in Rust. However, past a certain point, it hasn't been quite as useful as it once once. My `lib.rs` is littered with various attributes such as `#![cfg_attr(feature="clippy", allow(......))]`, because as an "intermediate" rustacian, I feel I know as if what I'm doing. That being said, clippy has certainly helped me progress to this point so far. I am excited to see clippy be included on the release versions of rust! It'll absolutely help reduce the steepness of that learning curve.
Great work! We will have maximally minimal modifications to meddle in magnificent optimizations 😊❤🦀
Text layout is done after layouting the rectangles, it's not influenced by constraints. I don't yet know how I can do the `wrap` property, simple centering / aligning is the first thing on my list.
Stacks as in callstacks. You cannot rely on them in async systems which is why pep 567 does not use callstacks at all. 
Small erratum: &gt;It would be nice to be able to define an impl that allows a value of **the never type ! to be converted into any type** (since such a value cannot exist in practice: &gt;impl&lt;T&gt; From&lt;T&gt; for ! { .. } "into any" vs "From&lt;T&gt;" ... Either the code is wrong or the description is.
That’s a bad system. Would not advocate for it :)
The code seems to be what is wrong. It makes no sense to be able to create a !-type from any other type.
I think that conversion makes sense and is doable but I can't find a usecase for it if that is what you mean?
Yes! I should have worded that more clearly. Although now that I think of it carefully there could be some use-case. It could make the question mark operator act like `.unwrap()` if the containing function returns `Result&lt;T, !&gt;`. Not sure if this is really a good idea though.
If I do get something set up I'll definitely let you know. Ideally we would have a wide range of skills showing up anyway. Getting involved in a local community is an effective way to get better too. 
There's an RFC on the `futures` repo to pass the task context explicitly by `&amp;mut` instead of using TLS - is that what you were referring to by `poll` being panicky, or are there other reasons it could panic? (I already pinged you [there](https://github.com/rust-lang-nursery/futures-rfcs/pull/2#issuecomment-363846343), I don't know if you saw it.)
Why is it called Wee and not Waa¹? ¹ Web Assembly Allocator
Specialization is a feature I'm following with great interest, and I think what is laid out in this blog post sounds very promising. Unfortunately I'm not deep enough in the type system aspects of Rust to fully grasp what exactly we will be able to do with this proposal. There are in particular two cases for which I would really be happy if someone could help me understand how this proposal impacts them. Implementing binary operators for existing types --- A current pain point for mathematical applications is the fact that we can not generically implement the arithmetic traits for scalar types. For example, consider multiplying a vector by a scalar - you would like to write `a * x` where `a` is a scalar and `x` is a vector of the same scalar type (could be `f32`, `i8` or some rational bignum type for that matter). Hence you'd like to have an implementation in the style of `Mul&lt;Vector&lt;T&gt;&gt; for T where T: MyScalarType`, but this is currently disallowed by the orphan rules. The only workaround that I know of currently is to manually implement `Mul` for each scalar type that you want to support, which limits applicability, but this still doesn't allow you to write `a * x` in generic code parametrized over `T`. Will this proposal somehow alleviate this issue? It seems that what Niko briefly mentions about blanket impls might be relevant, but I can't tell. Specializing associated types --- From the post: &gt; This implies for example that we don’t need to impose the restrictions that aturon discussed in their blog post: we can allow specialized associated types to be resolved in full by the type checker as long as they are not marked default, because there is no danger that the type checker and trans will come to different conclusions. I'm not sure what this implies. For example, say we have this contrived piece of code: trait MyTrait { type Assoc; } impl MyTrait for T { type Assoc = Self; } // Specialize for usize, not sure if correct syntax impl MyTrait for usize { type Assoc = isize; } Would this be allowed? Would greatly appreciate if someone could help me understand where we are currently heading :-)
I hadn't noticed that rule. Yes, I agree it's a violation. &gt; Under what conditions should this rule be broken? `¯\_(ツ)_/¯`
You dropped this \ *** ^^To ^^prevent ^^any ^^more ^^lost ^^limbs ^^throughout ^^Reddit, ^^correctly ^^escape ^^the ^^arms ^^and ^^shoulders ^^by ^^typing ^^the ^^shrug ^^as ^^`¯\\\_(ツ)_/¯`
Yes, I agree with that. The earlier PEP 550 was much closer to "let's add dynamically scoped variables to Python" which I think would have been a lot better. This restricted PEP 567 is a lot more complicated and a lot more tied into the functionality of asyncio, IMO. I think Rust should add dynamically scoped variables to handle this problem, too, but I feel like that's a lost cause, and probably something very problem-specific will be added instead.
Well yeah but the question is how do you know how wide and high the textbox will be given a text?
you should file ticket in actix repo https://github.com/actix/actix this could be bug in actix. i've never thought about ```impl Handler&lt;IrcMessage&gt; for System``` option
i created ticket https://github.com/actix/actix/issues/44
good bot
```IrcClient::for_each_incoming()``` is blocking, that is the reason why doesnt get called. here is code that works: ``` impl Actor for IrcActor { type Context = Context&lt;Self&gt;; fn started(&amp;mut self, ctx: &amp;mut Self::Context) -&gt; () { println!("IRC actor started!"); let addr = Arbiter::system(); thread::spawn(move || { let client = IrcClient::new("./conf/config.toml").unwrap(); client.identify().unwrap(); client.for_each_incoming(|message| { println!("Handling incoming message"); //self.system.send(IrcMessage::Notice { message: "test".to_string() }); addr.send(IrcMessage::Notice { message: "test".to_string() }); }); }); } } ``` 
PEP 550 implemented variables which were semantically (almost) identical to dynamically scoped variables in Lisp or other languages. I don't really want to write a long explanation of dynamic scoping, so if you are not familiar with the concept or don't see the resemblance, you should be able to find some articles explaining dynamic scope. There were also several posts in the python-ideas discussions about PEP 550 that made the comparison to dynamic scope. This one is decent: https://github.com/njsmith/pep-550-notes/blob/master/dynamic-scope.ipynb
/u/eddyb Do you have a generic explanation posted anywhere for why monads/generic-`do` aren't a good fit for Rust?
1. This is only for my self-study 2. I did look at diesel and that is why I'm doing this little project. I don't quite like its public API. However, the way it uses to generate result query could be reused 3. What I mean by native sql is that every compiled program is valid sql and it feels comfortable using it in rust 4. Just generate sql queries that are type safe for example: `QueryBuilder::select().from().wher().run()` 5. Using generics it is creating modified structurs for example you start with `SelectExpression&lt;NoSelect, NoFrom&gt;` when you use the select method on this object it returns `SelectExpression&lt;Select, NoFrom&gt;` which does not implement select anymore, but implements from and run methods.
Does this mean, the next nightly will be using LLVM 6? (please?)
AFAIK async Python has proper callstacks, the issue is setting up and tearing down data when the callstacks are switched from one routine to the next: as the callstacks are not torn down but merely suspended, the rollback code of one does not run, nor the setup code of the next, and so you end up with routine-local data stored in a threadlocal clobbering the data of the next routine. Dynamic variables shouldn't have that issue.
/u/eddyb delivers: https://www.reddit.com/r/rust/comments/7qq0rp/asyncawait_and_try_vs_donotation/dsr52h7/
Nice! My experience with writing Unix utilities has been similarly good. The `println!` gotcha you mentioned at the end has bitten me too, and led me to make an awful mistake in some of my tools that your proposed macro will also make: if `println!` fails because the pipe is closed, then the error should _not_ be ignored and the program should quit gracefully. Looking at your code, I have a few other comments. :-) * You are using a lot of `unsafe`. Are you sure all of those are really necessary for performance? I know I've personally found bound check elision to make a difference in a very small number of cases. Even the [pre-allocation of `Vec` for `Copy` types](https://github.com/gnuvince/ppbert/blob/5e32ad6fac9a54c77afd7133af493a1f25d5a703/src/parser.rs#L153-L154) I've found to not make much of a difference from just pre-allocating the capacity and using normal `push` calls (or `copy_from_slice` if possible, but that doesn't look possible here). * Popping up a level, have you considered just using `String::from_utf8` (no unsafe) with `self.contents` directly in place of [most of this code](https://github.com/gnuvince/ppbert/blob/5e32ad6fac9a54c77afd7133af493a1f25d5a703/src/parser.rs#L151-L171)? In particular, it looks like you're taking a pass over the bytes _just_ to check for ASCII, and I believe `String::from_utf8` does have optimizations specifically for ASCII. I don't mean to say switching will definitely be faster since `String::from_utf8` since has some amount of overhead for handling the more general case, but it might be worth experimenting. * I think [`peek_u8_unchecked`](https://github.com/gnuvince/ppbert/blob/5e32ad6fac9a54c77afd7133af493a1f25d5a703/src/parser.rs#L311-L313) is a misuse of `unsafe`. It is a private method so its scope is limited, but if a function is declared as safe (as this one is), then it should be safe to call *in all contexts*. This function most certainly does not meet that criteria, so it *should* be marked as `unsafe`. When a function is marked `unsafe`, this means the *caller* is responsible for ensuring the safety of the call. Great blog post though! I share many of your sensibilities. :-)
Oh right! I think the author is /u/gnuvince
[removed]
&gt; Doesn't async(IO) Python have proper callstacks The "native" representation of a task in Python is a future. The future usually gets invoked by the event loop so you loose your true call stack. &gt; Dynamically scoped variables shouldn't have that issue, and can more easily be reused by other types of control flow mechanisms (by having language-level dynamic scopes, you can provide hooks to copy/restore them even in non-coroutine-based async systems without needing to be aware of specific contexts). Precisely. You need a separate system to manage the context behind the dynamic variables. As far as I am aware the ones in lisp do not do that which is why I was asking. (In Rust dynamic variables are unlikely to work as such anyways because of the complexities wrt Sync types, unavailable data at runtime as the value might be unset etc. That just does not fit well in)
There is this: https://github.com/japaric/nvptx /u/japaric should be able to tell you more 
How much do you expect to leverage the typesystem? For example, is it intended that `select` can only range over column names introduced in `from`? Or does it no matter?
&gt; but at least semverver might be able to automate checking for breaking changes due to this. It already does! Its README lists "changes to the variance of type and region parameters" as one of the kinds of changes it recognizes.
Makes sense. Regarding address, you can clone address and send to different thread. Also I would create custom actor and use that actor as an irc message handler. You can spawn custom actor to different threads, you can store state on it. You can not do anything of this with System handler.
It does not matter, for now. It is a nice feature but in my opinion, its benefits come after easy to use library
Aye, quality acronym, but a bit like a bawling and greeting bairn than a wee allocator.
&gt; I modified the atom() parser to use Vec::new() and Vec::push() Why not `Vec::with_capacity()` and `Vec::push()`?
Take a look at PyO3. It offers two-way communication between Rust and Python: https://github.com/PyO3/pyo3 Milksnake is used to build rust extensions for python applications: https://github.com/getsentry/milksnake If github search weren't so terrible, I'd show you a list of projects using milksnake that you could reference. Two that come to mind include: https://github.com/torchbox/rustface-py https://github.com/Dowwie/pyfakers 
Hey, can some tell me why this code compiles? fn main() { let foo = String::from("foo"); borrow_str(&amp;foo); } fn borrow_str(s: &amp;str) { println!("{}", s); } &amp;foo is type of &amp;String and s is type of &amp;str, so is there an implicit conversion between String and str?
This is probably the wrong awnser, it did you try removing the semi-colon?
Just wanted to say thanks for the blog post! Even if you're the only user of this tool, having this sort of feedback is invaluable, and builds encouragement regarding Rust's ecosystem. Since you mention Erlang, have you attempted to use Rust for NIFs?
Yes, and in this instance, it's about the `Deref` trait. Types can deref into other types if they implement the conversion. As `Vec`s deref to `[_]` slices, `String` derefs to `str`. For a more thorough writeup, see [Holy std::borrow::Cow](https://llogiq.github.io/2015/07/09/cow.html). 
Yeah, the its index + 1, since it doesn't make sense to reserve a free list for zero sized allocations.
Even the main Go packages don't follow that rule everywhere. In the http package, Request holds a context internally. I quite hate Go's Context :)
https://github.com/MaikKlein/rlsl is the closest thing I know of. As for i128 on NVPTX that means either: - Someone fixes it in LLVM. The backend has some commit activity but I don't think 128-bit integers are very important or common in CUDA-type code, so I wouldn't expect it to get worked on any time soon. - Someone implements i128 lowering in rustc. Represent i128 as a (u64, u64). Figure out argument passing. Lower arithmetic to intrinsics from compiler-builtins. Implement conversions. For bonus points, implement the intrinsics using nvptx assembly with the uint4 type. This is an achievable amount of work, but again I'm not aware of anyone working on this.
It's also easier to use to automate my desktop reinstallation. I already have it tooled to install my Rust dev kit. I can just slap a few secondary binaries onto that list, similar to what I do with NPM.
Forbes publishing some Quora post from almost four years ago written by a person not privy to board-level communications is hardly authoritative. I did resign, but not because of any community backlash or frontlash or other reaction. Neither faction caused measurable change in Firefox market share. If you study the form 990 for 2014 you can find one clue to what happened. I can talk about that because it is a public document and does not say more about why I left, but I'm otherwise going to respect non-disclosure agreements I signed. As your comment here amply demonstrates, people project what they prefer as a "good story" with "just desserts" or "martyred victim" narrative onto the surface facts, but the facts do not support either reading. This has by now zero to do with Rust or its community, so my advice is to drop it.
Looking at the macro, it's expecting a path not a type: https://github.com/rust-lang-nursery/error-chain/blob/48c18a9747bb31b663d990f8971963706a4652d7/src/error_chain.rs#L48 Also having the error type be () implies that it never returns an error. Looking at the source confirms this: https://docs.rs/futures/0.1.11/i686-pc-windows-gnu/src/futures/unsync/mpsc.rs.html#171 So I'd say don't try to chain a non-existent error, just unwrap().
I’m in Nebraska, which is next to Kansas, which is a substring of Arkansas.
Errors starting out as a `Result` with the caller turning it into a panic/exception when an error value indicates a bug is one of my favourite parts of rust.
Something like pip/pypi but with a better story for native code could definitely be valuable. It's kind of annoying how pypi works well until you depend on a native library. Then the implementor of the library has build on a container (to get a ancient glib for the binary to work on 'many' linuxes) and the windows situation is even worse with 'just use VS to compile the dll and then distribute that as data). It's definitely annoying that interpreter languages have the definite advantage here of the user themselves having both the runtime and the compiler most times so code can be distributed everywhere (assuming sane practices).
I use this project. It's very useful and in my experience it just works. Thank you to everyone who contributes to it.
I actually wish Unix pipes standard was 'better' at handling multiple outputs on a standard 'encoding'. I mean something like 'i'm going to send 3 integers and then a stream, it's ok for you (the shell or any other programming language' to interpret this stream of bytes as a tuple (int,int,int,stream) and i'll be blocked on the stream until you read'. I'd be badass to have effectively unlimited 'libraries' without (much) native code needed.
I planned on doing that, but one step at a time. For using my own supervisor I need to implement: ```the trait bound `std::rc::Weak&lt;std::cell::RefCell&lt;actix::queue::unsync::Shared&lt;actix::dev::ContextProtocol&lt;MyOwnSupervisor&gt;&gt;&gt;&gt;: std::marker::Send` is not satisfied```
You should switch to master, it is much sillier. I am planing to release new version soon
Cool, I use StructOpt for all my CLI projects
This is really exciting. I was thinking about trying the [accel](https://github.com/termoshtt/accel) crate, and this looks like it could provide more compatibility. You say that RLSL is a subset, but you also say that you should be able to reuse crates. What's missing from RLSL? Just the reduced version of std?
This looks really interesting, but I haven't tried it yet: https://github.com/termoshtt/accel
I suspect the time is not really structopt itself, but more all of its dependencies: clap and syn must be the biggest and structopt is not possible without those 2 crates. You can try to `no-default-feature` it will disable optional features in clap and thus should reduce compile time.
It is not just the std, there are so many things that you just can not do in SPIR-V. RLSL is mostly targeted at Vulkan, which means you have to use the logical mode. In logical mode you can't really have pointers, I currently "sort of" optimize them away like C++ references. But you definitely can't have pointers inside structs, so a lot of things like iterators can't be easily expressed in SPIR-V. There many are other things that are not allowed like recursion for example. RLSL will include custom lints, if you try to compile a crate from crates.io you will get some (hopefully) descriptive error like "Detected recursion in foo, this is not allowed". 
Master instead of Supervisor? I like your humor nearly as much as your code ;) Thanks again, for the help and actix!
I did, in a simple test (I think I implemented collatz in Rust), and it was surprisingly easy to get working! For people unfamiliar with Erlang, NIF is the name of its foreign function interface, i.e., it lets Erlang code invoke Rust code.
ArrayFire is nice. It's Rust interface to C++ lib that will JIT compile stream of commands to openCL or CUDA. Not all the benefits of Rust, but performance is pretty solid.
:) actix master branch
My favorite change: &gt; structopt-derive crate is now an implementation detail, structopt reexport the custom derive macro by @TeXitoi It felt uncomfortable doing 2 imports for what is essentially the same library.
Oh, I was just meaning that since we don't know what to keep, the solution is to keep everything, as kind of a counterpoint to your last sentence. And that philosophy doesn't save us either when the time comes to get back assets as they were only really working in a specific context (and our pipeline moved a lot in the meantime). &gt; I always find it amazing that the facilities don't all get together and open source the pipeline thing This, I think, really is the crux of it all. It's such a shame, and that's too gigantic a project to be started without companies buy-in.
My opinion on this: Safe code is allowed to panic. As a point of good style you should only code a panic in situations where it would signal a programmer-level error - something which requires recompilation to fix. (This is the rule in rustc.) A library may panic if misused. It may panic if it hasn't been completely thought out. It *really really really* should panic instead of invoking UB. It should *not* panic from recoverable or user errors. Bottom line: safe code may panic. Most unsafe code may panic too. - Can you use "this library has unwrap calls!" as an airtight criticism? **No.** That would be like criticising split infinitives in English prose while paying no attention to the text's other merits. Can you ripgrep for panic, expect, unwrap, and assert as part of code review and ask "well, what situations would invoke this panic"? **Yes.** It's probably true that more libraries should. But seriously, panicking is how safe code gets out of sticky situations. It's just something that safe code can do and Rust expects you to live with it. 
I didn't imagine it was possible until the failure crate add this feature in v0.1.1. I've taken the trick from its code. I agree that's a great ergonomic improvement.
This is so good. I've been experimenting with a simple language on top of SPIR-V, but this just blows me away! At the end of the post you asked for requirements, I have one: It's a common pattern to build template shaders and a lazy shader cache that compiles shaders "on the fly" to make sure each shader is optimized for the specific set of features it needs. For RLSL, that would require conditional compilation and a compiler that can be bundled with the game engine. Either as RLSL or some intermediate representation. Would this approach be feasible? I would also be interested in contributing, this would be a huge boon for game development on Rust!
Let me give you some advice: never, *ever* try to read D code. You'll probably die.
But then who’ll defend K’un-L’un against the Hand?
But you don't need to, and many java / js / python / higher level language devs don't, know about them You don't need to know about the stack / heap to program in java, in fact it's really not essential at all
Excellent, are you aware of Rustler? https://docs.rs/rustler/0.16.0/rustler/
New Rust user, recently started looking at this and clap. I work at a company where there are a lot of programmers making one off utilities so command line parsing is actually pretty common. I really like the features clap has, but I'm hesitant to suggest using it if we ever look at Rust because it has 3 ways now to do the same thing -- I can use the builder pattern, the macro language, or struct opt. Is there any hope on reducing that down to just one? The choice means every programmer here will do it differently and we will lose some of the value of using one library consistently. Is there any way StructOpt isn't just better than the other two? If so why keep supporting the others and just enabling users to do it worse?
I have a borrow problem. This is a simplification of my issue which recreates it. I want to iterate over a vector, and within it build a string. My struct has both the vector and the string. When I do this it says I cannot append to the string as the vector is already borrowed. Is there a way to get this code to work? Here is a Rust playground of the issue: https://play.rust-lang.org/?gist=202c6b347765b6ef221bb2a5bd214781&amp;version=stable pub struct Example { ns : Vec&lt;u32&gt;, ns_str : String, } impl Example { pub fn new() -&gt; Self { let ns = vec!( 1, 2, 3, 4, 4 ); let ns_str = String::new(); Self { ns : ns, ns_str : ns_str, } } // This guy has the issue! pub fn run(&amp;mut self) { self.ns.iter().for_each(|n| { self.ns_str += &amp; format!("{}", n); }); println!( "{}", self.ns_str ); } } fn main() { let mut hw = Example::new(); hw.run(); } 
&gt; It's a common pattern to build template shaders and a lazy shader cache that compiles shaders "on the fly" to make sure each shader is optimized for the specific set of features it needs. For RLSL, that would require conditional compilation and a compiler that can be bundled with the game engine. Either as RLSL or some intermediate representation. Would this approach be feasible? Could you give me a more concrete example? I assume you are not talking about specialization constants but about actual different implementations. fn technique1(..) -&gt; Color{..} fn technique2(..) -&gt; Color{..} Where `technique1` should be used with GPU1 and technique2 should be used with GPU2. Something like this? fn vertex(vertex: &amp;mut Vertex) -&gt; Color { #[cfg(foo)] { technique1(..) } #[cfg(bar)] { technique2(..) } } I also assume that creating a new entry point would be not feasible because there are way too many possible combinations? I don't think I have yet considered this use case. Currently I have to build a custom version of rustc, this rustc then builds rlsl and rlsl requires the so/dll files of the initial rustc compiler. I currently store everything inside ~/.rlsl folder, and currently this is already 300mb big. Of course there are a lot of things there that are not needed. I am not sure how much I can throw away. I also want to statically link with rustc in the future and that should bring the size down. I think having the compiler as a library makes sense, but it is not very high on my priority list right now. The project is too messy right now for contribution, I am currently refactoring many parts of the compiler. I'll let you know when the project is in a more reasonable state.
In libraries, I think it's okay to panic as a bug, as in unwrap where you know the value is there and if it's not then it's a bug. There's little difference between panic and "check if okay, log error, exit with non-zero status code" I think it's also okay to panic in high-level libraries when the consumer has no option, but to panic himself. For example I know that output of `zpool list -o ...` is N rows with N columns separated by tab. So I can safely unwrap that result of `lines.next()`, because if it's not there - then either `zpool` has a bug, or I updated invocation arguments and didn't update parser. 
Yeah you do, if you want to be any good at it. Those who don’t light production code on fire regularly.
Although having multiple libraries for a task can be a problem in general (even if it's also a benefit in other ways), for the specific example of argument parsing I don't think it's a big deal to have multiple libraries. That's because unlike foundational libs (like e.g. serialization) that every other lib is built on top of, argument parsing libs are only of use to applications, and not of use to libraries, so there's no risk of mismatch via transitive dependence, and no risk of anyone being forced to use a certain argument parser just because they chose a certain dependency. Also, given that none of these libraries are products of the official Rust devs, I don't think the "why keep supporting" question makes sense; people work on these libraries because they have fun doing it, and other people use those libraries because they want to. :P I don't have enough experience with any of these libs to suggest one, but I think your problem could be solved by simply picking one library to use at your company and mandating that that's the one to use. If you have trouble deciding, then here's how you can break the tie: find the most important Rust CLI utility in the wild, and use whatever it's using, since that means the underlying lib is unlikely to vanish suddenly.
Why do you call it a HashSet? It's just a fixed-size 256-bit vector, it's got nothing to do with hashing.
It's an excellent feature, but one that should perhaps be reserved for executables themselves?
Yeah, you are on the right track. You can see an example of this in cesium: Cache: https://github.com/AnalyticalGraphicsInc/cesium/blob/master/Source/Scene/GlobeSurfaceShaderSet.js#L111 Shader: https://github.com/AnalyticalGraphicsInc/cesium/blob/master/Source/Shaders/GlobeFS.glsl Godot does something similar for its built-in shaders: https://github.com/godotengine/godot/blob/master/drivers/gles3/shaders/blend_shape.glsl#L29 The example from godot directly affects the number of inputs, and turns off chunks of code related to them. For the systems I used, this resulted in faster shaders and less bandwidth being used to transfer state. Specialization constants can disable blocks of code, but are fairly limited compared to macros. &gt; I also assume that creating a new entry point would be not feasible because there are way too many possible combinations? Yeah. But could be feasible if it was automated!
But they can still tear down a long-running process and waste a lot of time if you're not paranoid enough to have appropriate recovery machinery in place. (Which could significantly complicate the codebase) Imagine if a file copying GUI died on the first Access Denied rather than logging an error and continuing with the rest of the files.
Thank you for your answer. I really need to take a look at those procedural macros one day.
isn't it better solved by the use of specialization constants?
I like where this is going! It appears that your end game is sound, and we just need to narrow down the details. Here is some feedback: &gt; One downside is that you can not share code between files. That is incorrect. One can `#include` in HLSL. A similar effect can be implemented manually for GLSL, which is what WebRender and ThreeRS do. &gt; `fn frag(color: Vec4&lt;f32&gt;) -&gt; Vec4&lt;f32&gt;` Let's think about other possible returned values: multiple colors, depth, ability to discard a fragment. &gt; `fn vertex(vertex: &amp;mut Vertex, pos: Vec4&lt;f32&gt;, color: Vec4&lt;f32&gt;) -&gt; Vec4&lt;f32&gt;` The distinction between what goes into `&amp;mut Vertex` and the return value seems rather arbitrary to me. Why not return a tuple of `(Vertex, Vec4&lt;f32&gt;)`? &gt; `#[spirv(fragment)]` We could also consider an attribute-less way of specifying this, e.g: ```rust enum MyFragmentShader {} impl FragmentFn for MyFragmentShader { type Input = VertexOutput; type Output = (Vec4&lt;f32&gt;, u32); } ``` &gt; I don't explicitly specify the location of the input / output variables. This is all done behind the scene. I don't find this a good thing. A good user should have the same pipeline layout used by multiple pipelines, which sort of requires the descriptor binding to be pre-defined at pipeline creation time. All in all, amazing work!
Maybe. Macros permit parameterizing inputs. At least for the system I used, it significantly reduced bandwidth usage. But maybe it's possible to omit binding and setting the layout of uniforms and inputs without getting a linker error?
Thanks for all of your help! I'm not sure which option I'll take, but now I have multiple avenues, and they generally look promising 
That seems great! Gonna take it for a spin in my next Vulkan project :)
For transfer with block-level dedupe there's BitTorrent though, right? Like your NAS is on a de-duping FS, you hold a bencode'd .torrent of the file itself, when you need to transfer the artist's system requests the .torrent and then fires up a BT client connected to your private swarm inside the WAN, then they keep their copy on a local de-duping FS. And, if they stay active on the swarm, now you've got both additional speed for anyone else coming online and needing the same file, and some level of redundancy for block loss to BTRFS hiccups on any node of the swarm. Of course, now you've got the moment you explain to the auditor from Disney/Marvel that you're using BitTorrent and their knuckles go a few shades lighter.
I know one of the really big difficulties when doing crypto is there really is no substitute for hand-written assembly. So even though you may be using Rust to take advantage of its emphasis on safety, you'll still be building your safe abstractions on top of unsafe code. For example, one important thing in crypto is making sure a function will take the same amount of time regardless of the final result (to prevent timing attacks). Obviously this goes against the whole point of using an optimising compiler, so you'll often write assembly to be certain the machine does *exactly* what you want. So dont rule out a library just because a crypto library contains some C or Assembly code. That said, I've heard lots of good things about `ring`, and it doesn't have the annoying portability/installation issues that `openssl` has.
Ahh, no worries, read your meaning slightly wrong. And yeah, way too gigantic... I've taken some time off just to try and look at it from a different perspective, but it's so big it's hard to figure out where the thin end of the wedge is. Plus I'm not a business guy; I'm known -- or at at least known of -- by pipeline people at quite a few of these places, and I keep thinking there's a way of getting a ball rolling just *enough* to get them all to buy in, but you still need to demo a working *thing* and it *has* to be a non-vendor-lockin-stick-and-carrot like Shotgun Toolkit very definitely is... so how do you both get as far as a useable and obviously beneficial first product, afford to do so as open source, and while also actually addressing the deeper problems of asset knowledge and management that Shotgun -- frankly -- hasn't got the slightest clue about? Personally I keep thinking it's context-aware configuration ... the tool that looks at your facility/location/client/project/scene/shot/?department=comp type context and rolls up the shell, environment, packages, and breadcrumbs for the asset DB that describes a working area at any given time in the project cycle... Weta had it, DD and Sony and ILM have variants on it ... I've written different degrees of it over and over, but always too tailored to a specific company's ecosystem and investment in technical debt. It feels like something just aching for a simple, effective, and free solution... but if you build it, will they come? ;-)
For `curve25519` I strongly recommend an excellent [`curve25519-dalek`](https://github.com/dalek-cryptography/curve25519-dalek). For `chacha` take a look at [`chacha`](https://docs.rs/chacha/0.1.0/chacha/) crate. :) PeterReid (author of `chacha` crate) is a member of RustCrypto org, but unfortunately neither he or me didn't have time to incorporate it into RustCrypto. For `poly1305` I have a crate [draft](https://github.com/RustCrypto/rust-crypto-decoupled/tree/master/etc/poly1305) based on `rust-crypto` code, but it needs some additional work.
 #[structopt(name = "finish")] Finish(Finish), // subcommand "finish" Praise the rust godess, this is exactly what I needed! Thankyou thankyou!
gotcha. Ya, This is even more flushed out now, with `PathType` being `AsRef` for all the stdlib types as well. I want to implement `Eq` and other traits as well to make it even more ergonomic.
of course /u/japaric is writing code for GPU's... is there _any_ architecture he is incapable of targeting???
... which is an ecosystem that chose a to make a hammer for a specific kind of nail and now spends all of its time trying to sell more people on the idea that their hammer is the right choice for any given screw. I have these automatic, basically reflex or autoimmune reservations around any company that releases what is basically a an abstraction that makes their product *useable* under a license that says you can only use it as long as you're a paid-up customer. &gt; The following license to the Shotgun Toolkit Code is valid only if and while you are a customer of Company in good standing with either: (a) a current, paid-up (or free-for-evaluation) subscription or fixed-term license for Company's Shotgun Platform; or (b) a perpetual license and current, paid-up maintenance and support contract for the Shotgun Platform. That clause has always galled me ... facilities build their pipelines to outlive vendors because vendors go bust, get acquired, stop properly supporting their products, or drift off into idiot directions; making companies invest time and energy into learning and weaving together your -- often poorly designed and executed -- framework and core product into an actually useful tool, and then requiring them to continue that investment no matter how bad your core product gets in the future? If your central product is good, and going to stay good, then no one will ever need to redirect the abstraction to something else so full FOSS license would be fine... by creating that clause you're basically saying you're a) a pusher, and b) that your product might be good now, but someday soon you're going to start cutting it with baking soda or worse, but hey, that's okay, because c) your customers are already on the hook.
Thanks, I've been browsing the ring docs a fair amount this afternoon. Looks good, but the non-standard license is a bit strange. On another note altogether; damn do I love `cargo doc` and well written docs.
Fantastic, I was just looking at the dalek series of crates. That led to stumbling on one of the deps, `clear_on_drop` which is something I was considering implementing. Any opinion on [chacha20-poly1305-aead](https://crates.io/crates/chacha20-poly1305-aead)?
If you're wanting something "fully tested, proven, and supported", ring is what you're after. It's based on BoringSSL crypto, which is in use on billions of devices, and has a nice safe API. The maintainer is incredibly knowledgeable about security, which we should all find reassuring. 
Thank you. I'm leaning towards `ring` after some evaluation. It's been pretty interesting exploring the various different takes available out there, some of these projects look very promising and I hope they get the community they need.
Who here likes stats? Because I have some interesting stats. # The stats ## Movies Path: "Big Buck Bunny-720p.mp4" Total count: 180534 Unique count: 180534 Percent saved: 0.00% Path: "Big Buck Bunny-720p.webm" Total count: 150105 Unique count: 150105 Percent saved: 0.00% Path: "Caminandes_ Llamigos-1080p.mp4" Total count: 135690 Unique count: 135690 Percent saved: 0.00% Path: "Caminandes_ Llamigos-1080p.webm" Total count: 37150 Unique count: 37150 Percent saved: 0.00% Path: "Glass Half-1080p.mp4" Total count: 64220 Unique count: 64220 Percent saved: 0.00% Path: "Glass Half-1080p.webm" Total count: 43944 Unique count: 43937 Percent saved: 0.02% Path: "caminandes-gran-dillama.mp4" Total count: 113938 Unique count: 113938 Percent saved: 0.00% Path: "caminandes-llama-drama.mp4" Total count: 33369 Unique count: 33369 Percent saved: 0.00% Path: "cosmos-laundromat.mp4" Total count: 512733 Unique count: 512732 Percent saved: 0.00% Path: "cosmos-laundromat.webm" Total count: 175091 Unique count: 175084 Percent saved: 0.00% Path: "elephants-dream.mp4" Total count: 532753 Unique count: 532753 Percent saved: 0.00% Path: "elephants-dream.webm" Total count: 174469 Unique count: 174468 Percent saved: 0.00% Path: "monkaa.mp4" Total count: 155624 Unique count: 155623 Percent saved: 0.00% Path: "sintel.mp4" Total count: 219436 Unique count: 219436 Percent saved: 0.00% Path: "tears-of-steel.mp4" Total count: 493685 Unique count: 493685 Percent saved: 0.00% ## Archived projects Path: "caminandes-gran-dillama.zip" Total count: 535138 Unique count: 535138 Percent saved: 0.00% Path: "elephants-dream.zip" Total count: 4288108 Unique count: 4287927 Percent saved: 0.00% Path: "glass-half.zip" Total count: 229566 Unique count: 229566 Percent saved: 0.00% Path: "llamigos.zip" Total count: 3934410 Unique count: 3934259 Percent saved: 0.00% Path: "sintel.zip" Total count: 17539214 Unique count: 17535854 Percent saved: 0.02% Path: "big-buck-bunny.zip" Total count: 1056070 Unique count: 1056070 Percent saved: 0.00% Path: "monkaa.zip" Total count: 946163 Unique count: 945952 Percent saved: 0.02% Path: "tears-of-steel.zip" Total count: 3788243 Unique count: 3788212 Percent saved: 0.00% Path: "tears-of-steel-frames.zip" Total count: 6630287 Unique count: 6630287 Percent saved: 0.00% ## Projects Path: "big-buck-bunny/" Total count: 1118161 Unique count: 1117982 Percent saved: 0.02% Path: "caminandes-gran-dillama/" Total count: 836946 Unique count: 817372 Percent saved: 2.34% Path: "elephants-dream/" Total count: 5519015 Unique count: 5507650 Percent saved: 0.21% Path: "glass-half/" Total count: 260057 Unique count: 260006 Percent saved: 0.02% Path: "llamigos/" Total count: 9074072 Unique count: 8013790 Percent saved: 11.68% Path: "monkaa/" Total count: 969712 Unique count: 969198 Percent saved: 0.05% Path: "sintel/" Total count: 19086459 Unique count: 17878125 Percent saved: 6.33% Path: "tears-of-steel/" Total count: 4407785 Unique count: 4196216 Percent saved: 4.80% Path: "tears-of-steel-frames/" Total count: 6642126 Unique count: 6642126 Percent saved: 0.00% # My results (or tl;dr) - This appears to be more or less useless on already heavily compressed files. There may be a savings if I change the chunk sizes from 1024 to something less. - Things get interesting in projects. Some projects give almost no savings, while others get about 12%. My math could be wrong, but this does make things interesting.
Thanks, I got it.
I think this is a matter of some debate. I've seen discussions about this get heated before. There are a hundred package managers out there—should the developer learn how to make packages for brew, apt, yum, pacman, etc, etc, and update every single one every time there's a release? There are at least 3 major platforms with incompatible binary formats, and cross compiling to every other platform isn't really possible for Rust right now. So I think it's largely a matter of how many people are maintaining it. If it's just you, I'd say crates.io is perfectly reasonable. Maybe a binary tarball too, if you have time.
Oh right, I hadn't noticed that (didn't look too closely). After further perusal it seems to have parts licensed under: * Apache or MIT * OpenSSL * ISC * Intel License * SSLeay, though this isn't too clear regarding which parts. * Google I'm not sure what to make of all that...
You can license it under MIT, Apache, or WTFPL and let users choose 
Structopt builds on clap, so without clap there is no structopt 
The nvptx crate works !
From the FAQ Can’t you change the wording? It’s inappropriate / childish / not corporate-compliant. The WTFPL lets you relicense the work under any other license.
If you want something more basic you can also use Vulkan.
I actually didn't know anything about the portability vision of the std. That sounds very interesting, thanks!
There are tracking issues for all of this: https://github.com/rust-lang/rust/issues/32838 https://github.com/rust-lang/rust/issues/30172 https://github.com/rust-lang/rust/issues/29641 https://github.com/rust-lang/rust/issues/27779 https://github.com/rust-lang/rust/issues/22181 If they have no updates that typically means nothing has happened.
None of these were prioritized for this year right? Just checked the 2018 roadmap but I'm not sure
The terminology is misleading - Corona seems like fibers, not coroutines...
&gt;That is incorrect. One can #include in HLSL. A similar effect can be implemented manually for GLSL, which is what WebRender and ThreeRS do. Right I forgot about HLSL, but I know that you can manually implement it in GLSL but this requires some work and it is still like a C include right? So everything gets "copy pasted" behind the scene? &gt;Let's think about other possible returned values: multiple colors, depth, ability to discard a fragment. Everything should work, I forgot to mention how I handle outputs in the fragment shader. I'll also "unroll" the output of the fragment shader into their own variables. I haven't thought about discard that much and I am not sure yet how I am going to implement it. It should be as easy as calling a builtin function `discard()`, but it is probably going to mess up my structured control flow. &gt;The distinction between what goes into &amp;mut Vertex and the return value seems rather arbitrary to me. Why not return a tuple of (Vertex, Vec4&lt;f32&gt;)? It is pretty arbitrary. I actually thought that this would be the easiest way to implement it, but it has many edge cases. I think having it in the return should be much easier. But I need to think about it a bit more. At least having it as `&amp;mut Vertex` should be very obvious to beginners. &gt;We could also consider an attribute-less way of specifying this, e.g: I haven't yet figured out how I can access traits in rustc. I am I theoretically know how to do it, but the turn around times to are just too high. I am currently using [rspirv](https://github.com/google/rspirv) and that uses proc macros which means that I need to build the compiler to stage 2 and that takes at least 30min. It is just too long for experimentation, because for builtin traits I would need to touch the compiler. So I am currently just using attributes for everything because they are super simple. And what you want is already implemented, just as a macro. (Or was implemented) fragment! { "SomeFragment", fn frag(color: Vec4&lt;f32&gt;) -&gt; Vec4&lt;f32&gt; { color } } Which would produce the same thing. The idea was to use that struct for the actual reflection, and it would automatically check if is compatible with the fragment shader inside the `.spv` file. But I am going to implement this in a separate crate, might be a bit too subjective. &gt;I don't find this a good thing. A good user should have the same pipeline layout used by multiple pipelines, which sort of requires the descriptor binding to be pre-defined at pipeline creation time. I only implemented location for input and output variables, I haven't touched descriptors at all in RLSL. The idea is that you can share variables between entry points and RLSL computes the best possible overlap behind the scenes. Currently I only produce one big `.spv` file, but in the future I'll let the user specify which entry points goes into which `.spv` file. More things inside an `.spv` file means that more things like types, constants, variables, functions etc can be shared more efficiently. For example you could have one quaternion implementation shared between 100 entry points and managing that many input/output variables for the developer would be pretty annoying. Everything can still be retrieved before you create the pipeline. Also I think I'll allow explicit locations, and then RLSL will respect the location value for that particular variable.
What language feature in java depends on your knowledge of the stack / heap?
Yes to all what /u/kvarkus said. Also if you actually want different implementation, like different algorithms I think you can still do that with an branch + specialization constants. I am not sure if the pipeline can optimize the branch away, but even with a runtime branch the cost should not be high. I think the GPU uses a single branch prediction for every pixel in a chunk/patch (I forgot the proper name). 
Note that you can use almost all clap features in structopt using the `raw` attribute. Some case might be not so ergonomic. If you have some clap features that you can't use with structopt, feel free to open an issue, I'll try to find a solution using structopt or change it to a feature request.
Hmm, how is this different than may or coio?
&gt; The main reason being that it’s currently not possible to implement bignum arithmetic in rust in an efficient way Do you mean it's impossible in safe Rust?
Impossible in unsafe rust as well. We need to be able to expose the carry flag (directly or indirectly) and double word multiplication if we want to be able to implement efficient bugnums. IIRC there is work ongoing to add support for that though. I can elaborate on why it’s needed if you’re interested.
First of all, I want to say that I appreciate the current effort by /u/innovator12 to refactor the Rand crate. As a casual observer of that effort, however, I found it difficult to get information about its overall direction as there does not seem to be an RFC or similar background doc. So I am not sure I understood the intent of the PR: does it imply that no dinstinction will be made between distributions that are immutable or mutable during sample generation? There are many valid use cases for mutating distributions, such as the very common Box–Muller transform for normal distributions. IMO the distinction between mutating and non-mutating distribution should be maintained as it is common to use a thread-local RNG with a thread-shared distribution (which requires a non-mutating distribution). 