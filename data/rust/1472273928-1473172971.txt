I have security concerns with this but it's also an awesome tool that I've always wanted. A centralized place to just research crates to use is fantastic.
Crates.io is still the default registry for installing crates through cargo though...
Well, most people aren't relying on arbitrary Javascript in their docs, I'd suspect. A thick coating of server-side filtering, belt-and-suspendered with some CSP, and it's probably possible to make it work on-domain. Or just stick it in an `&lt;iframe&gt;`. But I wouldn't want to maintain it unless it was my day job (and, basically, this is my day job -- so I wouldn't wish it on the volunteers here!). It makes a lot more sense to just stick it on docs.rs and not worry about it.
&gt; So, it's not, strictly speaking, from the Rust Project proper, but what does that even mean, really? :p I'm worried that docs.rs will become unmaintained or simply disappear when their author becomes disinterested in the project. If it was controlled by the Rust team, this wouldn't be a worry. For example, that rust-travis service (which I don't remember the name) had a PR sitting for several months whose purpose was to fix a critical bug, but it has never been merged and deployed. 
Parsing floats is indeed extremely hard. Inaccurate parsing, especially if you don't need to support the whole range of float syntaxes, is a lot easier. &gt; I'm seeing 30× speedup, while OP reports only 5×. Rust's float parser has a fast and a slow path. You might be hitting the slow path where OP is hitting the fast path. Both paths are much slower than a custom implementation like you gave, but the slow path is particularly bad.
&gt; Well, most people aren't relying on arbitrary Javascript in their docs, I'd suspect. I plan on doing it. It's pretty useful. &gt; Or just stick it in an &lt;iframe&gt;. Oh god please no!
Why is an &lt;iframe&gt; so bad? It's only to provide a security barrier -- they can be made invisible from a UI standpoint. I think if you want active content in your docs, it's best if you just hosted it yourself. If I was the docs.rs folks, I wouldn't want to become a random JS-enabled webhost. You'll be hosting spam in no time flat.
OK. I haven't got a lot of experience with trying to make &lt;iframes&gt; seamless, as I think putting things on their own domain is the right choice. As I said: &gt; It makes a lot more sense to just stick it on docs.rs and not worry about it.
Async postgres I think makes little sense. Postgres sucks with large number of connections which is what async is good at. It's better to have a thread pool that works on postgres and communicates with the rest via a pipe. 
They were merged, somewhat. futures-mio is now tokio-core. I'm sure Carl or Alex will have a more detailed description. 
This will probably be the official place to get winapi 0.3 documentation, because I'm too lazy to build and upload docs myself and it provides all the targets I care about. Only blocker at the moment is https://github.com/onur/docs.rs/issues/29
Are you concerned about build scripts being able to run arbitrary code and perform malicious things?
I don't see any of those suggestions on my end. Are you sure that's not your browser suggesting it based on stuff you entered before?
That does sound like it needs a blog post of its own!
Is that transformation fully general? What if you have G(∃t. F(t)) where G is some type constructor? Instead you can: ∃t. F(t) ~ ∀r. (∀t. F(t) -&gt; r) -&gt; r (Which comes from the CPS transform A ~ ∀r. (A -&gt; r) -&gt; r with A = ∃t. F(t), and then currying (∃t. F(t)) -&gt; r = ∀t. F(t) -&gt; r)
Actually async postgres would be great. For a decent server you would probably have a decent pool of database connections. Queries themselves are relatively slow (lot's of waiting) and require relatively little work on the application side. Why use a relatively expensive threadpool (in terms of memory) and thread synchronization logic for something as benign as a simple dispatch and wait workload for database interactions. And finally you don't need to have the proper Send or Sync properties on your data like you fo when dispatching everything through threads.
What I like about it is a more consistent error handling of `Option`, `Result` and `bool`, it's the same pattern and therefore most likely a bit easier to recognize. It's IMHO not that important what `is_absolute` returns, as long as the whole expression is intuitive to read, which I think is the case. Also the displayed example is the most simple, if there's no direct conversion from `&amp;str` to the error type, then you need an additional `LibError::Whatever("fooooo")`, so the expression gets more complex. I'm not going to say that it's a lot better, but I like the pattern similarity.
Here's another using [impl Trait](http://www.ncameron.org/blog/abstract-return-types-aka-%60impl-trait%60/) **[Source](https://github.com/jonysy/range_step)**: #![feature(conservative_impl_trait)] pub fn range_step&lt;T,P,S&gt;(start: T, predicate: P, step: S) -&gt; impl Iterator&lt;Item=T&gt; where T: Copy, P: FnMut(&amp;T,) -&gt; bool, S: Fn(&amp;mut T,), { ::std::iter::repeat(()).scan(start, move |state, _| { let ret = *state; step(state); Some(ret) }).take_while(predicate) } fn main() { for elem in range_step(1.0, |&amp;elem| elem &lt; 7.0, |elem| *elem += 1.0) { println!("{}", elem); } } 
I would love to try this out in [imag](https://github.com/matthiasbeyer/imag), but I'm rather busy right now with the other things. If you want to try this out in a multi-crate project with a rather big codebase, I'd love you to submit a pull request! If you do not have enough time or motivation (please tell me right away) I will add this as issue.
&gt; `sudo cargo run` Why are you executing with root privileges?
Why several MB? How large do you think a thread is? :) you need more connections on the server if you do async not if you do blocking. I think you are mixing things up. 
Looks like a "late at night" issue to me :-). Okay, so I assume the `base64_decode` function reads some base64-encoded string from `src`, that is `sizeof_src` bytes long, and writes at most `sizeof_dst` bytes of the result to `dst`, and returns the total number of bytes that were written to `dst`, is that correct? I assume you want something more like this: pub extern "C" fn base64_decode(dst: *mut c_void, dst_len: size_t, src: *const c_void, src_len: size_t) -&gt; size_t { let source = unsafe { assert!(!src.is_null()); slice::from_raw_parts(src as *const u8, src_len as usize) }; let source_text = str::from_utf8(source).expect("Error while ingesting the input string."); let encode_text: String = source_text.as_bytes().to_base64(); let bytes_to_copy = std::cmp::min(encode_text.len(), dst_len as usize); unsafe { std::ptr::copy_nonoverlapping(encode_text.as_ptr(), dst as *mut u8, bytes_to_copy); } bytes_to_copy } What confuses me a little is that you call `to_base64` (is that from some library?) inside a function that supposedly should *decode* rather than *encode* base64, but without more context it's hard to say what the correct behavior should be.
Awesome. Just yesterday I was sad that crates.fyi no longer worked—or so I thought! One silly question, what happens is a crate named "about" is published? Since this is taken: https://docs.rs/about
 size_t base64_decode(void *dst, size_t sizeof_dst, const void *src, size_t sizeof_src); This likely means that the caller has specified a buffer for you to write into. You can write up to `sizeof_dst` bytes, and you return the number of bytes actually written. You can't write to `dst` itself, just the memory that `dst` points to. So the end of your function should look like: if dst_len &lt; encode_text_size { return -1; } // Or however you indicate that not enough buffer was supplied std::ptr::copy(encode_text.as_bytes().as_ptr(), dst as *mut u8, encode_text_size); return encode_text_size; And one more thing: Err(e) =&gt; panic!("Error while ingesting the input string."), I hope you're compiling your crate with panic=abort, otherwise that is undefined behaviour. And you don't want that.
Or embedding scripts into the doc pages.
In C#, when you spin up a thread, it reserves a megabyte of memory for that thread's stack. I believe it is due to the implementation of threads in the underlying OS, i.e. the Windows NT kernel. How expensive is a Rust thread? How does it deal with stack for different threads? How does it avoid this memory cost if it's something the OS does?
Try this: https://github.com/bluss/scratchspace SSE 4.2 supported 'find' function. Very fast )
I have already created a protocol implementation for [my own RPC library](https://github.com/sunng87/slacker-rust) based on tokio. Just wonder if it's possible to port it to tokio-proto and tokio-service.
It definitely should be! These upper layers didn't really change in the transition, they were just moved to new places.
`Option`and `Result` are very different from `bool`. They are wrappers while `bool` is a primitive. You're employing the machinery of unwrapping to deal with something that isn't a wrapper. That's not the same pattern, that's a new pattern. One that isn't recognizable at all, in my opinion. &gt; It's IMHO not that important what is_absolute returns It is important. If you're reading some code and you don't know what a function is returning, then you don't understand that code. Are you fixated on writing convenience over transparency? I brought it up because it *confused me*, and was actively detrimental to my understanding of the code.
The OP gives a more detailed summary.
FTP is barely a protocol. Have fun maintaining it :D
Ah, thanks. I skipped right ahead to the Tokio repository, thinking I would get the explanation there.
Seems like PostgreSQL is the go to database for a Rust web app stack cargo.io , doc.rs . 
1. I did not do this. :-( I spit on lines then parsed with nom. Mostly I used std for converting the `&amp;[u8]` to `f32` which is the slow part. Thanks krdln for finding that. Thanks to zenflux for showing how to do nom correctly. 2. Rust uses byte offsets for string slicing. So yes it is O(1). And `&amp;[u8]` is definitely O(1) indexing. The reason to avoid it is bounds checks that can slow things down a lot. 3. IO is very important. I used a buffered reader and had linear access. I should try mmap.
That sounds really interesting. How does one use it? Can it be used on stable? 
(summoning /u/burntsushi to help, since I would suppose he is the most capable of answering this question)
Pools and pgbouncer aren't mutually exclusive and both solve a different problem. For lower latency you want fully established (and authenticated) connections standing by in a pool. But using threads for effectively waiting for IPC or RPC of only one outstanding job is plain wasteful. Since a thread would still be doing marginally usefull work before and after calls a decent guess would be that a thread consumes at least 8kb for it's stack and some kernel memory. To prevent spinning up a thread for each connection, you'd probably end up having a decent number of them running. This memory is pure waste (the promise version would require a fragment of that). Furthermore we now have to go through some form of threading primitives. Some might have pretty low costs, possibly depending on the platform, but not having that problem is even better. And finallly you have two additional scheduling decissions for each query to return to the original thread (promises-rs is mostly single threaded in it's building blocks). So we're wasting memory, we're doing more work than necessary and adding latency for something that is (from an application developers point of view) a low level basic building block. And in this case it has a fleshed out api for the problem, so why not adapt that?
If it is thought to return an `Option&lt;T&gt;`, you'll be sitting there trying to figure out what `T` is. Which is unfortunate, because it clearly looks like there's a `T` being passed around in your version. That's kind of how functional programming idioms work. If you see a.do_a().do_b().do_c() ...then you can bet there's some data being passed through this system. But what data? A `bool`? Use `if`, `&amp;&amp;`, and `||` for bools. That's pretty standard. They are already very readable, and they're expressions, so they're totally composable. Why would you want to hide this behind function calls? &gt;after all there's the Rust type system, so you can't really screw it badly Just because your code compiles doesn't mean it is correct.
For me shows this error on OS X: ╰─λ curl https://sh.rustup.rs -sSf | sh 0 &lt; 17:53:13 info: downloading installer sh: line 250: 1828 Segmentation fault: 11 "$@" rustup: command failed: /var/folders/6w/5n3ml9qn02v5t3ylnp16pfcw0000gn/T/tmp.vfFG9CnY/rustup-init There is an issue for this: https://github.com/rust-lang-nursery/rustup.rs/issues/695
Isn't that the argument also used by the python creator for his dislike of lambdas? Ones man's opaque is another man's elegance 
&gt; Short of downloading nightly rust, is there any other option to run 'cargo bench'? Nope. One thing people often do is use rustup.rs to have multiple versions installed, and then "rustup run nightly cargo bench" for benchmark tests, and a regular old "cargo build" for development.
PostgreSQL server supports asynchronous mode. https://www.postgresql.org/docs/9.4/static/libpq-async.html
&gt; But what data? You mean like a `map` call were you can't directly say if it's an `Option`, `Result`, `Iterator` or whatever else? It doesn't matter that much to get an idea what the code does, to infere the core meaning of the operation, everything else - that's not different for the `map` - is ensured by the type system, because how far should you wrongly get with the `()`? It's fine to dislike it, have a different opinion, it might also be more of a bikeshedding issue, but I really can't see that this should be in any way more dangerous than any generic code in Rust.
I an aware. The only thing that does however is not wait for the result in libpg. 
All else equal, lambda's *do* make things more opaque. They're also often less flexible (try to zip with foreach!). That's why a lot of the standard library work has aggressively pushed for interfaces and solutions that don't require them. See e.g. https://github.com/rust-lang/rfcs/blob/master/text/0216-collection-views.md There's a reason so many Rust APIs are driven by state machines rather than continuation passing. Don't get me wrong: closures are great when they're great, and that's a non-trivial amount of the time. map? and_then? spawning threads? Great! They don't really do much more than cut out, like, a match statement that everyone's seen a million times. But fold is really on the edge of being valuable in imperative languages. It does something slightly non-trivial in rare enough cases as to be mentally cumbersome whenever encountered. Even places where fold is a slam dunk like sums and products, we got a lot of pressure from the community because *fuck* writing `.fold(0, Add::add)`. We just wanna say `.sum()`! 
I like the unit approach for things like this, but I think the customer benefits of the Apple-like integration could be partially acheived through the sentiment expressed in the original top-level comment: by having it picked up an official project. This would facilitate reliable, efficient, cross-project interaction, while keeping them easily maintainable per the unix mindset. 
Is there an easy way to make it work during tests without too much boilerplate code?
Absolutely wonderful! Thank you! /me pulls down latest racer and racer.el
&gt; PQgetResult can block if there's any active commands. But that is not really the problem I am referring to. The problem is that you do not know when a result on libpq is ready. The only choice you have is as part of your IO loop to constantly poll on open connections. &gt; You can dedicate one thread to the database and handle multiple queries in parallel, without a need for thread pool or multiple connections. What you usually do is let your IO loop (which in most cases I have seen is one per thread and no thread boundaries are ever crossed) handle it. If you consider this in a larger rust app you end up with a weird design that does not load balance very well: * You have N threads running to keep all cores you have in your machine happy * Each thread runs an IO loop * Each IO loop keeps multiple postgres connections open That's the worst case scenario because even with pgbouncer you are going to quickly exhaust your connection count. So you can go to: * You have N threads running to keep all cores you have in your machine happy * Each thread runs an IO loop for non postgres things * You have one thread which handles multiple postgres connections In that case you will need to send postgres commands from the individual tasks (request handlers etc.) into a queue anyways so that your background thread can handle it. In that case your main advantage is that you saved a few threads. However now that one thread will have quite a busy IO loop which effectively ends up in a cycle of "poll libpq; usleep(1)" since there is not all that much you can do with libpq. So why not just keep a nice thread pool to manage connections and delegate that work to the kernel who knows how to handle IO best? Especially super slow IO like database queries. For what it's worth I have not seriously investigated async postgres for many years now but a quick look at the manual does not reveal that things would have significantly changed and postgres' connection handling is all too familiar to me. //EDIT: evented/async IO is particularly useful if you have many different things to work with in particular the case that comes up with high number of socket connections. In that case many of them are super idle and only need to be dealt with at fairly irregular intervals. In that case, why spawn a thread? However your connection count on postgres is severely limited so the insignificant overhead of running a thread pool is not really a concern. You need to manage your concurrency to postgres anyways.
This has already been in VS Code for quite a few months now.
&gt; However now that one thread will have quite a busy IO loop which effectively ends up in a cycle of "poll libpq; usleep(1)" since there is not all that much you can do with libpq. That isn't necessary. When there are no active commands, you don't have to poll libpq. When there are active commands, you poll it in the blocking mode and it returns as soon as a first reply arrives. Yes, libpq doesn't fit into your usual epoll polling, but that might have to do with libpq working on the request / response level rather than on the TCP/IP packet level. &gt; For what it's worth I have not seriously investigated async postgres for many years now As I've said above, I've used it and it's blazingly fast. In my benchmarks it was 15 times faster than a synchronous connection in raw query throughput. More than enough to be useful for an asynchronous driver. &gt; but a quick look at the manual does not reveal that things would have significantly changed Maybe because it's good enough.
Thumbs up! I can only appreciate the work you guys do.
Cool. I have some plans for polishing, double checking every TODO before 1.0.0 and then adding some more drains, so I'm busy ATM so I'm telling right away.
I initially thought this would unbuffered I/O, but the author has since improved the Rust code posted (buffered I/O and removing some memory allocations) and yet claims that Rust is still as slow as before. I must be missing something, but I don't have rustc on this device... (nor do I have a profiler...) so if anyone wants a crack at it; have fun :)
Huh, I've just looked at some code of mine and turns out I was wrong: libpq *does* integrate nicely into the usual `epoll` polls! Check this out: https://gist.github.com/ArtemGr/179fb669fa1b065fd2c11a6fd474da8a (`waitFor` wraps a coroutine around libevent polling). On the downside, I can't find my benchmarks. IIRC I didn't thought them worth preserving.
This does indeed look like a bug. Here is another bug along the same lines: https://github.com/rust-lang-nursery/regex/issues/168 This is one of the reasons that the regex 1.0 RFC removed the various iterators defined on the Captures type. Instead, people can use the capture_names method on Regex itself.
&gt; g++ -Ofast -march=native and rustc -C opt-level=3 These are not equivalent compile flags, though I doubt that's the big difference - I'll have a look. edit: It's definitely the -march=native. Removing that drops the performance of the C++ code to near the rust code. Changing the u32 to i32's and avoiding recasts, removing error checking that doesn't exist in the C++ code, gets you a fair bit of the way closer. Storing the values and printing them *after* the logic, to avoid benchmarking println!, drops the code down 50%. At this point it's clear that the C++ and rust code are likely to perform very nearly the same.
According to the author the main time sink is specifically the last loop: fn manhattan_dist(a: &amp;(i32, i32), b: &amp;(i32, i32)) -&gt; u32 { ((a.0 - b.0).abs() + (a.1 - b.1).abs()) as u32 } let mut dist = u32::max_value(); for i in points_a.iter() { for j in points_b.iter() { dist = std::cmp::min(manhattan_dist(i, j), dist); } } which looks all right to me (and very similar to the C++ code, especially as Release should elide the overflow checks... no?
I didn't say it was dangerous, I said it was confusing.
&gt; but I remember constantly polling because partial data was in the socket Shouldn't happen. Nonblocking libpq should read what's there and leave the socket empty. If it leaves data in a socket then I'm 99% sure it's a bug. But it's more likely you forgot to call `PQconsumeInput` or something. &gt; I do not see this discussion as particularly interesting because your suggestion is vasically that the context switch is the expensive part which makes very little sense :) No, what I'm saying is that the asynchronous support in the PostgreSQL server is good enough to be used in an asynchronous Rust driver.
I suspect it's because println! takes a lock on stdout. Could try taking the lock outside of the loop instead, and using writeln! inside the loop
I'm unable to recreate the performance improvement with that flag and the rust code. Perhaps this is a case where gcc is doing something that llvm isn't getting. In fact the rust code is 6 seconds slower for me with march=native. edit: Tried with clang, kinda surprised. gcc -O3 -march=native = 7s gcc -O3 = 18s clang -O3 -march=native = 7s clang -O3 = 37s
[Should be fixed now](https://internals.rust-lang.org/t/beta-testing-rustup-rs/3316/168).
Fair point.
Funny thing is, I first ran the Python script to generate input with Python 2, which results in different output (due to how `print` was changed to a function). The C++ version happily ran and gave me lots of `0`s, the Rust version sensibly panicked.
There's another interesting different between the two languages: The C++ code collects the minimum distance as an `unsigned long long`, i.e. `u64`, while the Rust version uses `u32`. Using a `u64` in Rust gets me from 21sec to 15sec, which is the same time achieved by `g++` without the `-march=native`.
There has been a regression in the latest versions of vscode where the markdown is not being highlighted properly, but here is how it looks when it is working properly: [Here](https://cloud.githubusercontent.com/assets/947593/14992932/e068d3e2-113e-11e6-8f40-b7ae2be8e55e.png), and [another one](https://cloud.githubusercontent.com/assets/947593/14992958/f984f8a6-113e-11e6-9255-17637bbab85e.png). 
Perhaps, but how would that help them? As far as I'm aware you can't easily put new apps on their devices without going through *their* appstore, so they're not competing with anyone else there. And after the normal LLVM has done its job, I doubt that they can squeeze out much more performance with their "secret" optimizations.
Here we go: https://github.com/matthiasbeyer/imag/issues/671
This is a legitimate concern, but I am really passionate about Rust and this is my little baby. docs.rs actually requires minimal maintenance and hosting is sponsored. It is designed to run itself. And I don't think it is fair to compare this to rust-ci. The guy who made rust-ci was trying to develop a django (python) based web app. docs.rs built on top of cargo, and it is safe to say docs.rs will be exists until cargo gets abandoned.
It's a tradeoff. Creating binaries that don't crash if you run them on a different system seems like the more conservative option.
Since you mentioned python, have you seen this? https://magic.io/blog/asyncpg-1m-rows-from-postgres-to-python/ (repo: https://github.com/MagicStack/asyncpg). It is async, and they claim big benefits (haven't tried it myself). It implements the binary protocol directly, and does not use libpq (the [rust driver](https://github.com/sfackler/rust-postgres) seems to be native as well).
Nice! Does it work with `company-quickhelp`?
Hi, just wanted to say thanks for this. I've just followed it through and all makes sense.
`rustc -O` is ` rustc -C opt-level=2` while "Release mode" in `cargo` is `rustc -C opt-level=3`.
Sorry to nitpick, but you say a couple confusing things… - `if`is **a control structure, not an operator**. Now, contrary to some languages where the `if-then-else` syntax is a statement (doesn't have a value *per se*), in Rust it's an expression which has a value as a whole, so the distinction is less obvious. However, Rust has traits matching all its operators (real operators are really functions or methods, but with adhoc syntax to make them look like maths) - More importantly: the `if` structure **evaluates to whatever its branches return** so it's only a boolean expression if its branches are. What really always has to be a boolean expression is only the *condition* (whatever's between `if` and the *then` branch). Try https://is.gd/CC3Pqr
You can use `slice::from_raw_parts` like so: [PlayPen](https://is.gd/DnoCJU) fn print_slice(s: &amp;[u32]) { for element in s { println!("{}", element); } } fn main() { let r = 5; let s = unsafe { std::slice::from_raw_parts(&amp;r, 1) }; print_slice(s); } 
Yeah, that's my current solution - just seems strange that there's no safe function in `std` to achieve the same thing! Guess it's a pretty niche use-case?
Ah, I see, thanks. I will try cachegrind. But, even if the branches get predicted, there is still a least one more instruction in the assembly for the check, right? And one (or more) instruction executed 4 800 000 000 times on an Intel Core 2 Duo 2.4 GHz would at least lead to a time difference of 2 seconds between `[]` and `get_unchecked()`, no? (I don't really know, I am just asking, here.)
&gt; unwrap() has a performance cost, there's an assertion, then the code necessary for unwinding, etc. &gt; Using if let's instead of raw unwrap's, or ignoring results where possible, brings the Rust code down again. Huh. TIL.
&gt; Rust has traits matching all its operators Nitpick on the nitpick, not all of them: `=` cannot be overridden.
Well, you do if you're unable to multiplex the connections, which is currently (but maybe not necessarily) the case. I'm prepared to review my understanding of threads, if I'm making a mistake. Running this simple program: fn main() { let extra_threads = 100; for _ in 0..extra_threads { std::thread::spawn(||{ std::thread::sleep(std::time::Duration::new(60, 0)); }); } std::thread::sleep(std::time::Duration::new(60, 0)); } And replacing the numbers by 0 (only the main thread), 1 (2 threads), 2, 10, and 100, on Ubuntu 16.4, using the Rust 1.11 stable downloaded via rustup, I get the following results: ghexsel@ghexsel-desktop:~/Dev/thread_size_rust$ rustc -O main.rs &amp;&amp; (./main &amp;) &amp;&amp; (ps v $(pgrep main)) ... PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 25670 pts/25 S 0:00 0 331 17820 5832 0.0 ./main 25687 pts/25 Sl 0:00 0 342 21929 9896 0.0 ./main 25698 pts/25 Sl 0:00 0 342 26037 12104 0.0 ./main 25727 pts/25 Sl 0:00 0 342 58901 34728 0.1 ./main 25752 pts/25 Sl 0:00 0 342 326221 178632 0.5 ./main So, just from this simple, maybe naive test, a thread does seem to have an large resident (RSS) memory footprint. I admit, this is not really my area as I tend to work on JVM languages, but if this is not a valid test, can you explain? 
It does indeed! IMO the experience with a separate buffer is nicer, but company-quickhelp just works.
The `read_dir` iterator yields `io::Result&lt;DirEntry&gt;` items. `unwrap` will take the Result by value, and return the `DirEntry` (or panic). Your first loop consumes the `dir`; you are printing if it's a dir but couldn't use `dir` afterwards. If you'd want to continue using the unwrapped DirEntry, you'd have to write something like for dir in dirs { let dir = dir.unwrap(); println!("{:?}", dir.path().is_dir()); // use `dir` here } An equivalent iterator would use both `map` and `filter`: let filtered = dirs.map(|x| x.unwrap()).filter(|x| x.path().is_dir()); There's also a combined `filter_map` which could be useful here depending on what else you're doing with the items.
You don't even have to unwrap if you're willing to work inside the `Result`s, e.g. let filtered = dirs.filter(|r|r.as_ref().map(|r|r.path().is_dir()).unwrap_or(false));
This exact functionality is provided by the [ref_slice](https://crates.io/crates/ref_slice) crate - it used to be a function in `std`, but was deemed too niche.
Thanks for this very clear explanation!
I think this is similar to the issue I ran into here: https://www.reddit.com/r/rust/comments/4eel2v/when_can_an_object_with_a_static_lifetime/ That issue boils down to the fact that lifetimes in normal shared references are covariant, but lifetimes in traits and mutable references are invariant. (If I'm right that that's what you're hitting here, I'm actually still not sure whether it's the trait or the mut that's doing it.) The section on variance in the Rust book is really interesting: https://doc.rust-lang.org/nomicon/subtyping.html For some other fun magic corner cases, consider: - How does the `.` operator decide what method you mean to call, when there are multiple implementations available through references/dereferences? - Why is it possible to do this, even though `x` is not Copy? let x = &amp;mut y; f(x); f(x);
i'm curious, could you link me to something that talks about rust using state machines for its APIs? sounds interesting
Personally, I default to unsigned ints always, and only use signed where absolutely necessary. It removes one more test scenario that you generally don't need. 
I've an implementation of Parallel, a GTK3 TV series renamer, and a GTK3 systemd service manager: - https://github.com/mmstick/parallel - https://github.com/mmstick/tv-renamer - https://github.com/mmstick/systemd-manager I'd also recommend `exa` as a replacement for `ls`, and RedoxOS is a cool project: - https://the.exa.website/ - https://www.redox-os.org/
You are looking for /r/playrust. This sub is about [the programming language Rust](https://www.rust-lang.org/en-US/).
[removed]
Who said you had to use libpq? We already have a pure Rust PostgreSQL driver. Let's just use that.
Sure, then you can probably hook better into an event loop. Does not change the fundamentals however :)
This was a big focus of my thesis: https://raw.githubusercontent.com/Gankro/thesis/master/thesis.pdf (Chapter 6 in particular)
The user has to know this in order to stack allocate it, so it can't truly be internal. Low level programming tends to expose details like this. You can make the type more opaque with `impl Trait`, though. Associated types are similar, but apply to traits rather than structs, which is OK because traits are resolved statically.
If the lifetime of x in f(x) doesn't escape f, I don't see the problem. Or is there something more complex I'm missing?
It would be interesting to consider kernel self protection mitigation techniques in this design. For example, taking inspiration from grsecurity. These techniques have proven insanely effective. Also, I have no experience writing kernel software - is this something that can be easily broken up into smaller pieces? Is this something I could help with?
What word would you recommend instead of "pure" for a Markdown document where the only un-escaped markup constructs are specific to Markdown syntax? (or a Markdown renderer which escapes literal HTML prior to rendering)?
You could find some popular projects using [github search](https://github.com/search?o=desc&amp;q=language%3ARust+stars%3A100..200000&amp;ref=searchresults&amp;s=stars&amp;type=Repositories&amp;utf8=%E2%9C%93). Check ones that are still active somewhat. Of course, there are other interesting things, maybe less popular or simply outside of Github, but the search could get you started.
Really cool that people are already writing an operating system in rust. I know this project for a while but it really looks like an OS that will probably stay here for a while. Not the new Linux but more a look-how-safe-rust is project. Sadly enough I don't know any rust at all. Currently bussy with building web applications.
Ring is based on BoringSSL right? Can someone explain what is meant with being pure rust? 
I know it's somewhat controversial, but I wouldn't consider putting lots of comments into a code base a good practice. Reading "Clean Code" by Robert C Martin really opened my eyes to the issue. Comments rarely reflect the state of affairs if the code they refer to is changed. This is where they become misleading. Having lots of comments in a code base is definitely a code smell if you ask me. Well named variables, function names and classes, together with an occasional comment about non-obvious side effects should be preferred.
I couldn't disagree with you more. The amount I've learned from various codebases just by reading the comments is incalculable. The least I can do it is pay it forward when I can in my own code. I'd venture a guess that the problem here is that it can be easy to value comments just by their quantity and not their quality, and this can happen on *both* sides of the debate. (To be fair, the OP does seem to explicitly state what kinds of comments are valuable, and I agree with them.) I think quantity is mostly a red herring. For example, I don't think it's possible to have too many *high quality* comments. High quality comments aren't narrative, but rather, document the *invariants maintained by the code*. No matter how clear your code or your names are, it's almost always useful to document invariants. Writing high quality comments is hard (but so is writing high quality code).
IMHO - best practice is to treat comments like references in a technical paper. Most of my comments consist of URL links to blog posts, or otherwise keywords that explain what to look up to understand what is going on.
Yeah I learned the hard way today that you don't post servers in /r/rust or /r/playrust but actually in /r/playrustservers
&gt; For example, I don't think it's possible to have too many high quality comments. High quality comments aren't narrative, but rather, document the invariants maintained by the code. No matter how clear your code or your names are, it's almost always useful to document invariants. Was going to say this - it's particularly important when building abstractions over unsafe code to document why the abstraction is safe, what maintains the safety, etc. And to be clear about it. Actually it would be nice if there were a lint for undocumented unsafe blocks.
It's definitely not a problems. I guess what I meant to suggest is "isn't it weird that the compiler knows this is ok, even though it would not be ok for non-copy value types?" I think the underlying semantics has something to do with an implicit re-borrow.
Correct.
Spoilers below, view at your own risk. I'm having some trouble sorting out why this is a lifetime-constraints error instead of a "you didn't implement that trait correctly" error. It seems that fn get_from_wrapper&lt;'a&gt;(wrapper: &amp;'a mut Wrapper&lt;'a&gt;) -&gt; T { wrapper.convert().get() } is fine on it's own, the only problem is that its type doesn't agree with the function from the trait (lifetimes are de-anonymized below): fn get_from_wrapper&lt;'a, 'b&gt;(wrapper: &amp;'a mut Wrapper&lt;'b&gt;) -&gt; T; Am I reading this wrong?
&gt;Actually it would be nice if there were a lint for undocumented unsafe blocks. control-F for `unsafe`: https://github.com/BurntSushi/rust-snappy/blob/master/src/compress.rs https://github.com/BurntSushi/rust-snappy/blob/master/src/decompress.rs :-) (That was hard and exhausting work though. It's hard to keep it up. A lint would definitely help motivate me.)
The Emacs help API is not really intended to be extended -- I played with it for a while without getting it working properly. I think it could be useful to have multiple `*Racer Help: foo*` buffers around, but maybe that's retroactive justification :) &gt; Would be nice if it reused a single one instead. This is definitely something we should provide a customisable option for. We might even want to default to this behaviour. I've opened https://github.com/racer-rust/emacs-racer/issues/48
yeap! , that was the problem ,fixed now. Thanks a lot!
These are the kind of projects I like to see, I think they prove Rust as a very viable option for game development outside the realm of people just hacking games together. Consider it featured.
You may want to take a look at how other language modes implement this functionality. &gt; I've opened https://github.com/racer-rust/emacs-racer/issues/48 Thanks!
Not too bad! I like the separation of concerns. I have two comments. 1. You can update the result map directly and skip building the intermediate `HashMap` for each line. 2. There are functions like `BufRead::lines()` and `str::trim()` which can clean this up a little bit and avoid reading the entire file at once. A more compact version might look like this: use std::collections::HashMap; use std::fs::File; use std::io::{BufReader,BufRead}; fn main() { let f = File::open("/usr/share/dict/words").unwrap(); let lines = BufReader::new(f).lines(); let mut results = HashMap::new(); for line in lines { for c in line.unwrap().trim().chars() { if 'a' &gt; c || c &gt; 'z' { *results.entry(c).or_insert(0) += 1; } } } for (k,v) in results { println!("{} : {}", k, v); } } Since the "lines" don't really matter, if you were willing to use unstable code you could call `chars()` directly. let chars = BufReader::new(f).chars(); for ch in chars { let c = ch.unwrap(); if !c.is_whitespace() &amp;&amp; ('a' &gt; c || c &gt; 'z') { // incr count } }
&gt; … up to 4x faster than the C++ version Are you comparing your code with a *multithreaded* C++ version? I don't think it's fair to compare a multithreaded program to a single-threaded program. From a cursory glance, the code on nbabel.org (and even the website itself) looks very old and I doubt any of the codes up there are what I would consider optimal.
I have a pre-commit hook that I'm happy to share :) https://github.com/thepowersgang/rust_os/blob/master/Hooks/pre-commit -EDIT- That calls https://github.com/thepowersgang/rust_os/blob/master/UnsafeAudit.sh
How can I achieve the following using this crate?: #[derive(Default)] struct B { bar: i64, boo: u32 } #[derive(Default)] struct A { pub b: B, foo: i64, ... } A::default().bar(3).boo(5) Basically, I'd like to keep the `b` field and only the `b` field public and allow its members to be set through `A`
I'm working on [curtain](https://github.com/ivanceras/curtain) an database admin webapp with an online [demo](http://45.55.7.231:8080/) 
How much are the self-protection / grsec-style mitigation techniques irrelevant given memory safety? Stuff like W^X / PAGEEXEC / etc. only matters if there's a chance that control might flow somewhere where it isn't supposed to. But control-flow integrity (in the absence of dynamic control flows, like JITs) is I _think_ a subset of memory safety, right? And stuff like ASLR and kptr_restrict are all about making it harder to redirect control flow somewhere useful if you do manage to take it over, which, again, shouldn't be needed under memory safety. (Of course, they're still all relevant if you don't trust that you've upheld memory safety 100% correctly in unsafe blocks or assembly-language helpers, which is a good thing to be paranoid about.)
I've heard that about laptops due to overheating and processors not being designed to do that, perhaps that's what he's referring to?
The stdlib doesn't really do it as a matter of policy so much as that code tends to evoke "wow this code is interesting, let me explain it". There's plenty of undoc'd unsafe code because it's like "yep I'm derefing this unsafe pointer, better be ok".
I think it is partially implied in the [semver spec](http://semver.org/#spec-item-7) itself. There is no "public API" per se, but it MAY be incremented if there is a reason to do so (especially a backward-incompatible change to the *to-be-public* API), and it is reasonable and/or useful to assume that 0.y.z is compatible to 0.y.w (w &gt;= z) in some sense.
I think the rules after rule 4 are meant to assume that x &gt;= 1. It's a little ambiguous, I guess. Regardless, cargo assumes x &gt;= 0.
There is quite a difference between no comments, bad comments, and good ones. The good ones are additions to already clean code and explain the "Why?" instead of the "What?".
Parallel has been used in academics for quickly scaling serial simulations to all available CPU cores in a system, or multiple systems with SSH. The more cores that you have, the greater the benefit that Parallel can provide. You could, for example, have a text file with a million data sets supplied to Parallel, which will distribute each line of data evenly among all available CPU cores. The output of each individual command will be buffered so that the outputs are in the same order as the inputs, even if some tasks finish quicker than others. This means that the results are printed as if you had done the tasks serially. On the desktop, it's typically used for image and audio conversion, parallel wget/curl downloads, as well as compression, but there's no limit to what you could attempt to parallelize with it. https://www.usenix.org/system/files/login/articles/105438-Tange.pdf
It is an untextured object arised from `unsafe` code. More accurately, it indicates you are not in the correct subreddit; try again with /r/playrust.
&gt; A place for all things related to the Rust programming language, an open-source systems programming language that emphasizes zero-overhead memory safety, fearless concurrency, and blazing speed. And when you submit a post: &gt; The Rust programming language. For the Rust video game, see /r/playrust 
Working on my turn-based strategy game [Zone of Control](https://github.com/ozkriff/zoc), as always :) . ##### This Week in ZoC Last week was not very productive :( - [Published monthly report](https://users.rust-lang.org/t/this-month-in-zone-of-control/6993) - [Made discovered FoW tiles fade to alpha smoothly](https://github.com/ozkriff/zoc/issues/210) - https://youtu.be/eNwlOO_tTqs - [Derived `Copy` trait for all simple structs / cleaned up `clone` calls](https://github.com/ozkriff/zoc/issues/217) - [Cleaned up](https://github.com/ozkriff/zoc/commit/c95106) [some](https://github.com/ozkriff/zoc/commit/19140b) [code](https://github.com/ozkriff/zoc/commit/dbd4ea) [here](https://github.com/ozkriff/zoc/commit/dbb581) [and there](https://github.com/ozkriff/zoc/commit/ba6fe) Right now I'm working on [helicopters](https://github.com/ozkriff/zoc/issues/111). This week I hope to: - Finish helicopters - [Add vehicle towing system](https://github.com/ozkriff/zoc/issues/161) - [Add simple map selection](https://github.com/ozkriff/zoc/issues/213) - [Fix bridge slot count](https://github.com/ozkriff/zoc/issues/214) [@ozkriff](https://twitter.com/ozkriff)
I'm trying to get the [RFC #1623](https://github.com/rust-lang/rfcs/pull/1623) [implementation](https://github.com/rust-lang/rust/pull/35915) to pass the tests. Also my usual TWiR stuff (btw. nominate and vote for [CotW](https://users.rust-lang.org/t/crate-of-the-week/2704/172?u=llogiq), please?) and I've just started yet another small project that I believe will be quite beneficial for the Rust ecosystem (to be announced soon).
Hyper will use tokio and tokio uses futures, am I right?
It's not only laptops either... my 2014 5k iMac's i7-4790k goes up to 98°C (core temperature) with fans at full speed. I'm pretty sure keeping it at this temperature for a long time would damage it.
That really should not happen. Components are designed to endure long-term heavy use, and if you're managing to fry them on any reasonable use case, then you've simply purchased bad quality hardware. Linus Tech Tips even has [a video dedicated to this topic](https://www.youtube.com/watch?v=44JqNJq-PC0) (though granted, it's GPUs, but I'm fairly certain CPUs are held to at least as rigorous standards).
I meant that the TLS stack is pure-Rust. Sorry for the confusion.
Besides from my daily job involving *Kailua* (which is heading towards another interesting direction...), I've released [Encoding](https://github.com/lifthrasiir/rust-encoding/) 0.2.33 as a quick fix for the [recent regression](https://internals.rust-lang.org/t/regression-report-stable-2016-08-16-vs-beta-2016-08-26/3930/9) in beta. My bad, haven't really worked on Encoding 0.2 branch for months.
Still learning and enjoying. Building a (useless) web framework just for learning purpose. Asking questions on SO, having clear answers. Thanks to rust community!
This video is about performance degradation over time. Overheating over long periods of time does not cause permanent performance degradation (although underclocking does reduce performance temporarily), but can reduce the lifespan of hardware. CPUs don't really get slower, they either work or they fail. CPUs and GPUs should never overheat during normal use. Yet they routinely do in laptops and some desktop computers, because manufacturers assume most people will not run them at full load over extended periods of time. I define overheating as reaching temperatures high enough that the chip's thermal protection forces it to underclock. My 2014 5k iMac's GPU (AMD R9 M295X) goes up to 104°C, sometimes even 105°C. Its _absolute maximum_ operating temperature is 105°C. Keeping it at this temperature for a long time is most likely going to reduce its lifespan significantly. AFAIK, 'normal' desktops (i.e. towers), on the other hand, are usually built in such a way as not to overheat, and it should be safe to leave them at 100% load.
I've heard this called "why" comments, as in 'why is this code here?'. "How" comments are usually not worth it.
Sorry. I'm a dumbass.
I need to set up a script to post "Ruma" for me in this thread every week.
OK, that does sound more reasonable than my line of thought. I haven't really done any previous full research on the topic, I've just heard it mentioned anecdotally that running a laptop CPU too hard can be bad for the computer. On the other hand though, his warning might just be that, a warning to people who may have purchased less-than-ideal hardware. I know the first laptop I bought was from a not-that-reputable company, and I ended up fusing the lid to the case (so it couldn't open) within a year by overheating it. It was definitely badly designed, and if I had known more I wouldn't have gotten it, but it was my heavy usage of it that caused it to overheat.
Thanks! I read this: https://github.com/tokio-rs/tokio-hyper#status and this : https://github.com/hyperium/hyper/issues/881 so I think Hyper will use Tokio. But I'm not sure I really understood, that's why I ask.
&gt; Why y isn't f32 but f64? Well, it needs to pick _something_ as the default floating-point type (if it wants to have a default at all, of course). Why should it be `f32` rather than `f64`?
Minor nitpick: try! is not equivalent to the code illustrated, it additionally performs error conversion using From. https://github.com/rust-lang/rust/blob/8787a12334439d47e931be26fef53381ce337c3a/src/libcore/macros.rs#L223-L230 Sadly, the documentation doesn't mention that, too :(. *clones rust repository* https://doc.rust-lang.org/std/macro.try!.html
-Ofast is -O3 with -ffast-math: $ gcc -c -Q -Ofast --help=optimizers &gt; Ofast-opts $ gcc -c -Q -ffast-math -O3 --help=optimizers &gt; O3-ffast-opts $ diff O3-ffast-opts Ofast-opts &amp;&amp; echo 'Same' Same 
&gt; modern (x86) CPUs are optimized for double-precision so there's no real speed difference between the two Please note that this only applies if the operations can't be vectorized with SSE instructions for a bunch of numbers. A 128 bit SSE register can process 4 single precision numbers in one go, but only 2 double precision ones. So if you are doing a whole lot of computations in a tight loop (like say, vector dot products), single precision *does* have a significant performance advantage.
I find that it's a bit awkward to implement a cross-platform shading pipeline at this level of abstraction. When you target multiple platforms, you usually target multiple levels of hardware capabilities, which means that having your shader code compiled to different targets is not enough, you often want to employ different strategies that affect the entire rendering pipeline. Content creators (as opposed to engine devs like ourselves) reason about shading in terms of materials and there is a common misconception that a material == a shader program. This mindset was okay-ish when forward rendering was all we knew, but with architectures like deferred shading and (the new cool kid in town) object-space shading, parts of the material rendering code are spread at different points of the rendering pipeline. It would be very useful to have a lib that translates simple stuff into various shading languages (after all, why reimplement your BRDF function the same way in 4 shading languages), but I would use that as a helper that generates only parts of the shader and integrates into some higher level linker which knows about the platform details and is responsible for stitching together the different bis of shader code in the right place (potentially mixing it with hand-crafted platform-specific shader bits). For example: on mobile, do this lighting computation in the forward pass getting the normals from an interpolated vertex attribute, while on windows use the same lighting equation but in a deferred pass, with normals sampled from a texture, some procedural function, or whatnot. In this case a tool generating the shader code for the simple piece of lighting equation saves time, but that code should not even read from textures (should just be pure math) and the thing that attach these bits together should know about the platform it runs on. Being fully cross-platform at the low levels means you can only use the common denominators, which can be okay for some applications, but often the reason you are targeting multiple backends is to get the best each platform can offer in which case the low-level abstraction gets in the way. In my opinion, low level apis like gfx, glium and luminance (although I don't exactly know where luminance sits, since it does opinionated things like reordering drawing commands) should strive to be fast, safe and productive, erasing some of the platform differences when it makes sense while acknowledging that some things can not be properly abstracted away without enforcing choices that belong to a much higher level abstraction (assembling shaders being one of these things in my opinion). The beautiful thing is that these shader tools don't even need to be in gfx or luminance, they should be external libs that can be used to simplify the life of the gfx/luminance users, only if they need it.
&gt; How much are the self-protection / grsec-style mitigation techniques irrelevant given memory safety? To add on to /u/jdubjdub Security is also a moving target - what we consider "memory safe" today may not be memory safe tomorrow due to emerging attacker techniques. We can say rust is memory safe given a definition of memory safety that includes what we currently know. Kernel security also impacts userland - considering a design that allows for strong userland ASLR, for example, will impact more than just the kernel but that's where it would be implemented. Or a strong sandboxing mechanism, which should be enforced at the kernel level.
I was indeed talking about damage over time; this seems crazy at first because well, your computer just shuts down when it gets too hot right? The problem here is that the simulations with 32k or even more masses can take days to weeks to complete. The non-multithreading scripts would be completely safe to run for weeks, as they use up to like 25% on modern hardware. But when an old dusty lab Dell runs at 100% for weeks, it might just catch fire.
Yeah, when I first read the I thought it was doing float math, it isn't so the flag will have no impact.
To be clear, I definitely didn't mean to hold up my `snappy` code as code that was easy to understand. I definitely don't think it's easy to understand. Many of the optimizations make it quite opaque and hard to follow. If I had more time (or was smarter), perhaps I could make it clearer with a better abstraction. Where I disagree with you, I think, is that I don't think renaming `self.s` to `self.src_index`, for example, is really going to make much of a difference. In particular, I can't spot any comments that I'd be comfortable removing after changing any of the names.
\\seriousmode{off} You could say that this rustls my jimmies. \\seriousmode{on} In all seriousness, this is a great project!
Unfortunately, they are hard to compare. The version you linked uses only a small amount of masses (in your case, planets), and there is very little benefit of using multi-threading in that case. The logic required to spin off a thread would be tougher than just using a single thread. When simulating with loads of particles, you can 'squish' the O(n^(2)) complexity a bit in your favour.
Thanks for noticing! :)
You’re right, but `luminance` is not a 3D engine. The level of abstraction is pretty low-level, actually. You still have shaders. You still have framebuffers. You still have textures. I just felt the need to introduce `Pipeline` and all subsequent types because of safety. If I want something safe, those types are required, otherwise, you either end up making mismatched bindings, or you bind too much. Currently only OpenGL 3.3 is supported – OpenGL 4.5 is on the road and won’t be hard to add. Vulkan is planned as well but if I find something that could be changed so that I can use the backend in a more interesting way, I’ll change the design. It’s not a static / fixed graphics framework – it’s `0.*.*`, then not stable yet ;).
Back to improving Chomp and working towards a 0.3 release of the monad-like version, and a 2.0 candidate for the monad version. At least I think that is how I am going to split the crate, 0.x.y - 1.y.z as the version working on rust stable and 2.y.z+ for nightly (with a full-fledged monad).
All-in-ones tend to use laptops parts, so I'm not that surprised.
I continued to improve the performance of my [`des`](https://crates.io/crates/des) crate which is used by my [`uncbv`](https://crates.io/crates/uncbv) project. I published an updated last week to show the latest performance improvements. Now, it is only 5 times slower (in a single core CPU) than the DES implementation of OpenSSL. I was able to optimize the `E`, `P`, `PC1` and `PC2` transformations using this [bit permutation code generator](http://programming.sirrida.de/calcperm.php) (I used the offline C++ version). It is a great improvement, but it still needs work. I have some ideas on how to improve the performance a bit more, but it is becoming really challenging. I started to look at the DES implementation of OpenSSL to see the tricks they used. Since I did not get a noticeable performance improvement by using `get_unchecked()` over `[]`, [I asked a question on StackOverflow](http://stackoverflow.com/questions/39196594/why-dont-i-get-performance-improvement-by-using-get-unchecked).
You've got a good point. Indeed - different platforms need different shading approaches due to their capabilities. However: 1. Even within the same platform you may need different quality tiers, so making different profiles of features is not strictly limited to platforms and may live completely separately. 2. Platform != graphics backend, in general. So, on PC you'd still want DX11/DX12/Vulkan/GL/Metal backends doing the *same* shader code. Even on mobile you could be switching between GLES-3.0 and Vulkan for the same hardware. So, what I'm proposing is - compile shaders for all backends using some sort of universal shading solution. Then have several quality profiles, and then choose which of them are available on which platforms.
No, check Agner's tables. Signed and unsigned operations take the same amount of time (and in any case unsigned and signed add/sub are identical).
First of all I like Rust. I hope that it continues to evolve and improve, however posting something like this with zero effort, outside of writing the Rust version, is a disservice to the community. It really wasn't much effort to get an apples to apples comparison instead of just saying that you wrote something faster than 5 year old code. I assume you are looking at the c++ version [here](http://www.nbabel.org/codes/1). That page says that it runs over 4k bodies in 4m52.4. So I assume you ran your rust version and it took you about 1 minute and that's where your claims come from. I wanted to get my own results so I ran some unscientific tests. I downloaded and ran your rust version unmodified apart from bumping the hardcoded threads count to 12 since I have a 12 core processor. I also downloaded the c++ code from that page. Ran it unedited to get my results. And since that code is from 2011 and single threaded I spent 5 minutes updating it using std::thread. All I did was thread the acceleration() function in the exact same manner you have in your Rust code. Doing it in c++ with a lambda using std::thread is no more complex than doing it in rust, in all it is maybe 10 lines of code changing to thread it. These are the results I got (c++ with /O2): rust input4k - 0m42.62 c++ input4k (single threaded) - 4m49.64 So yeah, looks like your code is more than 4x as fast. Cool! But again single threaded is crap in this case and it's not any more difficult to multi thread this in modern c++ so let's look at that. c++ (MT) input4k - 0m15.94 OK, so now c++ is back to being the fastest and is 4X as fast as your code. For kicks I ran it on the input16k also to see if the difference holds up Rust input16k - 15m29.32 c++ input16k - 4m15.95 So looks like c++ is back to being the fastest by a good margin. However this is interesting and I don't know why rust is so much slower so I may spend some time figuring out what's going on and how to further optimize this problem. One of your comments you mention that the ease of writing it in Rust is the big advantage, however being 4x slower is still unacceptable for something like this unless it's exponentially harder to write it in one language, and in this case I would argue that there's zero difference in the effort required to write this code multithreaded in Rust or modern c++. 
&gt;Single-precision floating point (`f32`) accumulates rounding errors much faster than double-precision (`f64`), and modern (x86) CPUs are optimized for double-precision so there's no real speed difference between the two. Unless you care about vectorizing or the biggest bottleneck on modern CPUs, cache.
``` unsafe { (*dst) = dst_pointer}; ``` Um... you're assigning the address of pointer to a memory location under another pointer... Instead you should iterate over characters and copy them into `dst` one by one (or use some rusty `memcpy` equivalent). As /u/thiez said, "late at night" issue. I really doubt you meant to write `void*` into `void` :)
Nothing is ever 100% for certain, my understanding is that that's the plan, though.
Yes. If there's no way to tell a compiler "this must remain constant-time", then the compiler might change things so that it's not constant time. IIRC this is not paranoia, it's actually happened in the wild.
The GPU in that iMac is indeed a mobile chip, but the CPU isn't, and mobile components are not the reason it overheats. It overheats because the components are cramped together, thereby heating each other up, there's very little airflow due to the case design, the entire computer is cooled by a single fan and OS X waits a long time before bumping the fan RPM, in order to achieve near-silence during regular usage (although when things start heating up, the fan goes up to 2700 RPM and makes a lot of noise).
Yeah, I changed my mind :D
&gt; I find it interesting that the multithreaded version is 20 times faster, when I had expected 8-10 times with 12 cores. This is probably caused by simultaneous multithreading (hyper-threading is Intel's implementation), which can allow any given core to run two threads simultaneously, provided they use different units. It can provide a near-linear performance boost in ideal circumstances, IIRC.
Yeah, but wouldn't he have to define 24 threads then? In his case with 12 threads, every thread would map to a logical core.
In addition to the other explanations, I would note that this choice matches the behavior of today's C compilers, where `42` has type `int` (32 bits) and `1.0` has type `double` (64 bits). (The C language itself does not mandate an exact size for `int` and `double`, but almost all compilers use these values).
You know, openssl was once a modern, non-crufty approach to transport security. What I'm more interested in is how the authors of this stack plan to avoid the arriving to the same fate as openssl.
I think the other replies in this thread show that cargo's behavior doesn't match expectations until you read the docs!
Which other replies? There's only one other one. And we did it to match what other ecosystems do, it's behavior people do generally expect.
Agreed with the need for high-quality comments, but: &gt; document the invariants maintained by the code This is better done with asserts. Or [Design By Contract](https://en.wikipedia.org/wiki/Design_by_contract) if there is support for it.
I meant to ask for quite some time: why do you use a top-level Makefile instead of what `cargo` offers (binaries and stuff)?
The decision wasn't made because "that's what C++ does." We may have made the same decision for similar reasons, though.
That is an excellent point. I would like to see a governance model with a more aggressive approach toward deprecating and removing (non)features. I think Openssl grew organically in a way that made it very difficult to pare down once it was recognized that there were problems. In today's environment, we're very cognizant of the range of vulnerabilities, both logical and physical that could lurk. That should heavily influence the architecture. 
Ahh, I see. Well it looks like this ring crypto library is extremely well vetted, and implementing something in Rust is no guarantee of security anyway. (And right now can actually be less secure from those timing attacks.) So with this TLS library, how close do you think we are to an OpenSSL replacement written in Rust? Is that something that people are working towards? P.S. Thanks for all your work on Rust, you're awesome! 
What is the best way to return a *small* list of numbers from a function, preferably without allocating something on the heap? Think tight inner loop. * `[u16; 12]`, i.e. a fixed sized vector. I'm guessing just returning it as-is is Ok because RVO. * A list of numbers, sized between 0 and a known maximum of 24. Since a vector seems overkill, I am currently going with `[Option&lt;u16&gt;; 24]` and stopping at the first None. Could the function do something like `yield` does in python, so it can be used as an iterator directly?
Yes, these are similar problems.
also interesting, thanks.
Rust is still too low level of a language and the ecosystem still too undeveloped for my (current) domain, but I have a special place in my heart for communities with an emphasis on documentation. As someone with no programming experience at all (I was a supply chain analyst at the time...the domain of Excel), I learned Postgres and R quite easily (propelling me towards learning much more about programming) thanks to their approach to documentation. Things like this blog post give me so much hope for the future of Rust.
👍 …and again, a beautifully executed beginner-friendly article! Love everything about it. Writing style, diagrams, everything. --- &gt; [Also can't wait for your follow-up post on move, copy &amp; clone. * cough *](https://www.reddit.com/r/rust/comments/4xuds0/sharing_coloring_books_with_friends_in_rust/d6k71jl) Heh. 😋
So I'm reasonably new to rust but have learned a lot so far. One thing I haven't really been able to really find much documentation on is how to structure a crate. Obviously there isn't just a main.rs that gets run. Does anyone have any links to any sort of explanation on the general structure of a crate or even links to the repos for simple crates that I could dig through. I've done a bit of looking through some repos but most of what I find is far to complex for me to understand. 
If you're asking whether Hyper is going to use Futures in its API, probably not. It seems like it's going to remain callback-based, and then an abstraction using futures can be built on top of that. I have plans for an asynchronous client abstraction using futures and incorporating my work with `multipart-async`, but I'm mostly just waiting for the new Hyper async API to materialize.
We're actually close to having a [new modules chapter for the new version of The Book](https://github.com/rust-lang/book/pull/142/files) done. I would love if you'd read it and let us know if it helps! That chapter is more about the *mechanics* of crate organization though, and not so much the *reasoning* behind how you might want to organize your crate-- there are lots of different ways to organize crates, and at the moment imo it seems like more personal preference and how you think about your crate's domain rather than any "best practices" (insert doubt as to whether any "best practices" for anything really exist here). I think we as a community are still experimenting to find code organization that works for both crate author and crate user. There's a later chapter in the new book that is starting to shape up but we haven't gotten to yet on creating a library. It will include instructions on uploading to crates.io, creating API documentation, and multi-crate packages. Stay tuned for that! As far as crates to look at for examples, I'd say look at: - Any of the [rust-lang-nursery crates](https://github.com/rust-lang-nursery), especially log, which has env-log as a sub-package - [Servo's URL crate](https://github.com/servo/rust-url) has gone through some thoughtful API reorganization between 0.x and 1.x, [see this commit](https://github.com/servo/rust-url/commit/9e759f18726c8e1343162922b87163d4dd08fe3c) for the big changes and [this PR](https://github.com/servo/rust-url/pull/176) for the process - [Iron](https://github.com/iron/iron) and iron-related plugin crates 
Thank you so much. I'll give it a read later tonight and get back to you with some feedback. Should I go through any official channels or would you just prefer a dm on here?
Thanks you!
By definition, Rust could never *replace* OpenSSL, it just doesn't have what it takes. 
Yes, this is how pattern matching works in other languages. I'm only super-familiar with Haskell, but I believe it is in the ML family as well.
Use after free, reading from uninitialized memory, goto fail ... rim shot!
I found this answer to be useful: http://stackoverflow.com/questions/38276960/what-is-the-recommended-directory-structure-for-a-rust-project/38277274#38277274
&gt; When we write a program in Rust, your code starts off living in main Change of subject from 'we' to 'your', feels unusual...what about just "When you write a program in Rust, your code starts off living in `main`". But I think you should also define what `main` is -- you just mean the `main` function here right? IMO you should say `main` "function" then :)
Interesting. I guess that explains where those behaviors came from. Appreciate the quick feedback.
&gt; Rust has several properties ok.that makes it attractive to platform-agnostic developers: Seems like something slipped through here. (No idea if OP is the writer though.)
Gah! You're right; we try to keep the "we" phrasing throughout, but sometimes I slip. Thanks!
No problem. Familiarity is a tricky thing; it's so context dependent.
&gt; we try to keep the "we" phrasing throughout, but sometimes you slip. 
The patterns `y` and `_` are catch-all in different ways -- `_` matches anything but throws away the match, while `y` matches anything and binds it to `y`. IMO, thinking of it as untyped and unbound is misleading. The type is inferred (to be the same as the type of `c`). And it _is_ bound -- to the value it matches. The name is immaterial -- you could call it `c` as in the Stack Overflow post. The way I think of the `y` pattern is that you didn't put any restrictions on it, so it matches anything. For example, consider a slightly more complex case, matching an `Option&lt;char&gt;`: let c = Some('a'); match c { None =&gt; { /* there is no char */ } Some('a') =&gt; { /* there is a char and it's 'a' */ } Some(x) if x.is_digit() =&gt; { /* there is a char and it's a digit /* } y =&gt; { /* there is a char but it's not 'a' or a digit */ } } The cases are considered in order, so `y` catches anything that wasn't caught by the earlier, restricted, patterns.
uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuugh i am the literal worst :)
There's [`std::ptr::read`](https://doc.rust-lang.org/std/ptr/fn.read.html), which you can combine with casting between pointer types. [Example](https://is.gd/iDtgB8). Nothing to help with endianness, though. Alternately, for a more literal translation, you can combine [`std::mem::uninitialized`](https://doc.rust-lang.org/std/mem/fn.uninitialized.html) with [`std::ptr::copy_nonoverlapping`](https://doc.rust-lang.org/std/ptr/fn.copy_nonoverlapping.html). Which is [exactly what ptr::read does](https://doc.rust-lang.org/src/core/up/src/libcore/ptr.rs.html#135-139).
You can try the [pod](https://crates.io/crates/pod) crate. Especially the [`from_slice`](https://arcnmx.github.io/pod-rs/pod/trait.Pod.html?search=#method.from_slice) method. It seems that it doesn't check for alignment though :(
&gt; After researching on this for a minute it looks like the compiler breaking security conscious operations because they are intentionally wasteful is a big concern. It is, indeed. The thing is, crypto (and security) are the odd ones out there. Most developers are *glad* that the compiler removes operations left and right and make their program leaner and faster. Crypto requires specific guarantees, that no language I know of implement, and therefore has to dive into assembly to bypass the compiler's cleverness.
&gt; Rust’s brutal performance For some reason, I really like this adjective here :)
[itshappening.gif](http://i.imgur.com/Hf1yqgr.gif) --- ^(*Feedback welcome at /r/image_linker_bot* | )[^(Disable)](https://www.reddit.com/message/compose/?to=image_linker_bot&amp;subject=Ignore%20request&amp;message=ignore%20me)^( with "ignore me" via reply or PM) 
[Ruma](https://www.ruma.io/) would be honored to be featured. :}
Small point -- because your vector contains string literals, their type is `&amp;'static str`, and the clone doesn't actually copy the character data of the strings. The cloned vector will have its own heap array which in turn points to the same raw data as before for `"mercury"`, `"venus"`, etc. If you turned these into `String` values, then the clone would indeed get completely independent copies.
Cool this is exactly what I wanted. Thanks for the help.
Still reading through it. This update really helps as I was confused about this topic previously. One recommendation though - the demo made is using a project called "modules". As a newcomer to a technology, I've tripped up a few times with keywords not sure what all is necessary to get things working when the demos name things after the topic they are trying to impress. It might leave the reader with, "to make a module I have to start with `cargo new modules`". Naming it something more related to the example would help drive home the different keywords in code and command line.
This doesn't seem very safe, but it's super cool. Gotta be some kind of mad genius to want to add *this* to Rust.
While having the safety of a language with GC, not by having a GC.
Well, I mean, maybe what you really want is something like `Vec&lt;String&gt;`, because `chars` isn't enough. A `char` is a single codepoint, but a single *logical* "character" can be multiple codepoints. Of course, that's hideously wasteful, so maybe you *really* want something like `Vec&lt;SmallVec&lt;char&gt;&gt;`. But you *also* want efficient substrings and owned strings? Maybe `Cow&lt;[SmallVec&lt;char&gt;]&gt;`, then. Text is much bigger pain than people generally realise.
I'm not sure if people here will find this post interesting - but I haven't written anything in a while and wanted to share it. In my ongoing attempts to make [rulinalg](https://github.com/AtheMathmo/rulinalg) _somewhat_ efficient I wanted to add in-place transposition. It was an interesting challenge!
My understanding of `cargo` is that it'd be able to handle this kind of cases. What do I miss?
Sorry for kind of off-topic, but can you elaborate on the unsafety issues with crossbeam + coroutines? Edit: found #rust IRC [logs](https://botbot.me/mozilla/rust/2016-03-24/?msg=62800962&amp;page=5) from March where you say this: &gt; [unsafety caused by] leaping out of the scoped thread via coroutine then forgetting it Would love to hear more about this. Edit Edit: found [reem's](https://botbot.me/mozilla/rust/2016-03-24/?msg=62802095&amp;page=6) [example]( https://is.gd/E6cNPT) which demonstrates leaking a stack reference through a coroutine into a scoped thread, but I don't understand it. Can someone explain?
I would love any of: - Issues or pull requests to [the book repo](https://github.com/rust-lang/book/) - Email to me, carol dot nichols at gmail - DM here - DM on twitter @carols10cents It is totally up to you :) &lt;3
The volatile qualifier in C/C++ should be enough to prevent optimizations that break the constant time requirement. You'd just have to directly tie the runtime of your algorithm to memory accesses to the volatile region(s). Right? Does anything like volatile exist in Rust?
There are other ways to get compile time guarantees, with phantom types comes to mind, but then the API could only be consuming, because of changing types.
Yea, I think that makes a lot of sense, but the real confusion I was having was around the `Vec&lt;&gt;` manipulation. Are there easy ways to take subvectors based on index? I suppose I could just pop things off the `Vec` in a loop and build a new `Vec` but that seems really wasteful. I don't really need this to be mutable, I just want to copy chunks of it.
`Vec` has a `split_off` method that you can use to break it up. You can probably call this twice to get any contiguous subvector you want.
If you want a slice of a `Vec` based on two indices you can do `&amp;foo[a..b]` where `a` and `b` are indices. Leave out `a` to start at the beginning and leave out `b` to end at the end. If you want to convert the slice into a strong independent `Vec` then you can just call `.to_owned()` on it.
If a major vulnerability (with a working attack) is found for some protocol rustls currently implements; could you envision removing it from rustls? Or would it be more likely that an angry warning lint would be added or something along those lines?
&gt; The example shows us violating memory safety in all safe rust From the rust website: &gt; guaranteed memory safety So that is a bug in the compiler / the rust specifications, is it not?
&gt; The second example is completely different from the first one Yes I gave the second example just to show that the if alone is not a problem. Somebody on the chat thought returning the mut reference was the issue. Sorry it was not clear. Why does returning a borrowed value makes it go out of the scope?
No - if you don't use any external unsafe code rustc and std are all safe, this issue is only with a combination of two third party libraries which both expose "safe" APIs.
Great work! The only thing I stumbled on was the following: "The reason is that paths anywhere except in a use statement are relative to the current module." .oO(Oh, what suspense, do tell me what paths in use statements are relative to.) [...] "super:: is special and changes the path we give to use so that it is relative to the parent module instead of to the root module." .oO(Ah, got it!) I feel that this should be explicitly stated right after the first example of the use statement.
`&amp;mut T` is `Send`, so that won't solve the problem. One way to mitigate this issue is to have the coroutine only be able to reference `'static` data, but this is often an annoying limitation and not necessary without scoped threads.
&gt; I figured the time taken to allocate those large chunks of memory should add up. I know allocation gets a bad rap, but allocation itself is cheap. In place transposition messes with your memory, so you're replacing a cheap cost with an expensive one. &gt; Fortunately I can close the gap on square matrices by implementing a simple ptr::swap algorithm You'll want to at least use [loop tiling](https://en.wikipedia.org/wiki/Loop_tiling). See [this Stack Overflow answer for an efficient example](http://stackoverflow.com/a/16743203/1763356).
[Image](http://imgs.xkcd.com/comics/unicode.png) [Mobile](https://m.xkcd.com/1726/) **Title:** Unicode **Title-text:** I'm excited about the proposal to add a "brontosaurus" emoji codepoint because it has the potential to bring together a half\-dozen different groups of pedantic people into a single glorious internet argument\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/1726#Explanation) **Stats:** This comic has been referenced 10 times, representing 0.0081% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d72clei)
Concerning ownership the important one is impl&lt;'a, T, U&gt; AsRef&lt;U&gt; for &amp;'a T where T: AsRef&lt;U&gt; + ?Sized, U: ?Sized So if `Foo` implements `AsRef&lt;Path&gt;`, `&amp;Foo` does as well.
It only owns a value if you let it. If you provide a reference, you get this: impl&lt;'a, T, U&gt; AsRef&lt;U&gt; for &amp;'a T where T: AsRef&lt;U&gt; + ?Sized, U: ?Sized Which means nothing is owned.
[removed]
[removed]
Dunno about division, but to do two's complement multiplication, the only extra step you need to do is to sign extend both operands (and sign extension on the hardware is just wire splitting, i.e. you can do it for free).
&gt; (like write everything in GLSL, translate to SPIR-V, then translate the SPIR-V to whichever shader representation the API you are using accepts Hm, please, don’t do that. If you still want acceptable live code editing, you don’t want to do that ;).
Everything the library does is safe until you transmute the executable data into something that you can call. And that part does require an unsafe block.
http://stackoverflow.com/questions/4353780/why-floating-point-value-such-as-3-14-are-considered-as-double-by-default-in-msv
Interesting post! Out of curiosity, is it possible to implement transposition as a flag that describes orientation, and then select appropriate algorithms accordingly? I can imagine that most (all?) algorithms can be formulated in such a way that they operate on the (implicit) transpose of the matrix. Of course, this would need two implementations of each algorithm, but perhaps this too could be abstracted. Just thinking out loud here :) P.S: I've been following the development of rulinalg with some interest, and I just wanted to say that I am particularly impressed by how you handle community contributions. Always respectful, showing appreciation, while simultaneously giving fair criticism and ideas for improvement. Makes me want to contribute myself, except I can't spare the time these days! Keep it up :)
What are the applications?
Mainly writing JIT compilers.
It can be made fast (enough for you and me to not be able to notice the difference). Most of the time would be spent where the bulk of the optimization passes are run (so in the driver if you are outputting GLSL/HLSL).
The plan is to use them through the interface that the trait specifies. I'd actually like callers to know nothing about the concrete type
Thanks for the additional comments, /u/steveklabnik1/ and /u/burdkadurka/ . As I've spent more time reading about and playing with `match`, I've come to realize there are important benefits to the binding that goes on as part of the pattern matching process. It's basically necessary, I believe, for guards to work -- at least with the current syntax. That said, I think it is confusing for some people to the `y` in the examples above work the way it does. In the book, all the examples up until that point in time have used patterns that have a clear type, and the type does not always match the type of the variable specified adjacent to the `match` keyword. Think, for example, of the matching on enums, where each option has a specific type, and in most cases the type does not match the type of the variable adjacent to the `match` keyword (`c` in the examples above). Again, I understand now the reason for why things with `match` are the way they are. I'm just trying to share how I see things as someone new to the language and trying to understand its logic. P.S. - Has there ever been any thought to allowing patterns something like: y: f64 =&gt; { /* Do something only if there is a 64-bit float. */ } 
Slices in Rust understand their own length, so you don't need to pass the length back separately like in C. (This is why array accesses are safe in general; they'll panic if they're out of bounds.) You can also use a static array if you want, to avoid allocating, since its reference will coerce into a slice. Here's an example ([playground link](https://is.gd/ThfRwA)): fn write_three_numbers(buf: &amp;mut [u16]) -&gt; &amp;mut [u16] { for i in 0..3 { buf[i] = 42; } &amp;mut buf[0..3] } fn main() { let mut buf = [0; 24]; let output = write_three_numbers(&amp;mut buf); // The buffer is bigger than it needs to be, // but the slice we get back is of length 3. println!("{:?}", output); } 
I agree with you, though not as strongly: I do feel that using other examples is better, but sometimes, it's fine.
Thanks!
&gt; Right? So, I don't think so, but I am not 100% sure. My line of reasoning goes like this: 1. Clang is a C compiler. 2. The LLVM people say that getting it to do constant time operations is a no-go, just write asm. 3. Clang uses LLVM. 4. Clang is a C/C++ compiler. 5. This means that C features, aren't enough to guarantee constant-time operations. I could be wrong. Regardless, given 2, that's what Rust needs to do. &gt; Does anything like volatile exist in Rust? https://doc.rust-lang.org/stable/std/ptr/fn.read_volatile.html https://doc.rust-lang.org/stable/std/ptr/fn.write_volatile.html
That's hella cool; jumps were a huge problem for us in class
This is not just great, it's exemplary. :-D
I don't think you'd even need two implementations of algorithms. If you just changed the index operation to operate one way or the other, everything else just works on top of that transparently. In other words, if you can make "a[i][j]" access the i'th row and j'th column normally, and the j'th row and i'th column if the matrix had been transposed, then you are all set. The one downside to this approach is that if you optimize an algorithm to specifically access memory in a certain pattern (and not the matrix), this index transposition will change that memory access pattern and ruin your optimization.
There are plenty of indications that this is the wrong subreddit: check before you post. You want /r/playrust 
You can use an idea similar to flags - BLAS/LAPACK use the matrix strides to encode the orientation of the data. This means that if you supply some access functions (like indexing, row selection etc.) which use these strides to determine the orientation you can write all of your algorithms using these functions. However as /u/serpent describes this would break much of the optimization in rulinalg right now. I assume row-major in lots of places and use this for vectorization and cache utilization. Because of this I think we would need two implementations of many algorithms - but perhaps there is a smart way around this still. Also, thanks for your kind words!
Try out [AnyMap](https://github.com/chris-morgan/anymap) or [TypeMap](https://github.com/reem/rust-typemap) crates: they seem to fill the same role as your `Holder`.
Thanks so much, that's exactly what I wanted.
Agree. Using relatable names help a lot.
&gt; Don't worry about those warnings for now; we'll clear them up in a future section. They're just warnings, we've built things successfully! Is this really the attitude towards warnings we want to spread? ;)
Thank you for telling me! That wasn't clear to me, and now I understand the difference between `String` and`&amp;str` a lot better. I added a footnote about it in the blog post.
There's a reason why they're warnings instead of errors. I, at least, like to test my stuff *before* I clean up all the warnings. If you weren't supposed to do that, unused symbols would be hard errors (like they are in a certain language who's name starts with G, ends in o, and is two letters long). 
&gt; Out of curiosity, is it possible to implement transposition as a flag that describes orientation BLAS, and therefore its wrappers like Python's numpy, have an idea of "strides" which specify how far forward in your array you have to go to move to the next column, or the next row. It's then up to the programmer to consider the cache-coherency of this: using row-major exclusively or column-major exclusively is fine, but occasionally they can get a bit mixed after performing certain operations, at which point things get ugly.
I don't think that switching from a binary project to a library project in the middle of the chapter is a good idea. Since the libraries and modules subject are introduced at the same time, the reader might mix them up, or think modules are for libraries only.
Thanks for explaining. I should have been a bit more specific in my original post. I actually meant that you would need two _optimized_ algorithms. As /u/serpent pointed out, you can easily get by with a single algorithm if you don't care much about memory access order. What I was actually wondering, was if it's possible to come up with an abstraction that covers both optimized cases. After all, the memory access patterns in both cases are very similar, in the sense that they are loosely speaking "inverted" with respect to each other, so I was thinking that it is not inconceivable that it is perhaps possible to cover both with a single implementation (probably using generic programming or similar techniques to avoid runtime overhead).
Unsafe rust is a superset of safe Rust. Unsafe doesn't "turn of the borrow checker" or anything, it only allows you to do more things. One of the things it allows you to do is use raw pointers, `*const T` and `*mut T`, which aren't borrow checked. So, you have to be careful when you're using those to maintain the invariants that safe code expects. You can accidentally cause data races if you do the wrong thing.
And I still cannot find a convincing reason to want scoped threads. What's the point of doing work on a separate thread if you just block the current thread waiting? 
If you used block processing for each algorithm then you can swap the strides and the algorithm shouldn't get any slower. This depends on every algorithm being reasonable for block processing.
It seems like you're asking about how to handle string manipulation without having access to indexing operators. My high level advice is this: * Do you want to correctly handle any UTF8 string? If so, you'll be using iterators, not indexing. * Do you only want to handle ASCII? If so, convert to a `Vec&lt;u8&gt;` and you can write algorithms based on indexing to your hearts desire. I recommend choosing the former. Iterators are important to using Rust effectively. &gt; "thisword" -&gt; "thiswo" or "thisword" -&gt; "thisworddddd" So you want to convert the string to a particular length, truncating it or repeating the last character. What do you want to do if the string is length 0? I'll assume you want to repeat the null character to that length in that case. Here's the code, where `string` is the input string, and `n` is how long you want it to be when you're done. let last_char = string.chars().next_back().unwrap_or('\0'); return string.chars().chain(iter::repeat(last_char)).take(n).collect::&lt;String&gt;() &gt; And I also want to work with sub strings: It's hard to tell if you want to substring by position or semantically (e.g. after the whitespace). If its the latter, the [unicode-segmentation crate](https://crates.io/crates/unicode-segmentation) has some helpful iterators. If its the former, strings support both indexing by ranges and concatenation, but iterators are probably going to be more efficient. EDIT: Seeing your other replies, I see that you don't really want to deal with string data at all. Use `BitVec` or `Vec&lt;u8&gt;`.
Unsafe code encompasses FFI, so the compiler disclaims any responsibility for data safety if you mess up in unsafe code. Luckily, you usually don't need a lot of that, and what you actually need is easier to audit than a whole-application-written-in-horribly-unsafe-language.
I find this example is relatively convincing: implementing in-place, parallel, quicksort conveniently in all safe code https://github.com/reem/rust-scoped-pool/blob/master/examples/quicksort.rs The overall use even if you don't have more than one child thread is you can do things in that child thread and the main thread at the same time, then join the child thread later. This can enable patterns that are impossible all on one thread.
I think this is a big improvement! I have 2 open issues: 1) I am always wondering how to do proper imports. For example when I have lib.rs foo/mod.rs foo/bar.rs moo.rs And now inside `moo.rs` I want to use `Bar` from `bar.rs`. I think all of the following work: use ::foo::bar::Bar; use super::foo::bar::Bar; use foo::bar::Bar; And I always wondered what is best practice? My little c++ heart wants to go with the first one, because that is cleanest. 2) how about external crates? Most common practice seems to be to lump them all up in `lib.rs` or `main.rs` and then in `bar/foo.rs` just `use clap;` (or `use ::clap?`) but of course you could also do the external import directly in `bar/foo.rs`. What are best practices and why?
&gt; And I always wondered what is best practice? `use ::` is always redundant, since `use` always starts from the crate root. That'd make it identical to the last one, which is the standard way to do it. &gt; What are best practices and why? For me personally, I always put `extern crate` in the crate root, that way, `use clap` always Just Works.
A couple hundred clock cycles isn't particularly slow; it's the cost of a few sequential divisions. That is of course extremely important if you're boxing integers, but when you're dealing with 100kB matrices that cost is lost in the rounding error. 
*ONE JOB STEVE... ONE!*
I couldn't agree more with this, When it comes to the more advanced areas of Rust I'm still a beginner and trying to understand code that uses only names like foo, bar and baz makes the code incomprehensible to me. I really wish blog writers especially would stop doing it. Edit: I actually went and read the Traits chapter and it really does just adds to the complexity of it. I really don't know why it's so pervasive in the Rust community either, maybe it's just me or maybe it's just I read more Rust posts but in other languages there doesn't seem to be anywhere near the number of Foo, Bar or Baz's. 
It's not really true. `str` isn't a type, `&amp;str` is, and it's a reference to a string slice - think `*char`, but with length information on top. `String` is a heap allocated string - think `std::string`. `CStr` and `CString` are only ever used for C FFI, and you can actually convert them to `&amp;str` (assuming they're valid UTF-8, as native Rust strings are guaranteed to be UTF-8) . The relationship between `CStr` and `CString` is the same as `&amp;str` and `String`. `OsStr` and `OsString` are also mostly used for FFI, and are explicitly designed to be [freely convertible from Rust strings](https://doc.rust-lang.org/std/ffi/struct.OsString.html). Edit: for other things: &gt; borrowck rejects safe code. Not a bug - the idea is not to allow all safe code - it's to disallow all potentially unsafe code. Also, anything that doesn't solve the halting problem can potentially reject "safe" code. &gt; "Nonlexical borrows" will still reject safe code. After all, an "unsafe" block is not saying the contents of the block will invoke undefined behavior; it means the contents of the unsafe block have been erroneously rejected by the compiler. Same. Also, "erroneously rejected by the compiler" is an overstatement in many cases, especially when dealing with FFI and other invariants that may be outside the compiler's knowledge area entirely. &gt; borrowck is still there even when you're inside an unsafe {} block or function; to actually do magic, you need to call verbose names like sliceable.unsafe_get(index). By the time you're done writing it, it's the textual equivalent of a smoke alarm going off because the toast is burnt. That's the only way I know to make safe operations consistent between safe and unsafe blocks. The same code having different semantics inside an `unsafe {}` block would have been a different nightmare. &gt; Nobody knows what the rules of unsafe are. The rules are being formalized, but yes, that's an issue. &gt; This means the contents of an unsafe block are annoying to read, are by definition too complicated to obviously be right, are vaguely specified, and have to exist to write low-level code. Good luck, and screw you if you get it wrong, like C programmers have been for the last twenty years! Blah blah halting problem blah blah. &gt; Rust has exceptions. It calls them panics, and rarely uses them, but it has them, and your unsafe code needs to be transactional with respect to memory safety. So far, they've had about as much success consistently following this as C++ developers have had (exempting the C++ developers who ban exceptions, such as Google). Panics are not exceptions, and are not designed to be used like exceptions. Edit2: yeah, I'm not reading that any more. That page is ... let's just say "not entirely constructive". 
I actually _just_ wrote something for the new book about this: --------------------------------------- Strings are a common place for new Rustaceans to get stuck. This is due to a combination of three things: Rust's propensity for making sure to expose possible errors, that strings are a more complicated data structure than many programmers give them credit for, and UTF-8. These things combine in a way that can seem difficult when you're used to other languages. Before we can dig into those things, we need to talk about what exactly we even mean by the word 'string'. Rust-the-language has only one string type: &amp;str. We talked about these string slices in chapter four: they're a reference to some UTF-8 encoded string data stored somewhere else. String literals, for example, are stored in the binary output of your program, and are therefore string slices. Rust's standard library also provides a type called String. This is a growable, mutable, UTF-8 encoded string type. When Rustaceans talk about 'strings' in Rust, they usually mean "String and &amp;str." This chapter is largely about String, and these two types are used heavily in Rust's standard library. This is what Rustaceans mean when they say "Rust strings are UTF-8," since both String and string slices are UTF-8 encoded. Rust's standard library also includes a number of other string types, such as OsString, OsStr, CString, and CStr. Library crates may provide even more options for string string data. As you can see from the *String/*Str naming, they often provide an owned and borrowed variant, just like String/&amp;str. These string types may store different encodings, be represented in memory in a different way, or all kinds of other things. We won't be talking about them in this chapter, see their API documentation for more about how to use them, and when each is appropriate. Many options! As I said, strings are surprisingly complex. Most of the time, we'll be using String as an owned string type, though. So let's talk about how to use it. ------------------------------------- &gt; By the way, what about other Rust entries? https://www.reddit.com/r/rust/comments/4it4zt/rustaceans_will_you_please_contribute_to_the/?st=ishz0mgs&amp;sh=7bd6b946 and more specifically, https://www.reddit.com/r/rust/comments/4it4zt/rustaceans_will_you_please_contribute_to_the/d31nwdq?st=ishz53vo&amp;sh=2726cac9 (There's a grain of truth to some of this, but.... yeah. Also, just not a fan of the whole idea of this project.)
Honestly I just shoved OS interoperability under "FFI" as well. Not quite the same, but maybe close enough? Anyway, the main point is that you'll rarely run into that kind of stuff in your everyday Rust program.
If you want to stick to vanilla (std only) and not use any external crates (and honestly you should check out the try_opt). Also replace the `?` with `try!` macros where needed. fn get_album_art&lt;P: AsRef&lt;Path&gt;&gt;(&amp;self, real_path: P) -&gt; Result&lt;PathBuf, Err&gt; { let mut real_dir = if (real_path.as_ref().is_file()) {Some(real_path.as_ref())} // Notice no ; inside else {real_path.as_ref().parent()}; // only here toe end the // We make the Option into a Result to return what failed // String could be another error type. let files = real_dir.ok_or("Not a directory or file in a directory")?.read_dir()? let album_art = files.find(|file| { //Here we make the Result into an Option and chain it (*file).ok() // Makes it an Option for chaining .and_then(|file| file.filename().to_str()) .map_or(false, |filename| self.album_art_pattern.is_match(filename)) }); // Notice the two ?? checking the two sources of errors return album_art.ok_or("No album art file found")??.path() }
One thing the other comments haven't adressed yet: The next-to-last point: &gt; Every executable, by default, contains a copy of jemalloc, making "hello world" several megabytes in size. definitely isn't true (anymore?). Hello world is 646.2kb with the current stable compiler, as a release build. Goes down to 368.0kb if you strip debug symbols. There's probably a lot of room for improvement, but 650kb != several megabytes.
I want to see better support in Cargo for working offline. Right now, attempting a clean build will fail as it can't connect to the registry, even if all the dependencies are in the local cache. It kinda leaves you up a creek without a paddle if you wanna work somewhere without Internet and forgot to bootstrap your project before you departed civilization (or forgot to add a dependency when you bootstrapped).
jemalloc is still included, but the binary output is a bit leaner.
There are 6 distinct types that represent strings, yes. However, semantically, there are really only three - the other three are just fat pointers to those - and your code shouldn't be dealing with anything other than `str`/`String` except when crossing FFI/OS boundaries. 
This is tricky due to compiler flags; we'd have to hash them all or something, then check against the hash every build. I wonder how much it would actually save.
I don't want to maintain a local mirror to the registry, I just want to be able to work with crates that have already been downloaded to `.cargo`, that's what I mean by local cache. If you've run `cargo update &amp;&amp; cargo build` for the current project at least once online, you can work offline just fine. Adding dependencies to new or existing projects, even if the dependencies are already cached and the version numbers match, doesn't work offline.
Yesss so much this. As someone that lives in Rural USA (north country NY), the best I can get is either end-of-line, rotting-copper DSL or bandwidth-capped Satellite. Since moving here, I've really come to appreciate that 95% of the internet assume everyone is on a blazing fast internet. 
In general, I find that the steep learning curve comes from the compiler enforcing far more rules than other languages, especially C, and incredibly more so than Python. Its not that its a harder language (okay, its probably harder to get started than Python), but instead of other languages where the process goes: * Learn the basics of the language (syntax, environment issues like packages and compilers) * Learn about, and how to avoid, the important mistakes that are common to the language (Python: code runs, but doesn't work as planned due to wrong variable contents, unexpected behavior behind the scenes; C: double memory allocations/frees, bad usage of pointers) * Learn the idiomatic, pattern driven development natural to the language ("pythonic" code, many different "correct" patterns in C). Instead in Rust, the first two steps are combined. This is a good thing! The compiler doesn't know if you are new to the language, or just making a little mistake as an experienced person, so it enforces as many rules as possible. Now, instead of having something that *looks* like it works (but breaks later), it won't even let you run the code until at least the basics are correct. This is definitely an oversimplification, but I came from a C and Python background, and I am really enjoying Rust (definitely not an expert yet either!). It never hurts to learn a new language, though for me I find it doesn't really "stick" until I have some project to use it for. Once I get going, then you really start learning the lessons! But, don't feel bad if you read a bit, get confused, and walk away for a while. I did the same thing with Rust 2-3 times before I really started making progress. I can recommend [this podcast](http://www.newrustacean.com/), he goes over a lot of topics (both specific to rust, and describing what the broader concepts are about). You will find there is no one "right" language, just different tools for different jobs. Hope that helps!
OP wants to return an `Option` instead of a `Result` though, so none of that `?` mumbo-jumbo! :p
That's true, but I bet the vast majority of projects don't muck with the `[profile]` settings, so the flags will be mostly the same.
which means that &gt; You are basically saying now that most people don't have to deal with all 6 too often? is wrong: *nobody* should deal with more that two at a time: `&amp;str` or `String` almost everywhere, and one of them together with one of the other four in FFI code.
This is really stupid. My very first Rust code was to read a file. I gave up printing the path on error. I looked up the API docs and could not understand why the path must be consumed in a file opening method. This should really be fixed.
Just realised I never really answered your question :-/ The main difficulty Rust poses is that you have to explicitly say whether a piece of code owns a piece of data, or has borrowed it, or owns a view on a piece of data owned elsewhere, or has borrowed a view. The String situation in Rust is a perfect example: * String owns its characters. * &amp;String is a borrowed reference to a String that owns its characters * str is a view on some or all characters owned by some String (e.g. a substring) * &amp;str is a borrowed reference to a view, owned elsewhere, of some or all characters, owned by some String. The underlying character array is only freed from memory when the `String` falls out of scope: the other three don't affect it - though the `str` view has two variables indicating the start and stop of the of the substring they're viewing, which is why it needs to be owned. Lists work similarly incidently: `Vec` is the owner and `[]` syntax is used for the view. In Rust views are called "slices". Things then get even more complicated when you're passing closures (inline functions with references to outside variables) to other functions. Coming from Python or C#, where you never have to manually keep track of who was the owner of a piece of data, this strikes most developers as alien and confusing. The payoff is automatic memory management at almost no runtime cost, and data-race detection in multithreaded code. The syntax can also get a bit ugly compared to Python, with a lot more braces and colons and indents, but you get used to that. Finally the fact that it's POP rather than OOP is a bit confusing to those developers that have gotten habituated to the OOP mindset: however as a newish developer you may be more flexible. Like I mentioned in my monster comment above, you'll learn a lot of useful new concepts when learning Rust, so I'd say it's worth the effort.
To add to this. Suppose you only have callbacks of one type. Then you can avoid having to type erase the callbacks with a Trait, which avoids one dynamic dispatch per call back. So what you could do is write your own event loop that supports only callbacks with one type. Such an even loop would be very unusable, even for a very specific application. But if you gotta do it you can go and do it. Tokio allows callbacks of different concrete types which are put behind a trait. It is thus, not zero cost, since you could for a specific application _probably_ hand-roll down something better if you now all the callback types before hand (and e.g. put them behind an enum). Still, _the cost is minimal_, and it makes the event loop much easier to use, which makes it a better _default_ event loop. The other point where there is overhead is in the single allocation of a future chain. One could probably implement an arena that has the memory for a batch of futures preallocated and can reuse it, such that creating a new chain of futures requires zero calls to the system allocator. But this agains comes with a trade-off of usability vs performance.
The main issue that I see is that it requires a smart server, and potentially a decent amount of computing power to compute and compress the binary diffs. It would also require rustup to keep the original binary packages around somewhere. You'd at least need a caching layer to avoid recomputing the diffs of common upgrade paths. Finally, you need some way to determine what to use as your "base" image. For normal upgrades of a "tracking" toolchain, it's easy - just use the previous version as the base. However, you might be just installing a specific version, and rustup would have to figure out which other image is likely to be most similar. Luckily it looks like the HTTP delta protocol does allow the server to "override" the choice, and provide a full snapshot in the case that the diff would be larger.
You might be interested in how Haskell Cabal solved this problem. The `cabal new-build` command hashes all the info that affects the compilation. See http://blog.ezyang.com/2016/05/announcing-cabal-new-build-nix-style-local-builds/ for details.
Like how apm (for GitHub's Atom editor) used to download a 48MB linking library every time you installed or updated a package.
I wonder how well it'd work to replace cargo with Nix and a bunch of toml-parsing shell scripts...
For what it's worth, there's [lots of chatter](https://internals.rust-lang.org/t/the-operator-and-a-carrier-trait/3481/9) about `Option` compatibility for `?` ;) spooky stuff right
Honestly, it's a bit over the top. Strings are more complicated than most programmers care to think about, and Rust goes out of its way to help you handle them correctly, securely and, despite the number of types, surprisingly pleasantly. Once you understand the difference between a str and a String, the rest of types make sense given their context and clear explanation.
First you need to define memory safety.
I'm glad you enjoyed it! I had a lot of fun writing it
This is really cool work!
If we do follow this plan, we can do it opportunistically: Just give deltas for *a few* popular pairs of the old and new versions. For the Rust toolchain this would mean that only deltas from 1.N.0 to 1.N.M and 1.(N+1).0 are available. (And if you want to pursue this idea further... why not calculating a *series* of deltas required via a standard shortest-path algorithm.)
Anyone interested in this should also subscribe to the [Rust issue #21724](https://github.com/rust-lang/rust/issues/21724). :)
I started a discussion along these lines a while ago. https://internals.rust-lang.org/t/is-a-shared-build-cache-a-good-fit-for-cargo/3420 Locally it'd be nice and maybe make `cargo install` a bit nicer, but I'm still interested in it as part of a build cache for an arbitrary code execution platform. That said, my hack to seed the target dir of new projects with precompiled deps has been a sufficient until there is a real uptick in rust usage on the platform, so I haven't followed-up with any real work toward a solution.
Bashing on Windows is unfair here. The fact that Unix allows arbitrary byte sequences is also problematic :x
&gt;(assuming you use @safe with D, and don't use unsafe with Rust)? So what you're saying is D has opt in safety and Rust has opt out safety? Sounds like Rust is safer by default :P
Yes. $ cat e.d import std.stdio; @safe int f() { int* x = new int(6); delete x; // !! return *x; // !!!! } void main() { writeln(f()); } $ gdc e.d $ ./a.out Segmentation fault --- edit: $ gdc --version gdc (Debian 6.1.1-11) 6.1.1 20160802 Copyright (C) 2016 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. I don't use D so I'll be the first to agree that I'm probably Doing It Wrong, but unless I'm very much mistaken this is the sort of thing that `@safe` should ban. Right..?
Having traveled the world, much of humanity has pretty bad internet that is also expensive. If we want the highest number of people using Rust, we need to make it resilient, low latency and inexpensive.
[A number of things](https://dlang.org/spec/function.html#safe-functions), but apparently not ban explicit `delete`, for some reason.
Rust uses a definition that's pretty common, which is "absence of data races." Data races are defined here: http://blog.regehr.org/archives/490
[Fix lifetime rules for if statements](https://github.com/rust-lang/rust/pull/36029) Woo! Less borrowck punching! 
As I understand it, we have to move all of borrowck to MIR before we can implement non-lexical lifetimes. MIR is in, but borrowck is still done pre-MIR.
Is it really worth it to support binary diffs for crates? What is the size of the largest crate in `crates.io` anyway? The `rustup` use-case makes sense as long as it's not default. Binary diffs will actually slow things down for most users as a lot of processing time will be wasted creating a package from the diff. ----- &gt; I’m porting BSDiff to Rust Interesting. May I ask: * Why BSDiff? * Is it possible for a re-implementation to be less memory hungry than the original? * How much memory is needed to create a diff between two Rust releases? * Did you consider VCDIFF (RFC 3284)? I think it was 2-3 years ago when I tested all open-source binary diff tools available. My use-case was remuxing multimedia files to a modern container format while retaining the original in a binary diff. Needless to say, BSDiff was completely unusable due to its memory hogginess. xdelta3 (a VCDIFF implementation) was the best in my tests for both software packages and my multimedia use-cases: * It's not a memory hog. But you can use more memory for more efficiency. * Great two-level compression performance (`-9 -S lzma`). * It supports piping for both input and output. This was important for me as one of my use-cases required concatenating multiple input files in sequence.
Chiming in with a little research that I just did. After downloading the latest stable and the latest beta, I ran them through `gunzip` to get the raw tarballs. I then ran BSDiff comparing the two tarballs. It took roughly 15 minutes to generate the patch file. File | Size ---|--- rust-1.11-*.tar.gz | 115MB rust-beta-*.tar.gz | 120MB rust-1.11-*.tar | 337MB rust-beta-*.tar | 348MB patch-file | 98MB patch-file.gz (-9) | 98MB patch-file.xz (-9) | 98MB Essentially, the patch file appears incompressible. The savings aren't as exciting as I would like them to be... especially when we look at just compressing the beta tarball with `xz` (not even `xz -9`!) File | Size ---|--- rust-beta-*.tar.xz | 93MB I think we should absolutely support better compressed `rustup update`, but I'm not sure BSDiff is the answer. ---- **EDIT:** could someone explain why there are two different yet identically named `.so` libs in the rust-beta distro? They seem to be a waste of space, duplication, yet they have different hashsums. MD5sum | Path ---|--- 85cb34feabd2123c1cb8561972e7b9db | ./rust-std-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/librustc_llvm-cb30d86e.so b198070d4c5ffc3a1131883ebd9daf16 | ./rustc/lib/librustc_llvm-cb30d86e.so Each of them are 44MB. This just feels wrong.
I wonder if there are better ways to improve the quality of life for developers on low quality internet connections, since I imagine most (compressed) crates are tiny anyway, so tiny that I would imagine the latency of a connection is comparable to/higher than the time it takes to actually download a typical crate, and so nailing down the bugs where cargo hits the network unnecessarily would improve the experience more (as well as improving the experience for people who just need to temporarily work offline). Of course, I'm just hypothesising here, it seems the first thing to do would be some manual measurements. &gt; Perhaps a courgette-like scheme that will detect + recalculate the hashes could work to reduce the diffs? The hashes were just one example, and attacking them seems... like a lot of work for possibly little gain. It's definitely something that should be measured before diving in to an implementation (e.g. diffing stripped binaries).
I mean, because memory safety violations mean "everything is fucked" there's probably some circular reacharound that makes mem safety == data race prevention, but generally memory safety is defined in terms of staying in bounds of objects and avoiding reading uninitialized memory. Data races can lead to mem safety violations (they're UB). Edit: more broadly, memory safety and type safety are generally a shorthand for "the program has any meaningfully defined semantics", because any violations of these is Undefined Behaviour, which means anything and everything can start doing nonsensical things. Generally I don't think you need a particularly rigorous defn of memory safety, since you can pick some handful of standard issues (use after free, index out of bounds, access uninit memory, interpret memory as the wrong type), and anything that can lead to one can lead to the others with enough elbow grease; it's all the same stuff. Anything you left out of your list can subsequently be mapped onto something else in your list with minimal effort.
Is it really worse than having to have an encoding to understand weird alphabets or forcing all of your strings to be twice as big as they need to be in the most common case? UTF8 is fantastic. Yes, it's a pain to implement, but fortunately 99% of the pain is in well tested libraries. Microsoft made the wrong choice here; it took Plan-9 to get it right.
Well rustup.sh works as well and I can specify which component I want to exclude... In my case I exclude the doc because I am building a lightweight docker image... I don't see the benefit of rustup.rs so far... 
That sounds pretty much like how google blaze (and clones like pants and buck and bazel) work. They hash everything that can affect a binary, so you don't have to redo work that was already done. If you have a big shared cache (say, company-wide), it saves an enormous amount of work, because your packaging and CI and developers all share the cache. I'd even argue it's "table stakes" for any serious new large-scale build system.
^(I'm just an observer not a programmer, apologies if I say something embarassingly silly.) Isn't D opt-in for the safety? That being the case, practically speaking, I'd expect D programs to be generally less safe (even if `@safe` were fully safe) for the simple fact that developers are less likely to opt in when it's not the default.
That's fair.
But C has over 12 string types: *char char[] **char *char[] ***char[] ****char *****char ****char[] ******char *****char[] *******char[] string etc...
Rust does have all of those string types (well, it's `&amp;str` not `str`, but whatever). However, it does have the backwards-compatibility excuse, so that part of the statement is false; `CStr` and `CString` are for, you guessed it, compatibility with C. And `OsStr` and `OsString` are for compatibility with operating system APIs (these are different, since Unix APIs use 8-bit null terminated strings or `CStr` with no well defined encoding, while Windows APIs use 16-bit null terminated strings which are nominally UTF-16 but could contain invalid surrogate pairs so can't be depended upon being represented in any valid Unicode encoding). `OsString`/`OsStr` allow you to use the same type cross platform. So, why all of these pairs of types, like `&amp;str` vs. `String`, `CStr` vs `CString`, and `OsStr` vs `OsString`? There are two reasons for these distinctions. One is an "owned" vs. "borrowed" distinction, which is one of the fundamental strengths of Rust. In a language like C or C++, this distinction exists, but isn't made clear through the type system. If a function takes a `char *` (or any other type of pointer), there is no indication in the type signature whether the consumer of that `char *` has ownership of the underlying memory, and thus is responsible for de-allocating it when it is done, or whether it's a borrowed value, which the caller will de-allocate. You have to document this in the API, and if you're unclear or someone doesn't read the documentation or someone just makes a mistake, you've leaked memory, or caused a double free, or the like. There's no way for the compiler to know or check that you did the right thing. In C++, there is the option of passing in an reference rather than an pointer, and this acts somewhat like a borrow in Rust, but the safety checks are not nearly so comprehensive, it is still possible to get all kinds of undefined behavior if you use them wrong. So, in Rust, the `&amp;str`, `CStr`, `OsStr` are all of the "borrowed" variants, while `String`, `CString`, `OsString` are the owned variants. But why not just write `String` and `&amp;String`, like most other borrowed values? For values that are contiguous sequences of memory, like vectors or strings, it can be useful to be able to construct a substring by just referring to the start point and length of the substring. Then, rather than having to copy out sections of a string when you parse it, you can just hold on to references to the original string, with a start point and length. But what if you accidentally hold onto such a reference for too long, after the original string has been destroyed? Well, that's where Rust's borrow checker comes into play; it actually guarantees, statically, that you can't continue to hold a reference to something after it's possible that it's been deallocated. So, while an `&amp;` reference to a type simply borrows the value (or an `&amp;mut` reference borrows it mutably and uniquely), the specialized types `&amp;str`, and `OsStr` allow you to borrow just a substring, making zero-copy parsing much easier to manage and completely checked by the compiler for safety. Pretty much any language which has C and cross-platform compatibility is going to need to have specialized types for C compatibility and platform compatibility; it can be possible to avoid some amount of divergence by just using C strings or just picking one platform's string representation, but you then have the same issues on other platforms, and C strings are quite limited in the ability to do any kind of zero-copy parsing. In some languages, the compatibility types may not be in the standard library or may be hidden away somehow, but that doesn't really make them any more fundamental on Rust. In native Rust, you use `String` and `&amp;str`, and only deal with the others on the boundaries. Now, the two "native" string types may be surprising for those coming from GC'd languages; in GC'd languages, there is no distinction between ownership and borrowing, as all ownership is shared. Every pointer is considered to be an "owner", and the GC determines when there are no more owners and something can be deallocated. And that is great, for writing code without having to think about this distinction too much; it really simplifies programming, and there's a reason that GC'd languages have become so popular over the past 20 years. But GC, for all it's benefits, has its costs too, and in some domains those costs are just too high or too unpredictable. So, when not using GC, you do need to make this owned vs. borrowed distinction. In languages like C, there's no support at all, you just have to rely on documentation and everyone always getting it right to avoid bugs. In C++, you have a little more support, but still have plenty of rope to shoot yourself in the foot. In Rust, the distinction between the two types is made clear, and the types have built-in support for easy zero-copy compiler checked extraction of substrings with no memory safety issues. This makes two fundamental string types, along with two more pairs of compatibility string types, considerably more palatable. Is it all roses? No. It is more complex than most GC'd languages, when you're dealing with all native code (no FFI), in which you can just have one string type to worry about. Having to think about whether you want owned or borrowed strings is an additional layer of complexity when designing and writing your code. But luckily, in Rust, the compiler checks everything for you, so if you get it wrong, it will just give you an error and you can easily iterate. In unsafe languages like C or C++, you have to deal with the same issues, but with no compiler support, so if you get it wrong, you get subtle memory corruption errors that are hard to track down. Also, the comment is wrong about: &gt; There is no platform-agnostic way to convert between OsStr and OsString and any of the other four types of string There [are ways to convert from `OsStr`/`OsString` to `String`/`&amp;str`](https://doc.rust-lang.org/std/ffi/struct.OsString.html#deref-methods). One caveat is that `String`/`&amp;str` are guaranteed to be valid Unicode (encoded in utf8), so the conversion has to produce valid Unicode. You can either do a conversion that could fail, `to_str`, and it will succeed and produce an `&amp;str` if the input contained valid Unicode, or fail and produce nothing. Or, you can do a lossy conversion, which will give you the full contents of the original string if it is entirely Unicode, or a string with any bad characters replaced by U+FFFD, the Unicode replacement character. If you need to be able to deal with OS strings that may not contain valid Unicode in a non-lossy way, you will just have to keep them as `OsString`/`OsStr`, since that's the whole point of these types, to be able to represent what the OS considers to be strings in a non-lossy way. These issues are issues any time you have to deal with different string types of potentially different encodings. (...to be continued...)
(...continued...) &gt; By the way, what about other Rust entries? Like the entry you originally cited, the other entries are a mix of truths, half-truths, and falsehoods. Many of the "half-truths" are things which are a matter of perspective. &gt; borrowck rejects safe code &gt; "Nonlexical borrows" will still reject safe code. After all, an "unsafe" block is not saying the contents of the block will invoke undefined behavior; it means the contents of the unsafe block have been erroneously rejected by the compiler. True. Not a terribly interesting observation; working in a GC'd language rejects certain safe things you could do with direct manipulation of pointers. The whole point of safe languages is to allow you to write the vast majority of what you want to write in an entirely safe subset, and only drop down into unsafe for things that are actually safe but you can't prove so in the safe subset. &gt; borrowck is still there even when you're inside an `unsafe {}` block or function; to actually do magic, you need to call verbose names like `sliceable.unsafe_get(index)`. By the time you're done writing it, it's the textual equivalent of a smoke alarm going off because the toast is burnt. Yep. `unsafe` doesn't turn off the normal rules of the language; all it does is allow you to call more things, and use some operators, that you couldn't normally call in the safe dialect. This helps you not get surprised by an `unsafe` block accidentally changing the semantics of some safe code you had in between two unsafe lines. There is some debate about whether to provide better facilities for writing unsafe code that is a little more terse and potentially more readable. There are arguments to be made that even inside of `unsafe`, Rust doesn't provide quite enough to make that unsafe code easy and idiomatic. On the other hand, unsafe code is almost never easy and idiomatic. This is a point where there is some debate, and it's possible that there are designs that could make writing unsafe code easier. &gt; Nobody knows what the rules of unsafe are. This is a problem. It's a problem that's being worked on, but it does kind of suck that it's not solved yet. However, this makes it sound a little worse than it actually is. It is is possible to write unsafe code that everyone would agree on the semantics of. However, there is other code that there will be some disagreement on, so the Rust community does need to eventually agree on a memory model. I'll note that neither C nor C++ had a formal memory model until C11 and C++11. They respectively managed to be used for 39 and 28 years (precise ages depend on when exactly you consider them to have come into existence) without a formal memory model. &gt; This means the contents of an unsafe block are annoying to read, are by definition too complicated to obviously be right, are vaguely specified, and have to exist to write low-level code. Good luck, and screw you if you get it wrong, like C programmers have been for the last twenty years! So, the big distinction between unsafe blocks and C is that in C, everything is unsafe! And even in `unsafe` blocks, you get much better type checking, bounds checking by default unless you explicitly opt out of it, etc. So while in C, you have to audit entire codebases of tens of thousands of lines of unsafe code, in Rust, the unsafe code is far more limited, and for many applications, you can write none of it yourself, relying only on the standard library or trusted, high-quality third party libraries. &gt; Rust has exceptions. It calls them panics, and rarely uses them, but it has them, and your unsafe code needs to be transactional with respect to memory safety. So far, they've had about as much success consistently following this as C++ developers have had (exempting the C++ developers who ban exceptions, such as Google). Yep, Rust has exceptions in the form of panics. There are two things that make the situation better than for C++; one is that they are not used for signalling "expected" errors, like they are in some C++ code (by "expected" error, I mean things like failures to open files due to permissions issues). Panics in Rust are reserved for things that are truly programmer errors, and the default behavior of killing your thread in a way that's cumbersome to catch enforces that. The other is the limitation on the amount of safe code. Again, not that many people have to write unsafe code. You need to do it for some of your fundamental data structures, and for FFI, and a few other cases, but overall the surface is limited. I think it's a pretty big exaggeration to say that Rust developers have had as much success consistently following this as C++ developers, since in Rust, the vast majority of code is written without `unsafe` and thus without having to worry about this. (...further continued...)
If you are building a Docker image you probably should start from the [tarballs](https://static.rust-lang.org/dist/index.html) instead. (Just don't forget to squash the history, or put everything to one command.)
It's probably a great idea if you only use it in top-level functions. But people are gonna try and stick it in closures and confuse the craps outta me.
Oh, interesting. Yeah, that's a solution.
Really lovely, enjoyable post. Great to hear your perspective. And I largely agree with the areas for improvement you call out. BTW: we're working on a [project roadmap process](https://github.com/rust-lang/rfcs/pull/1728) and will hopefully be talking about the next year's goals in the near future. I'd love your feedback on the process, and your participation in the goals discussion when we start it (it'll be on the internals forum). Keep on Rusting!
Rust does have out in unsafety. It's not possible to have a language that doesn't have unsafety at the very bottom. (Though sometimes the unsafety is below the level of the language itself, like how all unsafety in Java is hidden in the JVM.) Theoretically, assuming @safe offers checking of an equal power to Rust's (judging by this thread it doesn't, but supposing it does) you could write a bunch of safe primitives and use those as wrappers around unsafe features in all your code. This would basically allow you to achieve equivalence with Rust, because that's exactly what Rust does.¹ 1: There are some more assumptions to state, but that's the gist of it.
Rust sure does have a lot of implementation bugs... https://github.com/rust-lang/rust/labels/I-unsound
I tried that, it didn't. Got the same error not accusing the new variable instead :c
I'll be sure to drop by and leave a comment. Thanks for taking the time to read it!
D is not safe even with `@safe`, but I think they want to fix that. There was a DIP(rfc) a while ago that would introduce something like Rust's lifetime system.
I've been quietly working on a (librarised!) port of zsync since @llogiq filed that bug. It carved off a third of the download size between a coupe of nightlies last week. I'll post some more accurate stats once it's going nicely. Won't be too hard to integrate into rustup.rs.
Great project, using it at work:)
Great writeup. Thanks for mentioning syncookied!
I thought it was a really neat project and it was cool to see it got better performance than the kernel. I love comp sec and seeing a tool for it built in Rust was too cool not to mention.
The issue is that, especially with binary diffs, it will often be the same parts of the binary that change with each update (eg. the parts that contains timestamps, addresses, etc.), so applying a series of updates ends up being very wasteful.
&gt; I really do not want to split the repository, imag is modular and the libraries might be used outside of the imag codebase itself, but the main goal was to use them internally, not to provide them for others. Isn't this a bit of a contradiction? If they are usable/useful outside of `imag` they've to be separate crates, otherwise how should they be used? If they're not usable/useful outside of `imag`, why are they in separate crates? You might take some inspiration of [git](http://fabiensanglard.net/git_code_review/), how they structure their subcommands. `git` has support for internal and external commands. External commands are just binaries - like in your case - which are just called with their arguments. Internal commands are functions that get registered during startup. They get their arguments in the same way like external commands, so you can easily exchange them, because their call interface is the same. 
Just one year of ~~love~~ Rust Is better than a lifetime ~~alone~~ of C++?
Sounds good! Any idea if you can exclude components with rustup.rs?
The problem is that it's quickly out of date and highly opinionated. Before the rust sections updated about two months ago it was entirely just incorrect information. It takes benign information and presents it in a wholly negative way with not attempt to explain the decision that led to that information. It's unconstructive because you can't really trust anything on it. It's goal is to be flame material, not educational.
&gt; Isn't this a bit of a contradiction? If they are usable/useful outside of imag they've to be separate crates, otherwise how should they be used? If they're not usable/useful outside of imag, why are they in separate crates? Yes, it makes sense as I want to split out the library crates in `1.0.0` - but we are really not there yet. We will not be there for ... I don't know. A Year? Maybe even more... two or three years (a lot of stuff has to be written)... depending on how fast the community around the project grows. I still want to publish a crate now to claim the name "imag" on crates.io - I really hope nobody claims it :-) --- &gt; You might take some inspiration of git, how they structure their subcommands. I do this, I have a `imag` binary (in `./bin`) which searches the `$PATH` for `imag-*` binaries and calls them. We are also in the transition to do this [with `clap`](https://github.com/matthiasbeyer/imag/pull/635) right now. We had a bash script before and rewrote it in Rust because ... Rust! And now we try to implement it with `clap` as clap has some nice properties. 
This is my take too. Java is memory safe, Python is memory safe, the toy language I made in Haskell for a university assignment is memory safe. Memory safety, by itself, is boring. Memory safety without the overhead of automatic garbage collection (either using ref-counting or a tracing GC) is interesting. 
[gluon](https://crates.io/search?q=gluon) is like this. All crates (except the language-server) is developed in the same [repository](https://github.com/Marwes/gluon) but when publishing they are each a separate crate on crates.io which is required at the moment as just path dependencies are not allowed when uploading. Apart from a bit of extra work when publishing this is ideal (at least in gluon's case) since it makes it possible to just use some* of the libraries but thanks to the [main crate](https://crates.io/crates/gluon) most users don't have to think about which parts are needed and can just pull that in. \* In theory. In practice that can be tricky as the vm crate is currently required for macros. The language-server crate would otherwise only need a dependency on `gluon_check` and `gluon_parser` (and `gluon_base`, which all of the crates depend on).
nit: Arguably, all unsafety isn't hidden in the JVM due to JNI. In fact all FFI support (in Python, Haskell etc) implies unsafety.
Due to the way the ecosystem works I feel that would be picked up very quickly, and the fix is also easy. The Rust community is awesome like that :)
&gt; OMG, it's like Best Korea in here! Seriously though, thanks for the downvotes. It tells me something about the cultishness of the community if discussing language design flaws is taboo. Uh, I'm quite sure that if you post a similar link in any programming subreddit you're very likely to be met with similar downvotes. The reason is that most subreddits are not interested in flame-based discussion, but something constructive. YLS *has nothing constructive*. All of the points are stated matter-of-fact, when they're often opinion, and the author provides no background. For example, there are obviously quite a few good reasons to have those string types, and it's not really difficult at all to determine which to use (hint: it's String/ &amp;str 99% of the time, and *many* languages have slice-syntax) but those reasons aren't brought up - it's simply "a bad thing" that these types exist, with no reason provided. The entry is actually incorrect (shocker!) - Rust doesn't have that many string types, the std lib does. You can say a lot of things are "bad" if you provide no context. But it makes discussion difficult and tedious, having to explain the obvious only because some webpage serves to confuse people. In theory a webpage that is critical of languages is not a bad thing, the problem is that the format is very flawed. Rust's was only updated a few months ago, before that it was so out of date that the entries were essentially nonsensical. So, yeah, of *course* you're getting downvoted edit: &gt; t highlights (potential) flaws in various languages to help people decide if they are suitable for their projects and whether to invest time learning them. Also, please do not ever use that cite for such a decision - so much of the information is, at best, a misrepresentation (this goes for all of the content across all languages), and often flat out incorrect.
The idea behind libfringe is to provide only context-switches (and not some higher-level abstraction, like scheduling, where the design is much more variable) and to provide them in a safe and self-contained way (which means that the switches must happen with a FIFO discipline, and not just between arbitrary contexts). For an example of a simple (bare-metal) scheduler see [scheduler.rs](https://github.com/m-labs/artiq/blob/49ba8aec18fecb917ae048b162dbed5aaa6712f9/artiq/runtime.rs/src/scheduler.rs).
Nice work!
It's amazing that you are using this in a embedded controller for quantum experiments.
17 is a lot? Several of those also only apply to `unsafe` code or unstable features.
`field` is a reference to a character (`&amp;char`) but you need a plain character so you can use `*field` to dereference the variable. By the way, you can write the complete function as `table.iter().all(|&amp;c| c != ' ')`
also, at the end, you can just write the returned value as return empty_fields == 9; or empty_fields == 9 (notice the missing semicolon)
since when does C have a `string` type?
Unrelated to the actual Strings and YSL there is something frustrating about talking about Rust in a public forum. It happens here on this sub, but also other places on the internet like H. You will see people be very hash about other languages, some more then others (mostly languages not liked but current PL community). Not in the name calling sense, but there will be a lot of strong criticism dished out about how old the other language is, how backwards it is, how unsafe it is, how the poster would never run into this problem if he used Rust. It gets pretty incessant. On the other hand the "community" has a very swift reaction to any criticism of Rust's short comings. It's very quickly dismissed as trolling, downvote. Talk about how slow rustc is.... trolling, talk about how lexical borrow hasn't be solve for 4 years... trolling. Talk about how/why choice green threads is bad (remember?)... trolling. It feels like the community has build this shared "enemy" of bad languages, languages that need to be fought and replaced. It's really not a uncommon strategy, the shared enemy that unites everybody. It start feeling pretty cargo cult when you combine that with "protectiveness of Rust" Just a guy trying to write a library in Rust. Who experienced the above. Rust can be pretty frustrating. Learning it, warts, unclear documentation.
&gt; A String is an owned string. It is also just a pointer to a character and a length Actually, a String is a pointer, a length and a capacity. It's [backed by a `Vec`](https://doc.rust-lang.org/src/collections/up/src/libcollections/string.rs.html#261-263), which is itself [comprised](https://doc.rust-lang.org/src/collections/up/src/libcollections/vec.rs.html#272-275) of a length and a [`RawVec`](https://doc.rust-lang.org/alloc/raw_vec/struct.RawVec.html), [which is itself a pointer and a length](https://doc.rust-lang.org/src/alloc/up/src/liballoc/raw_vec.rs.html#48-51).
&gt; because REASONS If a trailing comma wasn't required, there would be no way to distinguish between a 1-tuple and an expression that was parenthesized for clarity or precedence.
&gt; &gt; Isn't this a bit of a contradiction? If they are usable/useful outside of imag they've to be separate crates, otherwise how should they be used? If they're not usable/useful outside of imag, why are they in separate crates? I think the OP wants them to be in separate crates eventually, but stay in the same repository.
GC must run on other thread. Maybe it could be implemented as a signal handler (SIGALARM) but that might be even more problematic.
It is, in both cases. Python has the GIL (global interpreter lock) which prevents undefined behavior due to concurrency (except possibly through C FFI) and Java has a concurrency model that avoids undefined behavior by not allowing "interesting" mutations (that affect memory safety) to be larger than word-sized and making all word-sized mutations avoid UB (all such mutations use the equivalent of LLVM's Unordered atomic, which prevents only some optimizations).
That's simply not true. In Erlang, by design, GC does not run in a separate thread. I believe Nim operates similarly.
(Yes, this is what I meant)
Yeah, but how'd you do that? Just create a new crate with cargo and the same name and publish it as 0.0.0?
Nah, it's useful. Concatening tuples is nothing to be afraid of, this would also work in a statically typed language.
I think you can publish an empty git repo as 0.0.0?
This is the subreddit for the rust programming language. You are probably looking for /r/playrust.
Interesting. I looked at how in-place transposition is done in Eigen. For non-square matrices I think they do an out-of-place transpose, but for square matrices they do a specialisation and swap triangular views. See lines 245-300 in https://eigen.tuxfamily.org/dox/Transpose_8h_source.html
So you can mix them if you wish and use @safe if you do, unless you know what you're doing?
That doesn't really answer the question, though...
You are creating an iterator with `field.iter_mut()` which returns a `&amp;mut char`, which can't be compared. You should instead take it as a reference with `for field in table`, which will return an `&amp;char`. You can them compare it by comparing the value in that reference with `if *field != ' ' { }'. And you can also write it as pub fn full_table(table: [char; 9]) -&gt; bool { table.iter().filter(|&amp;x| x != ' ').count() == 9 } Or pub fn full_table(table: [char; 9]) -&gt; bool { table.iter().all(|&amp;x| x != ' ') }
I'm seriously mindblown. I was just sitting and thinking about making such as library, then I check out /r/rust, and what's the top post? Exactly such a library. Great work, though. I thought the ecosystem was really in the need of such a thing.
Even better, you could use an enum instead of `char` for the cells.
How do you exclude the docs with rustup.sh? The ability to not install docs is a desired feature but the metadata format used by both rustup.sh and rustup.rs doesn't support it. So it actually sounds like a bug to me that rustup.sh has a way to not install docs....
I've been able to find some scattered projects: [rust-ios-android](https://github.com/kennytm/rust-ios-android), [android-rs-glue](https://github.com/tomaka/android-rs-glue), [rust-jni-sys](https://github.com/sfackler/rust-jni-sys), [cocoa-rs](https://github.com/servo/cocoa-rs) And some blogs that mention it: [Compiling Rust to Android](https://ghotiphud.github.io/rust/android/cross-compiling/2016/01/06/compiling-rust-to-android.html), [Taking Rust everywhere with rustup](https://blog.rust-lang.org/2016/05/13/rustup.html), [Building an iOS App in Rust](https://www.bignerdranch.com/blog/building-an-ios-app-in-rust-part-1/) Not sure if those are quite what you're looking for.
I'm Rust beginner (currently learning ownership) and I'm writing and refactoring code of this game as I learn new things. Thanks for this, /u/CrystalGamma already suggested enum so I'll keep it in my mind.
I haven't experienced this; most people seem to be very straightforward about Rust's shortcomings. And most will tell you that Rust will never "replace" C or C++. I wonder why our perspectives are different. Specifically, as for your examples, everyone knows `rustc` isn't super fast. It's been a major focus of development work. Same with nonlexical lifetimes. And Rust hasn't had green threads in 18 months. The latest iteration of the RFC process is specifically framed around "what problems does Rust have?" If you ever see people being harsh towards other languages here or in any of the official project spaces, report them. Rule 4 is taken seriously by the mods.
2nd section, bullet point #3, is that a "certainly"? &gt; the code I’ve certaibly gotten better
 struct Bar; trait FooBar&lt;T&gt; {} trait Foo where Bar: FooBar&lt;Self::Type&gt; { type Type; } fn foo&lt;F: Foo&gt;(_: F) {} Error: **the trait bound `Bar: FooBar&lt;&lt;F as Foo&gt;::Type&gt;` is not satisfied** Fix: fn foo&lt;F: Foo&gt;(foo: F) where Bar: FooBar&lt;F::Type&gt; { } I would like to limit the associated type `Type` for `Foo` to only types where `Bar: FooBar&lt;Foo::Type&gt;` is satisfied without having to specify that in every generic function.. How can I achieve this? Or is there no other way? 
I guess I don't see it as parody. Maybe I'm misunderstanding - but so is the creator of this topic, who bases language decisions on it.
&gt;It will make you bitter Take it from you, right?
haha pwnd
This looks like one of the more complex GUI programs I've seen written in Rust. How was using Rust for GUI?
The perfectly represents the project as _this is the name of the project_.
This feels like an instance of [#20671](https://github.com/rust-lang/rust/issues/20671): "where clauses are only elaborated for supertraits, and not other things" I am not aware of a workaround.
I think we will use this for kernel threads in the [Redox kernel rewrite](https://github.com/redox-os/kernel). Good work!
Python is another counter example. It is garbage collected and runs on one thread (because of the global interpreter lock).
Technically, I missed the deadline for Fedora 25 Alpha, but it will be there for Beta. And I also added the new packages for Fedora 24 updates, but of course the featured Changes only talk about what's new at the start of each release.
This is not correct. In most implementations, even in Java (with some collectors) the garbage collector is single-threaded, and it is run upon an allocation failure. Actually, even in a multi-threaded scenario these allocation failures are the main reason for collections, this can be observed e.g. in JVM GC logs quite easily.
&gt; independant independent :)
What would be the best way to convert the following unsafe code to safe code: struct Foo{num: i32} struct Bar{foo: *const Foo} fn main() { let mut foo = Foo{num: 1}; let bar = Bar{foo: &amp;foo}; foo.num = 0; unsafe {println!("{}", (*bar.foo).num)}; foo.num = 1; }
Yes, I wrote it. :) These are just the sources of UB that are intrinsic to the language (and not just some library's API).
Hoping to see a windows implementation using fibers soon! 
The borrow checker can and should be improved, but rejecting safe code is unavoidable (see: halting problem). It'll always be a tradeoff you make, but it's not a negative - the benefits outweigh the costs for a lot of programs. Different semantics in safe/unsafe won't be a nightmare on the implementation side - honestly, I have zero idea how something like that could be implemented. It'll be a nightmare for the person reading the code, as you have to always keep track of which semantics you're currently in, and switching between the two mindsets won't be easy, especially when you need to do it often. Panics act like exceptions, but they're not intended to be used like exceptions. Panics are closer to assertions than to runtime exceptions - you use the `Result` type to handle errors that you know can happen, and `panic!` to indicate a situation that shouldn't happen. 
3ns doesn't seem believable, that smells like something isn't being measured. I understand the kernel isn't being used so the overhead should be much less than a typical context switch, but surely dumping all registers to memory and saving a copy of the stack involves more than ~9 add instructions?
I haven't looked into this yet, but I know the messaging app [Wire](https://wire.com/) is using their version of the Signal/Axolotl Encryption algorithm written in Rust. I could be wrong but I believe it's used in both their iOS and Android app. I haven't seen any tutorials, but you could probably grab the code off [github](https://github.com/wireapp) and see what they did.
You could pester /u/retep998 :)
Thank you! Could you provide any insight into the performance of `Cell`? I couldn't gain any from the linked documentation.. Edit: The [Rust book provides a bit more information](https://doc.rust-lang.org/book/choosing-your-guarantees.html#cellt) and it looks like it's only for `Copy` data :( `RefCell` isn't restricted to `Copy` types, but a runtime cost comes with it.
Right. `Cell` is zero-cost, but `Copy` only. `RefCell` has a runtime cost but works for non-`Copy` types. Standard references are zero-cost and work for non-`Copy` types, but you have to abide by the borrowing rules and can't mutate with an outstanding borrow. I'm pretty sure anything else will require unsafe code because aliased mutability is explicitly against the design goals of safe Rust.
Won't marking every register as clobbered just force LLVM to implement the copy?
The code that performs the context switch is inlined into the point of switch (i.e. `yielder.suspend` or `generator.resume`), and if the code you wrote didn't happen to use, say, r8 at the point of the switch, r8 won't be spilled (even if it's callee-save).
I hope we can continue to encourage an open, honest discourse that is humble and willing to respect the successes of other languages and communities. We've had a very good run of that, but it's always hard to keep ourselves in line as our community continues to grow at a steady rate. Unfortunately some folks are still in the honeymoon phase and tend to get a little... excited.
Saving all the registers would take about 17ns, which early `libgreen`-derived versions did. As whitequark explains, we save only live registers, and the benchmark is hence somewhat contrived because it only has a few live registers and only perfectly predictable branches.
:( I guess it only has over 11 then.
A GC, in itself, does not require multi-threading. It is however my understanding that multi-threaded GCs have a higher throughput and, by delegating work, allow for a lower latency in user threads. This seems to be corroborated by the GCs offered by the JVM, for example. Given that the lack of a high-throughput GC is a significant sore point of D, I jumped to the conclusion that in order to offer a state-of-the-art GC they would need a multi-threaded one (with a background thread doing most of the work). It may have been a premature conclusion, since while a multi-threaded GC is probably necessary anyway for multi-threaded applications, an alternative runtime could be swapped in for mono-threaded programs which does not have this dependency, I suppose.
I had the same question... Quite interested in the answer! Also (nitpicking...), GPL does not mean the redistributor needs to send _you_ his/hers modifications. It requires the redistributor to make accessible to _the user_ of the binary his/hers modifications to the binary. It's all about the end-user's freedom, not necessarily yours...
Just for interactive mode mode I think
It works for me, both in Linux and OSX. Not sure if the wget and/or bash versions matter here, I had wget 1.15/1.17 and bash 4.3/3.2 respectively for Linux/OSX.
Ah, OK; I had it bound to 80, since I'd thought that would be the main one to listen to for standard web browser use. Didn't realise that that port required root privileges to listen to...
I'm interested in resources for how to improve compile time. In [Rusoto](https://github.com/rusoto/rusoto) we're translating AWS service definitions from JSON to Rust code. With our 36 implemented services we end up with somewhere north of one million lines of code to compile. On TravisCI it can take 45 minutes for a full translate+compile run. If we can adjust our code generation to emit faster compiling code it'd be a huge boon. We're doing quite naive things now, but I don't know where to start with making faster compiling code.
So, distros will find it much easier to package Rusts after 1.10, due to changes we made in the build system. Now was when it was eligible to fit in, and so some people stepped up and made it happen. It was super close to the deadline, actually, but it managed to work out for 25.
I don't really want to get into an argument about semantics, but I do owe you an explanation. So let's just say that if your comments were about me, I would understand what you wrote as to claim that I could not be a contributing member of the community, that I am a "malcontent", that I intended to cause harm and mislead, and that you would sorely wish to insult me more directly had the situation not forced you to bite your tongue. Now, I'm not that person - to the best of my knowledge I've never touched a word of YourLanguageSucks. But many of the points there could well have come from my mouth (albeit less strongly phrased), and I only count one strict inaccuracy in Rust's section, at least as far as is evident to my eyes. Further, although neither of us think this is a good way to go about it, you yourself admit that "admitting the shortcomings of a language is a very beneficial thing to do" - and that is surely the *intent* of the authors of this page. &gt; From my point of view, linking to YLS should be considered a breach of the CoC since it is not constructive and it is full of half truths and immature language. If the article was posted alone, I would agree. However, asking a question about it, as done here, seems fair game. 
It's probably failing because you're asking rust to start a process and that process doesn't know how to handle the '&amp;&amp; touch' part because that's stuff the shell would be interpreting and handling. Besides, I'd do the 'touch' natively from Rust, or I'd invoke `bash -c "one &amp;&amp; two &amp;&amp; three"` etc.
Oh! Weird! I'll give it a shot when I got home. Sorry I wasn't more helpful.
Thereby challenging the nature of reality and the very existence of free will.
It's generally discouraged to have major updates to stable Fedora, but ultimately it's left to maintainer discretion. I asked for opinions on the devel list, and only one person replied that Rust users will probably prefer having the latest. I agree, so for now I'll try to keep it updated even on stable Fedora.
"Awful text situation" - could you elaborate a bit/point me to a discussion?
It was 1.10 when the build improvements landed, and I've got 1.11 queued up now. I doubt I'll be able to get 1.12 before the beta freeze, but we'll see. In general, I plan to keep pushing updates even after Fedora release, unless that proves painful.
I figured it out. It's too embarrasing to detail. The above code works just fine.
Cool. We're interested in what various distros need, so make sure to keep us abreast of what's up regarding these kinds of things! (And thanks so much for your work on this. I've haven't been paying as much attention to the distro packaging stuff as I'd like, but it's such important work, and I'm really glad people are doing it.)
b) is so, so so important.
Wow cool to see Rust find its way into Artiq. A good friend of mine did his postdoc with Wineland at NIST; sadly Rust was still to unstable to use it in my own PhD :(
The standard string type is a (linked?) list of chars &amp; is so slow that multiple only partially compatible replacements have emerged, each with their own benefits &amp; problems.
| What can I say... I totally understand. I was digging for a crazy long time though, so I just wanted someone to help me with my search to be honest. I don't blame you, and I'll document the parts that I mentioned after I'm done. Ok well, I kind of can't start down this direction until tomorrow during working business hours because I have two jobs, and I have to attend to the other one first. I appreciate the reply - let me look at the links you gave me and see if I can't just work out what to do from there before I start back with a better understanding before I unnecessarily burn your time. In the mean time, I will fork your repositories so that I can add any documentation that might be needed that I feel is missing for an ultimate merge request before I'm done.
Google "imag" and see what you get.
I don't even bother with CStr most of the time. Most API's I use also give a length so it's more like: // inside unsafe block. let b = slice::from_raw_parts(ptr, string_len); let s = try!(str::from_utf8(b).map_err(thingo)); Or more manually: let b = slice::from_bytes(ptr, max_len); let len = slice.iter().position(|&amp;b| b == 0).map(|n| n-1).unwrap_or(b.len()) let s = try!(str::from_utf8(&amp;b[..len]).map_err(thingo)); CStr as far as I can tell doesn't have a limit on how long the string can be. So you'd have to check that the buffer is null terminated beforehand. (Which in my case doesn't seem to be guaranteed, or it's at least not documented).
Neither is any compiler for any other language.
I think you'd probably have to ask the author of the Quora answer why they are writing that (granted the "default" implementation still appears to be slower in Rust, but even that's quite a cherry pick out of the literally infinite benchmarks one could pick, IMO). I am not familiar with the author(s), but the tone in the two posts seems quite different -- are you sure that the Quora author is also the author of that benchmark posted to reddit? I think it's quite a stretch to call a lack of TCO "memory leaks" -- if that's the case then nearly all code in the entire world leaks memory under recursion. But C programmers sometimes seem to have a different definition of "safe" than Rust programmers -- perhaps this is due to a difference in background? The author of the Quora post seems very interested in functional programming where TCO is du jour and expected. Re: differing hash functions, that kind of depends on what you're benchmarking, no? If the challenge is to insert tons of stuff in a hash table as quickly as possible, then differing functions are fair game, right? If the challenge is to insert tons of stuff in a hash table *while using a particular algorithm*, then they aren't. It seems to me that specification is the mother of all disagreements over benchmarks.
Now someone needs to write a Cargo-&gt;COPR or Cargo-&gt;OBS script, and we're settled :D
Well...currently, this software DOES crash pretty badly :P Although, as I plan to separate it into a rock-solid server component and a client, Rust's error-handling philosophy should prove to be very helpful.
Congratulations! Another milestone reached. :)
Not actually all that bad - the GTK+ bindings for Rust are very nicely done, and largely translate very well from C (which makes it easy to e.g. use instructions from Google). I had to make copious use of `Rc&lt;RefCell&lt;Thing&gt;&gt;` - it's also necessary to be a bit careful that you don't borrow something twice when e.g. destroying UI objects that have signal handlers. My only large niggle is that I was unable to implement my own GTK+ things (like my own TreeModel), but this wasn't that much of a roadblock.
I would change the Bracket type to encode the dual nature of braces. So something like this: enum BracketDirection { Open, Close } enum BracketType { Round, Curly, Square } struct Bracket { type: BracketType, direction: BracketDirection } In particular, I believe that an implementation based on this structure would support evolution to a new kind of bracket (angle, &lt; and &gt;?) with fewer code changes.
I still want to improve its speed, but providing an overview for its current speed: In release mode, its about two times slower than libvorbis (brought to rust via the vorbis crate). That's still more than enough to play it in realtime though. In debug mode, its still faster than realtime. Decoding a 44100 Hz stereo file thats 118 seconds long takes 48 secs in debug mode, and 3 seconds in release mode.
&gt; one must ask why the compiler isn't doing this for us... The reason is actually simple: With manual memory management (and full control), Rust can be made _an order of magnitude_ faster than F# and OCAML (~10x, 0.2s vs 2.0s). Without it, it has more or less the same performance (~1.4s with the good hash, worst case 2x slowdown). Have you tried to make the F# and OCAML versions as fast as the optimized Rust version? I doubt it is easily possible to get them even close (say only 4x slower than the Rust version, that is, in the 0.2-0.8s ball park), but if you are able to get that close, from experience with other languages, i am pretty sure that your code would need to actively fight the automatic memory management of the compiler. In Rust, OTOH, garbage collection is optional. You need to explicitly opt-into it via e.g. `Rc&lt;T&gt;`, `Arc&lt;T&gt;`, or soon `Gc&lt;T&gt;`. This makes it more verbose for tasks in which you don't care about memory management, but as a tradeoff it lets you easily build applications when you need full control over memory management. Such a language decision is just an engineering tradeoff, and one or the other make sense depending on which applications you have in mind. Want to write a shell script? Then automatic memory management is probably a very good idea. Want to write an operating system kernel, a game engine, a web browser, the garbage collectors that OCAML and F# use? Then automatic memory management is a very bad idea. While Rust looks higher-level than C and C++, it was designed for targeting the tasks that these languages are good at. Still, some people use Rust to write shell scripts because they like it, in the same way that some people (probably) will use F# to write parts of an operating system kernel, but that doesn't mean that these languages are good at those tasks, since in particular they weren't designed with those tasks in mind. 
A client of mine, yes, but I cannot reveal anything about the app, sorry.
 I'm not using rustup, I'm on nixos unstable. They have updates fast enough for me and I can easily rollback. Just saying...
That's Great! All I wanted to know if its battle tested 🙂
&gt; self.input.chars().collect::&lt;Vec&lt;char&gt;&gt;().into_iter() This makes me twitchy. `self.input.chars()` already does what you want! I can understand not wanting to expose another type's API, but you're doing that anyway, and paying an expensive `collect` to do so. One thing that might be better is wrapping the type in a `BracketIter` struct. But that's a lot of busywork for little gain. Soon you can use `impl Iterator&lt;Item=char&gt;`, though, which is the best of both worlds.
[removed]
Good idea, I added it. Thanks! I preferred avoid talking about very specific cases. The point is to try to make things more clear. :p
Memory safety isn't interesting in itself, but Rust's memory safety facilities automatically guarantee there's no data races in multithreaded applications - which is definitely interesting.
Flashbacks to circa 2006 when Javaland was busy convincing itself FactoryFactories were a good idea.
It's more than that, though. Without some sort of `Cell` type, you end up invoking undefined behavior. If you want to do things like this without `RefCell`, then you need `UnsafeCell`. EDIT: I should elaborate here slightly. Rust gives information to LLVM when you use `&amp;mut` to say "hey this pointer is the only one, please optimize according to that assumption." The core of UnsafeCell is to remove this information to LLVM, instead saying "hey, for this thing, you can't make any assumptions." So that's why you'll get undefined behavior without it.
opus consists of two encoders iirc. Anyhow, I have all my music in opus format. Before that, I was a FLAC lover. I could easily tell the difference between mp3-320 and flac on my HS-7 speakers, but I still have to find something annoying in opus. I'm very impressed by the codec... Files are smaller than mp3, at 96kbit, and I hear no difference with flacs.
Yeah it has one encoder for voice, and one for everything else, and can switch between those two seamlessly, including a hybrid mode: https://people.xiph.org/~xiphmont/demo/opus/demo3.shtml opus is an awesome codec, I really like it.
RefCell, Cell, UnsafeCell all assume multiple mutable references when I'd like to have just one mutable reference (with another immutable reference). Is there a safe way to go about this? I would like to simply read the immutable reference(s), not return or modify it.
&gt; (with the 2 other immutable references). This is the issue. You can't have mutable and immutable references at the same time without `UnsafeCell`. &gt; Is there a safe way to go about this? There is not, because it's by definition not safe.
He baits people into disagreeing with him so he can portray himself as an innocent victim and everyone else as a jerk. (This is usually called "trolling.") The tone of his individual posts actually doesn't seem aggressive; rather, it's the persistence of his overall campaigns. Once he's decided he doesn't like a language, he puts a ton of effort into building a case for why you should be fearful and uncertain of using it for anything at all. Then he posts that stuff in the language's own community, dismisses criticisms of his posts by changing the subject, and when he's satisfied that his opponents have failed to disprove his claims, he comes back a week or a month later with a bolder, scarier version of the same thing. Anything you say in his presence is liable to turn into ammo for an argument with someone else, even if it seems like you're getting along with the guy. People who have been burned this way learn to ignore him as a rule. [Here's a comment thread](https://www.reddit.com/r/programming/comments/8egod/when_haskell_goes_bad_io_and_ffis/c090znu) from *seven years ago*, during the height of his anti-Haskell campaign. (I didn't feel like searching any further back.) He'd already had a reputation as a troll (previously anti-Lisp) for quite some time. I assume he'll spend the rest of his life doing this.
Not as far as I know.
Opus is awesome. It got my whole music collection down from 70 GB to 8.8GB. 70GB included mostly flac and some mp3.
[removed]
I opened an issue to fix this problem [here](https://github.com/rust-lang/rust/issues/36191). Feel free to fix it! :)
But then we get "Abstraction Obsession", which is much worse. There's nothing wrong with using primitives. As an aside, it will probably compile faster as there are less indirections for the compiler to tackle (and also less indirections for a human reading the code).
&gt; Maybe multirust / rustup.rs could be packaged by fedora as well? This is currently kind of an open question; distros range from "it's already there" ([arch](https://aur.archlinux.org/packages/rustup/)) to "never" (debian, I think...)
`tidy` is the last step of `make check`.
Why not an anonymous rsync server? It is able to detect the modified chunks in big binary files, and download only the differences
Thanks for the info!
What do you use OpenAL for, and it isn't that primarily for positional audio?
Wow, everything is much clearer now. Thanks!
Great! You're welcome. I really need to write better docs for the Cells.... I also edited my post above, you may want to make sure to read that too :)
I also thought the addition of the Brackets struct in this code is not adding much in terms of readability. It's worth noting that the author has a different version in the exercism repo. I'm still not sure if this is better than working directly on String but I think it's an improvement over the first version. https://github.com/exercism/xrust/blob/master/exercises/bracket-push/example.rs Excerpt of the code implementing Brackets: impl Brackets { pub fn new(s: String, pairs: Option&lt;Vec&lt;(char, char)&gt;&gt;) -&gt; Self { let p = match pairs { Some(x) =&gt; MatchingBrackets::from(x), None =&gt; MatchingBrackets::from(vec![('[', ']'), ('{', '}'), ('(', ')')]), }; Brackets { raw_brackets: s.chars().filter(|c| p.contains(&amp;c)).collect::&lt;Vec&lt;char&gt;&gt;(), pairs: p, } } pub fn are_balanced(&amp;self) -&gt; bool { let mut unclosed: Vec&lt;char&gt; = Vec::new(); for &amp;bracket in self.raw_brackets.iter() { if let Some(last_unclosed) = unclosed.pop() { unclosed.extend(self.pairs.unmatched(last_unclosed, bracket)); } else { unclosed.push(bracket); } } unclosed.is_empty() } } 
I saw your question on IRC. In my implementation I can see a difference, for large enough matrices (For example a 1000x1000 f32 matrix). Pick a matrix whose total memory size is larger than all your cpu caches. Imagine your matrix is tiled with cache lines, being normally 64 bytes each. Do you see that you can fit 8 f64's into one cache line? And every time you go down a row in your matrix, you always end up in a new cache line. That means to read/write one element in a row requires bringing all of the 64 bytes of that line from memory into the current cache if needed. Visiting all rows of the 0th column will fill the cache with the first line of each row. If there are too many rows, the first ones will be evicted from the cache, and have to be re-read when we restart with the 0th row, 1st column and go down. The principle is simply that memory locations visited over a short time, should be close to each other.
For Arch: it's only in the AUR (Arch User Repository), so it's [not in the official Arch package repositories](https://www.archlinux.org/packages/?sort=&amp;q=rustup&amp;maintainer=&amp;last_update=&amp;flagged=&amp;limit=50).
Ah right, it's been a few years since I ran arch, so I forgot about the split. :) Thanks.
The player example is, as its only an example, kept very simple and churns on the data as fast as it can, regardless of whether playback can catch up or not. So what you probably see is the decoded samples piling up inside the playback queue. Also, if the decoder would be slower than playback, the example code detects this, and doesn't start the playback until everything has been decoded. Here stuff will pile up as well. What you see probably won't turn up in a crate that uses the library in a more proper fashion than the example does.
&gt; b) But most importantly, once packaged, Rust can be used as a stable build dependency, so distros can choose to ship Rust software by default! The maintainers of a venerable piece of systems software are currently trying to rope me into rewriting bits of its surrounding ecosystem in Rust (from ancient C), and I believe this software ships on Fedora by default. I don't want to say more until I've got the details finalized, but this definitely advances the situation. :)
New Fedora releases come approximately every 6 months. But as I said, updates within a release are still possible as long as we keep stability. I think Rust's track record is pretty good there.
I was able to install cargo and rust after changing the repo mirror I was using and cleaning the cache. Thanks )
The first &lt;anything&gt; in an ecosystem have a way of spreading far and wide.
I'm not sure what's wrong with hyperbole, and I'm not sure how you decided they intended to misinform (they clearly [intend to fact check](https://wiki.theory.org/Talk:YourLanguageSucks), even if you think they're bad at it), but it seems to me they did it to make their biases explicit.
&gt; Recently I saw a few question where people said "it worked before upgrading rustc/cargo". Which questions are these? This shouldn't happen. We care a lot about this, and the community survey said that most people have never experienced code breaking, and if it did, it was trivial to fix. (We have made one or two small changes to fix soundness bugs) &gt; doc.rust and rustbyexamples provide snippets that... don't compile Please open bugs for these. This shouldn't happen; docs examples are run as part of the tests.
&gt; Which questions are these? This shouldn't happen. I know, I responded the same to the author. I saw 2 question like that in last 3-4weeks.
Yes, definitely!
/r/playrust
Out of my mind I can point one place where documentation is not specific and forces user to more googling. I promise I'll try to come back to you when I find more! https://doc.rust-lang.org/std/fs/fn.create_dir_all.html#examples My code: fn create_default(path: &amp;str) { try!(fs::create_dir_all(path)); } Failure: src/main.rs:47:5: 47:56 note: in this expansion of try! (defined in &lt;std macros&gt;) &lt;std macros&gt;:5:8: 6:45 help: run `rustc --explain E0308` to see a detailed explanation &lt;std macros&gt;:5:8: 6:45 note: expected type `&amp;str` &lt;std macros&gt;:5:8: 6:45 note: found type `std::result::Result&lt;_, _&gt;` error: aborting due to previous error More gooling leads to this [reddit-thread](https://www.reddit.com/r/rust/comments/4cgean/cant_figure_out_how_to_to_work_with_errors/), which fixes the code to: fn create_default(path: &amp;str) -&gt; Result&lt;&amp;str, io::Error&gt; { try!(fs::create_dir_all(path)); Ok("Dir created") }
So, if you click the little "run" button, you'll see that there's some extra context there. This is an issue that trips some people up. It's a choice: * We can add extra visible context that distracts from the example * We can assume people know this behavior about `try!` * We can show non-idiomatic code. The standard library docs are written for an audience of "I know Rust, how do I do this thing", and so we've chosen #2 here. It does lead to this failure mode sometimes, though. I am very much open to changing this, but I don't know how to fix it. One option is something like the second example here: http://rust-lang.github.io/book/ch06-00-enums.html You get a little arrow button to expand and show the content inline. I'm still not sure people would know it, though. Oh, and also making this error message better would be really great. I'm hoping the new `?` syntax means that we can do so; see https://github.com/rust-lang/rust/issues/35946 &gt; I promise I'll try to come back to you when I find more! Thanks! You can either message me, or just open issues on the repo.
Are there any good Rust tutorials aimed at someone new to programming? I already have experience (mostly Python), but I find those sorts of tutorials easier to learn from.
Telling you to be careful. The simple truth is that when it comes to `unsafe`, try your damnedest to work out a safe solution, because even the slightest mistake can invalidate the rest of the code in your program.
Yeah, did you see the example in the link? What do you think?
As far as writing Android apps in Rust is concerned, please remember that Android apps are *not* designed to be written in a non-JVM language. If you want to write an Android app, write it in Java/Scala/Groovy/Kotlin. The NDK is for porting big things (like a browser engine or a game) and places where performance is the number one priority that doesn't involve constant calls to Android APIs (like a browser engine or a game). For example, with Firefox, Gecko is run using the NDK but the app's UI is written in Java.
Well for one, do you have mingw-w64 installed? You'll need that to cross-compile from Linux to GNU ABI Windows.
Yeah, he should've really introduced some more Traits and some generic code in general. There's also some crates that already solved some of his problems, so he could utilize those.
I suspect this is one of the problems with the nature of code examples; any code example that's short enough to be readily understood is too short to accurately represent the kind of large codebase where primitive obsession is a large enough issue to actually care about.
This is one of the reasons we need an RFC dashboard! That RFC in particular seems to have been implemented in a very much piecemeal fashion, which seems to be a bit against the spirit of the process (though I think it was caught up in the rush to 1.0 when a lot of things got triaged).
Do you have a nix expression you use for a project shell if you want to peg a specific rust/cargo version to a project? I prefer being able to encapsulate on the project level rather than cramming dependancies into my configuration.nix
Since we are airing our grievances, I'm disappointed by how scared the rust community is about releasing 1.0.0 versions for packages. Everyone is using 0.x.y months after the crate is useful, making it so that every time I run `cargo update`, my build is broken for hours as I go through and manually update all my packages to choose the correct versions. Just publish 1.0.0! Breaking changes are Ok, and you need the major version in order to show that you are going to be breaking things!
I would also like to see more crates go to 1.0. I'm not sure how to encourage that, though.
&gt; is there a better way of doing it one year later? two years later? FWIW C++ has this problem as well. A lot of C++03 answer turned useless in C++11, and the same happened again in C++14 and C++17. The same is also true about assembly. Every time a CPU adds a new instruction there is a better way to do something. This happens like every couple of years as well. The only way to solve this problem is to use a language that doesn't evolve over time. Every language that gets better with time has this problem, and since it has no solution, it is not a problem, but a reality that programmers must accept.
Yeah, I was half thinking about that when I wrote it. Having a bracket direction be different from bracket type is an obvious improvement.
The little expand button is nice, but kind of opaque, even after clicking on it—for whatever reason, I expect code-snippets in running prose like that to form a working program when concatenated, so I thought maybe the button was "include all the previous code-blocks on this page"... but none of the later blocks on the page have that button, which was confusing. If mousing over the code-block added a label next to the expand-button, saying something like "show support code?" or "show scaffolding?" that would make things clearer. Even better if, in expanded mode, the support/scaffolding code is still a bit dimmed/darkened/greyed out, to communicate that it's not the important part of the code-block.
BSD flavors in tier 2 and 3 makes me sad as well, and even if rustc supports them, cargo may not, and then after that, rustup might not either. I hope to have freetime at some point so I can try to help make these things smoother. It just strikes me as strange that windows gets tier one support while posix systems don't. This is one thing that makes go so nice, it really works everywhere.
OpenAL is primarily for positional audio, but it also happens to be one of the best-supported cross-platform audio APIs.
Ah, interesting. So what I generally do is, repeat the full example each time, but hide all but the line I'm talking about. They should be working programs, though. (And only ones that have something hidden shows the `&lt;-&gt;`)
This weeks video will be recorded and released today rather than yesterday (AEST) due to a CPU kaboom. THANKS AMD!.
&gt; It just strikes me as strange that windows gets tier one support while posix systems don't. While it's true that most of us come from a POSIX-y place, as a project, we consider Windows incredibly important. It's a massive, important market/constituency for Rust.
It's probably not the same people making mistake over and over. The issue is likely people submitting content from front page and entering name of this sub manually. The game is quite popular (atm 4th most played game on Steam) so no wonder we get some stray content from them. As the volume isn't that high I find it more entertaining than annoying, it's just one submission per couple of days. :)
Yeah, this sounds excellent. (And the -sys thing is on the build scripts page.... gah I need to find time to work on Cargo's docs :( )
&gt;Code that is 6 months old, HAVE TO compile on latest cargo/rustc and on one that will be released in 3 years. I feel like this should not be that large of an issue. The compiler version becomes a dependency just like any other library dependencies for a project. If upgrading the compiler I would expect there to be some maintenance required and if it can't be afforded then postpone the upgrade. Where I work we've done this for ~10 years managing Java/JVM versions. The only case where I see this as a valid argument would be for patch/bugfix releases, such as 1.10 compiling without changes on 1.10.1, etc. Maybe this issue is just more apparent due to how fast rust versions are released and/or how easy it is to upgrade systems? Add a spyware browser toolbar to the rust install so developers take more precaution when upgrading~
I didn't say they were strings, I said they were "string types." Do you think Rust's `&amp;str` is a string? Or `String`? Anyway, `*char` is a pointer to a char, not a string. `char[]` is an array of char, not a string. C has no strings. It only has char arrays that happen to be null terminated. But maybe not? So it really matters what your conception of "string" means. 
Got to admit, seeing 'SQA' made me do a double take. See SQA also stands for the Scottish Qualifications Authority so seeing them doing something with rust would of been very odd.
I'd argue it's a social, not technical issue. If everything you use is pre-1.0, you still feel like it's the wild west, early days, anything can happen. If anything, I'd take this comment to encourage _more_ people to go post-1.0; if it's really the same, then there's no reason to not start off at 1.0.0.
Is there a specific reason why the standard library is important to you, or is it more about having some kind of mature library in general?
It took me a while to notice the `&lt;-&gt;` too. Some options: * Move it to the left side, perhaps as a short '...' button to minimize visual disruption * Use code folding, like what's found in [intellij](http://imgur.com/Ikscn6i.png) or [kate](http://i.imgur.com/DSiZ07x.png).
Rust, like C++, does not have a specified or stable ABI. Not even between two versions of rustc.
So, it's _slightly_ different: there are a few different C++ ABIs, mostly based on each compiler, and my understanding is that they're pretty stable within them. In other words, the ABIs aren't standard, but they're specified.
&gt; We've laid down a good foundation with...incremental compilation (expected to hit alpha this week) This made my day. Thanks everyone who worked on that!
&gt; Hmmm, interesting. On every compile, or the first compile? On every compile that has a changed file. If nothing changes, the next compile is instantaneous. That's why I avoid using dependencies in my project—they make the “edit/compile/view errors” workflow slow. If you'd like, I can open up an issue with more details/benchmarks in the next day or so? &gt; And yeah, incremental compilation will help. 🎉💕
Iiiinteresting. It shouldn't affect external crates, as they're not recompiled. I wonder if it's something about linking. If it is, then incremental compilation won't help :/
Please don't do this. I don't think lack of 1.0 crates in rust is so much of an issue that we need to somehow "encourage" new crate authors to start with 1.0. I agree that the rust ecosystem needs more 1.0 crates, but trying to achieve this in such an artificial manner doesn't inspire confidence in me. The road to 1.0 is a natural one. In fact, I see this as one of the strengths of the rust ecosystem. A 1.0 crate inspires a high amount of confidence. When a crate is 1.0, I *know* that the crate is stable, mature and reliable. Rust 1.0 crates give me much, much more confidence than 1.0 packages from npm or elm. I think this phenomenon of conservatism has arisen naturally in the rust ecosystem, and I wouldn't try to change it too much. I think rust itself is probably partially to blame for this, with all that buzz around "rust 1.0" and all its guarantees. "1.0" has become some sort of *final stamp of approval*. It has transcended it's semver origins. I consider this a good thing. What we really need to do is help move crates to 1.0 by **actually improving them**, instead of bumping crates to 1.0 just to satisfy some misguided craving.
Here's a [gist](https://gist.github.com/davidbarsky/297ff30d7ba1de9be8699cd094340669) that describes the issue. Do you have any suggestions as to where I can go and investigate possible causes? (thanks, by the way.)
I would be happy with any working library with documentation and a few examples. 
Huh. I don't know much about how long linking takes, but I wonder if that's not it. It might be worth a post on the internals forum.
Thanks a bunch; [posted here](https://internals.rust-lang.org/t/slow-linking-with-external-crates-trying-to-investigate-internal-cause/3986).
Is there a way to loop through the fields of a struct instance? if not, is there a better way to do this? Perhaps Hashmaps? Thanks.
[nit] Buildbot is spelled "Buildbot"
Why does the entry API in HashMap takes a value as opposed to a reference? Assuming my keys are strings, the string will have to be copied whether it is occupied or not. I would have preferred a reference to something that can be cloneable.
Right, which is why rustup is still beta. Checking signatures is not straightforward. It is straightforward on _Linux_ (most distros). You need some kind of rust gpg lib. Bindings could be written, but the rustup devs prefer something that's mostly pure rust.
You could also create a site like http://gophervids.appspot.com (code at https://github.com/dgryski/gophervids )
The dependency crates should definitely not be recompiled on every change to the main crate.
Oh, I forgot about allocation failure. Thank you!
I understand your position, but application I'm trying to write is dedicated on Linux/BSD market and I have zero plans to support Windows platform :&lt; 
That's exactly why I'm developing imag in one repository. Before we hit some 1.* version, all the things are in flux and I do not want to do multi-repo breaking changes, that'd be just to complex to handle for such a small team (me) and also draw away possible contributors (imagine I need to tell them that if they file a PR for this crate, they also need to file PRs on three other crates as they break with their change applied... who would contribute to such a project? :-) )
Well, if I understand correctly, GIL makes it "less multithreaded" - it introduces unnecessary locks. That's the cost. In case of Java, that's interesting. Anyway, I believe that Rust makes it more easy to find even higher-level data races (which don't cause memory bugs but logic bugs). And I believe all bugs are important to find before production releases, not just memory bugs.
Not being an expert on the Rust ABI specifically myself, I would say that the reason `rbp` and `rsp` are touched is because its the most basic function prologue (and epilogue with the `pop rbp` at the end) you can have. That is to say... there doesn't seem to be any fancy checks around whether the stack is required at all within the function and instead it just includes a basic epilogue+prologue anyway with the option to increase the stack not needing to do the setup (you just go "Oh we need 32 bytes of stack space? Sweet, output `sub rsp, 0x20` and rely on the prologue being there to store the base pointer). So from that perspective, I can see how this makes code generation easier.. but I agree I would not expect it if it is not actually required by the function. It also frees up `rbp` to be used freely in the function if need be.
This looks pretty reasonable. It's hard to make a generic abstractation for this because it would need some way to be generic over primitive arrays (arrays with length in their type). Currently there is no way to work with `[T; N]` generically. This is not intended to permanent. There are some RFC's around this, eg: https://github.com/ticki/rfcs/blob/pi-types/text/0000-pi-types.md
This is by far the biggest issue for me. Exploring a new API where namespaces and methods are enumerated, with hover docs, just by typing their names into the file you're currently editing is orders of magnitude quicker than having to go back to the docs for every line.
`cargo publish` could suggest 1.0 by evaluating the following criteria: age of the crate, number of previous versions released, number of closed GH issues and PRs, number of downloads, number of other published crates using the crate (bonus points if those other crates are 1.0+), etc. Or some sort of `cargo activity` or `cargo diagnostics` or `cargo pokedex-rating` command that gives you these metrics and suggestions. I'm sure there are more tasty stats and tips that could fit into this functionality.
One confounding factor here is that website is implicitly adding debug information to the compilation, which results in more stack spilling in Rust for some reason. The asm is slightly better on the playpen (or locally) where the `-g` flag isn't passed.
I don't think anything should be removed or excluded from the publication in the first place. But we could have a "badge" or something visible for crates that pass some criteria.
I tried in C (training for my future school) but a have severals problem. I try with SDL, I had a transparent window but when I draw my image, there was a black background. -&gt; http://imgur.com/a/3SbkF But I really want to do the same in Rust (fully Rust if possible)
Maybe show the boilerplate code, but in a color that has lower contrast with the background? (And maybe switch to normal color on tap/hover.)
If you have to use transmute, qualify it: `transmute::&lt;InputType, OutputType&gt;(…)`
Are you able to post some code that we can look at? It's not really clear what you're trying to accomplish. Do you just need a transparent window? Does it have to use Rust? Windows, Linux, or both? 
Well, I'm interested in what assertions would need to be made. I'm not doing any of that with those two function calls (which is obviously why it's unsafe). That's my main concern with unsafe code—covering all the bases. I'm using portaudio. It's all the gist, actually, if you're interested in how it works. I'm fairly satisfied with the rest of it, using bounded-spsc-queue to send messages back and forth.
Buildbot Buildbot I still don't see the difference. :-/
If the C API only allows for one pointer (void*) then I would suggest to require `T: Sized` in your generic wrapper. Everything else can be solved with an additional layer of indirection, e.g. using a `Vec&lt;i32&gt;` instead of `[i32]` or using a `Box&lt;Trait&gt;` instead of `Trait` as your `T`. From what I know, `T: Sized` is enough to make `*mut T` a thin pointer. To be on the safe side it would be nice to have the program not compile at all if the size of `*mut T` is different from `*mut c_void`. I just tested the conversion using `as` but this operator apparently ignores the additional v-table pointer. I would suggest to use `mem::transmute` instead for this reason. It only wants to convert between types of the same size and the compiler will reject the code if sizes differ.
According to /u/SimonSapin [in an old thread](https://www.reddit.com/r/rust/comments/38tv6n/when_is_a_reference_not_a_pointer/cryasew) a bound of `Sized` should be necessary and sufficient: &gt; `&amp;T` is a represented in memory as a single pointer iff `T` implement `Sized` […] otherwise, `T` is a dynamically sized type (DST) `&amp;T` is represented as a "fat pointer" Although for some DST (slices mostly) you should also be able to "unpack" the fat pointer into a sensible C struct you can recover the original DST from.
Why would is_balanced() return BracketBalance instead of a bool? If the name of the method starts with is, I expect a bool so that I can write: if is_balanced(b) { ... } The following seems odd, and doesn't even work without adding a derive to BracketBalance. if is_balanced(b) == Balanced { ... } 
To elaborate, I think one very easy change could be to add additional columns on the crates.io search results page; things like minimum Rust version required, last update, number of dependent crates, and how long the crate has been around. This would make it much easier to pick out interesting crates from the list, rather than forcing the person searching to open every one (they won't).
We support Windows because we have developers that care enough about Windows to put in the time and effort to support it as a tier one platform. BSD stuff is tier two not because we don't want to support it, but rather a lack of developer time and effort to bring it up to tier one spec.
If I'm understanding it correctly... with! { { Some(a) = x; Some(b) = y; Some(c) = z; } { a + b + c } else { 0 } *vs.* if let Some(a) = x { if let Some(b) = y { if let Some(c) = z { a + b + c } else { 0 } } else { 0 } } else { 0 } *vs.* (|| { if let Some(a) = x { if let Some(b) = y { if let Some(c) = z { return a + b + c; } } } 0 })() I'm not sure `match` would be *quite* the same, since you have to construct a tuple *before* starting the match, which can complicate binding by reference to the interior of things. In addition, that forces you to eagerly evaluate the three expressions, which might not be what you want. 
This is exactly what I was looking for, thank you!
That was exactly my thinking. Generic wrapper with other layers on top of it. The ignoring seems dangerous to me, I would definitely use `mem::transmute` if I were to implement it. Thank you for info!
This seems like a thing you still want to assert. Eg. `assert_eq!(slice.len(), 64);` or if you _really_ care about one extra compare use `debug_assert_eq!`. The problem is that you have an invariant that which when it is broken causes memory unsafety, which is a big no-no. If you really don't want to assert the length, consider marking the function that is relying on this invariant `unsafe`. I still recommend to just `assert_eq!` it.
That has the same "issues" as `match` does.
Carl explained the plan when he announced [Tokio](https://medium.com/@carllerche/announcing-tokio-df6bb4ddb34). Aaron Turon and Alex Crichton decided to join forces as part of the [futures](http://aturon.github.io/blog/2016/08/11/futures/) work and the three announced an [expanded effort](https://aturon.github.io/blog/2016/08/26/tokio/) last week. There's supposed to be a blog post with more details sometime soon but I think they're all busy writing code. I don't know if it's mentioned in the articles or on one of the related reddit threads, but I remember the overhead being ~3% in microbenchmarks.
Thanks for those links - but AFAICT none of them mention completely changing MIO's API as part of the work. (I suppose I am in the minority but I have no interest in using futures or Tokio, only the underlying mio library..)
Using example from Book: let mut args = ::std::env::args(); with! { { Some(first) = args.nth(1); Ok(val) = first.parse::&lt;u32&gt;() } { println!("First argument is {}", val); } else { panic!("You need to supply one, numeric argument"); }} It is like filler for lack of `do`-notation or chained `match`es
thanks for mentioning that; the Primary failure for newbies in learning new language happen when they try to leave comfort zone of their previous language and IDE, for such if the IDE dont create projects and files from the Menu, they will need decades to move here, so for speeding up adoption its important to make sure "Absolute Newbies" an use Rust from IDE on their first day.
Only allow versions &gt;= 1 on crates.io. The problem stems from people having a notion of version numbers that doesn't mesh with how semver is supposed to work. In particular people think 1.0 means *everything I want this library to do* and versions after that are for new development. I don't think it's a good idea to encourage putting crates on crates.io before they're actually ready to use. That just leads to new people having a bad experience when they try to build something with libraries they find, and experienced people having to sift through all the little learning projects to find actual usable crates.
I would prefer something like this: let val = ::std::env::args().nth(1) .and_then(|first|first.parse::&lt;u32&gt;().ok()) .expect("You need to supply one, numeric argument"); println!("First argument is {}", val);
It was 0.3%.
&gt; lack of proper IDE with debugger This is huge for me. Sure, we already have some autocomplete in Visual Code, but there is still no decent debugging experience from what I've seen. And no, debugging through command line via GDB does not count as a decent debugging experience since it's 2016 and the language is no longer in pre 1.0 Heck, what about profiling in terms of performance?
&gt; but at the same time the "1.0" brand comes with a lot of useful guarantees This is only true for bigger projects. All other projects normally just keep on evolving until they reach some form of equilibrium (lack of developer time, rough feature completeness, that big refactor that will happen some time (possibly never), ...). Most of those maintainers never get to the point of saying: Since we're not changing anything anymore and there are apparently users using this, let's just rerelease as 1.0.0 and maintain stability. Often this is because their point of equilibrium is "lack of developer time", which implicitly will mean no time to do this version bump. But I think many times it's the consequences of those "1.0" brand "useful guarantees" that actually scares the developer away from bumping the version because they then become maintainers. And a lot of developers don't want to be maintainers. So it all boils down to: Do we want the perceived maturity of 1.0+ crates by defaulting to starting with 1.0.0 or are we fine with most crates probably never reaching 1.0 and only let the 1.0+ crates signify that a project has maintainers committed to the implied useful guarantees.
The Parallel command is typically used for that, and we have a [Rust implementation](https://github.com/mmstick/parallel) now. find -type f -name *.flac | parallel 'ffmpeg -i "{}" -c:a libopus -b:a 128k "{.}.opus" Or parallel 'ffmpeg -i "{}" -c:a libopus -b:a 128k "{.}.opus" ::: $(find -type f -name '*.flac')
That works. I think the ideal situation would be to have a default.nix which does not depend on nixpkgs at all. That way you could have a very stable compiler version and whatnot. If I get something working I will post it here.
Transmuting raw pointer into a reference can be dangerous regarding lifetime. The reference will have an unbounded lifetime (see https://doc.rust-lang.org/nightly/nomicon/unbounded-lifetimes.html). See https://is.gd/CONUSC for an example of the issue.
The first link is the current book. The second link is the re-write I'm working on. &gt; I wasn't sure if the work on the latter was outdated and/or not the correct go-to for Rust information anymore. The existing book is not out of date, it's just not as amazing as it could be. I started working on it months before 1.0, and, well, to quote myself: &gt; there’s a funny thing about writing a book for a language that isn’t done yet: you don’t really know how to write a good book on something that doesn’t exist, because, well, you haven’t played with it yourself. I could make decent guesses, given that Rust was evolving incrementally, but that’s still very different. I think “technical debt” is a great metaphor here: the evolution of the language meant that I had to keep up with the work of many, many people to try and update the documentation. Furthermore, since nobody had tried to learn Rust 1.0 yet, since it didn’t exist, I couldn’t write a book that would do an amazing job of teaching them, since I hadn’t used it either! A lot of people tell me that they really, really love the current book, but I'm always hard on my own work. Now that time has passed, I can do a lot better. The issues with the current book are more about how the info is presented than the info itself. Compare https://doc.rust-lang.org/stable/book/ownership.html to http://rust-lang.github.io/book/ch04-01-ownership.html , for example. The first is short and very theoretical. The second is longer, focuses on a problem that many new Rustaceans have (working with strings), it has diagrams to illustrate its points... it's just better in every single way. That's what the relationship between the two editions is.
`Box&lt;Trait&gt;` still requires a fat pointer. You would need a `Box&lt;Box&lt;Trait&gt;&gt;`. Similarily, you can't make a `Vec&lt;T&gt;` into a `void *` and back, you would need a `Box&lt;Vec&lt;T&gt;&gt;` for that.
I disagree with the "the compiler doesn't need to change" part, as it very much needs to change to facilitate those things. Without that, all IDEs/tools need to reimplement parts of the compilers to make auto completion/jump to definition work. I'm happy to see that we have an RFC to fix that (https://github.com/rust-lang/rfcs/pull/1317).
Can you elaborate on what specifically it is that causes a lack of productivity for you? For example, for me, it's a lack of libraries. But there's all kinds of other things that could be too; when I was starting out, it was the type system.
You're absolutely right. I didn't realize how much strong IDE support mattered until I didn't have it (while writing a bit of Rust). I rely on it a lot to learn the API - _and_ learn how to write idiomatic code. I'll often jump into definition to see how others have written functions, what constructs and techniques they use, etc. And no, emacs w/ racer doesn't come close. I tried it, and...walked away.
The proposal for a new API was discussed [here](https://github.com/carllerche/mio/issues/360).
Yes, that was a concern of mine. But I'm immediately pushing the buffer onto a ring buffer queue, and since it's an array it uses `Copy`. I could probably wrap this whole segment in a scope to ensure that the reference doesn't get accidentally used again.
&gt; I don't believe it has support for futures still. My understanding is that what was `futures-mio` is now `tokio-core`, so yeah, mio shouldn't have futures built-in.
I know. I understand this.
I'd rather like to see a decent RFC for something like this: if let Some(a) = x() &amp;&amp; let Some(b) = y() &amp;&amp; let Some(c) = z() { a + b + c } else { 0 } This is very intuitive to read, however, there are still problems to solve. E.g. what are the semantics of `if let Some(a) = x() || let Some(b) = y() { ... }`? If `||` is invalid, as it doesn't make sense, then perhaps the syntax shouldn't use `&amp;&amp;` as well. And do all pattern matches yield a boolean result or are they still unit-expressions out of `if`s?
I have in the past resorted to doing something like x().and_then(|a|y().and_then(|b|z().map(|c|a + b + c))).unwrap_or(0); But I'll admit that it's hardly ideal.
Is imaginary syntax allowed? A new syntactic sugar like `?` but does not return, and works with `Option`, would be great. // Either evaluates to None or Some(a+b+c) let s = a` + b` + c`; match s { None =&gt; 0, Some(sum) =&gt; sum }
Nice! Thanks. I did update with a link for the list
&gt; That's God damn heroic. Thanks! &gt; do you have the code anywhere for the development of the decoder Some of it is already/still inside the codebase. There are over 20 unit tests which contain test data and the expected outputs for parts of the code. Sometimes they are based on examples from the spec, and sometimes I've extracted them from what the stb_vorbis crate has read using test files. They are a good first way in the initial development. Implementing was probably about half of the time, the other half was spent on finding various bugs in the code. Maybe bugfixing was even more than half. You will have to do the ogg part first, as its the basic groundwork for the rest, but you can use my ogg crate, so you can focus only on the vorbis part if you want. The simple first task is reading the ident header. Then you'll have to implement bitpacking (which is complicated but once you master it you feel better xD) as preparation reading of the setup header. Once you have done that you can do the hufmann decoder layer, and after that you can do the final part: the audio decoding, which consists of 12 steps described by the spec. Then you are done! For header reading testing, I've implemented a simple tool that takes the header, and attempts to read it. I could root out lots of bugs with knowing that the input is valid alone, and finding out why the tool rejected the valid input. Similar things can be done for the audio decoding. About how to use gnuplot: Make your code `println!` your data to a file (one number per line). Then you can start the gnuplot prompt and do `plot '/your/filename/here' using 1;`. Also, you can do `awk 'BEGIN {OFS=" "}{ getline line &lt; "b"; print $0-line; } ' a &gt; c` to get the per-line difference between two files, useful if you want to compare output. It has helped me with a couple of windowing bugs. Generally, gnuplot was only helpful to me in the part where I have implemented the audio decoding (the last step). Happy coding!
Sounds like a rationalization to me. That particular programmer has a bit of an ego :P I don't think he'll ever budge on his opinions.
Switch statements in other languages already generate different code depending on the compiler's whims. Whether it ends up as a jump table, an if-else chain, or an if-else binary search depends on the discriminant values and the compiler's optimizing engine. I think Blow might be under the impression that any high-level construct that doesn't have a simple, direct mapping to assembly instructions is a potential performance hazard. I think the opposite is true, since simpler instructions give the compiler less room to optimize based on the user's intent. Instructions like "for each value of i between 1 and 100, take the value of v[i] and add it to the result" are unnecessarily overconstrained compared to instructions like "sum this array", and provide less opportunity to do optimizations like automatically vectorizing or parallelizing the code.
No true Scotsman would shame others into accepting their opinion by questioning the skill and knowledge of those who disagree. I think people sometimes confuse verbosity and "control". Pattern matching on an enum is just a tag lookup. It generally does the exact some thing that you would have written by hand, except that you won't accidentally read/write the wrong type to a union. Pattern matching on a struct is even easier. I would be very interested in seeing an example where using pattern matching to solve a problem introduced "non-trivial control flow", while *not* using pattern matching resulted in much simpler control flow.
Finally. :) These are good changes and I was looking forward to them. I need to find some time and update mioco's mio dependency.
I was talking about types `T` so that `*mut T` is a thin pointer. You don't need `T = Box&lt;Box&lt;Trait&gt;&gt;` for this. `T = Box&lt;Trait&gt;` suffices. Another way of rephrasing what I tried to say is: `Box&lt;Trait&gt;` is sized.
It was removed in [this commit](https://github.com/emk/heroku-rust-cargo-hello/commit/4eee5781a3e929f849e200b3c87aa428990868ea). This is why you should always use the commit hash (press 'y') when linking to a Github repo!
Ah, great! I was assuming it was outdated (as the tutorial was written a year ago), but wasn't quite sure. Thank you!
Blow's problem is that he is good at what he does, but not for the reasons he thinks he is. He knows his methods and tools so well that he can easily accomplish his goals. He knows his limited stuff perfectly, so he does not consider learning something different even if he could work more effectively in the long term. He would be a beginner if he really tried C++ and he would have to develop a different mindset, which is very hard for an expert. He is so immersed in his little low-level C world that any sane language addition looks suspicious to him. He is "too busy to improve" anyway.
I love the safety aspects of pattern matching, but really dislike that enums take 8 bytes for the tag.
SIMD &amp; Auto-Vectorizing is very important for high-performance numeric computation!
With a modern optimization pipeline, you don't have that level of control over control flow anyway. Optimizers can do things like rewrite recursive functions into loops, perform [loop fission](https://en.wikipedia.org/wiki/Loop_fission), do [loop fusion](https://en.wikipedia.org/wiki/Loop_fusion), perform autovectorization, do data prefetch, recognize loop idioms and completely rewrite them, etc. etc.
The idea that you somehow know better how your code will map to assembly in C over rust is pretty silly. Optimizing compilers are crazy, they will do all sorts of nutty things - and even *more* crazy things in a language like C with UB optimizations.
You can run `cargo rustc -- --error-format=json` instead of `cargo build`.
&gt; but really dislike that enums take 8 bytes for the tag. Usually, at least. In the case of non-null references there is no memory increase. Also you can do pattern matching on things that aren't enum values.
I've actually found out: by using `RUSTFLAGS`
I'm still just happy I knew who we were talking about. Thanks for confirming it.
Nice results with SIMDArray!
The discriminant is rounded up to alignment after sizing it for its range. Some more enum layout trick could help in some cases, but not many. Unions will give you full control if you need that.
`enum Test { A(u8), B(u8)}` is 2 bytes, so it looks like that's correct. It's equivalent to adding an extra `u8` unless you're using absurdly high discriminant values, so it's the same padding issue that `struct Test { a: u64, b: u8}` has.
Yeah, unions with destructuring could be a nice improvement over what I'd do with C. By chance do you know when alignment actually matters on a modern x86-64 cpu? It seems like processors have gotten much better at handling misaligned loads, so I'm wondering whether the 8 byte alignment for enums is overzealous.
Ah, whoops. Somehow I didn't see [this](https://medium.com/@royalstream/rust-development-using-vs-code-on-os-x-debugging-included-bc10c9863777#.vr5i7gq2o) last time I fiddled with rust a while ago.
Cool! Stuff moves fast, so if you haven't used Rust in a while, it's possible that it didn't then, but does now :)
Yeah, I guess confirming is too strong of a word. He was my first thought though, so I felt validated when I saw his name in the comments.
He is certainly using a C++ compiler. And he picked a few C++ features that don't "remove control flow from the programmer". But he does not use C++ in an effective way because he does not see the bigger picture - that a more powerful language allows for abstractions that are both performant at runtime and productive for a programmer.
&gt; Just found out when my project failed to build. You broke that fully knowing what might happen by explicitly changing the version from "0.5" to "0.6", right? Because you would never use wildcard dependencies, right? :)
(even with a wildcard dep, Cargo.lock should have kept it building, you'd need `cargo update` to cause it to fail)
&gt; And no, emacs w/ racer doesn't come close. I tried it, and...walked away. Especially in threads like this, it would be great to elaborate on the _why_, rather than just saying it didn't work. "I walked away" isn't actionable, "It didn't do X and did Y when it wasn't supposed to" is. :)
The compiler's ABI is stable-ish but can change.
Right. sorry, I must have read through your comment to quickly :)
My Rust implementation is only using one core and despite that initial reports show it tying with Clojure for second place after C++ I would love to see someone refine my naive solution.
I don't understand any of what you have written. On the original benchmark, Rust no faster. On the different benchmark, Rust is no faster. &gt; Rust can be made an order of magnitude faster than F# and OCAML (~10x, 0.2s vs 2.0s) Yeah, by changing the benchmark. If you keep the benchmark the same across all languages then Rust is no faster. &gt; Have you tried to make the F# and OCAML versions as fast as the optimized Rust version? Yes. The performance is the same. &gt; I doubt it is easily possible to get them even close Eh? &gt; but if you are able to get that close, from experience with other languages, i am pretty sure that your code would need to actively fight the automatic memory management of the compiler. Pre-sizing the hash sets has nothing to do with automatic memory management. &gt; as a tradeoff it lets you easily build applications when you need full control over memory management. Rust doesn't give you full control over memory management. You cannot walk the stack and identify pointers so you cannot write decent automatic memory management. &gt; Such a language decision is just an engineering tradeoff, and one or the other make sense depending on which applications you have in mind. Want to write a shell script? Then automatic memory management is probably a very good idea. Want to write an operating system kernel, a game engine, a web browser, the garbage collectors that OCAML and F# use? Then automatic memory management is a very bad idea. &gt; While Rust looks higher-level than C and C++, it was designed for targeting the tasks that these languages are good at. Still, some people use Rust to write shell scripts because they like it, in the same way that some people (probably) will use F# to write parts of an operating system kernel, but that doesn't mean that these languages are good at those tasks, since in particular they weren't designed with those tasks in mind. Ok. My point here is that Rust is advertised as being "blazingly fast" but clearly isn't. Futhermore, the original benchmark used purely functional sets in order to reuse results but Rust apparently doesn't provide them because it struggles with memory management. That deficiency is actually more important to me than the not-blazingly-fast performance. 
I have it open in a tab, but I don't think I'll be able to check it out until the weekend.
Awesome, thanks for clarifying! I'll keep my eye on the re-write now that I'm almost finished with the current book.
a language with a bunch of random "features" (that mostly feel half baked to me) that certainly do things which the user might not intend
Sweet, rayon ties with OpenMP. Always nice to see. 
Note that there is no fixed alignment for Enums, it depends on contents. Option u8 is 1-aligned and option u32 is 4-aligned and so on.
Cool, thanks. I'm a vim user, and don't use racer, so it's tough to know what the particular problems are here :)
I don't think they expect to know how fast the optimizer can make it. I think it's coming from a viewpoint of: I know that the C code will translate to assembly that is at least this fast, and the optimizer may make it faster. There are places in rust where I don't have the same confidence. I'm not saying that rust is slow, but there are things in rust that rely on the optimizer in order to be fast, and that requires a bit of faith (or in depth knowledge that I don't have). I don't think pattern matching is a good example of that though. Personally, I need a bit of faith for closures passed to iterator adapters.
`match` *is* flow control. It's just a more capable version of `switch`, and can be compiled down to if-else chains the same way (in a hypothetical version of Rust that allowed you to directly read the discriminant and value of an enum, that is).
If [this is the conversation you are talking about](https://twitter.com/Jonathan_Blow/status/771589892727988224), it seems like you're talking about somewhat different things. Note that he says &gt; pattern-matching involves nontrivial / looping / recursive code. I'm not sure where looping comes in, but I can understand the rest: Blow is talking about about complex matches, where you can have things like internal equality checks, multiple levels of unpacking and so on. If I were to reply, I'd point out that a *simple but sufficient* implementation of pattern matching is equivalent to a `switch` on the discriminant. In fact the result is basically syntactic sugar for a C-style enum plus a union. I somewhat agree with Blow, though; using a `match` forces you into a *specific* representation of your enum, whereas writing it yourself allows you flexibility of representation and more control over unpacking if you need it.
I still like the once proposed `let … else …;` syntax, e.g. let Some(a) = x else return 0; let Some(b) = y else return 0; let Some(c) = z else return 0; return a + c + b; It's maybe not as nice as `with!`, but at least it prevents your code from drifting to the right too much. On the other hand, `?` and `catch` seem to fulfill most of what you'd get from `with!`, and looks kinda like this: match catch { let a = x?; let b = y?; let c = z?; } { Ok(n) =&gt; return n, _ =&gt; return 0, }
Well regardless of how you think about pattern matching, the point is that it isn't the same as writing a switch statement. Even the most simple pattern matches relinquish control because you can't explicitly force the compiler to output certain code to perform the matching. The switch statement isn't entirely predictable about what code it will produce either (See some of the other answers here for why). So long as you aren't directly writing in assembly or machine code, you're going to have to rely on the compiler to generate reasonable code for what you're trying to represent. The person that OP was responding to is paranoid (perhaps through bad experiences) of allowing the compiler to make too many decisions about what is a reasonable representation for pattern matching. 
I was quite interested in the language until he introduced for loops with implicit `it` as current element (I think -- I skimmed his last video). looks too much like perl's `$_`
Plugging [libpnet](https://github.com/libpnet/libpnet) for doing raw networking, which you could use to implement ping. I understand you are making a different comment, but it should be possible as of the latest release. 
Are there any examples on this new API out yet? I just started messing with `mio`, so I'm mostly looking to port the pingpong example in the getting started guide. I could do a PR with my ported version if that would be helpful, but I probably won't do it the "best" way since I'm learning.
It is possible that I set something up incorrectly. Honestly, I'm a vim user and setting up emacs felt a bit like cut-n-paste coding for me. I was pasting pieces of lisp into my init.el so...maybe I missed something? I was able to get auto-complete with racer, so whatever it was, that part worked.
If you're a vim user, spacemacs is way easier to use when using Emacs anyway, so I'd recommend trying it. 
You want /r/playrust
my bad 
What is the discriminant? Is it like a tag on a tagged union in C?
I was working on my own implementation yesterday about the same time you submitted your original PR. I intended to put a little more time into it this weekend if I get a chance. I stopped playing with it when it wouldn't run the benchmark locally on like the second try. Right now mine looks largely similar to yours, except with fewer method chains. What I really wanted to do was come up with a way to do multi-core and then have competing implementations for tokio, websocket, and maybe futures. Edit: [PR sent](https://github.com/hashrocket/websocket-shootout/pull/26), though I didn't go as all-out as I'd hoped.
yes. (enums are tagged unions, "discriminant" is another name for "tag")
Works for me in vim. IIRC I had to set `CARGO_HOME` for it to work. Like `export CARGO_HOME="$HOME/.cargo"` in your shell rc file.
Does it stay synced up correctly when you move it? (it might be using the screenshot hack or the get-wallpaper hack)
Hmm, that did it. I had my `CARGO_HOME` set to `$HOME/.cargo/bin`. Thanks.
You're looking for /r/playrust!
&gt; I know that the C code will translate to assembly that is at least this fast, and the optimizer may make it faster. I am afraid that it's a fallacy. As a fellow C++ developer, I've been bitten back by a compiler more times than I dare to count. Undefined behaviour is of course the bigger concern, but I've also been bitten by the absence of what I thought would be trivial optimizations (notably pertaining to NRVO). Sometimes you expect too much, sometimes too little, and in the end the compiler always has the last word.
callerche responded 2 hours after you: [here](https://www.reddit.com/r/rust/comments/50srx0/mio_api_has_changed_substantially/d77565u). It's a draft of a guide though, not complete.
How does folding the discriminant in a subobject interact with the ability to form a reference to that suboject? I thought this was why objects have to have "standard" layouts.
&gt; I do not really like pattern matching since it removes flow control from the programmer The claim is pretty absurd. Modern compilers complicate control flow way more than what match does. match is just switching on the discriminant, with type safety. In fact, in the Rust intermediate representation, it's basically represented as that. More complicated match statements with nested patterns do produce more complicated code, but that's basically just checking multiple discriminants instead of one, or whatever. Why is "non-trivial control flow" a bad thing, anyway? I agree it can be, with things like hidden returns and whatnot. But you _know_ what a match does, and by looking at it you _know_ what the control flow is. It's like saying a for loop has nontrivial control flow. Of course it does. But you know what that flow is by looking at it. &gt; imagine he's referring to some hidden cost/complexity of compiled code Well, matches are pretty zero-cost in the sense that it's the same cost as a switch or an if-else-if chain.
Are borrows and references the same? Is every borrow a reference and vice versa? 
Not sure it would be really good UX-wise, but there is a [`oncopy`](https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/oncopy) event (and `oncut` too) which could be used to "augment" the copied snippet. I would probably favor a "hover" behavior, though. Less surprising.
An `Entry` needs to be able to take ownership of the key you give it in order to support the `.or_insert()` method. If you instead gave it a reference and had it clone it if it needed to insert, you wouldn't be able to use the map with non-`Clone` keys.
I don't think we should move crates to 1.0 unless they're really worth it. I don't mind the low version numbers as much as: 1. Does the crate do what's advertised on the tin without having to fiddle with nightly compilers or asking questions on IRC? 2. Does it have decent documentation, with examples? 3. Does it handle a number of real-world cases (for example, consuming JSON from various APIs etc.) and spit out reasonable errors? 4. How stable is the API? Has it changed dramatically in the last 3 months? 
Just my 2 cents: Intra procedure optimizations on the control flow graph are the mostra mature optimizations in compilers.
I'm not a C++ developer, but you're probably right that you can write C++ code that relies on the optimizer to avoid being very slow. I don't think that is good style if the performance matters. Relying on optimization of undefined behaviour is especially bad style, and I think the compiler writers made the wrong choice in that regard.
You shouldn't have to set this value explicitly; it's the default.
&gt; Relying on optimization of undefined behaviour is especially bad style, and I think the compiler writers made the wrong choice in that regard. At the end of the day, compilers are judged by the speed/efficiency of the code they generate. A compiler that uses ub for optimization is going to beat one that does not. I think the *real* problem here is that you can't fart without accidentally introducing three different types of ub in a C/C++ program, unless you are an expert and you fart very carefully. Forgot to end your source file with a newline? Undefined behavior! 
Non-js version: https://webcache.googleusercontent.com/search?q=cache:https%3A%2F%2Finternals.rust-lang.org%2Ft%2Fsetting-our-vision-for-the-2017-cycle%2F3958
It's only one enum actually, but Arrayvec just lost two drop flags (mir trans in nightly), so the world is looking up.
Oh, yeah, I figured. I guess I was just theorizing over the question rather than explicitly stating its a rustc/LLVM thing. I am aware LLVM is involved.. but I am by no means qualified to talk about it in any detail!
Wow, this is way, way simpler than before. Thanks for the improvements!
Why is the "Run" button ever hidden?
Except that he's done both already for his fairly substantial compiler...
Wouldn't it be better to stick to semver since lots of people assume minor version number changes are non breaking Edit: im wrong
&gt; Yeah, by changing the benchmark. Calling `reserve` is not changing the benchmark and others have shown how this change already is enough to beat F# and OCAML. Arguably if anything that makes the benchmark fairer since preallocating memory is what any GC does. Quoting veedrac (emphasis mine): &gt;in this case, (i &lt;&lt; 10) ^ j on top /u/Aatch's code makes the code stupidly fast. This is _fair_ IMO because the F# uses a similarly specialized hash function. &gt; implementation|time :--|:-- original|2.6 Aatch|1.5 PairHash|1.4 Aatch + PairHash|0.28 So yeah, by preallocating memory and using a custom hash, the program you submitted can be trivially be made 10x faster than what you wrote. I expect that you can reproduce a similar speed up in your computer, so my question is, can you modify the F# and OCAML version to compete with the fast Rust version? I say you cannot. If you can, I'd love to see the code (this is pure true interest since I love F#, but I wouldn't know how to make the code faster there). But I also understand that we all have better things to do than writing code to prove people wrong in the internet. It is fine to disagree. Then you say: &gt; You cannot walk the stack and identify pointers so you cannot write decent automatic memory management. You can: - take a pointer to any variable on the stack, - increment/decrement/dereference the pointer at will, - write a procedural macro that automatically do this for you, tracing references to types allocated wherever you want, including the heap of some garbage collector, - use this information to implement _whatever garbage collector algorithm is better suited to your problem domain_, - or to use the garbage collectors of different languages, - and even to have multiple different garbage collectors in the same program (each better suited to a different task). Being able to do this is nice, but if you don't want to do this yourself lucky for you it has already been done, see [rust-gc](https://github.com/Manishearth/rust-gc). In particular, the largest open source Rust project (servo) uses this to hook with javascript's garbage collector. &gt; My point here is that Rust is advertised as being "blazingly fast" but clearly isn't. Rust _can be made_ blazingly fast. Just because the Rust code you wrote isn't the fastest way to solve your problem doesn't mean that there isn't a faster way to do that in Rust. Others have shown how minor changes significantly improve the performance of your program, but Rust allows you to optimize your hotspots up to the exact assembly that is generated (using `asm!`), so you can invest as much time into this as you want in Rust, and could probably get another order of magnitude speed up if you needed it. My point is not, yay rust is fast by default, but rather that it allows you to write code that is as fast as it must be. As I said above I love F#, but it doesn't allow you to do this. If the performance you get by default is good, and if the garbage collector latency is ok, everything is fine. But the moment it isn't, the answer is typically go on and rewrite that part of your program in another language, like C _or_ Rust. 
`ToOwned` would be a better trait bound. I can't think of any good reason why it couldn't be used with the entry API, except maybe causing inference issues and introducing another type parameter. Either way, the API is stable now so there's not really anything that can be done backwards compatibly.
Perhaps the trouble is: mio is widely used through the ecosystem, so people expect it to be stable-ish - but indeed 1.0 isn't out yet.
Here's [a previous thread](https://www.reddit.com/r/rust/comments/34rszb/pony_type_and_memory_safe_language/) on Pony, that contains a comment with Rust analogues of Pony's reference types.
It's fairly common in games to build a mini virtual machine. I present [the byte-code pattern](http://gameprogrammingpatterns.com/bytecode.html).
Ah shit, more complexity. Alright, I will take the updates into consideration.
Almost none of those should have a runtime cost, though.
This summed up my impression perfectly, but I also got the sense that sometimes he *does* get the benefit of a high-level abstraction eventually, but doesn't want to admit it after he spent serious time arguing against said abstraction before he even properly knew what it does.
Hey thanks! That's exactly what I'm looking for!
Not new to programming , But The official Rust book made rust so much easier to learn , even some really basic ideas are introduced . The book assumes you are new to programming.
I watched a Pony video a few months ago and distinctly remember Sylvan (I think his name is) saying that it's impossible to deadlock. But it is possible to "live lock".
A "reference" is an `&amp;T`. A "mutable reference" is an `&amp;mut T`. Both of these borrow, but are distinct types.
I think the argument is that C translates more predictably into assembly than C++ or Rust. You can't dispel that by showing that C++ isn't very predictable.
There is this package https://github.com/rustr/rustr. But still isn't ready to production enviroment.
beware of videos those will get you bogged down in alternate realities
I used the Java bindings to call into R for a small project and came away pretty frustrated tbh. 
While Martin Sústrik does not work on Nanomsg anymore, Garrett D'Amore is actively maintaining it. Version 1.0 has been recently released and there have been some discussions about what will happen next on the mailing list. I do plan on adding stuff to scaproust: like making the transports pluggable, adding more transports (IPC, Inproc, and later TLS and WebSocket). But mainly I would like to expose an async API based on **future-rs**, which would require breaking quite some stuff. There is also some work to do regarding performances.
In that case I'm unsure, sorry! :(
&gt; what happens to `it` when you nest loops? Why is this a problem? It's either shadowed or not allowed, and neither option is particularly confusing. 
&gt; if let Some(a) = x, Some(b) = y, Some(c) = z { if let (Some(a), Some(b), Some(c)) = (x, y, z) { ?
This one will execute x, y and z before doing the pattern match, it doesn't stop on the first non match.
On one hand rust is soaked with these sorta shrug and hope the optimizer will do a good job, iterators are a perfect example of this, and generally rustc does a good job. On the other rusts lack of undefined behavior, type safe tools like sum types which have very predictable optimization (though there's some low hanging fruit with folding to be done here). What I take from this is not that languages are missing tools to optimize hotspots. Rust's zero cost abstractions are a good fit for this. But that languages need more tools to keep the warm parts of our programs fast. Haskells rewrite rules seem like one of the few language constructs that seems directly targeted at this middle layer. 
But following [documentation](http://doc.crates.io/manifest.html) on the manifest format we read: # The default set of optional packages. Most people will want to use these # packages, but they are strictly optional. Note that `session` is not a package # but rather another feature listed in this manifest. default = ["jquery", "uglifier", "session"] # A feature with no dependencies is used mainly for conditional compilation, # like `#[cfg(feature = "go-faster")]`. go-faster = [] And: # A list of all of the optional dependencies, some of which are included in the # above `features`. They **can be opted into** by apps. jquery = { version = "1.0.2", optional = true } uglifier = { version = "1.5.3", optional = true } So if optional dependencies enabled by default as you say why do we need `default` feature keyword and why it's writen that optional dependencies **can** be opted in, but not that they can be disabled? Do we have something like `exclude` keyword but for dependencies, passing every time `--no-default-features` seems overly redundant.
I'll be honest, I've yet to find any problems at a cursory glance. Some stuff like `let &amp;CipherText(ref vec_bytes) = self;` is a little frivolous, and would be more readable as `let vec_bytes = self.0;`, but beyond that is just personal preference. With the testing I'd recommend more failure tests, and using more than a single key.
 (Some(1), Err(Foo { x: "string", ref y, .. })) =&gt; ... Languages other than Rust also frequently have things like (x, x) =&gt; ... that check both elements are equal.
If you think most people will want it, put it in the defaults, otherwise, don't. Once you mark the dependency optional you don't have to create a feature for it. Features and dependencies share the same namespace (which I think was a bad design but that's irrelevant) so people can just use "rand" as a feature name.
Oh, nice find! Looks like the `set_len` is just unnecessary in general. Instead of out[i] = // ... It could just be out.push(/* ... */); Why bother zeroing it?
Rust has owned, `&amp;` and `&amp;mut`, while Pony has `iso`, `trn`, `ref`, `val`, `box` and `tag` so I would say it's a little more complicated! Especially since it's exponential in terms of how you can combine or convert them.
The libc crate: [Cargo.toml](https://github.com/rust-lang/libc/blob/3635c1a9398160bb6540d7289b16a547be0f02b3/Cargo.toml#L16-L18), [lib.rs](https://github.com/rust-lang/libc/blob/3635c1a9398160bb6540d7289b16a547be0f02b3/src/lib.rs#L85-L88).
Thanks, that's what I was looking for. I guess the only remaining question is what default does for builtin types. I assume it's just 0 for everything? 
Actually I use Java (JEE) and planning a migration for Rust in next year.
Yes.
There has not yet been a release, no.
&gt; He would be a beginner if he really tried C++ I had thought Braid and The Witness were written in C++. If not, what are they written in?
They could easily fall back on PPs.
Thanks :)
Have you thought about using Julia? Julia was designed to replace R, and have performance similar to C, but perhaps the simplest syntax we've ever seen in a language to date.
If that was allowed, another crate could `impl&lt;T&gt; Add&lt;T&gt; for TheirType` and it would conflict with your generic implementation. (It would be impossible to do that for the first example, because you can't `impl&lt;T&gt; Add&lt;T&gt; for i32`, since i32 is defined in core.)
Not possible here? I do not want to implement `Add&lt;T&gt;` but Add&lt;`Foo&lt;T&gt;&gt;` and `Foo` is local.
Jesus Christ wrong subreddit, my bad. 
&gt; impl&lt;T&gt; Add&lt;Bar&lt;T&gt;&gt; **for T** { That's the part the compiler is complaining about.
Kotlin does this too.
Thanks for your answer. Do you know if there is an open issue for this? Just based on the semantics of the operator traits it seems weired that it is possible to implement `Add`for `T` in the right hand side position but not the left hand side. 
I couldn't find an open issue when I searched. It would be pretty difficult to come up with a completely backwards compatible extension that allows more `impl`s without any additional annotations for existing traits, which is why most of these discussions happened before the rules were frozen in 1.0.
Maybe a build script would work for you.
It doesn't matter that your implementation has `Bar&lt;T&gt;`, *another* crate could implement `Add&lt;T&gt; for TheirType` like I described, and it would conflict with your implementation, because `T` unifies with `Bar&lt;T&gt;` even though that other crate has no knowledge of `Bar`. The specific instantiation that would conflict is: `Add&lt;Bar&lt;T&gt;&gt; for TheirType` Both your implementation `impl&lt;T&gt; Add&lt;Bar&lt;T&gt;&gt; for T` and their implementation `impl&lt;T&gt; Add&lt;T&gt; for TheirType` would be applicable for the above instantiation, and the coherence rules are specifically designed to prevent that. TL;DR: not a bug, the binary operator are not symmetric in this sense: they give preference to one side, because to do otherwise would allow conflicting generic implementations.
Submitted a pull request with improvements for file handling.
"design"
You should be able to put something like `println!("cargo:rustc-link-search=native=path/to/libsample")` in your script so that your `#[link(name="sample")]` attribute works.
Just from this piece of code, it's hard to tell you anything. The reason why your won't work is because you borrow the result to get its state, and then try to borrow it again to pass it into the function. I don't know how you implemented your state machine, but one way you could do it is by creating an enum of states. Have the enum contain the state information. Then use a match statement take the state enum and return the new state.
Ah my bad, seems I misinterpreted the problem. Maybe using `cargo rustc -- -C link-args=objc` instead of just `cargo build` would work?
Hmm, is there something else to the state machine to change than the state itself? Because if it's just a state → state transformation, I don't see the need to pass the entire machine, but just state itself, mutably. Or possibly by value, even.
Rust is quite restrictive in terms of mutable aliasing, and it takes some time to step in to the mindset that you design and write your code from the start to take that into account. It's often worth it though, because it's easier to see what mutates what just from the signatures. Sometimes it's a pain, though.
[this](https://github.com/jappeace/5ol/blob/master/src/main.rs#L99) stuff. Before I tried passing the statemachine to enter and render functions but that is basically impossible since the statemachine needs to modify the state by calling a function on state which also modifies itself. I fixed it by forcing the states to return if they want to change a state or not, if so the state machine will do it (I got the inspiration from the other comment, yes I need a rubber duck).
You can use [`.split_first()`](https://doc.rust-lang.org/std/primitive.slice.html#method.split_first), or store an iterator and call `.next()` on that.
Well the point was that states could decide when they wanted to be "over". So the "Java" way of doing that is passing the state-machine which has the one public method [change state](https://github.com/jappeace/5ol/blob/d8900c6fe6a239be9c77789bbe8f089975262098/src/main.rs#L130). I now have a [variant](https://github.com/jappeace/5ol/blob/master/src/state_machine.rs#L21) in which the state functions return an Option&lt;State&gt; on none the state stays the same and on some(x) it changes to x. (Note I arbitrarily decided to not allow exit to change states, since its just for cleanup).
You can still put the 'c' module in a separate file, but you'll need to do: use super::c::*; rather than: mod c; use c::*; in a.rs and b.rs. 
I don't think so, but as an alternative you can write the data to a file and use [`include_bytes!`](https://doc.rust-lang.org/std/macro.include_bytes!.html).
One set of impl rules is already *really* complicated. Having more than one set seems... out of this world.
The current behavior is 'correct,' insofar as Rust has these coherence semantics. While Rust could have adopted different semantics, this was just a trade off between conflicting goals and there isn't an 'improvement' that would make this code work without costing anything else. The operator traits are an area where this rule looks odd, because though the syntax is symmetric, the left side has the additional constraints that being the receiver of a trait implies.
Well, we have `#![fundamental]` ;)
Excellent. Thank you for your help.
Thanks, I think I get it now. I guess `use super::c::*;` is sort of like the Rust equivalent of `from .c import *` in Python. Right?
Why is this here?
r/playrust
Not necessarily, as it was kernel code which would have likely been at least partially `unsafe` in Rust.
&gt; that enums take 8 bytes for the tag [Enums take 1 byte for the tag unless padding is required for alignment, or there are too many variants.](https://is.gd/TWQMbI)
I agree with his view, especially for kernel-level work, but it does mean you have less opportunities to use the type system to help you out with SIMD and atomic operations. 
I believe it was caused by a use-after-free in their XML deserialization code. If you've ever used Java's ObjectInputStream, you will understand how it gives each object an ID as it is serialized, and then each time that same reference is encountered in the object graph, it simply uses the ID, and this allows it to serialize cycles. This XML deserializer did a similar thing, except that it kept a table of ID -&gt; object mappings while deserializing, and it did not increment the reference count when objects were placed in the table. That meant that if an object was deserialized, then its reference count reached zero, and then the deserializer encountered its ID in the input stream, it would try to use the already freed object in the table. The equivalent function written in the safe subset of rust would not have suffered from this problem, since use-after-free is statically prevented. Using `Rc` would make it impossible to forget to bump the reference count.
I can't make it on Friday (Darmstadt is a bit far) but if you happen to be in cologne tomorrow (monday), I'd be happy to [meet you](http://rustaceans.cologne/2016/09/05/compile-to-js.html)! :)
I'll be around!
it's not even the rust game
Indeed, it was the result of a muscle-memory 'cargo update' to get clippy to play well with the nightly compiler. In the end switching everything over to the new API wasn't too bad, and I haven't suffered any severe performance penalties as a result.
I think you can use `cargo update -p clippy` or something similar to just update one crate's version.
It's not a pretty solution, but here's what I did. I didn't know about the objc crate, so I wrapped all my objective-c functions in a c wrapper. Then I compiled things using a simple build script: extern crate gcc; fn main() { gcc::compile_library("liblibrary.a", &amp;["src/library.m"]); } The rust link looks like this: #[link(name = "library", kind="static")]
FYI you can find Rust code for the benchmarks game (incl. k-nucleotide) here: https://github.com/TeXitoi/benchmarksgame-rs
It'd be awesome if Rust had static assert...
This looks interesting, from where you got that information? Also, how use after free can lead to security vulnerability?
If another process (or even the same process) re uses this free memory, an attacker can manipulate the memory to appear valid, but pass execution control to attacker controlled functions through function pointers.
There are others. But this is the most common method. Im on mobile, apologies for the brevity.
One of the well-known performance bottlenecks of HashMap is the default hasher. It uses SipHash which is cryptographically secure and resistant to collision attacks, but is a bit slower than some other alternatives. Try to initialize the HashMap with a different hasher. Check this out: http://cglab.ca/~abeinges/blah/hash-rs/
So if I understand it correctly, a process allocates some memory, creates an object in it (puts pointer to vtable there), then accidentally frees the memory, the memory gets allocated second time and some user (attacker) input is written there, so then, when pointer to that location is used, the program jumps to some other function (within provided vtable). Do I understand it right?
&gt; Use std::io::stdin().lock() when print!ing lot's of times. Did you mean `std::io::stdout()`? Also, avoid using `print!()` in hot code when you can. Writing a byte slice to a locked stdout directly can be much faster. I ran a benchmark, and [here are the results](https://gist.github.com/yberreby/70df45bdec53ca8455e9d581ad49d5f1). Of course, printing to stdout directly will be slower if you have to allocate and do advanced formatting... but if all you're doing is _printing a newline_ and you need every last bit of performance, use a locked `stdout` handle directly.
I assume that vtable is from c++ and that a vtable contains pointers which can point to an object method.. if so you got it!!
The error message could just say something like "expected parameter type `bool`" or whatever.
First of all, I'd like to point out I'm also pretty new to Rust, so don't take what I say as gospel! Basically, when you access data from a vector, what gets returned is an immutable (aka read-only) reference to the item, not the item itself (this is what the `&amp;InventoryItem` means in the error - read it as 'an immutable reference to an InventoryItem'). The reason for this is pretty simple: one of the core design ideas in Rust is that there can only be a single owner of a piece of data. If you accessing a vector gave you the item itself rather than just taking a reference to it, by definition the item could no longer be part of the vector, as otherwise it would break that rule. Vectors would be pretty useless if elements fell out every time you accessed them! To get this to work, you need to be more explicit about what it is you're trying to do - `inventoryVec.remove(0)` takes the item out of the vector, shifting all the subsequent values left by one, and then returns it. So something along the lines of this would probably work: ``` playerVec.push(inventoryVec.remove(0)); ``` Pretty much everyone struggles with the ownership rules when they start out in Rust, I think, so try not to get discouraged! I would recommend reading the chapters on [Ownership](https://doc.rust-lang.org/book/ownership.html), [References and Borrowing](https://doc.rust-lang.org/book/references-and-borrowing.html) in the Rust Book to help you get a better understanding of how it all works. Additionally, the [documentation on the `Vec` structure](https://doc.rust-lang.org/std/vec/struct.Vec.html) will tell you all the different things you can do with a vector.
&gt; I gave you +1s for participating Thank you. Have an upvote yourself. &gt; calling reserve is fair game. You're missing the point. This program has a purpose: to find the nth nearest neighbor shells in a potentially infinite graph. For the purposes of this benchmark I chose to use an infinite 2D Manhattan graph. I did not optimise the generic core of the program for the specific input graph or even its topology in any language. In fact, I originally used the generic built-in hash functions in both OCaml and F# and in both cases they performed just fine. Only in Rust was the default hash function very slow (because it is crypto) so I replaced it with FNV which is much faster making, I believe, a fair comparison. I have tested a variety of inputs and Rust is sometimes 3x faster and sometimes 3x slower but I found no evidence that Rust is generally significantly faster than either F# or OCaml. In contrast, these guys have optimised not only the generic core of the program for the specific input graph but even the argument to `reserve` for the specific problem size used in my run of the benchmark and they did this only in Rust and not in OCaml or F# and then concluded that Rust is blazingly fast. Calling `reserve` is just a small part of this. To me, that is a completely different benchmark. You can easily optimise the F# and OCaml versions in the same way and achieve the same performance, of course. You could just use a 2D array to represent the 2D Manhattan graph, pre-sizing it because you know how big the output will be. But that is completely missing the whole point of this exercise. &gt; The lack of this method in .Net is its own failing True. &gt; and on this micro-benchmark F# is soundly beaten. Not true. I have written an F# version equivalent to this new benchmark and the performance is, again, the same. 
What does that mean?
&gt; Or maybe you can use another data structure? The shootout wants it to be `std::collections::Hashmap`.
You cannot recreate that specific function lazily since it checks the entire buffer for IO errors and returns an error if any of them errors. A lazy version would have to look into the future. Maybe you want to do something like [this](https://is.gd/TGYoC8) instead? Basically, I introduce a new function `lift_opt` which creates an `Option&lt;Result&lt;T,E&gt;&gt;` from an `Result&lt;Option&lt;T&gt;,E&gt;`. This way the error is kept and only `Ok(None)` values are filtered.
I'm not quite sure I understand what you're looking for. What are we calling user data/ user requests? From what I can see you may want a wrapper type that handles this sort of authentication - you get a token from a user and a request for the data, and then you use that token to access the data through the user data wrapper type's API. This is similar to how this works at a service level, but you would mimic the pattern in your code.
There's a clippy lint for this! [builtin_type_shadow](https://manishearth.github.io/rust-clippy/current/#builtin_type_shadow)
Oh, I'm actually visiting Darmstadt next Friday by chance! I'll try to turn up!
You're right, `with_capacity` does the same thing as `reserve`.
It'd be straight forward enough to include a custom Hasher though, no? [fnv is pretty small](https://github.com/servo/rust-fnv/blob/master/lib.rs#L71) but given the context a specialized hash could even be conceived
You can check out https://github.com/hjr3/mob for an example. I updated it to 0.6 right when it was released. I also have some blog posts on how to use mio, but they were for 0.5 http://hermanradtke.com/tags/mio.html
It is usually not a problem. However, for complex cases you might want to use a more memory efficient structure. Pattern matching is not a silver bullet, but Rust allows you to do both. I don't think of this as a compiler issue, but as bottleneck due to memory bandwidth. Also, some transforms are more expensive with algebraic datatypes because it introduces more recursion in the algorithm. This might not be evident if you are new to Rust, but can be worked around by careful thinking and using the right smart pointers.
The error here is the opposite here, and "expected type parameter `bool`" kind of makes it sound like there should be a `&lt;bool&gt;` in there somewhere.
He is talking about data tainting so you can't accidentally hand a buffer currently holding a password to a method that might log it.
Yeah, I'm not sure, I just know that I had a conversation with the maintainer and he made his wishes very clear with regards to it _has_ to be `std::collections::HashMap`.
I see. Thanks. How can I prevent that? I don't think I'm understanding the use of `test::black_box`.
All `black_box` does is pretends to use the value. That is, it just makes the optimizer not throw away the entire function call because it returns an unused value and has no side effects. Trying to "avoid" optimization like that is wrong - the compiler is completely justified in doing those optimizations, and you should be happy it's doing them. If you want to benchmark your implementation, do it on random numbers, or maybe on a deterministic sequence of numbers generated at runtime.
You don't need to `black_box` the output – the value returned from the closure is blackboxed anyway (remember not to put the semicolon). You should `black_box` the input though, so compiler can't treat it as constant! I did b.iter( || test::black_box(m).get_population() ); and got it to 1ns/iter (which is ridiculously fast, but seems reasonable (1ns is almost 4 cycles, which is over 10 instructions, if they're independent enough)). The compiler may also recognize your manual popcount and do something smarter instead. *edit*: I may be wrong, as `black_box` seems to pretend only a read, not a write. *edit*: If you check the IR of u/K900_ version with the `black_box` added, it seems to that llvm does not optimize out, so probably the `black_box` could be used to obfuscate input after all.
Ah, that makes some sense to me. And whether or not you're wrong, it seems as though that's what they've done [in the book](https://doc.rust-lang.org/book/benchmark-tests.html): #![feature(test)] extern crate test; b.iter(|| { let n = test::black_box(1000); (0..n).fold(0, |a, b| a ^ b) })
Yup, it seems to work, although the docs could be clearer on this. Anyway, I don't know if you wanted to test this particular implementation of popcount, but there's a method [`count_ones`](https://static.rust-lang.org/doc/master/std/primitive.u64.html#method.count_ones). It's equally fast (13ns) as your implementation on my CPU on this test: b.iter(|| { for _ in 0..10 { test::black_box(test::black_box(m).get_population()); } }); But, if you add `-C target_cpu=native`, it goes down to 7ns!
That's awesome, works perfectly. I'm very happy now.
That's the magic of the POPCNT instruction most likely. 
Look at the full code, they're using `&lt;bool&gt;` as a type parameter in a generic impl. So Rust is interpreting `y: bool` as `y` being of the type parameter `bool`, not of the actual type `bool`. Replacing `bool` with `T` in the generic definition would make it `y: T = true`, which doesn't make sense. 
 impl&lt;bool&gt; X&lt;bool&gt; for Y{ // ^^^^ bool is declared to be generic but a `bool` already exists. The normal one // which includes `true` and `false`. fn foo(x:bool){ let y:bool = true; // ^^^^ The type is the new generic `bool`which does not include `true` and `false`. // ^^^^ The value is the normal rust `bool` type. These are not the same.
Yes I switched to gcc crate recently. It's much neater than a bash script and puts the library in the right spot.
https://news.ycombinator.com/item?id=12321311 is the convo
For ease of information: the custom C hash function is (key ^ (key&gt;&gt;7)) where key is a u64 of packed bit pairs of the given sequence
This is an option for `rustc`, so you could pass it to `cargo rustc -- &lt;rustc options&gt;`, although it would get complicated if you try to mix it with bench. The easiest way would be to do it via environment variable. This affects only one invocation: env RUSTFLAGS="-C target_cpu=native" cargo bench You can also configure it in `~/.cargo/config` (for setting it globally) or `.cargo/config` in your project directory. See `rustflags` option in [docs](http://doc.crates.io/config.html#configuration-keys).
Unless the usual premature optimization devils kick in. I am usually disappointed every time I check some cool Rust packages that turn out to have unsafe sprinkled around, when it isn't actually needed. 
Problem is `include_bytes!` is dependent on `std` while I want to write some tests which will work with `no_std` too. So unfortunately i can't use it. But thank you anyway.
Gee, someone should send a PR to update that RFC then.
Thanks, good idea!
I'm also getting this bug when using older (06-24) nightly. It looks like it's trying to use avx-512 instructions, which are available only on server skylakes. Up-to-date nightly works fine. As of speed difference, I was talking about difference between non-native vs. native on this code: fn get_population(&amp;self) -&gt; u8 { self.count_ones() as u8 } It would surprise me if the usage of popcnt instruction (enabled by target_cpu) won't improve the times significantly for you. I'm also on skylake (i5-6600) so we should get similar results.
You need to add `optional = true` to your `dev-dependencies`, `default-features = false` just means the features of the dependency itself, not that it is an optional feature. Sadly `dev-dependencies` [cannot be optional at this time](https://github.com/rust-lang/cargo/issues/1596), so you will have to move them to normal dependencies and add a feature if necessary. See [Chomp's `Cargo.toml`](https://github.com/m4rw3r/chomp/blob/1e5aca879dca19572b9f75ca35f40edd604aa87f/Cargo.toml#L31-L38) for an example where the `unstable` feature which is used by default by Travis enables `compiletest_rs` on unstable. Clippy is there to enable Clippy lints locally with `cargo build --features clippy`. Edit: Seems like I did not read through the toml as thoroughly as I should have, hopefully the above will still be of some use for you.
Yes, my focus was on this line: let mut table: [Field; 9] = [Field::Empty; 9]; Which gave error *the trait bound `Field: std::marker::Copy` is not satisfied*
In some cases I want to modify the returned value, so I have them return mutable reference. In the rest of the code it works fine, because I either change the value and drop the reference, or copy the value out.
Wrong sub
I believe you're looking for /r/playrust
Yes, getting a mutable reference to a value in `SuperDB` is exactly what I want. My problem is that the compiler does not let me combine multiple lookup functions, even though I borrow `self` again only if the last borrow should to be over.
I liked it too and it's a really nice idea. Maybe you want to add some kind of mini-interview: ask the same questions all four authors and include their answers in the video. For example: - How long have you been working on this project ? - What are your plans for the future ? - What problems did you have ? - How could s.o. who is interested in the project help you out ? - What did you like / dislike about rust and its tools ? - ... 
It really looks like this may be blocked by [non-lexical borrowing](https://github.com/rust-lang/rfcs/issues/811). Your issue almost looks similar to [this one](https://github.com/rust-lang/rust/issues/21906). Fortunately, MIR has entered orbit, as they say, so non-lexical borrowing is no longer blocked by that. Hopefully this will be fixed soon. [Over here](https://is.gd/b6WxbR) on Rust Playground, I have created a very interesting test file that seems to demonstrate the borrow checker acting in what seems like an illogical way, but really it's just being "lexical" I guess. Maybe someone else will come along with the perfect solution to this problem. I am not an expert in Rust by any means.
sorry 
Trait objects are stored as fat pointers. The internal structure is a data pointer (which is always a valid pointer to the concrete type) followed by a vtable pointer. The vtable pointer is a normal pointer to a table of function pointers. Each of these pointers correspond to a method on the trait. For a given type/trait pair, the vtable pointer is the same. In short, that means you can cast between trait objects by simple replacing the vtable pointer with the correct vtable pointer of the new trait. The data pointer can be left unchanged. Getting the correct vtable pointer for a trait known at compile-time is easy: just make a null pointer to the underlying type, and cast it to the trait object, the compiler will automatically fill in the correct vtable pointer. The hard part is getting the vtable for a trait specified by a `TypeId` and only known at runtime. To solve this, I use the `implements!` macro: this generates a big switch statement, comparing the `TypeId` against all of the IDs corresponding to traits which the Self type is known to implement. 
Sounds OK -- the intent is to show full-function library implementations and being able to drop-in a custom hash function is fairly ordinary.
Great! Thanks. :)
Thanks for the feedback I was honestly expecting this to meet a fair amount of negativity but im glad you liked it. I REALLY like the idea of interviewing the people involved with the project, I will see how possible that is. As for counting down backwards, not a bad idea I just dont want to go to far and make this clickbait, I still want the series primary purpose to expose Rust projects so that the whole ecosystem grows.
It seems like the problem is that you want to perform this transformation: Iterator&lt;Item = io::Result&lt;T&gt;&gt; -&gt; io::Result&lt;Iterator&lt;Item = T&gt;&gt; The problem is that we can only know if the `Result` is `Ok` or `Err` once the Iterator has been fully processed. I think the Futures library may provide a better API for what you want to do.
WTF-8 is an implementation detail of OsString, I don't think you're supposed to touch it as such. There's a crate for it though.
I don’t know if `size_of` could work in a static/const context, especially with a generic type.
I had no idea rust let you do that. Seems like something that should be banned or at least give a warning.
I added your video on the topic Frameworks and Tools from: http://daily-rust.github.io/2016/09/01/videos.html With more time I will create a page with videos about Rust =)
Oh wow thanks. Do you keep that page updated, If so Ill be sure to give it a mention in one of my videos.
Is this a COM joke? or an actual library
Open a ticket?
I would either write lookup2 as a macro that takes the actions you want to preform as arguments, or probably take a closer look at `std::cell`.
This is why we need a `#[deny(unsafe)]` lint...
I actually found [String::from_utf16](https://doc.rust-lang.org/std/string/struct.String.html#method.from_utf16) shortly after posting my comment. I haven't checked the implementation but it seems to work identical to `OsStringExt::from_wide`. But I might just pull in wio so I don't have to have the null trimming in my code. Thanks
`from_utf16` is not at all identical to `from_wide`. Wide strings can contain lone surrogates which would cause `from_utf16` to fail due to them being unrepresentable in UTF-8, while `from_wide` will work fine because it is converting to WTF-8. This is also why you should stick to `OsString` and `PathBuf` when working with strings from the OS, and avoid converting to `String`/`str` unless you absolutely have to.
I did find a version on the benchmarks game, using arena allocation. Is this the only way to get good output for lookup and insertion? http://benchmarksgame.alioth.debian.org/u64q/program.php?test=binarytrees&amp;lang=rust&amp;id=1 
It's late, so all I'll chime in with right now is this: &gt; Is there a reference anywhere about exactly how Rust types map to representation/C types? Unless you use `#[repr(C)]`, no particular representation is guaranteed. Of course, they're obviously done _somehow_, but they're not guaranteed. This is to leave room for future optimizations. 
&gt; // Would hope Box&lt;Option&lt;Node&gt;&gt; should be just *Node where null = None `Option&lt;Box&lt;Node&gt;&gt;` works like that (Box is guaranteed to be non-zero, so None means null, Some(o) means pointer to boxed object). I think you see the extra dereference because of the `&amp;Tree` in your code, that is an extra indirection.
For two reasons: 1. Compile times would suffer dramatically. Seriously, implementing a single trait for thousands of types results in a huge amount of codegen. 2. You can already do `let foo: BAR = unsafe { mem::zeroed() };`.
True, but the representation of trait objects is sort of de facto stabilized, since stable code can and already does rely on it. Changing it would definitely break things, so it probably won't happen. 
I wouldn't currently know of any. I know of crates opting into raw and then using that structure (which would be strictly safer, as they will break on compilation, should it change).
hyper has depended on trait object repr since before Rust 1.0. This was done after talking to rustc devs acknowledging it would not change. 
I'm going to extend my lifetimes-in-statics PR (which got merged last week – yay!) with a proper feature gate. Also I benchmarked some things from the benchmarksgame and found that Rust can beat gcc handily in n-body with a single command line option. I don't know what, but I'll try to do something about that. On Friday I'll meet some of you at the Rust Regulars' table in Darmstadt (see my other post).
&gt; His hash function is trivial in the F# version, giving F# a huge advantage Then why are the results the same even if you use F#'s built-in hash function: open System.Collections.Generic type [&lt;Struct&gt;] P = val i : int val j : int new(i, j) = {i=i; j=j} let inline iterNeighbors f (p: P) = let i, j = p.i, p.j f(P(i-1, j)) f(P(i+1, j)) f(P(i, j-1)) f(P(i, j+1)) let rec nthLoop n (s1: HashSet&lt;_&gt;) (s2: HashSet&lt;_&gt;) = match n with | 0 -&gt; s1 | n -&gt; let s0 = HashSet() let add p = if not(s1.Contains p || s2.Contains p) then ignore(s0.Add p) Seq.iter (fun p -&gt; iterNeighbors add p) s1 nthLoop (n-1) s0 s1 let nth n p = nthLoop n (HashSet([p])) (HashSet()) (nth 2000 (P(0, 0))).Count &gt; This is also kinda of a test of malloc vs GC throughput, which while fair, is incredibly specific and won't be representative of most programs. Really? 
In which case I don't understand why it hasn't been stabilised, TBQH.
Ahh alright. That didn't even cross my mind as a problem since I'm rarely ever recompiling winapi in my project or creating fresh projects using winapi. It is pretty long; 60~ seconds on my machine.
The definition of a leak you're using here is one I'm with which I'm completely unfamiliar, whether in practice or academia. 
I'm really new to programming and know very little. Should I work on learning something else or does it matter? Is rust easy to learn once I get into another language?
Why spend time for something people won't accept as bug?
Maybe it could be checked when the implementation for that type is instantiated.
Yeah, I already learnt earlier that this transformation is impossible. I'd like something like: (Iterator&lt;Item = io::Result&lt;T&gt;&gt;, FnMut(Iterator&lt;Item = T&gt;) -&gt; B) -&gt; io::Result&lt;B&gt; So it would collect the iterator into a result but expose the `Ok`'s before the possible `Err` in another iterator to be consumed and collected. 
For me, the leap from being a "very new" programmer into one where I was actually productive came from discovering REPLs, and in particular DrRacket. It let me explore a programming language, try out expressions, and was relatively forgiving when I wrote code that didn't handle all cases. It also had nice tools for understanding why my programs behaved the way they did. Rust is not like that. It forces you to handle a lot of things "up front", and that was the kind of thing I found very frustrating as a beginning programmer. The tools for debugging are powerful but hard to use, and there's not a lot of exploratory programming you can do easily. The value that Rust brings to the table for me are rooted in my experience of systems programming and a firm understanding of the problems it helps avoid. That makes me more productive, because I fairly rarely bump into the rules of the language, and when I do, it's usually because of a bug I'd have introduced. Because of that, I think Rust isn't a great place to start programming. Once you're more experienced, moving between languages is relatively easy. I'm fairly confident in my own ability to use any relatively widely used programming language with a few weeks of training, at most. Of course, the best thing for me to learn new things is to have a concrete goal that I want to solve for a project. If you have something you want to do, I'm happy to try and make recommendations. I'm sure other people may disagree with me, and that's fine too. Everyone has different experiences, ways of doing things, and expectations.
You're right, looks I can make it more readable like this: fn parse_buffer&lt;B: BufRead&gt;(buffer: B) -&gt; Result&lt;Vec&lt;Benchmark&gt;&gt; { buffer.lines() .map(|r| r.map_err(Error::from)) .map(|r| r.map(|s| s.parse().ok())) .filter_map(lift_opt) .collect() }
Yes it does, you're right the readme and docs are very thin atm. Will improve that soon.
My favorite is a funny consequence of having a bootstrapping compiler. The Rust compiler is written in Rust. This includes the parser. [`syntax::parse::char_lit`](https://github.com/rust-lang/rust/blob/86995dc8c56b3c2923dff8a4bb79fed7c60a4396/src/libsyntax/parse/mod.rs#L294) is called to parse backslash-escapes. When `\` is followed by `n` in a string literal, this represents a newline. So this function returns a newline character. This function is written in Rust, and the way to write "a newline character" in Rust and `'\n'`. So there’s a `match` arm that looks like `'n' =&gt; Some('\n')`. Effectively, the definition of `\n` is `\n`. Nowhere in the current compiler source is it written that the byte for it is 0x0A, but that information still ends up in the compiled binaries.
On holiday and noticed my laptop didn't have `whois` so I wrote it in Rust. Still needs formatting work though. 
Ah! So because you first need to compile the new source code with the old compiler and the old has `\n` defined as 0x0A the new compiler inherits that definition. So it's a bit like mathematical induction, where some version had the definition and each compilation carries over the definition to the new version, the latest version still has it even though it's not in the source code anymore. That's pretty curious! Nice little fact!
I think its a bug that the HEAD of futures is only working on nightly right now, so it'll probably work on stable soon.. This code certainly doesn't depend on nightly. A future is essentially a lazy Result - it represents a state of success or error which is not known yet. This is actually exactly your use case. The main motivation for developing the library is to build a high performance async io platform (tokio), but the Future and Stream traits are more universal.
Production compilers aggressively reuse stack slots and `null` out unused ones based upon the results of liveness analysis before making calls in order to minimize floating garbage. I wouldn't be surprised if Python got this wrong because it didn't originally support either garbage collection or tail calls properly. I'd expect Java to do this though. Certainly F# and OCaml don't leak. 
I suppose the traits work well here. I just think it might suggest that something async is going on by using them, a potential point of confusion for others reading the code. I suppose converting to a stream and almost immediately collecting+waiting makes things fairly clear. As long as you know the futures library. Bleh, I'm nitpicking here really. I'll gladly use it once it works on stable. Thanks for pointing out the library, I wouldn't have thought of it myself :)
Turns out it stops at OCaml: https://github.com/ocaml/ocaml/blob/trunk/lex/lexer.mll#L41
Is there any tweening library with a nice rusty syntax? I found https://github.com/tilpner/tween.rs but it's old and discontinued it seems. 
&gt; There are updated benchmarking sites with consistent methodology; the most famous is http://benchmarksgame.alioth.debian.org/ Although the Bechmarks Game has been improving recently, by no means is it well controlled.
&gt; The only solution I can see that might make this Rust fast is to write a new hash set implementation that is optimised for fast hashes. Strange, because OP points out that my version was faster than F# and OCaml, just by changing the hash function. Obviously the only way to make a mostly *fair* benchmark is by using equivalent hash sets. &gt; None of these languages are pure functional. You're being pedantic, and it doesn't help. If you want to write pure functional code, you want F# and not Rust. &gt; Why does the Rust program and only the Rust program have to include `drop(s2)` to avoid a memory leak? It's hard to take this question seriously when you have answers already. If you write unidiomatic recursive code in a language that doesn't have tail recursion, your stack is going to grow. Using `drop` doesn't even fix the problem, since you're still "leaking" stack space. A lack of TCO is indeed a downside of Rust, but calling Rust leaky because you act like it's there is like sitting on the roof of a bungalow and complaining that you get wet. 
I'm proud to have, I think, the struct with the most template parameters in the ecosystem: https://github.com/tomaka/vulkano/blob/af1aa0383db4e1726b18ed3413e3a27498972f31/vulkano/src/pipeline/graphics_pipeline/mod.rs#L60-L61 Which leads to funky where clauses: https://github.com/tomaka/vulkano/blob/af1aa0383db4e1726b18ed3413e3a27498972f31/vulkano/src/pipeline/graphics_pipeline/mod.rs#L239-L260
I'm not sure if this is terribly beautiful or beautifully terrible. That function is definitely impressive though!
 |n| (2..n).all(|d| n%d &gt; 0) With this piece of code I won a code golf competition once. Not necessarily my favorite piece of code, but pretty sweet anyway :)
How readable the code is. What I mean is that Rust sits in that sweet spot of being verbose *enough* that almost all relevant details are *in* the code, but not *so* verbose that you can't see the bigger picture. Every time I've come back to code I haven't looked at in a while, or look at totally unfamiliar code, I'm *usually* surprised at how easy it is to get a handle on what's going on. The *big* things here are explicit borrowing, and constrained generics, allowing me to more or less skim code and quickly look up the pertinent details. Lack of implicit coercions helps a lot, too. On the other hand, *local* type inference helps limit noisy details. Not to say you *can't* write unreadable gibberish in Rust, of course. The lunatics who constantly abuse the hell out of macros are *particularly* guilty of this. There are also times where it'd be *really* nice to have a few extra "signpost" types scattered here and there in the middle of particularly complicated iterator chains.
Newb here, I have no idea what this does. Can you explain it? My guess is that it takes two numbers and outputs every number between 2 and n that is bigger than d and not a multiple of n. But that is probably wrong. 
If I'm reading it correctly, (and I'm not a rust hacker), it's a one-liner telling you if a number is a prime or not. The equivalent python, I believe, would be all(n % d &gt; 0 for d in range(2, n)) Example: for n in range(2, 14): is_prime = all(n % d &gt; 0 for d in range(2, n)) print('%i %s a prime' % (n, 'is' if is_prime else 'is not')) Output: 2 is a prime 3 is a prime 4 is not a prime 5 is a prime 6 is not a prime 7 is a prime 8 is not a prime 9 is not a prime 10 is not a prime 11 is a prime 12 is not a prime 13 is a prime Edit: equivalent C#. var n = 13; var isPrime = Enumerable.Range(2, n).All(d =&gt; n%d &gt; 0);
Yes. Updates are done at least once per week. Would be nice!
Never heard of all and any, but they look really neat. I really have to finish reading the book. Thanks for the explanation. 
Assuming you trusted the interpreter runtime.
Traits!
Thanks, I know :) In code golf it's all about short code, not about performance. Adding `.take_while(|d| d * d &lt; n)` would certainly improve performance a lot...
[removed]
The Error Messages, The Book, shepmasters SO post history. Popping into IRC to get a question answered in a decent amount of time. New nightly errors are sweet.
Distributed Cross Compile solves this, as well.
In Python, you don't need the `&gt; 0` part because nonzero ints are truthy.
Local type inference. Being able to just write Vec::new() and have everything work is so awesome!
Shure, but their 'knowledge' of the program that is executed is much more restricted than a compilers. It would be much harder to pull of.
I found some related links * https://bluishcoder.co.nz/2013/08/15/phantom_types_in_rust.html using [Phantom Types](http://rustbyexample.com/generics/phantom.html) to do taint checking of template strings. * https://www.reddit.com/r/rust/comments/39we2h/are_there_any_attempts_at_implementing_taint/ I was thinking of implementing a `LockBox&lt;T&gt;` that would have a `.unlock(id: ?)` method but it looks like Phantom Types would give compile-time safety. 
A second compiler wouldn't be as hard as the first one. If all you're trying to do is verify the lack of backdoors, you can throw out all of the error checking, borrow checking, and optimization.
Does anything like python's `shutil.copyfileobj` exist in the rust ecosystem? A function that'll will copy bytes from a `Read` into a `Write`?
This game was developed by 23 students during a programming practical I oversaw. In case you're interested, I wrote about a few thoughts we had about `rustfmt`, `glium` and other Rust related things [here](http://lukaskalbertodt.github.io/2016/09/01/developing-a-game-with-Rust-beginners-some-thoughts-on-tools-and-libraries.html). This also includes a link to [another, longer post about `rustfmt` and codestyle](http://lukaskalbertodt.github.io/2016/08/10/rustfmt-and-yet-another-opinion-on-code-style.html) I wrote before. I'm interested in any kind of feedback! :)
That's a really nice way of doing it! Do you know how cache friendly BTree is? 
Will this be further developed?
I guess I don't get it. The only potentially useful one is for type conversions, and there's already good ways to accomplish it. I feel like that post was mostly a solution in search of a problem, which is why it's under "fun" I suppose. Pretty neat mental exercise though.
From the Readme: &gt; Development will probably stop after the practical has ended. If there is enough interest in the game idea, the game is probably rewritten from scratch (the code in this repository often is far from optimal). Don't hesitate to make suggestions or file PRs, though! Just keep the status of this project in mind ... So: rather not. At least not without rewriting a big part first...
Also: `collect`/`FromIterator`: let hash_map = vec1.into_iter().zip(vec2).collect(); // voilà 
In this https://github.com/pingcap/tikv/pull/977/files#diff-33c81e227baf639277c9d52a8a171ba7R478 Wouldn't be better to let the compiler know the size of the boxed array statically? Like word_buf: Box&lt;[u32; WORD_BUF_LEN]&gt; I wouldn't be surprised if that generated noticeably better code.
macros
Cool. Nice work - the site is simple and easy to use. Please use encryption - you can get a free certificate with [LetsEncrypt](https://letsencrypt.org/getting-started/).
Not if you pass `hash_map` to function expecting a `HashMap`. E.g: https://is.gd/OJCk8S
I'd assume because it's a pain to work with large static arrays in rust right now, since there are no trait impls for them in std.
I agree that `as` is not great for numerical casts. Using `From` and `TryFrom` is much better. Something like `let i: u32 = !0u64.truncate();` is a nice idea. I might use that in the future.
That's true. I left it in to help readers understand how I moved the code around.
Another funny tidbit: Rust (the fungus) isn't composed of rust (the material)! Somebody'd better call the naming police.
I don't see why that would be the case. The interpreter has access to the whole code, just as a compiler does. The way to solve this, I think, is to write the code in a very simple language (core lisp, for example), so that just about anyone could write their own interpreter if they wanted to. The original trick doesn't work if there are *two* C compilers, which would be much more likely if they were easy to write.
I thought they were implemented for up to 32 or something (it's 9 in the code I pointed to).
So I re-read the hn link, and it seems you don't want the use of a nucleotide_hashmap extern library created for the benchmark, but a specialized hashtable was ok, I though it wasn't, so it would have been at odds with the Java code. I am not a native English speaker and didn't want to infer conspiracy ideas, whatever the word funny means to you. 
It's not easy to see why `id` is useful at first. It's common to initially question its usefulness in fact, but it does have practical use cases. There's some more discussion in the context of Haskell: http://stackoverflow.com/questions/3136338/uses-for-haskell-id-function http://stackoverflow.com/questions/25987085/what-do-people-use-the-identity-function-for I find I use it a lot as function parameters with higher-order functions to keep something from changing, constructing modifiers or accessors out of one core generic function and avoiding boilerplate.
Oh, well, um, yes. (I assumed it was larger)
The new nightly errors are a godsend. They were already good, but now with the nightly ones they're out of this world!
&gt; Is there a reference anywhere about exactly how Rust types map to representation/C types? As far as I know, the rules are: Structs are represented with their member variables all in a row, with appropriate padding to make sure everything is aligned properly. Members may be reordered to shrink padding. Enums are represented as a tag and a blob of data. The blob is equal in size to the largest variant's data, and is missing if there is no data associated with any variant. The tag is the smallest unsigned integral type that fits all needed discriminant values. As a special case, enums with two variants, where one has no data and the other has data including a non-nullable pointer such as `&amp;T`, `Box&lt;T&gt;`, or `NonZero&lt;T&gt;`, re-use that field as the tag, so anything with zero there is the empty variant, and anything with a nonzero value there is the other variant. So `Option&lt;Box&lt;T&gt;&gt;` is the same size as `Box&lt;T&gt;`. This goes transitively through substructures, so `Option&lt;Vec&lt;T&gt;&gt;` is also the same size as `Vec&lt;T&gt;`. [Examples](https://play.rust-lang.org/?code=%23!%5Ballow(dead_code\)%5D%20%2F%2F%20STFU%2C%20I%20know%0A%0Amacro_rules!%20sizes%20%7B%0A%20%20%20%20(%24(%24t%3Aty\)%2C%2B\)%20%3D%3E%20%7B%0A%20%20%20%20%20%20%20%20%24(%0A%20%20%20%20%20%20%20%20%20%20%20%20println!(%22%7B%7D%20has%20size%20%7B%7D%22%2C%20stringify!(%24t\)%2C%20std%3A%3Amem%3A%3Asize_of%3A%3A%3C%24t%3E(\)\)%3B%0A%20%20%20%20%20%20%20%20\)%2B%0A%20%20%20%20%7D%0A%7D%0A%0A%2F%2F%208%20bytes%20if%20padding%20is%20removed%20by%20reordering%20fields%2C%2012%20bytes%20otherwise%0A%2F%2F%20Looks%20like%20it%20isn%27t%20here%2C%20but%20don%27t%20rely%20on%20that%20without%20%23%5Brepr(C\)%5D%0Astruct%20PaddingTest%20%7B%0A%20%20%20%20a%3A%20u16%2C%0A%20%20%20%20b%3A%20u32%2C%0A%20%20%20%20c%3A%20u16%0A%7D%0A%0A%2F%2F%20a%20really%20fancy%20bool%0Aenum%20SmallDiscriminants%20%7B%0A%20%20%20%20A%20%3D%200%2C%0A%20%20%20%20B%20%3D%201%2C%0A%7D%0A%0A%2F%2F%202-byte%20discriminant%20needed%2C%20would%20also%20be%20needed%20if%20it%20you%20left%20off%20the%0A%2F%2F%20specified%20values%20but%20had%20more%20than%20256%20different%20variants%0Aenum%20LargeDiscriminants%20%7B%0A%20%20%20%20C%20%3D%200xFF%2C%0A%20%20%20%20D%20%3D%200xFFFF%2C%0A%7D%0A%0A%2F%2F%202%20bytes%20of%20data%2C%201%20byte%20of%20tag%2C%201%20byte%20of%20padding%0Aenum%20EnumWithData%20%7B%0A%20%20%20%20E(u8\)%2C%0A%20%20%20%20F(u16\)%2C%0A%7D%0A%0Afn%20main(\)%20%7B%0A%20%20%20%20sizes!(%0A%20%20%20%20%20%20%20%20Option%3CVec%3Cu32%3E%3E%2C%0A%20%20%20%20%20%20%20%20Vec%3Cu32%3E%2C%0A%20%20%20%20%20%20%20%20Option%3CBox%3Cu32%3E%3E%2C%0A%20%20%20%20%20%20%20%20Box%3Cu32%3E%2C%0A%20%20%20%20%20%20%20%20Option%3Cu64%3E%2C%0A%20%20%20%20%20%20%20%20Option%3C%26u8%3E%2C%0A%20%20%20%20%20%20%20%20Option%3C*const%20u8%3E%2C%0A%20%20%20%20%20%20%20%20PaddingTest%2C%0A%20%20%20%20%20%20%20%20SmallDiscriminants%2C%0A%20%20%20%20%20%20%20%20LargeDiscriminants%2C%0A%20%20%20%20%20%20%20%20EnumWithData%2C%0A%20%20%20%20%20%20%20%20%5BEnumWithData%3B%2010%5D%0A%20%20%20%20\)%3B%0A%7D&amp;version=stable&amp;backtrace=0 ) ---- So for your specific case, `Option&lt;Box&lt;Node&gt;&gt;` is exactly what you want. It has the same bytewise representation as `Node*` would in C, with `NULL` represented as `None`. Then your function `lookup` gets a pointer to a `Tree`, which is itself a pointer to a `Node`. You can remove one of your levels of indirection by directly passing a `&amp;Node`, since you can easily get a `&amp;Node` from a `Box&lt;Option&lt;Node&gt;&gt;`.
And to add to the pile: Paul Rust (the comedian) was not born in Rust (the Austrian city)[.](https://en.wikipedia.org/wiki/Rust_(disambiguation\) "I cheated")
Interesting. I'm that case, wouldn't a closure be just as easy?
Interesting resource! I was considering setting up a site to demo a project I'm working on, and this seems like the perfect fit!
Time to hit reverse: Rust, the 2010 film, was not filmed in the Austrian city Rust.
I get mildly annoyed that I have to `use` the trait in order to use it. It's not a big deal, but if the compiler knows where it is enough to tell me how to import it, why can't it just do it for me? Other than that, traits are pretty cool, and I think I might prefer them to Go interfaces (though Go interfaces are pretty sweet too).
This isn't my area and I'm sure someone can give you a better answer about the the current state of things. From what I know, there was a company putting significant effort into a framework called [leaf](http://autumnai.com/) but they decided [tensorflow would win](https://medium.com/@mjhirn/tensorflow-wins-89b78b29aafb#.g14u289iz) and stopped working on it in May.
There is this summary: http://www.arewelearningyet.com/
- Type system (+ traits) - Error handling (Result + try!) - Variables (bindings) immutable by default - cargo + crates.io - Writing tests - Writing documentation
&gt; It's not a big deal, but if the compiler knows where it is enough to tell me how to import it, why can't it just do it for me? Naming conflicts, for one.
Enums and `Option`. Taken together, they enable me to describe data formats in a much more elegant way than most languages; this is especially useful when writing fn signatures. Other than performance, this is the only Rust feature that I really miss when using scripting languages. In my experience, it's rare for a typesystem feature to make statically-typed code *more* concise than the equivalent dynamically-typed code; it's rare for it to make me feel empowered rather than constrained.
Whether or not Java's escape analysis successfully dodges this issue all the time (it doesn't, by the way) is beside the point. Your definition of a memory leak is flawed, and is not one I've seen from anyone else, FP enthusiast or not.
A package exposes a trait called `Book` also has a `read` method. You import it. Now, when you want to call `read` on something... do you get `io::Read`, or `Book`? We could in theory say if there's only one in scope, just use it, and if there's two, make you import it, but Rust prefers explicitness, generally.
Exactly. There are public wifi hotspots that actively inject advertisements into the users browsers. It will also add privacy for the user. There are [several other reasons, too](https://scotthelme.co.uk/still-think-you-dont-need-https/).
Working around reborrows seems useful to me. YMMV
There are so many `arewe[a-z]+yet\.com` that I almost trust [this](http://arewedoneyet.com/)
[serde_wat](https://github.com/mgoszcz2/serde_wat) uses macros in the `build.rs` to print macro definitions into a source file, to be `include!`'d in the `lib.rs`...
It would be interesting to consider an error only on conflict... Though on the other hand the trait error leads me to the trait documentation (which isn't always easy to know just based on the method name). 
And Rust, the Austrian city, was not home to Paul Rust, the comedian. Edit: my reading comprehension needs some work...
Looks kind of like a periodic table of elements to me
Right, but you can always look it up on the documentation for the type. I've found that part of the documentation particularly useful (and another reason I prefer Rust traits to Go interfaces). I think conflicts are rare enough that it's more surprising to have to be explicit here. Why do I need to `use std::io::Read` to use an `std::net::TcpSocket` when I don't use `Read` anywhere in my source? It's just a surprising piece of syntax IMHO, especially coming from languages with interfaces like Go or Java.
I honestly can't think of a case where there could be a conflict. For example: use library::Fantasy; use std::net::TcpStream; let book = Fantasy::new(); let stream = TcpStream::connect(...).unwrap(); book.read(...); stream.read(...); Both know what type they are, so the compiler should know which trait they're referring to. If I were using UFCS, then I have to explicitly use the type. I suppose if a type had the same fn name implemented for two different traits it could be ambiguous, but I imagine that's quite rare and needs to be handled special today anyway (the recommendation I've read is to use UFCS). Perhaps if I saw an example of ambiguous usage I would understand. In any case, I find this quite surprising, especially since I don't actually have `Read` anywhere in my code, so it looks like an unused import.
&gt; Both know what type they are, so the compiler should know which trait they're referring to. This example is different from what I'm talking about, it's the &gt; I suppose if a type had the same fn name implemented for two different traits it could be ambiguous, case. &gt; I imagine that's quite rare Right, but that means that it's suddenly surprising, because it is rare: now something starts acting like something else all of a sudden, and you may not even notice at first. &gt; and needs to be handled special today anyway Right, you _can_ bring both into scope, and then use UFCS to disambiguate. (And, UFCS didn't always exist...) It's all about tradeoffs; as I mentioned above, we could probably make what you're talking about work. But it would add more complexity, at the cost of not needing to type one `use`. Language design is all about choosing which tradeoffs you want. And that means others might prefer different tradeoffs :) &gt; it looks like an unused import. Rust will warn about unused imports, so you don't need to worry about that.
I think this might be the hackiest thing we do in Diesel. We bhave to parse struct bodies in our macros, and for tuple structs we end up giving back the same "field type" over and over again in order to use it as an ident and abuse macro hygiene. https://github.com/diesel-rs/diesel/blob/master/diesel/src/macros/parse.rs#L249-L271 When we use it on [these lines](https://github.com/diesel-rs/diesel/blob/master/diesel/src/macros/queryable.rs#L113-L114) We basically end up generating code that looks like: let (bare, bare, bare, bare) = row; Foo(bare, bare, bare, bare) but because of hygiene each occurrence of the ident `bare` is considered a different variable.
Why does your `new` function return a `Gpw&lt;T&gt;` instead of `Gpw&lt;English&gt;`?
Did you submit the hash change to glium? Looks like this issue was open about it: https://github.com/tomaka/glium/issues/946
&gt; Java's escape analysis This has nothing to do with escape analysis. &gt; Your definition of a memory leak is flawed How so? 
At this point it's probably cargo. I can get used to a lot of pl things, but I can't deal with dealing with dependencies manually anymore. Parametric polymorphism is super rad too though.
https://github.com/rust-lang/rust/pull/34096 it's in nightly as default, so I consider the answer to be a yes. I remember an announcement that the switch to disable MIR was now removed from the latest nightlies, but I cannot find it anywhere.
&gt;You get the same results even if you use F#'s built-in generic hash function: Your post certainly doesn't show that. &gt;changing the hash function only in the Rust program Do you actually believe the stuff you type? &gt;Where the OCaml and F# programs have been subjectively "de-optimised" by the author of the site, Isaac Guoy. Of course they were.
This is so cool. I've been thinking about `no_std`, zero-allocation buffers for intermezzOS; definitely gonna check this out.
[I'm using it](https://github.com/serprex/rg)
I'm still not sure of the design of my kernel overall, but was considering what to do for interrupt handlers. I was thinking about having them place an Event enum on a ring buffer that the main kernel polls for events. I have no idea if this is a old idea or not; I haven't had much time to study OS stuff lately :/ I don't have a kernel heap yet, so just wrote up a quick buffer around arrays.
"Funny tidbit: Rust (the game) is not even made with Rust (the language)!" Not particularly funny, I think, but also didn't deserve the amount of downvotes he got. I wonder if I'm missing something.
https://wiki.mozilla.org/Areweyet ← Someone with an account, add it there!
What's your `cargo --version`?
This is both beautiful and sickening.
I'm guessing using "real" engines like Unreal is currently blocked by the missing C++ FFI in Rust? Or would it be practical to write a bridge? (Pretty new to Rust, sorry if that's a stupid question. :))
Oh, right. good call.
/u/tomaka17's is certainly longer, although I'd argue that typenum's may be as ugly: https://github.com/paholg/typenum/blob/18795fdbd071de92c853f85cc61bdabbce0f1592/src/uint.rs#L1162-L1178 [Dimensioned](https://github.com/paholg/dimensioned) creates structs with as many type parameters as a user wants, but it's all hidden behind a macro, so no one gets to truly revel in its full hideousness.
&gt; Personally I'm more interested in getting Rust up and running for HPC and super-computing. Not quite the same field, but I hear that Rust is quite popular in the high-performance trading industry. Of course, HPT being what it is means it's hard to find any evidence. I heard from a guy that heard from a guy (though both are generally reliable sources).
On phone, so I'm just gonna guess. Instead of `Some(Box::new(*node))` make a new node.
If you're not comfortable solving problems with another language, I wouldn't recommend picking up Rust quite yet. How easy Rust is to learn is pretty variable, but generally it's not the easiest language to pick up and learn. It has a number of concepts that are sometimes difficult for experienced programmers to learn, so if you're also still figuring out how to effectively write programs on top of that, you're going to have a difficult time. I'd recommend starting with a language like Python or Ruby. Something a bit more... gentle in terms of letting you make mistakes. That way you can focus on your skills as a programmer, without having to learn a ton of extra stuff about memory management and pointers at the same time.
Won't that allocate a new node, ie copy the whole tree?
OK
It's not as cache friendly as it could be, but it turns out that's not the dominating performance factor for a good in-memory btree. Wasting some cache by including parent pointers (and needing to descend into "unimportant" children to fix their parent pointers) ends up being worth it to avoid paying the cost of maintaining a stack for traversals. All in all the current implementation is competitive with using Rust's HashMap, often beating it.
Someone needs to make an areweareweyet.com
Ah, but I mean, I'd have to create a new node at every level of the recursion, so it'd end up copying the whole tree
I have a question about borrows and mutability. In my code below I have a struct that owns a File type and I want to be able to write to the file. I have read that I want to use a BufWriter to avoid incurring the cost of system calls each time I write to the file so I'd like to have my type own a BufWriter&lt;File&gt; instead of just a file. If I borrow the File as immutable and bind to a mutable variable I can write to it just fine, but if I perform the same operations on the BufWriter I get a compiler error. I am curious why this suceeds for the &amp;File but not for the &amp;BufWriter use std::io; use std::io::{BufWriter, Write}; use std::fs::{File, OpenOptions}; use std::error::Error; struct BufWriteTestType { file: BufWriter&lt;File&gt; } struct FileTestType { file: File } impl BufWriteTestType { fn get_file(&amp;self) -&gt; &amp;BufWriter&lt;File&gt; { &amp;self.file } } impl FileTestType { fn get_file(&amp;self) -&gt; &amp;File { &amp;self.file } } fn main() { let file = OpenOptions::new() .create(true) .append(true) .open("test_file") .expect("failure opening file"); let file2 = OpenOptions::new() .create(true) .append(true) .open("test_file2") .expect("failure opening file2"); let t = BufWriteTestType { file: BufWriter::new(file) }; let t2 = FileTestType { file: file2 }; let mut result = t.get_file(); let mut result2 = t2.get_file(); // This call will fail with "cannot borrow immutable borrowed content `*result` result.write("hello".as_bytes()).expect("Error writing"); // However this one suceeds but should also be an immutable borrow result2.write("hello2".as_bytes()).expect("Error writing 2"); }
That you can write a kernel in it. So many languages has taken the easy GC way out; but that throws some (for me) very relevant use cases out with it, namely those which require deterministic worst-case latency and memory usage. That, and that things are so well documented across the ecosystem. Rustdoc helps with an enormous amount here - when skimming a new crate, it's rule rather than exception that there is API documentation (and with docs.rs that's improving even further!). In addition, people have, in general, bothered commenting their functions, and add examples, Readme files, and so on.
More or less all syntex users have that (as an option) to use syntax extensions on stable.
You would only recreate the path that the new node lies on. So the runtime the same. And RVO might make "recreating" cheaper than you'd think.
&gt; why this suceeds for the &amp;File but not for the &amp;BufWriter It succeeds because there is an [`impl&lt;'a&gt; Write for &amp;'a File`](https://doc.rust-lang.org/src/std/up/src/libstd/fs.rs.html#368). There is no similar implementation for `&amp;BufWriter`. The implication is that the underlying implementation of `File` is correctly synchronized on each supported OS and it is safe to have many concurrent writers the same way you may have many concurrent `&amp;T` borrows. However there is no such synchronization guarantee around the buffer used by `BufWriter`. Edit: This is the same pattern you have with the `Cell` types and the concept of "interior mutability". You can technically mutate through immutable references, but you should only do so with care, with proper synchronization, and ideally when the effects of mutation are not observable. The common example here is memoization: filling a cache of results based on some calculation, which is totally fine because on any given call the result is the same whether you get it from cache or recalculate it.
Note for people who didn't get the "wat" part: it appears to be a reference to [this hilarious talk](https://www.destroyallsoftware.com/talks/wat).
I'm super excited to be teaching this class again. Today is actually going to be our second lecture. [Last semester's class website](https://cis198-2016s.github.io/) has all of our old slides and homeworks, and the final projects that our students did. I've been doing a lot of general editing on the course material, so hopefully this semester will be a nice improvement all around. Everything about this class is hosted on GitHub, including the homeworks! GitHub Classroom lets us make private homework repos for students to use. The creation links for these are all public (posted with the assignment) if anyone wants to follow along. This is one of my favorite things about the class -- it's awesome seeing other people participate! The 19x series of classes are half credit classes (most classes are 1.0 credits) that are generally taught by students instead of professors. Last semester I co-taught CIS 198 with /u/kainino0x and /u/thedmal. Both of them ~~are jerks and~~ graduated from Penn last May, so I'm on my own this time. ...I'm honestly a little terrified. I feel way underqualified to be actually teaching anything.
Any logic encoded in the type system is beautiful by its very nature. :)
If you really want cache friendly, you may have to drop down to a special purpose data structure. For example, in this case, you could use a ~~128B (~2 cache lines)~~ *(edit) [u32;128] (~512B, 8 cache lines)* table for ASCII (assuming mostly western text) and a `BTreeMap` for everything else. However, that's probably overkill for most cases...
Could this be made to integrate with standard logging macros like debug!()? That would be awesome!
&gt; Attempt #6 The reason why you haven't seen a parametrized struct might be because there is usually no point in doing it. It does not give any additional capabilities, serving only to force users to add unnecessary constraints. As an example, if you wanted to implement `Debug` on the buffer. With constraints on the struct impl&lt;T : AsMut&lt;[u8]&gt; + Debug&gt; Debug for LogBuffer&lt;T&gt; { ... } without impl&lt;T : Debug&gt; Debug for LogBuffer&lt;T&gt; { ... } (The only reason to actually use a constraint on a struct is if the type has associated types that you need, but even in that case I have found that to be problematic. Usually adding additional parameters is the better choice in the long run.) struct Struct&lt;T: Iterator&gt; { iter: T, value: T::Item } vs struct Struct&lt;T, I&gt; { iter: T, value: I }
Ah, that too. But still - is my assumption correct? Since all you need to implement for `fmt::Write` is `write_str(&amp;mut self, &amp;str)`, it kind of follows from that, that there is no way the other functions could create non-UTF8 data...?
Didn't know about that post \*\_\* thank you!
Thanks for mentioning [ZoC](https://github.com/ozkriff/zoc)! :) Oh, I really need to improve docs and UI, but want to solidify basic game mechanics first. The goal of the game is to get Victory Points by controlling Sectors (groups of hexes with chess-board texture on them). You do this by selecting units and giving them orders like 'move', 'attack', etc. Gameplay video: https://www.youtube.com/watch?v=WJHkuWwAb7A (actually this is smoke system demonstration, but it has all the gameplay elements in it)
Yes, all you have to do is [initalise a logger](https://doc.rust-lang.org/log/log/index.html#use-with-no_std) based on this crate. :-)
It's tracked in [issue 27751](https://github.com/rust-lang/rust/issues/27751): We expect that this will be insufficient in the future, particularly if/when we support trait objects like Trait1+Trait2 (which would require 2 vtables). We could stabilize it, since it would remain correct for all existing objects, but it'd be a shame. In contrast, the various "custom DST" proposals would allow for a much more flexible way of extracting and creating this data (sort of like from_raw_parts), so we think we'd prefer to explore that route.
Oh, it never occurred to me that traits have TypeIds too. (At least the object safe ones have it...?) And you use it to work around the lack of built-in RTTI in Rust. Nice! 
Wow, that's cool! I wish my uni did a Rust course. Maybe I have to ask the correct persons ;-) No need to be afraid, I'm sure you'll be doing well! One small typo in the schedule: You mentioned Rust 1.13 twice, 12/22 should be Rust 1.14.
&gt; I think conflicts are rare enough that it's more surprising to have to be explicit here. You might get conflicts in the future by changes in your used crates. Being explicit about your trait imports makes your software just a bit more stable against breakage.
Call it zealotry, but in this case, what I said above is true: you are relying on a structure that will not be stabilised because it may change, even if the layouts these projects describe may be future-compatible. Given the number of projects that hack around this, it might be worthwhile to invest efforts on that route.
[removed]
rust noob trying to figure out what all of this actually means.. I'm wondering why you need the unsafe annotation for taking a mutable reference? it's a mutable array, why is it necessary? Doesn't it break requirement 2?
&gt; I cannot read anything by the OP. That was basically what was said, and I did give source code. &gt; You applied an algorithmic optimisation only to the Rust program. F#'s hash function was i + 4000 * j Rust's, before the change, was let mut hash = 0xcbf29ce484222325; hash = (hash ^ ((i &gt;&gt; 0) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((i &gt;&gt; 8) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((i &gt;&gt; 16) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((i &gt;&gt; 24) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 0) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 8) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 16) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 24) as u8 as u64)).wrapping_mul(0x100000001b3); Rust's, after the change, was (taking a guess at which one was used) (i &lt;&lt; 10) ^ j which one seems like a fairer comparison to you? &gt; I don't see why I shouldn't be able to choose in Rust just as I choose in other impure languages like F#. Rust isn't for you, I get that. &gt; Stack space isn't the problem. Given the asymptotic complexity of the algorithm the sun will burn out before the stack overflows. The problem is the huge data structures on the heap leaked with every single stack frame. But stack usage *is* the problem. Without TCO or some derivative thereof, any value left on the stack will not and can not be removed. This applies **even** in a language with GC, because those items on the stack are rooted so cannot be collected. The only thing this measures is whether you have TCO, and whether this results in running out of stack or heap space first is a measure of uninteresting constants, not leakiness.
Hello, I'm very new to Rust and am having trouble using multiple files. No matter how many examples I look at, I can't make my code work. Here's an outline of what I have: main.rs mod gb; (imports) fn main() { (variables) gb::get_info(file_buf.as_mut_slice()); ... } gb/get_info.rs (imports) pub fn get_info (file_buf: &amp;mut Vec&lt;u8&gt;) { (code) } gb/mod.rs mod get_info; Thanks in advance!
Note that while the structure for "multiple trait" trait objects has not been decided, it won't break existing code using this crate since existing code can't be using them. The representation of "single trait" trait objects is almost certainly not going to change in a breaking way (since any larger form would be less efficient, rust favours zero-cost abstractions, and I don't actually rely on anything other than the first pointer being a data pointer), so there's no real danger to relying on that.
ok, I'll add it, thanks
Ah yes, that's true, but that's not what I meant; even if I just added the constraint on the impl I would still have effectively stored an AsRef&lt;[u8]&gt; (since that's the only thing I know about T) inside LogBuffer, and I haven't seen that before.
I almost want to send a PR to OCaml to remove the number there.
If you could safely take a mutable reference to a global mutable array, you could create a race condition in a multithreaded environment, which in Rust is unsafe. Anyway there is no real need to use that particular piece of unsafe code, since `static mut l: LogBuffer&lt;[u8; 256]&gt; = LogBuffer::new([0; 256]);` would work just as well with the AsMut constraint.
The function `get_info` lives in the get_info module, so it's absolute path is `gb::get_info::get_info`. However the get_info module is not declared as public (with the pub keyword), so you won't be able to access it anywhere above the gb module. You probably want to reexport the get_info function from there though, so inside the gb module you write `pub use self::get_info::get_info;` The pub makes the imported symbol available (reexported) from outside the gb module. The self is necessary because absolute paths are the default.
Austria != Australia
Austria != Australia
&gt; &gt; You applied an algorithmic optimisation only to the Rust program. &gt; &gt; which one seems like a fairer comparison to you? Using the same hash functions across all languages is completely fair. I did it myself. It doesn't change the results. Rust was still not faster. You even [did it yourself](https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/d1s2tpp) and posted your own measurements that showed Rust slowing down from 2.6s to 3.6s! Correct me if I am wrong but what you are actually doing is applying algorithmic optimisations in the form of calls to `reserve` to the Rust program **and only the Rust program** in order to conclude that Rust is faster, which is entirely disingenious. Right? &gt; &gt; I don't see why I shouldn't be able to choose in Rust just as I choose in other impure languages like F#. &gt; &gt; Rust isn't for you, I get that. Ad hominem. &gt; But stack usage is the problem. Without TCO or some derivative thereof, any value left on the stack will not and can not be removed. This applies even in a language with GC, because those items on the stack are rooted so cannot be collected. The only thing this measures is whether you have TCO, and whether this results in running out of stack or heap space first is a measure of uninteresting constants, not leakiness. All references to the collection `s2` are dead by the time the tail call is made. You don't need TCO to collect `s2`, you just need liveness analysis. Plenty of languages without TCO will collect `s2`. The leak in Rust is caused by being scope-based and has nothing to do with TCO. 
Thank you for posting this, super helpful for us that are new to Rust &amp; programming!
working on it :)
I think it's pretty cool that you're doing this even though you're unsure of yourself. 
I opened a glium PR [here](https://github.com/tomaka/glium/pull/1517)
Regarding the integer conversions, I drafted up a small example introducing `.widen()` and `.truncate()` [here](https://is.gd/ZXM75H)
&gt; If I want to use WebSocket for some of the connections and not to reimplement the protocol or modify the existing libraries, the options are rust-websocket or ws-rs. I'm still relatively new to rust, but I'm working on implementing a Rustic API to a network protocol-- which includes a parser. If I had to guess, this is the current problem. My understanding is that, ideally, we have protocol parsers that just [deal with bytes](https://www.reddit.com/r/rust/comments/4wobls/network_protocols_sans_io_are_the_future/)-- basically just returning their current state-- possibly returning how many bytes were used, how many bytes were unused, which bytes were unused, or some combination thereof (e.g: similar to what nom parsers do). That way, the same parser can be used whether you using the blocking API, non-blocking API, futures, tokio, or something else entirely. Alternative APIs to the standard blocking ones are still relatively new-- the post I linked was only posted a month ago-- as was the futures and tokio announcements. While mio has been around a bit longer, its API is still in flux-- designing clients/servers independent of the underlying socket API seemed rare (and difficult). With tokio, and the current direction of sans-io parsers, your goal should get easier-- but you seem to have encountered one of the areas that's still a big work in progress.
The thing is that authors of OCaml had no way to predict that a new language (Rust) will be developed and without that knowledge, they would have to basically code AI which can understand the purpose of the code and inject backdoor. And if they could write such advanced AI, they would probably use it for something else.
Thank you! I had missed the impl&lt;'a&gt; Write for &amp;'a File in the documentation.
This is really great, thanks for sharing! There is a lot I can learn from this repository. You should be very proud of your students!
If CloudFront happens within Amazon's network I would stick with that. With CloudFlare, the CDN servers still make an insecure HTTP connection to your servers, so a MitM is still possible at that point.
Not performance, but I'm still looking for a Rust framework. I want support for Windows, ATI GPUs, easy compilation, etc.
Really excited to see this happening again!
https://github.com/shadowmint/ue4-static-plugin/
You can send cache invalidations by URL path to the CloudFront distributions. [Documentation link](http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html) 
I don't really agree because I can't read Rust code I wrote a few weeks ago. //if two liberties read ladder //returns None if can't capture pub fn capture_ladder(&amp;self, group: &amp;Chain) -&gt; Option&lt;Move&gt; { let player = group.color().opposite(); if group.liberties().len() &gt; 2 { return None; } if group.liberties().len() == 1 { let liberty = group.liberties().iter().next().unwrap(); let m = Play(player, liberty.col, liberty.row); return Some(m); } let mut liberties = group.liberties().iter(); let liberty1 = liberties.next().unwrap(); let liberty2 = liberties.next().unwrap(); //if one move gives more than 3 liberties, forget about reading out the other move, it won't work let decide = |liberty: Coord, other_liberty: Coord| -&gt; Option&lt;Move&gt; { let other_liberty_move = Play(group.color(), other_liberty.col, other_liberty.row); if !self.new_chain_liberties_greater_than(other_liberty_move, 3) { let m = Play(player, liberty.col, liberty.row); let mut cloned = self.clone(); if self.next_player() != player { cloned.play_legal_move(Pass(self.next_player())); } cloned.try_capture(group, m) } else { None } }; let cap = decide(*liberty1, *liberty2); if cap.is_some() { return cap; } //do the same test for the other liberty let cap2 = decide(*liberty2, *liberty1); if cap2.is_some() { return cap2; } None } How horrifying.
&gt; Yeah, even if we did have a competitive and feature robust machine learning framework in Rust. We'd still have to compete with Google's support and documentation. Never mind the existing market share of C++. One of the biggest problem here is sponsors/investors. Such a large endeavour needs funding and once Google enters a market, basically all companies get their funding cut because of fear. I had that problem with a company building video conferencing software and Google released Hangouts in between.
Scalability: you do not need many thread to handle lots of requests, see [this SO question](http://stackoverflow.com/questions/8546273/is-non-blocking-i-o-really-faster-than-multi-threaded-blocking-i-o-how). 
See my other reply above :). Not much content, though. I refrained from adding some text because I wasn't sure I could add anything useful.
Have you considered going 1.0? No real harm in doing so, you can still go 2.0 if you want to make breaking changes, but it will communicate to the users of your package better. 
Summaries and opinioned recommendations for specific crates would be very nice. Haskell has nice resources in this regard with https://github.com/Gabriel439/post-rfc/blob/master/sotu.md and http://haskelliseasy.readthedocs.io/en/latest/.