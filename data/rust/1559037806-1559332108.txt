Well, if you enable the last 2 verbosity levels, the performance might be slower. By default (with #3 verbosity level), the program works fast enough.
Thanks mate.
Yes, I completely forgot, added as issue: [https://github.com/jesusprubio/leg/issues/5](https://github.com/jesusprubio/leg/issues/5)
What should be the starting point in learning #rust language? Where to find some good free learning materials?
what is your opinion on generics of both the crates?
That's right. \`nalgebra\` does require lengths to be known at compile time which makes it too strict for certain problems/use cases. \`nalgebra\` have generics available though too primitive for now but \`ndarra\` doesn't seem to have.
If your are new to Rust, do not use \`mrustc\`: * [**https://www.reddit.com/r/rust/comments/btuj8v/rust\_toolchain/ep337pr**](https://www.reddit.com/r/rust/comments/btuj8v/rust_toolchain/ep337pr?utm_source=share&amp;utm_medium=web2x) * [**https://www.reddit.com/r/rust/comments/btuj8v/rust\_toolchain/ep4ctvs**](https://www.reddit.com/r/rust/comments/btuj8v/rust_toolchain/ep4ctvs?utm_source=share&amp;utm_medium=web2x)
Foo/bar/baz are just more pronounceable and memorable than single letters. It's not like foo/bar/baz are any more "meaningful" than X/Y/Z. It's the same reason cryptography uses Alice and Bob for communicating parties, rather than A and B (although some are actually mnemonics, like Eve for eavesdropper). In this particular case, they just sound too much like real words, making it confusing to say "The Unfooable Barness of Baz" because it actually sounds like a real title even for those who know that foo/bar/baz are typically used for arbitrary variables.
My experience with hpc workloads mirror yours. It's rarely worth it to try to rebalance your work; the overhead and especially cache performance negates any advantage. The few workloads I know that do rebalance well (Namd comes to mind) do it at much longer timescales (seconds to minutes) and only across nodes rather than within a node.
I don't understand why we need the website translated. First, I have never met a programmer who doesn't speak English. And more importantly, all the learning material for Rust is in English, as well as GitHub, stackoverflow and internet forums. Someone who wants to learn Rust is completely screwed if he doesn't understand English, and a translated website won't change that. Almost all other programming language communities have only an English website.
I agree. a huge part of this is that people get bored of projects. I am also the same and have been bored out of projects. Its is important to create an environment where people are able to come up to speed on projects quickly and are able to solve issues of the project. There are a lot of programmers who are interested in contributing to open source but are not able to contribute due to project complexity.
Linux is numa aware but it needs to be configured. Pinning isn't great either as you can get heat issues if you stick your workload to the same cores.
/r/rustjerk
Slices and Chains are very powerful features of Rust, it took me some time to get into this way of thinking. Please checkout [https://www.reddit.com/r/rust/comments/bkskdl/chaining\_primitive\_bytearrays\_in\_rust/](https://www.reddit.com/r/rust/comments/bkskdl/chaining_primitive_bytearrays_in_rust/)
This looks nice, thanks for sharing. One thing about the API that stands out is that each call to \`info\`/\`success\`/etc seems a bit verbose. Perhaps you could use the builder method to build up a \`logger\` one time, then just \`[logger.info](https://logger.info)("my log")\`. Or if you think users will want to change the settings for each print, you can offer a builder like syntax for each log type. Also, I would change the \`ln\` (line) argument from an option to a custom enum.
I think it's earlier than that. I'm pretty sure I first got involved at 0.6, that was definitely self-hosted. I have a feeling that 0.1 is either the OCaml compiler or the first self-hosting one.
Why cat? `&lt; out.txt grep Hi`
I personally reported the JSON output was broken. The solution from the team was to remove the feature. I am currently trying to have a usable XML output but it is not usable yet.
Mrustc was built to build the Rust compiler, it's not meant to compile anything else. Rustc is the only Rust compiler you can use.
This allocates 2 extra vectors
Yes but they are unnecessary
My view is that your suggestion is a good one, and probably simplifies things a *great deal* down the line when we might want to go from “Meters per Second” to “Centimetres per Day.” Even more so if we want something like “Grams per Square Meter per Kitten”: `Frac&lt;Frac&lt;Grams, MeterSq&gt;, Kitten&gt;&gt;` In OP’s approach, we would need to explicitly name each possible unit we might want.
Did you... forget to take your meds this morning?
no even that comment wont stop destruction of JS, you LOSERS
Of course Mozilla want to silence you, your are obliviously propagating hetertic idea. Unless it is the Russian hackers who want other countries to use inferior technologies that try to make you believe that Mozilla (one of the companies working on wasm) is the responsible of your misfortune.
Sure, I wanted to avoid it to keep it as simple as possible. But you're right, probably I need the change if more parameters are needed.
I think it's a personal taste – I have never had a problem with semantic move not always implying memmove, as I've always assumed the compiler can do anything with my code if it keeps the output the same. I think I would have less problem with "transferring ownership" phrase if it was used more specifically. For example, I think in the book we talk about "transferring ownership of a `String`". I prefer to think that the small String \_struct\_ itself is moved, but this move's implementation detail is that the ownership of the buffer is transferred without copying. But of course, that's more wordy and less beginner friendly. So we need to simplify a little bit. So we can either say the whole String is moved or that the ownership of the String is transferred, neither of which describes precisely what's happening. Of these two simplifications I prefer to simply say that the String is moved, with an asterisk saying that "moves in Rust are usually cheap".
I've been working on a small dynamically-typed scripting language called hana with [an implementation in Rust/C](https://github.com/ffwff/hana). The core language has been implemented with prototype-based OOP, arrays, first-class functions and a working garbage collector. It is fairly stable and I'm currently improving the language and working on a standard library. Hopefully I'll have some documentation and easier-to-use Rust interface soon.
That’s a good point. You don’t need the vector if you just assign to the mutable array in a loop.
BS, you all dont matter since Reddit is not the only forum out there, you cant stop me on every platform.
Have you considered storing your guid in a wrapper struct, like: struct Command&lt;T&gt; { guid: Guid, payload: T, } impl&lt;T&gt; Command&lt;T&gt; { fn id(&amp;self) -&gt; &amp;Guid { &amp;self.guid } } If you want, you could add a trait for the payload, like: trait CommandPayload { type Output; fn run_command(self) -&gt; Self::Output; fn do_other_stuff(&amp;self); } impl&lt;T: CommandPayload&gt; Command&lt;T&gt; { fn do_stuff_with_a_command(&amp;self) { self.payload.do_other_stuff(); } } Then, instead of putting your guid inside of every command newtype, you separate the guid-tracking logic from the rest of the logic. This pattern won't necessarily solve your problem, but it's a common one in Rust, and it's probably easier/less gross than writing a procmacro to invasively modify the definitions of your commands.
\`iter::once(1);\` if you are really concerned with allocation
You can avoid the loop and potential bounds checking (which I suspect would be elided anyway if the length is known at compile-time) with another level of `zip`.
Updated [blob-uuid](https://github.com/ivanceras/blob-uuid) to use 2018 edition, added a cli to convert/generate blob string from command line.
There should be a "Useless use of "[Useless use of cat award](http://porkmail.org/era/unix/award.html)" award". There's really no reason not to use cat, and I'd say it improves readability and flexibility (because you can more easily change / add more parts to your pipeline when using cat instead of an argument or redirect)
Does this have SIMD optimizations or is on the charts?
It’s around a thousand, There were roughly 300 bootstrap compilers, three stages each, plus the new versions since we changed how bootstrapping works.
Yeah, I meant "builds" as in "a full build of one compiler version, end to end".
Why are you planning an offline build of Rust? What's the end goal here? As for your questions: 1. You can build mrustc alone, but it's only _really_ useful as a bootstrap compiler for rustc proper - compiling your own Rust code with mrustc is probably not a good idea. 2. Yes, you can. 3. You need the source code of the standard library to build anything _useful_, either with mrustc or with rustc proper. You don't need it to _just_ build mrustc itself.
Hmm I'd try `.as_ref()` to get rid of the Box around it.
You can create `Option&lt;Pin&lt;Box&lt;dyn Future&lt;Output = T&gt;&gt;&gt;` by `Box::pin` and convert `&amp;mut Pin&lt;Box&lt;dyn Future&gt;&gt;` into `Pin&lt;&amp;mut dyn Future&gt;` via `Pin::as\_mut`. If you want to extract an inner future from your struct, you may find [`pin_utils::unsafe_pinned`](https://docs.rs/pin-utils/0.1.0-alpha.4/pin_utils/macro.unsafe_pinned.html) helpful.
You need `Pin` not just some reference, check signature of `Future::poll` or use `FutureExt` https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.16/futures/future/trait.FutureExt.html#method.poll_unpin Note that second option requires `Unpin` in your future type.
How can I have the same color theme I normally have using TextMate? I'm having a hard time seeing stuff between all the blue... I tried exporting the current color theme but it doesn't seem to work that well. I'm using [Gruvbox Dark (Medium)](https://marketplace.visualstudio.com/items?itemName=tomphilbin.gruvbox-themes). [Here](https://imgur.com/a/FIuOvmc) are the pictures.
Not a mrustc contributor but I've been tracking it fairly closely. You can certainly build mrustc offline but you can also build rustc offline. Currently the only real purpose of mrustc is to short circuit the "bootstrap from original sources" of rustc. Without mrustc, you'd need to complete ~1000 compilations of the Rust compiler in order to get a recent compiler from the original OCaml sources. That said, your questions feel very much like an [XY problem](http://xyproblem.info/) to me. What are you actually trying to do?
That's something I could use :-). Do you happen to know what's the difference between libgen.io and libgen.is?
FYI, unless I'm missing something, you just need a stage2 compiler. `rustbuild` doesn't even build the third one anymore unless you mess with your `config.toml` file. For much of the distant past, you could probably get away with a stage1 compiler as well. The only thing that doesn't work in stage1 is proc macros and for many years, those weren't even a thing.
It is not that hard to sanitize them a little bit: ``` cd assets/ # Extract archive. tar xzf quote-pack.tar.gz cd quote-pack # Check which files are not ACSCII. file * | grep -v 'ASCII text' # Fix most files: # - Replace fancy quotes. # - Remove multiple spaces. # - Repace fancy dashes. # - Change nonbracking space (0xc2a0) with regular space. sed -i -e 's/“/"/g' -e 's/”/"/g' -e "s/’/'/g" -e "s/‘/'/g" -e 's/ \ \+/ /g' -e 's/–/-/g' -e 's/—/-/' -e 's/\xc2\xa0/ /g' * # Check which files are not ACSCII (all French ==&gt; maybe remove them): file * | grep -v 'ASCII text'|cut -f1 -d ':' 16dd101d-b720-4a63-b03a-35b659bdfd30 4a6fad29-6aeb-4e5d-9831-c49ecf226724 60d15560-fc67-4dc5-92be-ad036b71eb2f 6f513300-b0e4-4ffc-bb92-a4d529f6f74a 6fe83e1c-f672-4472-82dd-533e794882c0 775175f9-d36d-4b23-b619-97375f54d9e8 881f3ec4-8ef1-481d-a283-ac059e02eb89 88577208-ea98-415b-9fa7-48c2eb1d66cd 8933ba99-4bb1-4117-b373-6c456b528654 9e4a2e65-07d0-4327-9d4e-83b73e5c665d acdca835-1af2-4071-865e-76fae3b5cb0b c1c7b419-2570-4489-b02e-f272b04949f7 ca0bd1f3-bc26-41b6-a9d6-d48c7ed7cb98 db635644-80db-4cff-b604-2475570060e5 ```
I *guess* you could skip stage2, sure, given that it’s already been validated to work. I imagine that if you’re gonna do this, you should just really do it, though.
&gt; ’m working actively on drafting a stabilization report to propose stabilizing a minimum viable version of async/await in the 1.37 release of Rust. 1.37 will be released in mid-August, and branches on July 4. Woohoo!! I applaud the Lang team’s unflappability. Looking forward to seeing this feature bloom in the Rust community’s hands.
&gt; I’m working actively on drafting a stabilization report to propose stabilizing a minimum viable version of async/await in the 1.37 release of Rust. Great work!
You can generate [rls save-analysis](https://stackoverflow.com/a/50426846). It creates json files that contain quite a lot of info. Not sure if it's enough for your purposes.
Thank you so much for the kind words! I'm glad the experiment in teaching-via-podcast worked, because I was genuinely unsure it would when I started. 😆 Also, quick clarification: I'm just *using* Buttondown (a really fantastic email newsletter service) to do updates! I do expect a fair bit of what I build will be open source—in fact, my *default* will be open-source for most of the components—whether I originate it or contribute to existing libraries, so you'll see me around!
Ah, derp -- I noticed you're using a markdown to email campaign service, and I forgotten that was the name of it. Pardon my assumption!
I heartily support this plan! Also, I never in a million years expected to encounter the phrase "Chris Krycho was a podcasting Voltron." 🤣 Made my morning!
All good!
Now that it is official, can we still publicly make polite jokes about it? Writing something like &gt; we're looking.forward to seeing this feature.bloom in the communitys.hands I know this would help me welcome it into my heart.
I'm not exactly sure what you mean? Both libraries are generic (i.e. you can have a `Vector3&lt;f32&gt;` or a `Vector3&lt;Foo&gt;`. `nalgebra` has a flexible type system, which allows you to optionally specify a static length for each axis. For example, a type something like `Matrix&lt;f32, Dynamic, 7&gt;` has exactly 7 columns but a dynamic number of rows. With `ndarray`, each axis is always dynamic, but you can have an arbitrary number of axes. If your data is more than two dimensional, you could not represent it with `nalgebra` alone. So I use `ndarray` to operate on large chunks of data (often in three or higher dimensions), then use `nalgebra` once the problem is reformatted into smaller pieces.
Hi there! Recently, Someone on twitter tweeted this &gt; Rust’s await is full of footguns due to the cancellation behavior. I wish people discussed that more instead of the syntax. https://twitter.com/mitsuhiko/status/1131957120700747776 There were no examples in that thread that can help me understand what was he referring to. I am curious, Please help me understand why is await full of footguns?
This is great news! I can't.wait for this to become stable \o/
I can't (until this syntax lands in stable).wait!
Some companies have strict requirements to the binaries being run in their environment, that's why some components must be bootstrapped from source code. Being offline during bootstrapping ensures that the existing code cannot download any non-audited code/scripts/binaries during build.
Here's some reading: * [Post: Async/Await - The challenges besides syntax - Cancellation](https://gist.github.com/Matthias247/ffc0f189742abf6aa41a226fe07398a8) * [Discussion: Async/Await - The challenges besides syntax - Cancellation](https://internals.rust-lang.org/t/async-await-the-challenges-besides-syntax-cancellation) * [Discussion: Can we reduce the burden of cancel-correctness for async Futures?](https://internals.rust-lang.org/t/can-we-reduce-the-burden-of-cancel-correctness-for-async-futures) * [Post: In nightly Rust, 'await!' may never return (dropping futures)](http://www.randomhacks.net/2019/03/09/in-nightly-rust-await-may-never-return/)
I understand why it can be an issue for people, but OP is very clearly new to Rust, and I'd say this is not something they should be concerned about _right now_.
I doubt you could justify a redesign of the number hierarchy inside std, even with editions.
Thank you so much. I'll read these in a few hours.
We're having a party on the public server at 1800 GMT. Veloren is developed by its community. If you'd like to get involved, please join us on our Discord server or on /r/veloren! https://www.gitlab.com/veloren/veloren
As a Dutch person I have the same feeling, but in some/many countries English is not a mandatory school subject, or they dub foreign series and movies instead of adding subtitles. Wouldn't it be nice if Rust is a gateway drug to English, rather than English being a requirement for Rust? Besides, as an English speaker communities in other languages are probably invisible to you. That doesn't mean they don't exist.
From my experience you shouldn't do this. Its going to create a lot of boiler plate code for very little payback. Are you using 'real' Guid that you generate once and get from storage i.e. Is it unique in the universe? Or is it a unique hash? or unique for the lifetime of your program? or is a unique location in memory enough ? In any case. The best solution is most likely a wrapper struct containing your Guid and the Command. Macros might be a solution, however procedural macros are probably overkill.
Basically, the state machine which is your async block can be dropped by other code at every yield (i.e. await) point. This can be a problem when locking mutexes and similar operations across yield points. This is because the async sheduler (for instance the tokio runtime) can decide that the result of the current operation will no longer be needed. This is somehow equivalent to the panic! problem: locking a mutex and then panicing will poison said mutex and make it unusable for all further operations.
The first document argues that it's difficult to abstract over IOCP efficiently when dropping implies cancellation. But to me, I think the same problems arise if dropping a future is allowed _at all_, regardless of what dropping is supposed to mean for the underlying operation. Even if Rust went with a different design based on cancellation tokens, forbidding dropping would require some sort of tricky language extension kind of like Pin I think?
oh, this is perfect! thank you.
You had it backwards. It should be "I can't (for this to become stable).await".
Is that an intentional dot-macro syntax?
Wouldn’t locks and other shared resources be acquired in a way that they would be relinquished automatically when exiting scope, though?
I'm pretty sure your sentence should be written like this: &gt; we're forward.looking to seeing this bloom.feature in the hands.communitys ;P
I'm working on a `phpMyAdmin`/`Django Admin` style proc macro for `diesel`. It's pretty early even for a proof of concept, but it seems very doable. I have a few hangups currently, but I think I can get through it far enough get feedback this week or next.
The [take_mut](https://crates.io/crates/take_mut) crate might be of interest to you. It lets you move out of a `&amp;mut T` and makes it safe by converting any panics to a hard abort while you're stealing out of a reference.
You have to break into unsafe code in order to get a reference to a struct member, either pinned or unpinned, from a pinned pointer to the structure. This is unsafe because [it is up to the programmer](https://users.rust-lang.org/t/when-is-it-safe-to-move-a-member-value-out-of-a-pinned-future/28182/3) to decide whether the field shall be exposed as structurally pinned or not, and keep to that projection everywhere the reference to the field value can be exposed to safe code. When your future struct contains inner futures that need to be polled, they need to be structurally pinned unless they satisfy `Unpin` (which means their entire content does not care about pinning). In any case, `poll` needs a pinned reference that can be constructed with `Pin::new_unchecked`, or using the `unsafe_pinned` macro from `pin-utils` as another commenter suggests.
Wouldn't the future just be removed from the scheduler's queue and the lock then get dropped normally, though?
I looked at the cancellation document. It makes statements that I do not see as true. &gt; The main drawback is that the underlying operation must support synchronous cancellation. [...] Examples for APIs which do not support synchronous cancellation are IO completion based OS APIs, e.g. IO completion ports (IOCP) on windows I'm not sure how the author comes to this conclusion, but implementing cancellation w/ IOCP is perfectly doable. You just initialize the cancellation process in the drop. In the case of IOCP, you pass a callback, so you have no problem doing further work after the cancellation op completes. If there is no callback, you just need a way to schedule cleanup work, which can be as simple as spawning a cleanup task. Then the author goes on to use `async fn receive(self: &amp;mut Socket, buffer: &amp;mut [u8]) -&gt; Result&lt;usize, Error&gt;;` as an example... IMO you are going to have a bad time no matter how you do this... to make this work, you would have to have **very** strong guarantees in the task scheduling system. If what the author showed is permitted, even minor bugs in a scheduler implemented with no unsafe code could result in memory unsafety... in short, IMO, it should not be possible to do what the author is requesting in the first place given the level of danger. &gt; The surprises of synchronously-cancellable subtasks The point probably has some merit w/ regards to it being a surprise to someone who does not learn about Rust async / await... this should be handled by documentation. If a unit of code needs to be run to completion, it needs to be spawned on a task.
You obviously don't get how this works... It should be &gt;We're forward.looking to (this feature.bloom in the community_hands.of).seeing
The problem is not with the mutex itself, but with a data that it protects. If a thread had panicked it means that it potentially left the shared data in an undefined state. Even if we would release all mutexes in correct order, that would not prevent other threads from accessing the poisoned data. The goal is to eliminate UB from the code in the first place.
&gt; You can create &gt; Option&lt;Pin&lt;Box&lt;dyn Future&lt;Output = T&gt;&gt;&gt; Isn't this useless when creating a field of a struct which is not necessarily pinned itself? The time to structurally propagate the pin is when the struct is used under a `Pin` pointer, otherwise the field can and should be accessed normally. It is unsafe to mutably reference structure fields from under the pin in any case.
Neither do `build.rs` files
&gt; that they would be relinquished automatically when exiting scope, though? The magic of `yield`, however, is that the variables do *not* exit the scope. In practice, variables that are live across yield points are stored inside the state machine.
I would imagine that dropping the state machine also drops the live variables, otherwise it's leaking any kind of resources: memory, locks, etc... No?
I don't think there is anything else you can configure, this chip is very simple. The only 2 things that affect execution speed are clock frequencies and flash wait states. When I create default project for you chip in STM32CUBE I see that it sets frequency to 48MHz and 1WS for flash, the same as Rust code does. One other difference is that CUBE configures SysTick interrupt to run every millisecond. But it's only couple of instructions, so overhead should be in microseconds. But you can try to get around this by disabling interrupts before your main loop. Also you can use DWT to measure the number of cycles. This will give you some value that will not be affected by frequency, but it may still be affected by flash wait states. I did some measurements on my F429 board, and I see this numbers: WS: 0, Cycles: 3145986 WS: 1, Cycles: 3289737 WS: 2, Cycles: 3431648 WS: 3, Cycles: 3573764 Here is the code to print cycles: [testDWT.tar.gz](http://88.99.85.67:1337/testDWT.tar.gz) Please note that you need to have debugger attached with semihosting enabled, or else the code will hang on call to print. Also what compiler are you using for C? Maybe you compile without optimizations?
Why redirection? grep Hi out.txt
Why not just use fn convert&lt;'a, T&gt;(of:&amp;'a Scalar) -&gt; T where T: std::convert::From&lt;&amp;'a Scalar&gt;, T:std::fmt::Debug, { T::from(of) } And let the compiler tell you when you can't convert? Also no one will be able to implement `From&lt;&amp;'a Scalar&gt; for Option&lt;T&gt;` because both the type and the trait are foreign. ([proof](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=76646d530e5e4005cff9742b2c91d739)) And if you want fallible conversions fn convert&lt;'a, T&gt;(of:&amp;'a Scalar) -&gt; T where T: std::convert::TryFrom&lt;&amp;'a Scalar&gt;, T:std::fmt::Debug, { match T::try_from(of) { Ok(x) =&gt; x, Err(_) =&gt; unreachable!("Can't convert {:?}", of) } } You can't get the name of a type without using [intrinsics](https://doc.rust-lang.org/std/intrinsics/fn.type_name.html) or a [crate](https://crates.io/crates/typename) that only supports *some* types
It requires nightly and the use of an intrinsic, but this works (I made it fully generic for the sake of the example): #![feature(core_intrinsics)] use std::intrinsics::type_name; fn convert&lt;'a, TA, TB: std::fmt::Debug&gt;(of: &amp;'a TA) -&gt; TB where Option&lt;TB&gt;: std::convert::From&lt;&amp;'a TA&gt;, { let name_a = unsafe { type_name::&lt;TA&gt;() }; let name_b = unsafe { type_name::&lt;TB&gt;() }; let x = Option::&lt;TB&gt;::from(of); match x { Some(value) =&gt; value, None =&gt; unreachable!("Can't convert {} to {}", name_a, name_b), } }
I´m seriously thinking about having a bunch of `enums` but the memory performance worries me
Wouldn't a wrapper create additional complexity in the rest of the code? I.e. Everywhere in the code, I wouldn't have just commands, but wrapper of commands?
Wouldn't a wrapper create additional complexity in the rest of the code? I.e. Everywhere in the code, I wouldn't have just commands, but wrapper of commands?
The conversions are on dynamic data, so I can't rely on compiler time errors. The Option&lt;T&gt; is a trick for bring some stuff from another crate around (that I also have) because this can't implement TryFrom. &amp;#x200B; So, I have it solved except for how print the intended type from the generic (currently I pass a string like "char").
I'm not sure on the syntax, but wouldn't it be more like: if !(await_is_stable).ready { panic!("can't wait"); } Though I guess that's a bit too literal :)
Why can't you implement `TryFrom`? Coherence issues? If that is the case then there isn't much you can do because you can't implement `From&lt;Bar&gt; for Option&lt;Foo&gt;` even if you own `Foo` and `Bar` https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e15b6902162b3b70126bf8df31748298
What is this? r/ProgrammerHumor?
You're looking for /r/playrust, not /r/rust
Oh sorry, thanks !
Eh, perhaps it was in bad taste. Honestly, I don't really care _what_ it looks like, I just want to use it. I'm glad that it's moving forward with an actual timeline. I feel like every time I see one of these posts, I'm basically checking "can I use it yet?" and the answer always seems to be "no". I feel very much like I'm polling instead of getting notified when it's finally ready. I definitely wouldn't top-post stuff like that, I just felt it wasn't completely out of place given the context.
Wait, we are not going to discuss the `.await` syntax here, are we?
Oh damn, this shit sucks
If you enjoy this style/theme you could write a backend for `log` or `Drain` for `slog`. Eg. just fork `slog-term` and change the style. Otherwise it is quite stand-alone, feature limited and not very usefully crate.
Not in this subthread. We've already deviated enough from that discussion :)
Which is mitigated by using RAII (which is actually good practice)
Indeed. Syntax downsides are skin-deep, cancelation-safetty is a real gotcha. Basically: shared.rate_limit.increase(); dosomething.await; shared.rate_limit.decrease(); is incorrect, because `await` might never return for any (external) reason. Mistakes like this will lead to plenty of hard to rarerly occurring (hence easy to overlook and hard to debug) production issues. Any cleanup **has to** rely on `Drop`, as it's the only way to reliably implement cleanups in `async` code. At very least I wish we could come up with some good clippy heuristics to warn about cases like this. But I"m not sure if it is even possible.
Love watching your progress!! (Been following it from the start)
I would. I’m cheap though so not enough to pay for it.
This week in Rust - Podcast edition! With some deep dives on recent updates in the language RFCs etc. EDIT: And with some interviews every now and then!
You indeed can't move `(usize, &amp;mut usize)` there because it isn't `Copy`. The reason that passing `input.1` there is allowed, despite it not being `Copy`, is that IIRC it implicitly creates a new borrow (i.e. it desugars to `&amp;mut *input.1`).
This is the kind of thing I would definitely listen to. Great idea!
What does this have to do with await? If this is synchronous code and that's a regular function call that panics the same issue appears. How is this new or surprising?
The two operations are doing something slightly different, in practice. The first line, `f(input)`, is moving `input` wholesale; and of course can only move once. The second line, `f((input.0, input.1))`, is creating a new tuple with a different lifetime by re-borrowing `input.1` automatically. A slight twist to force the borrow of `input.1` to continue past the function call will also cause an error in the second case: fn f&lt;'a&gt;(_: (usize, &amp;'a mut usize)) -&gt; Vec&lt;&amp;'a ()&gt; { vec!() } fn call_f_a_bunch_of_times&lt;'a&gt;(input: (usize, &amp;'a mut usize)) -&gt; Vec&lt;&amp;'a ()&gt; { let mut result = vec!(); for _ in 0..10 { //f(input); // ERROR: use of moved value result.extend(f((input.0, input.1))); // OK } result } Results in: error[E0499]: cannot borrow `*input.1` as mutable more than once at a time --&gt; src/main.rs:9:35 | 5 | fn call_f_a_bunch_of_times&lt;'a&gt;(input: (usize, &amp;'a mut usize)) -&gt; Vec&lt;&amp;'a ()&gt; { | -- lifetime `'a` defined here ... 9 | result.extend(f((input.0, input.1))); // OK | ^^^^^^^ mutable borrow starts here in previous iteration of loop 10 | } 11 | result | ------ returning this value requires that `*input.1` is borrowed for `'a` Reborrows are transparent in general, but as all magic when you look closer there's a moment of confusion before you grok it.
Thanks!
[Veloren 0.2](https://www.veloren.net), a multiplayer voxel RPG written in Rust, just got released!
Follow this https://rust-lang-nursery.github.io/futures-rs/blog/2019/04/18/compatibility-layer.html but then instead of await!(..), do .await. I followed it and got it to run pretty quickly! Then you can start writing functions like async fn do_something() and then .await the result
Interviewing people in the Rust community. The New Rustacean had a few of these and I really enjoyed them.
This is very good news! For those who cannot live with that decision: If you still feed strongly about \`await\` in about 3 years, you can always propose a better syntax for Rust Edition 2022 ;)
It used to be much more understandable before NLL, although some things that are possible today weren't back then.
Well, it's indeed possible to lock mutex, partially modify data leaving it in an inconsistent state, await, and finish data modification. If future gets dropped on the await step, then mutex will be freed with an inconsistent data. While it's indeed not a great situation, I don't think it classifies as UB or memory safety bug (assuming you don't have `unsafe`s which work with lock). I think it should be solved by education (e.g. "usually you should not keep lock across await points"), not by introducing additional `defer`-like constructs. And we should be careful with panic analogy, as it clearly leaks with mutexes and other poisonable constructs.
Isn't this true in synchronous code as well? If `dosomething` was `-&gt; !` or panicked then `shared.rate_limit.decrease()` would also never get called, unless I'm missing something?
I don't know enough to tell you whats best going on just the names `Command` and `Guid`. If a wrapper is the right solution then there is probably a better word for it in your problem domain. At the end of the day, the data has to be owned/stored by something, somewhere. ---- If your problem is just writing out every `fn id(..)`, know that you can do the following without procedural macro's : trait Command { fn exec(&amp;self) -&gt; bool ; fn id(&amp;self) -&gt; u32; } macro_rules! impl_id { () =&gt; { fn id(&amp;self) -&gt; u32 {self.id} } } struct A{ id : u32} impl Command for A { impl_id!(); fn exec(&amp;self) -&gt; bool { ... } } struct B{ id : u32 } impl Command for B { impl_id!(); fn exec(&amp;self) -&gt; bool { ... } }
I don't know enough to tell you whats best going on just the names `Command` and `Guid`. If a wrapper is the right solution then there is probably a better word for it in your problem domain. At the end of the day, the data has to be owned/stored by something, somewhere. ---- If your problem is just writing out every `fn id(..)`, know that you can do the following without procedural macro's : trait Command { fn exec(&amp;self) -&gt; bool ; fn id(&amp;self) -&gt; u32; } macro_rules! impl_id { () =&gt; { fn id(&amp;self) -&gt; u32 {self.id} } } struct A{ id : u32} impl Command for A { impl_id!(); fn exec(&amp;self) -&gt; bool { ... } } struct B{ id : u32 } impl Command for B { impl_id!(); fn exec(&amp;self) -&gt; bool { ... } }
Postfix macros 4 lyf!
It goes deeper than that. IOCP involves a message from the application to the kernel saying "you may write to this region of my virtual memory". Rust conceptualizes that contract as a temporary transfer of ownership - a loan. And "temporary" is understood as the special case where a borrow fits inside a single function call. So it's fundamentally incompatible with asynchronous procedures. If a loan starts with one function call and ends with another, that's not supported. That's fundamentally why the Mutex and RefCell APIs involve deadlocks and poisoning and runtime overhead. Since IOCP is also a kind of shared memory, it can't avoid those difficulties either.
TextMate rules doesn't work for Rust. You need to fill ```json "workbench.colorCustomizations": { "[Dark+ One Monokai]": { "syntax.type": "#56b6c2", "syntax.function": "#98c379", "syntax.variable": "#c5c8ce", "syntax.string": "#e5c07b", "syntax.comment": "#777d88", "syntax.keyword_directive": "#e7a0a6", "syntax.keyword_constant": "#56b6c2", "syntax.keyword_control": "#e06c75", "syntax.keyword_operator": "#e06c75", "syntax.storage_modifier": "#e06c75", "syntax.punctuation": "#c5c8ce", "syntax.number": "#c678dd", "syntax.namespace": "#b4cdff", } }, ``` With the wanted colours
While you can't have variadic functions in Rust, you can abuse tuples and generics to create overloaded functions to have a very similar effect. Here's a simple example: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ef8df1525664d8a4e09fe306bf1b79e0 A more complicated example, and where I discovered this trick, would be: https://docs.rs/wasmer-runtime/0.4.2/wasmer_runtime/struct.Func.html
You won't get a panic or any other indication that you've screwed up in the async case. It's not that killing the task is the problem, silently killing the task is the problem.
The example from downthread seems more reasonable: shared.rate_limit.increase(); dosomething.await; shared.rate_limit.decrease(); This code is, strictly speaking, incorrect, because the future isn't guaranteed to complete. There's no way to indicate in the signature, "this future must be spawned as a task to be used correctly". I suppose it might be correct if you use RAII instead, but that's non-obvious. The whole point of async code is that you can pretend it's synchronous.
Why?
yeah, if you are following the unix way. ie, small programs which are designed to compose. Lots and lots and lots and lots of programs are monoliths and this is a \*horrible\* idea for those style programs. Context matters.
&gt; Any cleanup has to rely on Drop, as it's the only way to reliably implement cleanups in async code This is already true today for all rust code, not only async code.
It's hard to say more without knowing the exact details of what you're doing, but it might be helpful to note that an enum is usually only as large as its largest variant. All the different variants get to use the same space, with a tag byte or something like that distinguishing between which variant is active. (And sometimes the compiler doesn't even add a tag byte, when the it uses something like the "null pointer optimization".)
&gt;I suppose it might be correct if you use RAII instead, but that's non-obvious. The whole point of async code is that you can pretend it's synchronous. This is similar to Mutex. You don't call any unlock() method to release the data, so it is expected to do the same with a semaphore.
**Update**: I added the `Vec::with_capacity` optimization, and a new `?` operator. The `?` operator can be applied to array literals containing `Option&lt;T&gt;` expressions: assert_eq! { vecmerge!([Some(1)]? + [None]? + [Some(3)]? + [4, 5, 6]), vec![1, 3, 4, 5, 6], } If one of of the expressions is `None`, then none of them are concatenated to the final vector: assert_eq! { vecmerge!([Some(1), None, Some(3)]? + [4, 5, 6]), vec![4, 5, 6], } Conversely, if all are `Some(_)`, then all of them are concatenated to the final vector: assert_eq! { vecmerge!([Some(1), Some(2), Some(3)]? + [4, 5, 6]), vec![1, 2, 3, 4, 5, 6], }
Well written Java runs currently close to C++ speed. A lot has changed on the JVM in the last couple of years.
The unit of logic itself would spawn a task and return a handle to that task. This would avoid the leaky abstraction.
For me the big difference is that a panic shows up somewhere. For many people this will typically result in terminating the application or crashing down to a point where the world gets re-created. A cancelled future just happens. There is no message, no warning, nothing.
I think the two situations are similar, but I also think it's fair to say that the await version of the problem will be more common, since the panic version only comes up when you catch your panics.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/negativewithgold] ["Oh damn, this shit sucks" \[-19\]](https://www.reddit.com/r/NegativeWithGold/comments/bu4rwg/oh_damn_this_shit_sucks_19/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I can't (`async for` this to land)@?!
Well, this particular program was NOT well written java:) My point was that it is easier to write rust code that handles resources correctly than java, i.e. if you don't know rust well and don't know java well either then you are probably better off with rust. That's the message:)
I have Rust know-how and a mic suitable for podcasting, but I know nothing about audio editing or how best to host the files/hook into podcasting services like iTunes. What tools did you use and how did you learn?
Is it possible to implement `TryFrom` for the serde `Serialize` trait? I'm getting some conflicting implementation errors, is TryFrom only meant for concrete objects?
Meanwhile it will meme to troll Rust
It’s almost like we need ACID semantics (controlled by RAII containers where destruction-before-commit-is-rollback) for any mutating side effects produced by an async function that can be cancelled in any way. This is... actually not a bad rule of thumb for any language that works with async. I wonder if Rust’s checker, either now or over time, can make it increasingly difficult to deviate from this guideline.
My best guide to that is [here](https://winningslowly.org/bonus.04/). I also watched a handful of courses on something like Lynda.com* on things like compression, etc. For getting started, Audacity and Garage Band are *just fine*, or, if you have an iPad, Ferrite is *incredible* (esp. for its price). Lots more details on that guest lecture, though! * now LinkedIn Learning; I used it before it was acquired by LinkedIn and also before I joined LinkedIn myself, lest anyone think that's why I'm mentioning it.
Trying my best to learn [Amethyst](http://amethyst.rs)! Very challenging for me, but also extremely rewarding and fun. My end goal is to make an RPG-like 2D horror game. I wrote a little more [here too.](https://vaporsoft.net/mwia-1/)
As a potential listener, there are a few things I'd *love* to see somebody do: - Talk to Rust users (an interview show)! I had a handful of contacts with more folks who were up for being interviewed and I just never had time to get to those, and with the Rust community booming the way it is, there are lots of people to talk to. Take the model of something like Elixir Fountain and run with it. - Do news episodes! They take a surprising amount of time to prep to do well (as I mentioned on the last ep. of New Rustacean), but if someone did a biweekly show they could sync with releases every third episode and talk community happenings the two in between, and those could be 10–15 minutes and be *great*. - Do something like the Crates You Should Know format I was doing, surveying and kind of 'teaching' the ecosystem. Doing it the way I did is also harder than it looks; one way you could make it a bit easier is to combine it with an interview format: have maintainers on to explain their projects – talking a bit about the history and origin of the library/tool/etc., and a bit about how you use it.
sounds like you've been pretty busy! Zemeroth is looking cool as ever. (nice to see the blog in Zola too)
I'm not happy about the syntax choice, but things are what they are. I'm pretty sure the choice was already final when the last post was published, but the community was given a final opportunity to voice their concerns. At least the endless discussions are now over. `DOT async` will work just fine and will be a great feature for the language in the networking/web domain. Looking forward to the ecosystem (mainly tokio, hyper, ...) migrating over to `std::futures::Future` and `await` ( which will take many months though).
Thanks a lot for the feedback! These are all things I've love to see as well.
The big difference to me is that I would just run with panic=abort. Panics shouldn’t happen in non buggy code, and trying to handle them might be a futile attempt, whereas restarting is a lot more deterministic. Cancellation is however not buggy behavior but an expected one.
then how am I getting that highlight from? The rust rls extension? Strange. I'll that that out and let you know, thanks!
As you can see I have changed the description of the project to clarify it's thought to implement the output of CLIs, not for logging. But I keep the idea. Eventually I would need a logger for my HTTP APIs and I like this style. Thank you!
Printing to stderr now :)
The problem isn't the English website but rather that not everyone can speak English or understands it well. Localization efforts help to bring Rust to a wider audience! Europe in particular is quite diverse in terms of languages.
This is essentially what Rusty Spike was before it also closed down, and it was great. A short ~5m summary even is enough, IMO.
&gt;Europe in particular is quite diverse in terms of languages. Well yeah, I'm European but you're gonna have a very hard time programming if you dont know English.
I agree it will be a problem, but this is a natural consequence of the low-level design. I don't think this could be made any better without much more integrated runtime support. This will just be a thing you have to learn when writing async/await code in Rust. It will cause plenty of weird, hard to trace bugs, but the best we can do is fill the gap in the ecosystem. Things like a attribute macro for async functions that panics if not run to completion. #[must_complete] async fn f() { ... }
Yes, it is true. It's a similiar situation. But while people might be aware of panic (exception-) safety, there might not be aware how `async` in Rust works.
But people are generally aware of exceptions and such. They will generally not be as aware of details of how Rust's `async/await` are implemented: differently to run-to-completion in other languages.
Yeah I guess it is worse in async code. But I still think it's kind of a moot point because I think we should be discouraging this kind of resource clean up anyway, regardless of whether the code is synchronous or not.
IMO, main difference is: (some) people are familiar with exception-safety - a lot of languages uses exceptions, so `panic`s are Rust's special-try-not-to-use-them exceptions. The are no other languages (IIRC, might be wrong) with poll-based `async/await`.
&gt; Then the author goes on to use async fn receive(self: &amp;mut Socket, buffer: &amp;mut [u8]) -&gt; Result&lt;usize, Error&gt;; as an example... &gt; IMO you are going to have a bad time no matter how you do this... to make this work, you would have to have very strong guarantees in the task scheduling system. If what the author showed is permitted, even minor bugs in a scheduler implemented with no unsafe code could result in memory unsafety... in short, IMO, it should not be possible to do what the author is requesting in the first place given the level of danger. Author here. I'm not sure what you mean with that. Can you further elaborate? The function signature is not any different from any other function that borrows across await points (which was one of the main motivations for async/await). Scheduler need to support this. The only difference is that this would not be a compiler generated function but one that is hand-implemented in terms of a Future.
&gt;This is somehow equivalent to the panic! problem: locking a mutex and then panicing will poison said mutex and make it unusable for all further operations. On POSIX shared memory mutexes can have the "robust" attribute set which makes it possible to do a "repair" operation on them after a process holding the mutex dies while holding it. I have no idea how this works under the hood, but in principle it suggests there is a way to do it.
&gt;The problem is not with the mutex itself, but with a data that it protects. If a thread had panicked it means that it potentially left the shared data in an undefined state. In many scenarios this is completely fine. You may lock a mutex just because you intend to read the protected data, and crash because you try to copy it to off the end of an array. Or you might be copying into the protected data and the problem is you are trying to copy from off the end of an array. The data doesn't get put in an invalid state because you never even change it. Or maybe you did half write data, but it's the kind of thing that the next writer is going to completely overwrite anyway.
&gt; I'm not sure how the author comes to this conclusion, but implementing cancellation w/ IOCP is perfectly doable. You just initialize the cancellation process in the drop. In the case of IOCP, you pass a callback, so you have no problem doing further work after the cancellation op completes. If there is no callback, you just need a way to schedule cleanup work, which can be as simple as spawning a cleanup task. The post explicitly mentions this alternative: &gt; There exists a workaround for this problem: We can provide the operation an owned buffer instead of a reference - and make sure that the ownership is kept intact over the whole duration of the operation. [..] Alternatively, the implementation of the Socket can use an owned buffer internally to read new data from the OS and can copy it to the user-provided `&amp;mut [u8]` when finished. In this case the original function signature can be kept. However the downside of this approach is an extra copy of each read byte.
It's because of type interference. By default, integer literals has `{integer}` type that's internal to compiler. Then it goes through code to find first concrete type usage. In your snippet it was `vec![T; usize]`. If you never specified concrete type it would use i32 IIRC.
You never explicitly declared the type of `width`, `height`, or `vector_size`, so it has to infer it based on usage. Calling `needs_u32_args` hints that `width` and `height`should be `u32`. The multiplication of them hints that `vector_size` should be `u32`. The vec macro later hints that `vector_size` is actually a `usize`, which conflicts with preexisting inference. Rather than letting the compiler resolve the conflict for the programmer, the programmer is required to resolve it themselves.
https://doc.rust-lang.org/rust-by-example/types/literals.html says it would default to i32, but it doesn't need to default here. The specific inferences are (I believe) as follows: Without `needs_u32_args` the compiler sees the `vec!` call which constrains the type to usize, so it makes all the integer literals usize. With `needs_u32_args`, but without `as usize`, it still sees the `vec!` call first, and so still makes all the integer literals usize (thus the error about calling `needs_u32_args` with usize arguments). Finally, with `as usize` the `vec!` call is no longer a constraint the compiler can use to infer the type of the literals. So it looks to the `needs_u32_args` call and thus infers that the literals are u32.
got it, so since vector\_size needs to be a usize for the vec! macro to work, I should explicitly say that it is usize? Is this the idiomatic way to rewrite the code in the original post? &amp;#x200B; fn needs_u32_args(w: u32, h: u32) { println!("width = {}, height = {}", w, h); } fn main() { let width = 32; let height = 24; let num_pixels = width * height; let values_per_pixel = 4; // 4 if RGBA, 3 if RGB // changes here ------------ let vector_size: usize = (num_pixels * values_per_pixel) as usize; // ------------------------- let image_data: Vec&lt;u8&gt; = vec![0; vector_size]; needs_u32_args(width, height); }
To add a bit to that: the `as usize` part made it work because that expression fragment still accepts any `{integer}`, so the _next_ expression is checked to find a concrete type, which forces `u32`.
Still silly.
&gt; and the lock then get dropped normally, though? Yes, but the data protected by the lock can be left in an inconsistant state.
I don't think it's moot. People will be coming to Rust and writing bugs. It's a main theme of Rust to prevent bugs. I.e. by avoiding planting pitfalls in the language. I don't know if we can do anything about this one, but it's important to recognize it, look for possible ways to improve the situation, and at very least be very vocal about this one when educating new users.
I think I get it. So if I dont use `as usize` the compiler sees width = 32; and assumes it is a i32 by default, then after some calculations it sees that im using vector\_size (which is also assumed to be i32 because it is derived from previous i32s) and it should be a usize. The compiler here assumes that if the derived variable is a usize, that means the variables previous to it should be usize as well. The error comes at the end when the compiler reaches `needs_u32_args` and at this point it has already decided that all the above variables should be usize, and the function call gives an error. &amp;#x200B; But if I explicitly say `as usize` then the compiler only casts that specific variable to usize, and not the rest. it still thinks that width and height are i32 by default, but when it reaches the function call it decides that they are actually u32.
Either way is fine since you're only using `vector_size` in the vec macro.
It's not generally possible to do this, since you don't have any actual value of `T`. I'd recommend something like the following, if you control everything: trait HasTypeName { fn type_name() -&gt; &amp;'static str; } impl HasTypeName for A { fn type_name() -&gt; &amp;'static str { "A" } } impl HasTypeName for B { fn type_name() -&gt; &amp;'static str { "B" } } ... You could also make it nicer to implement with a macro, like macro_rules! impl_has_type_name { ($($name:ty),* $(,)?) =&gt; ( $( impl HasTypeName for $name { fn type_name() -&gt; &amp;'static str { stringify!($name) } } )* ); } Then you could depend on this trait inside your function: fn convert&lt;'a, T&gt;(of: &amp;'a Scalar) -&gt; T where Option&lt;T&gt;: std::convert::From&lt;&amp;'a Scalar&gt;, T: HasTypeName, { let x = Option::&lt;T&gt;::from(of); match x { Some(value) =&gt; value, None =&gt; panic!("Can't convert {:?} to {}", of, T::type_name()), } } Of course, this is a bit more work, but it does allow you to get this functionality. I don't think there's a general, stable way to get type names. This works, at least. --- The other way, as others have mentioned, would be to have your conversion code produce the failure message. If you use `TryFrom` instead of `From`, you could then have the conversion itself generate the error. For instance, use std::convert::TryFrom; fn convert&lt;'a, T&gt;(of: &amp;'a Scalar) -&gt; T where T: TryFrom&lt;&amp;'a Scalar&gt;, &lt;T as TryFrom&lt;&amp;'a Scalar&gt;&gt;::Error: std::fmt::Display, { let x = T::try_from(of); match x { Ok(value) =&gt; value, Err(err) =&gt; panic!("Can't convert {:?}: {}", of, err), } } Then in your conversion code, instead of using `Option`, you could write something like: impl TryFrom&lt;&amp;'_ Scalar&gt; for i64 { type Error = &amp;'static str; fn try_from(scalar: &amp;'_ Scalar) -&gt; Result&lt;i64, Self::Error&gt; { Err("Failed to convert scalar to i64") } } This way it'd be on the type implementing the conversion to generate a proper error, including the type name. This could be a good way to do this too! You could then even include more specific errors for particular failures, and get better errors than just "couldn't convert to i64".
(I found a solution that I'll outline in a separate comment.) Adding the generic argument works for this specific example, but not for the case I'm actually trying to solve. When I try to apply your advice to a modified version of the example in my grandparent comment, I end up with: use std::error::Error as StdError; use std::result::Result as StdResult; type Result = StdResult&lt;(), Box&lt; dyn StdError &gt;&gt;; type SndResult = StdResult&lt;(), Box&lt; dyn StdError + Send &gt;&gt;; struct Callbacks&lt;'a, R&gt;(Vec&lt;Box&lt;dyn FnOnce() -&gt; R + 'a&gt;&gt;); impl&lt;'a, R&gt; Callbacks&lt;'a, R&gt; { fn new() -&gt; Self { Callbacks(Vec::new()) } fn add(&amp;mut self, cb: impl FnOnce() -&gt; R + 'a) { self.0.push(Box::new(cb)); } } fn main() -&gt; Result { let mut callbacks = Callbacks::new(); callbacks.add(|| -&gt; Result { Ok(()) }); callbacks.add(|| -&gt; SndResult { Ok(()) }); // &lt;= Error Ok(()) }
I would love a podcast that reflected cppcast in format and style. I thoroughly enjoy most of their episodes. They have guests for most episodes. In the beginning, the hosts and the guest(s) talk about recent interesting libraries, articles, news and conferences. They then dive into the projects the guests are associated with. It's really very well done and I imagine a LOT of hard work. One of my favorite episodes was the one with Matt Godbolt discussing Meltdown and Spectre: https://cppcast.com/2018/01/matt-godbolt/
&gt; So if I dont use as usize the compiler sees width = 32; and assumes it is a i32 by default Your understanding of how the compiler infers the type for width is correct, but I want to clear up one small detail. The default of i32 doesn't come into play until the compiler has reached the end of your code and found no other way to determine the type of `width`. When it's reading through your code, it just has a placeholder for `width`'s type `{integer}`. It's a subtle detail, not really too important. I think it would only come up if you left off the `needs_u32_args` call and kept `as usize`. In that case there is no place in your code where the compiler can be sure what type any of the variable should be, so it'll just default them to i32. &gt; But if I explicitly say as usize then the compiler only casts that specific variable to usize, and not the rest. it still thinks that width and height are i32 by default, but when it reaches the function call it decides that they are actually u32. Not quite. `vector_size as usize` tells the compiler nothing. When it gets to that line, it doesn't know the type of `vector_size`. It just knows it's an integer of _some_ kind. It could be i32, u64, who knows. And all of those are valid types in the expression `vector_size as usize`. But you are correct, once it sees needs_u32_args, _then_ it knows, since `width` and `height` could have been any integer type up until that point, that because of how you used them they _must_ be u32. It then infers the types for all the other variables (because u32 * u32 gives u32, hence num_pixels and vector_size must also be u32). Anyway, this is all basically just a convenience feature of the compiler. Rather than having to write the types for _every_ variable _all_ the time (like we do in Java, C, and older C++), Rust's compiler is smart enough to figure out what type variables should be based on how we use them. You can always specify the types explicitly if you want or are confused about what the compiler is assuming.
This is also why you can do e.g. let x = &amp;mut 1u64; foo(x); foo(x); The `&amp;mut u64` there isn't `Copy`, but nonetheless we're able to pass it "by value" twice. It's the same re-borrowing trick, and I believe the compiler does it magically because life would be very inconvenient if it didn't.
See [StackOverflow](https://stackoverflow.com/a/56350903/1034080) (new link) or [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4f46077d20dcd2184ca2ac23d4b47c0c) for the solution I came up with.
See [StackOverflow](https://stackoverflow.com/a/56350903/1034080) or [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4f46077d20dcd2184ca2ac23d4b47c0c) for the solution I came up with.
You might be running into the limitation that you're only allowed to implement a trait for a type if your crate defines either the trait or the type. That avoids the problem where crates A and B both try to implement trait C for type D, leading to a conflict and a build failure.
Could you elaborate on why you need to do this? The usual convention is to just take `&amp;self` whenever that will suffice.
No, it's the GUIDs that are adding complexity to your code. The `Command&lt;T&gt;` wrapper struct encapsulates that complexity, clearly displaying where it's used and allowing you to ignore it when it's not used. All of the other solutions suggested here will probably increase code complexity by a similar amount --- I'd argue that either maintaining a procedural macro, or writing the GUID code on a per-struct basis would both be considerably more complex than using a type wrapper. A lot of the grammar, syntax, and design work of the Rust language is dedicated to conveniently writing generic types, generic methods, and generic functions. In my experience, any time you can use those tools instead of other, more complex options with less support, that's a win.
&gt; I’ve seen in these discussions costs that people weigh heavily raised to the level of absolutes. This negative attribute of some proposal is held to be so bad that we simply can’t do that. For the most part, this is an unhelpful framing. It closes the discussion because it makes it difficult for people who disagree to acknowledge that this problem is real while also arguing that we should hold a different balance of the cost, which makes it still the preferable choice. We need to be able to avoid this sort of dichotomizing, black and white thinking. Life lessons.
I’m so confused. I wanted to know what this library was about so I followed the link: &gt; metrics is a high-quality, batteries-included metrics library for Rust. Is that supposed to be descriptive? &gt; general features &gt; Provides counter, gauge, and histogram support. &gt; Access to ultra-high-speed timing facilities out-of-the-box with quanta. &gt;Scoped metrics for effortless nesting. &gt;Speed and API ergonomics allow for usage in both synchronous and asynchronous contexts. &gt;Based on metrics-core for bring-your-own-collector/bring-your-own-exporter flexibility! Can someone explain what this library is?
It's a library for collecting metrics in your application, and sending them somewhere to allow you to see them. Here's a decent primer on what collecting metrics is typically used for: [https://blog.digitalocean.com/observability-and-metrics/](https://blog.digitalocean.com/observability-and-metrics/)
Yeah but I think `as_mut` is cleaner than reference manipulations.
The idea is to have something like this, in for example a hypothetical HTTP server configuration: HttpServer::confg() // returns a default `HttpServerConfig` builder .set(BindAddress(Ipv4Address::parse("127.0.0.1:8888").unwrap()) .set(ConnectionTimeout(1000)) .set(LogLevel::Trace) .handle(Handler::new()) .start(); Where you only have the one `.set()` method in a simple trait and then each configuration type (`BindAddress`, `ConnectionTimeout`, `LogLevel`) implements `Parameter&lt;HttpServerConfig&gt;`: impl HasParameter for HttpServerConfig { fn set&lt;P: Parameter&lt;Self&gt;&gt;(&amp;mut self, param: P) -&gt; &amp;mut Self { param.set_param(self); } } impl Parameter&lt;HttpServerConfig&gt; for BindAddress { fn set_param(self, config: &amp;mut HttpServerConfig) { config.bind_address = self.0; } } // etc.. _In theory_ this is more flexible, e.g. if you wanted a generic config loader that was decoupled from the actual config struct: pub fn set_bind_addr_from_env&lt;P: HasParameters&gt;(config: &amp;mut P) where BindAddress: Parameter&lt;P&gt; { let bind_addr = load_bind_addr_from_env(); // magically load the bind address from an env var config.set(BindAddress(bind_addr)); } However, it rarely gets more than the basic usage, so you have all this extra code for what should just be a simple setter method. It's extra pointless in `png` since it's just one parameter. And you've already found the primary downside: if you don't already understand the pattern, it's quite difficult to figure out just from the API alone. For these reasons, this idiom seems to never have really taken off outside of the Piston project (which is where all the crates in the `image-rs` org originated from).
It’s probably overcomplicated, but I want to use move semantics for (i.e. copy) Copy objects.
The syntax is very important, it is permanent. While caveats can be fixed in the background without anything breaking. The syntax needs to be worked out. It needs to be it needs to be it needs to be it needs to be it needs to be it needs to be
Could you be more concrete? It could be that you know exactly what you're doing and I'm not getting it, or it could be that you have some misconceptions that we could clear up, but without more detail it's hard to tell. Are you worried that calling `.clone()` on a `Copy` type will be less efficient than moving it? Or that taking a `&amp;self` reference to a (small) `Copy` type will introduce unnecessary indirection? In both cases the compiler usually sees through the abstraction and generates the same code.
Yes! Tokio / Futures
Thanks! So this is focused on web/networked services?
The latter, largely because this is a public trait that may well be implemented on a foreign type. I’d like to be explicit instead of relying on the optimizer, if at all possible.
I think it's fair to say that it's focused on networked services, yeah. Typically, "metrics" is sort of a catch-all term for values that describe system behavior: requests per second, how long a database query took, etc etc. That's definitely the context in which I was in when creating the library, but I've also seen metrics libraries used in applications that weren't networked services. &amp;#x200B; As an example, I've pushed metrics out of batch processes that run very infrequently, etc. A lot of the properties inherent to the systems used to visualize collected metrics -- viewing values as a time series, being able to group metrics, etc -- are generally useful for other types of numeric data that is measured over long time scales.
Ah well. I appreciate the "sigil syntaxes" section on the benefits of not adding new sigils, but I do wish the choice of _which_ existing sigil to reuse was better addressed. It seemed like using `-&gt;` over `.` would retain the benefit of not introducing a new sigil, without the ambiguity of confusing it for field access. Plus it conveys the feeling of motion of "piping through" a keyword that seems like it'd be a boon if it were every expanded to cover `match`, `yield` or whatever. Oh well, in the end, it's only syntax.
&gt; Usually at this point, someone mentions the accessibility aspect It's a fair criticism for a project that leans so heavily on a reputation of "inclusion". Once again we see that it's empty self-praise.
 use std::borrow::Borrow; impl&lt;T: Borrow&lt;Bar&gt;&gt; Trait for T { fn thing(self) -&gt; usize { 1 } } The [Borrow trait](https://doc.rust-lang.org/std/borrow/trait.Borrow.html) can be used to abstract over types which can give you a reference to a type, and it's implemented for values.
&gt; It gives a lot of people like me concern that there's a chilling effect going on. There is, and it's always the same few people behind it. (How many minutes will this comment last before one of them removes it?)
https://github.com/ryantenney/metrics-spring this in rust is my dream. @ExceptionMetered and @Timed are godsends in the java world for me (I just slap these on every public function and it covers 95% of my metrics needs). A rust macro for these that integrates w/ any metrics provider would be so useful
&gt; The top three commands are: &gt; &gt; 24.43% rayon::iter::plumbing::bridge_producer_consumer::helper::h0ada5e1800c62e78 &gt; 22.70% rayon::iter::plumbing::bridge_producer_consumer::helper::hb8c28d295f89a4a0 &gt; 17.43% __memcpy_ssse3_back &gt; &gt; This leads me to believe that Rayon's work stealing schedule ultimately undermines its performance for this benchmark, as so much time is spent copying data between threads. Note that because of inlining, this is probably still going to be mostly your own code. The helper doesn't actually do much, and even the parallel iterator code for `Vec`'s `par_iter()` and `par_iter_mut()` is just calculating pointers, not moving the actual data. It may still be that OpenMP is more clever about how it divides data for cache/NUMA awareness, but I'm not sure. Is the omp version of your code available for comparison? I don't see it in the repo.
I see. I work on more HPC oriented applications so metrics to me are more performance oriented, memory bandwidth, cache misses, hot paths, etc. I understand this is a domain specific term, however. Looks very cool! Thanks again!
I'm creating a modeling language and architecture that works in harmony with ipfs, protobufs, and wasmer. It will make it relatively easy to create secure community developed p2p mmos.
It does, so the mutex would be unlocked. The problem is cases like this: let inner = some_mutex.lock(); inner.foo = fut1.await; inner.bar = fut2.await; If this future is cancelled during `fut2.await`, the value in the mutex will be left in an inconsistent state. Unsafe code has to account for cancellation to be correct; for example, this is not correct: let inner = some_mutex.lock(); unsafe { inner.temporarily_break_safety(); something.await; inner.fix_things(); } If this future is dropped while awaiting `something`, the mutex is left in a broken state. This reminds me of [Pre-Pit Your Peaches](https://cglab.ca/~abeinges/blah/everyone-peaches/); in that post it's "you can't expect your destructor will be run," in this case it's "you can't expect your future will be polled to completion." It also seems similar to threading-mutexes being poisoned by threads panicking while holding the lock; will async-mutexes need to poison themselves if a future holding the lock is cancelled?
I didn't wanna be that guy again but: `await &lt;cancellation_predicate[optional]&gt; { &lt;future&gt; }` There could be place for `&lt;future&gt;.await { &lt;cancellation_predicate&gt; }` but that's just a semicolon away of changing totally its meaning.
Thanks, I think this is probably the right direction, but in my case, I don't have anything concrete for `Bar`. Basically, I'd like to write something with the semantics of the following: trait Trait where Self: Copy { fn foo(self); } trait Trait where Self: !Copy { fn foo(&amp;self); } I'm stumbling over how to write this using `Borrow`, as I'm not really sure of a way to write `self` that encodes this.
Thank you so much for your efforts!
Your code is fine for the vast majority of cases. There's a hidden bug here though: if you're targeting a platform that has a pointer width less than 32 bits (say 16 bits), and vector\_size is more than your maximum pointer size (say `std::u16::MAX`), your conversion using as will silently overflow, leading to a vector\_size that's too small. It's more idiomatic to perform integer conversions using `from` or `try_from`, because these functions will always preserve the original value if they succeed. This is a pretty extreme edge case, but I find that it's better to be in the habit of doing things the right way, especially when doing things the wrong way can potentially lead to bugs that are hard to track down. I would write the line as: let vector_size = usize::try_from(num_pixels * values_per_pixes).unwrap(); You don't need to add a type ascription because `usize::try_from(...).unwrap()` will always return a `usize`, so there's no ambiguity.
So the [`metered`](https://docs.rs/metered/0.2.0/metered/) crate does exist and basically provides this! There's a to-do item on my list to see if there's some way if support could be added to \`metered\` to become \`metrics-core\`-compatible so that \`metrics\` could be a backend for it. Alternatively, we could write our own macros but I've been trying to resist the urge to just copypasta things into the crate and instead focus more on being interoperable.
Back around Rust 1.0 (when I was living out of a backpack in the SF Mozilla offices) a friend said he had some expertise making podcasts and asked if I wanted to be his co-host with the responsibility of being his technical specialist and community contact. He gifted me a rather nice mic and we did several interviews with prominent developers and library authors, but I don't know if he ever actually got around to editing any of them and so they've just never seen the light of day. I do still have that mic and I'd still be happy to help people make a podcast, I just don't have any audio editing chops or podcast know-how. :P
I've seen metered before and it seems great! One thing that was missing iirc was the autodisxovery of a compatible metrics registry like my link above does. Basically since dropwizard/micrometer have standardized interfaces I don't even need to tell it about my metrics registry it just knows where to look. Not sure how doable that is
Heh, nope, even earlier than that: the self-hosted compiler first bootstrapped in mid-2011, whereas 0.1 was released in early 2012. :) We didn't even have a version number back at that point!
Down voting me dont bring back the OBSOLESCENCE of JAVASCRIPT, BLOODY FUCKING SCRIPTING incompetent RETARDS
I wouldn’t be surprised if, in such an organization, answering questions like this is a prerequisite to being allowed to allocate one’s time to learning.
What exactly does `Copy` do and how does it differ from `Clone`? The docs says `Copy` works with "Types whose values can be duplicated simply by copying bits." and `Clone` is "A common trait for the ability to explicitly duplicate an object". How does `Copy` work if it does not just duplicate the object? What makes it cheap? Can you use `Copy` in `no_std`?
I meant, would it be possible to define a type like below and use the same in defining generic functions? `type Data = Vector3&lt;T&gt;`
Nice! Yeah, I think I see what you mean overall. Inspired by your comment, I created an issue on the repo to see if we can explore a PR that might let the two crates work together: https://github.com/metrics-rs/metrics/issues/16
That's awesome! Ill take a peek this week maybe this is my chance to make rust tick all the boxes so I can use it at work
Copy is basically a flag to the compiler that you may simply make a byte-for-byte copy of the struct (i.e. `memcpy`), and nothing will break. This is done implicitly: when something expects an owned value of an object, the compiler will automatically copy it and the object will now be owned in two places. **This is the default behavior in languages like C.** C++ does this by default, but has copy constructors that allow you to force a Clone on a specific class. Implementing Copy *requires* that you also implement Clone: however, the Clone implementation should just use Copy (e.g. `*self`). Copy may **not** be implemented unless all of the struct's children also implement Copy. Copy is fine for most normal types: there's no problem simply copying numbers, booleans, etc. and other structs made up of just those. However, what if they have a custom destructor? A `Vec` will automatically free the memory that it owns. However, if you just make a byte-for-byte copy of it, the memory will be freed twice after both `Vec`s are dropped! Clone is different, as it allows you to write custom code to duplicate your struct. In the case of a `Vec`, the clone function will allocate new memory and clone over each element from the original `Vec`. Note that if a type also implements Copy, then Clone (should) just execute a copy: meaning that cloning a type that implements Copy is a zero-cost operation. Both traits work on `no_std`. Typically, you will implement Copy whenever you can, and only implement Clone when your struct or any of its children have a custom Drop implementation.
All `Copy` types are `Clone`. The difference is that while `Clone::clone` might do something extra or different (like bump a reference count, or allocate memory), `Copy` *must* be a bitwise copy of the data, and *only* a bitwise copy. This also means that for a type to be `Copy`, any values it contains must also be `Copy`, which usually just means things like numeric types, `bool`, and immutable references. For instance, `struct MyInt(i32)` can be `Copy` because `i32` is `Copy`, but `struct MyString(String)` can't be since safely making a copy of a `String` requires allocating memory, copying the contents of the string over, and changing the copy's storage pointer to the new allocation. Because `Copy` happens implicitly, rather than explicitly like `Clone`, it's also considered good practice to only derive `Copy` for relatively small types. There's nothing stopping you from implementing `Copy` for your type containing dozens of `u128`s, but people would get caught off-guard by a `Copy` type being that heavy.
I second this! Great show!
I'd listen. I get my rust news from new rustacean so more in depth or finely targeted subjects would be of the most interest to me.
Oh, wow! Thank you for my first gold, kind anonymous Redditor!
In general if you want to put multiple different kinds of objects implementing the same trait in a single vector, you want to create boxed trait objects. Those are of type `Box&lt;dyn MyTrait&gt;` and you can create them with `Box::new`. See https://doc.rust-lang.org/book/ch17-02-trait-objects.html. But note that the example you linked to is using "specialization", which isn't yet part of stable Rust.
Something like Python Bytes for Rust, I'd love that 👍
I think you're looking for /r/PlayRust, friend. This sub is for the Rust programming language
Whether it’s `foo.await` `foo@await` `foo~chiilllll` or `foo.honeybadger` I don’t care, I’m just ready, willing, and excited to use it already!
Thank you, Chris, for your wonderful contribution to the community. Best of luck to you and your family and all your future endeavors!
If you have a collection of different types which all implement a common trait you will need to use a `Box&lt;dyn Trait&gt;`. This signals to the compiler that you will be using dynamic dispatch to call the trait interface. `impl Trait` is monomorphised under the hood and uses static dispatch, so you can't mix different types together in the same collection.
You're talking about incorrect but completely defined and deterministic behaviour, whereas having UB in your code is another story.
I understand trait objects, my problem is that one of the traits requires `Sized` but rust explicitly disallows `Self: Sized` traits from being trait objects so `Box&lt;dyn Visitable&gt;` results in a compiler error which is where I'm a bit lost at. I'm not sure what (if anything) needs to be done to get this specific solution working, or if I need to go in a different direction entirely.
This issue does not seems to be Rust specific? In Go it would lead to the same, unless I am misunderstanding the issue. You could put your application behind a rate limiter.
I've been teaching a Rust course and an audio course this quarter, with many of the same students. Better Rust audio information and infrastructure would be great. A separate Rust audio forum seems eminently reasonable. I'd really prefer /r/rustaudio here on Reddit to a Discourse site, but it looks like that ship has sailed. *areweaudioyet* sounds fantastic. I'm pretty swamped with end-of-quarter right now, but I'd really like to help. What's the best way for people to get involved?
They can definitely be elided in further usage! `fn funcname(x: Foo) -&gt; Bar` is totally valid with your above definitions. Lifetimes can be elided almost everywhere, it's mostly just on struct/enum definitions where you still need them.
This feels like another rash decision. Some of the previous were: * leading bars in match expressions * if let instead of is expression * rust lang website redesign * tons of rarely needed apis stabilized in each release like option.transpose, iter::successors or slice.rchunks_exact It’s not enough to stop rust from taking over the world, it’s just rust is slowly drifting towards popular, quality but inelegant languages like JavaScript or Objective C++.
I've been working on [a crate for interpreting G-code](https://crates.io/crates/ngc); it parses the LinuxCNC dialect properly now and I want to finish the evaluator now, which resolves expressions and translates G/M codes to a sequence of "programmer readable" instructions for the machine.
Yes, but also OP could just, like, say that? They seem to be avoiding answering the important question for some reason, even though there are definitely many reasonable answers.
&gt;From the article it also seems like they now built a hand-written state-machine inside constellation. That might be ok depending on the complexity of the component. But it might also be possible to run that as a more synchronous looking workflow with Futures or async/await. &amp;#x200B; I'm going to build a bit of a straw man and take your comment probably much further than you intended to, because it reminds me of other things I've heard before and I'd like to address it all at once. The way I think about it, is that Servo is itself implementing a "runtime" on which "more synchronous looking workflow with Promises or callbacks" are possible(think the JS running on a web-page). &amp;#x200B; This "runtime" not only has to run the "logic" of web-pages, it also deals with their rendering on the screen, and finally a wealth of API's are offered to the "code" running inside that runtime(think how adding an `img` element to a page magically fetches the image and then renders it, and that your JS code can also still interact with that element and change the image data and so on). All of this isn't an excuse for "messy" design choices, however it's just a fact that the system you're trying to implement is "messy". And somehow, that's exactly what makes it fun to work on Servo. So an argument in favor of "why don't we just use XX for this" often breaks down in the face of the countless details that need to be finely handled, which often means you need to implement a "messy, hand-rolled, XX-like" system just because you need full control. And that's just one part of Servo. To coordinate those various parts require "hand-rolled" lots of things everywhere. Also, that doesn't mean contributors can't use libraries like rayon/futures/crossbeam. People don't necessarily "implement their own thread-pool", instead people might use rayon where they can, but its use is going to be buried somewhere inside a "hand-rolled messy" component, which then needs to be plugged-in into various others such components in a messy way. I remember being involved in a discussion on "multi-producer/consumer" channels, and thinking "that's basically what the constellation is, it's a giant stateful mpmc channel of communication between the various 'top-level' components of Servo". Another example I worked on is the [task-queue](https://github.com/servo/servo/blob/master/components/script/task_queue.rs) used for JS tasks, which involves prioritizing some tasks over others in some cases. Perfect for a [BinaryHeap](https://doc.rust-lang.org/std/collections/struct.BinaryHeap.html)? Nope, because the "priority" logic of the queue doesn't as much relates to its data, as it relates to "how the event-loop runs". So you end-up with a "half-baked" priority-queue, which however does comply with the concepts of [running an event-loop in HTML](https://html.spec.whatwg.org/multipage/#event-loop-processing-model). And finally, a "hand-rolled state-machine" or "half-baked priority queue" doesn't mean that the code itself is messy and half-baked. It just means that the components you're working on do not fit into an existing concept. Trying to make them fit into such well-known concept would be a good example of a [Procrustean bed](https://en.wikipedia.org/wiki/Procrustes). &gt; The parent task must outlive the subtask. The parent task can wait for synchronously for responses from the subtask. The subtask should never do that, since it can lead to a deadlock. I'm not sure if this has been followed in this case: It's not obvious how the relation between constellation and embedder is. Someone tried to write down the relationship between various components: //! * Layout can block on canvas //! * Layout can block on font cache //! * Layout can block on image cache //! * Constellation can block on compositor //! * Constellation can block on embedder //! * Constellation can block on layout //! * Script can block on anything (other than script) //! * Blocking is transitive (if T1 can block on T2 and T2 can block on T3 then T1 can block on T3) //! * Nothing can block on itself! And that's the type of list that has a similar effect to unit-tests: it only tells you about the presence of certain relationships, not the absence of other unknown ones. So simply said, relationship between components, let alone the "tasks" that they run, is never going to be obvious. And this messiness doesn't mean Servo consists of a bunch of code just slapped together. Some pretty decent designs "emerge" over time, but they need to accommodate a certain level of messiness. It's a little like how a city like NYC is "messy", yet it's also beautiful, much more beautiful than say a "high-end" suburb designed with "optimal live-ability" in mind. This is a pretty good overview of the constellation by the way: https://github.com/servo/servo/blob/84ec1316c90a29f07c60e10f78ba4d9f3185bef8/components/constellation/constellation.rs#L5 &gt; If you perform communication between two tasks, always have a clear definition of what is the parent task and what is the subtask. That hints exactly at the peculiarities of something as complex as implementing the Web. People are often unable to precisely pinpoint such type of parent/sub-task relationship, which is not made easier because those relationships can change as a result of page navigation. Example of the type of discussions that occur at the level of "defining" relationships: https://github.com/whatwg/html/issues/3691
This is the first time when I'm not happy that rust moves forward.
This is why you are encouraged not to perform computationally expensive tasks within I/O futures. If an attacker can send you data faster than a tight loop can run, you have other problems.
This quoted sentence is sad. The post doesn’t just notifies public about final decision (different people can different opinions, and sometimes there’s no best solution, and someone has to make the decision), but also accuses people who would prefer different decision of black and white thinking.
I'd be up for helping produce a new Rust podcast! Please message me if you're also interested, and maybe we could share the work!
What do I need to import to use `div_rem` on an `i32`? I see it's defined in the trait `num::Integer` ([https://rust-num.github.io/num/num/trait.Integer.html](https://rust-num.github.io/num/num/trait.Integer.html) ), however importing `num::Integer::div_rem` doesn't actually define it for i32. Also, I'd appreciate if you can tell me how to figure it out for next time. Thanks!
Why is this relevant in here at all? foo borrows x, then by the time you need to use it next time, it's not borrowed anywhere. It's perfectly sequential and if rust borrowck cannot prove such trivial thing without anything special, I'm not sure how it can even work at all.
If you're not using unsafe in your code, dropping futures at weird points won't introduce UB. And if you _are_ using unsafe, well you probably already need drop guards now, so it's not something new.
I agree, it read to me like a fundamental incomparability between IOCP and async, or a limitation of the IOCP design, rather than a problem with Rust's async design. GC languages don't have the problem because they effectively *always* use a buffer that has shared ownership (rather than a borrowed one). You can do that in Rust too. In other words, it can't be safe and zero-cost in *any* language (in the way the author means anyway). It's fundamental, not a language limitation. I may be wrong.
Look if you're going to be using unsafe, you need to think real hard about safety already. Drop guards are required now if you call user provided code. It's not that big of a change.
If the attacker is calling connect faster than the server can process, this is just a straight up DoS attack. Unless you're doing heavy computations on an IO thread, this is not likely to work with just one attacking computer. With multiple attacking computers, this becomes a DDoS attack. The only real good ways of dealing with those are either firewalls / proxies with DDoS protection or some kind of IP blacklisting, hopefully automated. If you're concerned about this kind of attack, I'd recommend using protection services like those offered by CloudFlare, or other DDoS protection providers. With that said, DDoS attacks take a lot of resources or a large bonnet to pull off, so you're not likely to be a target. And with non-blocking-IO and correctly relegating all heavy computations to side threads, I find it very unlikely that you'd run into this issue outside of coordinated attacks (or very large (100,000+ client) workloads, at which point you may want to invest in distributed infrastructure or more servers.
Working on Pushrod 0.3.x series now. I finally got over the hurdle that was causing Mac OS X builds to fail, and worse - not render on the screen. MoltenGL's OpenGL ES lib helped fix that. So, now the library is drawing 100% using GL graphics, which makes me very happy. CPU usage dropped by a factor of 3, which tells me I'm on the right track. The next optimizations will depend upon how well I can optimize for objects that are invalidated. Hope to have a release in the next week or so. Lots of fun things happening! [See here for more info!](https://www.github.com/KenSuenobu/rust-pushrod)
Disagree all you want, but the decision was not rash. I believe big parts of the community is tired of bike shedding over the syntax and just wants something stabilized so we can start using it.
It's not accusing all, or even most, of those people. "I prefer this syntax because..." is a perfectly fine discussion point. However, "This syntax has problem A and therefore all its advantages are null and void" is not.
Our chance to make that explicit in tutorials, stdlib documentation, and examples.
No it doesn't. u/desiringmachines preferred prefix `await fut` themselves, so apparently they are accusing themselves of black and white thinking? That makes no sense.
`use num::Integer` should work. To use trait methods, the trait needs to be in scope.
It's a trait, and it's implemented for `i32`, as you can see on the left bar of the page you linked. To use the methods of a trait, you just need to bring it in scope: `use num::Integer`, after that all the methods should be available on all the types that implement it you have in scope. I don't think there's more to figuring out than "knowing how traits are used". I want to link https://doc.rust-lang.org/book/ch10-02-traits.html, but it does not actually talk about this explicitely...
Looked through the code a bit and I have a few questions about it: &amp;#x200B; Why send metrics over a crossbeam channel to a different thread where each element is batched until a certain size and then demuxed and pushed into a \`HashMap\`? &amp;#x200B; This appears too performance-heavy. Also, why use a \`HashMap\`? Why not make a struct like: &amp;#x200B; \`\`\` struct MyMetrics { total: Gauge, gaugeA: Gauge, frameCounter: Counter, } \`\`\` &amp;#x200B; And then, in code, do a simple \`MyMetrics.gaugeA.record(123)\`. This avoids the 25 ns lookup time of a \`HashMap\`, the 30-50 ns crossbeam channel message time (approximate from my machine). The above is compiled to simple memory accesses and is fast. If you want to periodically send the above \`MyMetrics\`, then having all metrics in a struct like that makes accessing this "batch" very cache-friendly.
Where I work we have very high standards for line-coverage in unit testing (the wisdom/folly of this is debatable). 90% line coverage is considered the minimum. Testing a base class without also testing the subclasses can be infuriating. Testing abstract classes is almost impossible. Most Java mocking frameworks have the concept of a partial-mock - the pernicious idea that you can mock out only part of a class! Beyond the obvious conundrum of "you have to literally rewrite the class at test-time using the mocking framework so you can test it, ergo you aren't really testing the class you wrote anymore," what this has created in the legacy code I maintain is multi-thousand-line God-classes with either no tests or useless "line coverage tests" which use lots of partial mocks and encapsulation-breaking private method calls to exercise the code. The result is that we have no test which actually tests what the class does, and the tests that do exist are difficult to maintain because of their nonsensical nature. The classes themselves are unmaintainable and engineers go on weeks-long spirit journeys into the code to understand how to make simple changes. Not all of this is an inheritance problem, but the true moral of the story is that just because you can do something doesn't mean you should. The cleanest code is often the simplest, the easiest tested, and the most reliable. Rust's current lack of a mocking framework is a positive thing in my book: the tests I see are clear and obviously demonstrate use cases. Rust's lack of complicated encapsulation and polymorphism creates code that is simple, declarative, and deceptively obvious to read. It's very easy to test, it doesn't surprise you with strange and exciting behaviors. I expect Rust to get some more enterprise-y testing tools later, but having a strong base upon which to build without those tools will make the end ecosystem stronger, I think. Truly, less is more.
&gt; I feel very much like I'm polling instead of getting notified when it's finally ready. Nah, you're just receiving spurious wakeups.
hi, don't you just miss the "with_capacity" on your vector creations ? it's just a guess but you might be resizing your vectors a lot.
Continue https://github.com/request-for-explanation/podcast
A good inspiration could also be the [Micrometer](http://micrometer.io/) library which is gaining popularity in the JVM ecosystem. It's essentially a facade around multiple metric backends.
There's https://www.open-mpi.org/projects/hwloc/ to deal with hardware locality, which appears to have rust bindings too. Don't know if it's relevant for this benchmark test.
None, they all seem to be unmaintained. https://github.com/shepmaster/snafu looks good though.
I prefer learning things by getting my hands on, so here is my recent take on replacing the old async combinators with the new async/await syntax: https://github.com/nearprotocol/nearcore/pull/976/files#diff-5aae9dc5316d68d492a5a37b85981b8f. I hope you find some tricks there useful.
For `Recipient` I'd need to store one for each type of message I wanted to send right? I might go with `Box&lt;dyn ...&gt;``.
I don't use any. Never felt the need to use one.
err-derive as a thin layer on top of std::error::Error, since std::error::Error is getting fixed to provide the same benefits as \`failure\`. [https://users.rust-lang.org/t/announcing-err-derive-yet-another-error-handling-library/23594](https://users.rust-lang.org/t/announcing-err-derive-yet-another-error-handling-library/23594) &amp;#x200B; See also [https://www.reddit.com/r/rust/comments/8lt8k6/do\_i\_really\_need\_failureerrorchain/](https://www.reddit.com/r/rust/comments/8lt8k6/do_i_really_need_failureerrorchain/)
I use simple_error. I'm sure it would be an awful choice for libraries, or polished applications, but most of my coding is academic, where I just want a nice human readable error out in the least amount of work
I've got recording equipment (in Berlin) and also like talking about Rust, I just don't have the time to do the production. Feel free to ping me.
I think there could be a way to do this but let me make a case for why you shouldn't. Blanket implementations are prone to be problematic as special cases arrive and something you may expect to work a particular way could break a safety contact. The rust type system is built to encourage additive features. That means, define a functionality and then have different types of data opt-in to said functionality. This means that generic functions only ever accept types that are *hypothetically* guaranteed to work. If you want to do it quickly you can write a macro that puts in the trait implementations.
`trait T: Sized` means that the memory size/layout of the type implementing the trait must be known at compile time, everywhere it is used. This is incompatible with trait objects by design. You would need to remove the `: Sized` restriction. As an example, I've done something similar to enable cloning of trait objects (Clone requires Sized). ``` trait MyTrait { fn clone_boxed(&amp;self) -&gt; Box&lt;dyn MyTrait&gt;; } ```
Rust does not feature this kind of overloading. You'd need to abstract over the Self-type via another trait (`type S: SelfParam; fn foo(self: Self::S);`) but it's impossible to generalize over by-value vs by-reference in this case because `Self::S` needs to implement `Deref&lt;Target=Self&gt;` to qualify as a Self-type which is does not if it is a value-type (because it's not a reference you could derefence). At least I could not get this to work, not even with `#![feature(generic_associated_types, specialization)]`. With specialization, you can at least implement `SelfParam` differently for `T: !Copy` as for `T: Copy`, but `Deref` is the bottleneck. Another solution is the definition below even though it might not be the most satisfying one: trait Trait { fn foo(&amp;self); fn moving_foo(self) where Self: Sized { self.foo() } }
"Solved by education" looks much like the mess that is C++ today to me.
\`Sized\` information isn't available when using a trait object, so you can't have the trait require it and use it as dynamic trait, since requiring it implies that you trait will make use of the \`Sized\` in some way. &amp;#x200B; \---- I thought about this answer and decided to not delete it since I want feedback on this ---- &amp;#x200B; I assume you are using \`Sized\` to get some information inside a provided method. You can circumvent this by not relying on \`Sized\` on the trait definition, and add a blanket implementation for anyone that implements \`Sized\`, and implementing that functionality in there. I don't know if there's a way to provide a blanket implementation for non-\`Sized\` and have some panicking code in there. &amp;#x200B; \---------- Sensible answer starts here ---------------------------- &amp;#x200B; You could also "abuse" the trait system. I can't remember the name of this pattern (found it, Sealed Trait) but basically you create a new \*private\* marker trait and you make your \`Visitable\` rely on that trait. Then blanket-impl the private trait for \`Sized\` ;) &amp;#x200B; [Here](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5c7bd0890b5b36370b8b104e92c1efa8)'s an example!
I haven't found one that I enjoyed using.
Pathfinder is a GPU vector graphics renderer written in Rust by /u/pcwalton as part of his work in the emerging technologies team at Mozilla. This post explores the details of what happens on the GPU when using pathfinder.
Thank you. This worked great. Haven't tried `unsafe_pinned` yet but will surely keep it in mind!
None. All of those I have checked are more like liabilities than real breakthrough in Error management.
There are other ideas about using `-&gt;` for solving an aliasing issue around raw pointers, replacing `(*rawptr).member` by `rawptr-&gt;member`.
I've just been using std::error after experimenting with failure. Most of the error crates seem to be being left unmaintained and there seems to be some improvements to std::error that originally motivated things like failure. However I'm really unclear on what is the best practice or recommended community approach is going to be in the future. I am assuming std::error?
I just updated my (first) crate and thought I'd post it here as it might be useful for other people as well. The crate provides a macro that returns the string name of a binding/type/function. This helps with refactoring in certain cases (renaming), because names can be written as Rust identifiers instead of as strings and can thus be more reliably be renamed automatically. Any thoughts or feedback is greatly appreciated!
I've been happy with quick-error, so I'm probably going to stick with that for the foreseeable future.
i'm curious about this too.
Good to hear. I had plenty of hacks over the legacy [https://github.com/posix4e/rust-metrics/](https://github.com/posix4e/rust-metrics/) , added influxdb reporter, function based gauge. &amp;#x200B; Metrics was the key tool for going to production. It's cheerful to see it back to life.
Very good explanation, thanks!
Or maybe he just wants to have some fun building the compiler. You all trying to solve this guy's problem, but he may not have any in the first place.
So for me, the argument against dot-postfix syntax has always been about control flow visibility. I really think this problem reinforces that argument. If `await` is a point at which the function can exit (permanently; `Drop` is called for all local variables), then that is a very serious thing that programmers aiming to write robust programs must be aware of. It's not that a prefix `await x` syntax magically solves the problem, but my personal feeling is that it reduces the confusion by a teensy amount.
"I want to have fun building the compiler" is a totally viable answer to "why are you building the compiler".
Yes, even though I belong to the vocal group (minority?) that would definitely prefer a different syntax, I think it would be very wrong to characterize this process as being rushed. It seems to me the team has made every reasonable effort to carefully consider the options and make a decision they believe is right based on sound arguments.
This is as good as a pull request ;)
Clippy is tied to Rust releases so to get a very recent one the easiest way is to install nightly rust then rustup component add clippy to that version.
I'm uneducated on the subject but a thought crossed my mind, could the tiling process benefit from emitting tiles of different sizes (16x16, 32x32, 64x64..)? If so, what if the viewport was split into a quadtree instead? (Branch would split only if it was beneficial) Thanks
I find it very unlikely that I'd ever use a helper crate for error handling inside a library. Doing it out by hand is not that much work, and once the code is written, it rarely changes. And adding new error variants is typically very easy: add a new variant to your enum, then add a new branch wherever the compiler complains (e.g., in a `fmt::Display` impl). In other words, IMO, error handling helper crates don't provide enough benefit to justify both their additional complexity and the additional dependencies they bring in. The key advantage, IMO, of a error handling helper crate is backtrace support. In an application, this can be quite useful. I've been experimenting with using snafu for this in ripgrep, but haven't had time to finish the port yet. I could imagine something like `failure`---but with `derive` support as good as snafu---also being useful here. (Notably, last time I checked, `failure` doesn't support deriving `Display` impls that use file paths.) The key difference being that `failure` biases toward the use of trait objects for errors, where as snafu biases toward concrete error variants. I still don't like using trait objects for errors in libraries outside of particular circumstances, but in applications I think it's fine, especially if it comes with backtraces for free.
Does this enable something that you cannot do with stringify?
Tbh i have no clue,I just use libgen.is because it's the first I've discovered Regarding the project,well it's basically over I need to do the cli interface.I'll send you the github with the project itself when it's over
Isn't this a much bigger problem than just poisoning the mutex if the future gets canceled? It seems like it would be very easy to deadlock by holding the lock across the yield point, since a second task might be attempting to lock the same mutex. It could also cause performance issues because the lock is held for a potentially very long time while waiting on IO. The deadlock issue could be partially solved by making the mutex async-aware so that locking is also a yield point. It seems like it's a really bad idea to hold the lock across a yield point in general, and that yield points should be treated as possibly taking a very long time. It's a bad idea to hold a mutex across an unrelated slow computation even in normal threaded programming, and await should be treated the same way.
It will only compile as long as what you're stringifying actually exists. So if you use it in an error message, or in debug/log output, it will alert you when you make a change causing the message to no longer be correct.
Ah okay, I thought `stringify` would complain aswell if given an identifier which doesn't exist, however it does not [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;code=%0Afn main() {%0A stringify!(b)%3B%0A}). Now I understand the value of this crate!
Sorry, didn't put the right link in the OP. [https://github.com/UoB-HPC/BabelStream](https://github.com/UoB-HPC/BabelStream)
It should be noted that many things in the UCG reference are *not* guaranteed, until such time that an RFC guarantees them, and are part of an evolving discussion of what might be.
[https://github.com/timsueberkrueb/visit](https://github.com/timsueberkrueb/visit)
This is correct, it does this by creating an implicit usage of the symbol using some (closure) trickery, which will be optimized away by the compiler.
I think you could get something close to this with a PhantomData marker for your indexes? That way you can't use an index obtained from an Arena&lt;u64&gt; into an Arena&lt;u8&gt; but beyond that it won't work if both Arenas are of the same type. I've made a [proof of concept demonstrating this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d70402ee247ff65110ce6c1a745f441a) however I don't think this matches exactly what you were looking for.
It depends on what you are trying to optimize. A quad tree is often useful when you want to compress a sparse representation. For example it could help with reducing the size of the storage of solid tiles and generate fewer vertices for the solid tiles pass. The solid tile pass is very fast so it's not where I would look for optimization at the moment. There's an up-front cost when going from a straightforward random access organization of data (an homogeneous grid), to something more branchy like a tree. So I would be rather challenging to not regress the performance of the tiler if it had to build a tree instead of a uniform grid. When rendering mask tiles into the float texture, the taller the tile the more overdaw you get from having each per-edge quad extend to the bottom of the tile. So making tiles taller isn't necessarily a win even if there are few edges on the tile. Extending the width of the tiles would reduce the number of edge quads since they have to split at tile boundaries. So my answer to you is that from where I stand it is unclear whether a tree structure would provide benefits to the way pathfinder works. That said, there exist rendering approaches that are similar to pathfinder but not quite the same which do benefit from using trees. An example of this is the *random access vector graphics* paper linked at the bottom of the post which works on a regular grid and was improved upon in the *massively parallel vector graphics* paper but introducing a tree structure to store the edges. In these two papers there is a shader that loops through nearby edges in order to determine the color of a pixel. Due to the execution models of shaders, having a roughly homogeneous amount of edges to visit within a paquet of parallel shader execution is key to the performance. Pathfinder, however, doesn't have a shader that loops through edges, so it is not very sensible to divergence in the number of edges per tile.
Thank you for the quick response! This is pretty much what I'm already doing, but I rather use the whole arena as the type parameter to the index so that the error message is a bit more descriptive. I was wondering if it's possible to solve the case where the types are equal at compile time.
Hey friend, a quick update. I'm using a fork of tui-rs in the meantime while I was for my pull request to be included in a new release, so I'm using a git dependency which does not play nice with Cargo. I don't plan on publishing my fork of tui-rs, so you're going to want to hang tight until the new tui-rs is released. In the meantime, you can do: `cargo install --force --git` [`https://gitlab.com/DarrienG/terminal-typeracer.git`](https://gitlab.com/DarrienG/terminal-typeracer.git)
I noticed integrating Pathfinder is on the roadmap: https://github.com/servo/servo/wiki/Roadmap Is there an bugzilla/Github issue to track the progress? Also, where does [font-kit](https://github.com/pcwalton/font-kit) fit in all of this?
This is awesome. Thanks for making this, I'm using it a lot already.
None of them. I refactored away from ``failure`` some time ago, replacing it with custom error enums and transformation traits. Thus far, I have gotten by just fine. However, I might adopt ``derive_more`` crate to reduce boilerplate I typically write.
How about adding a unit struct for each instance of the arena, and add it as a type parameter for both index and the arena? Maybe with const generics you could have an instance ID which would be incremented every time you instantiated an arena.
I guess I didn't highlight it well enough but \`metrics\` is moving away from the event loop model into a pure atomics-based approach -- there's a PR open for this right now. There's still two ways to record a metric, a "slow" and a "fast" method, but everything underneath, data-wise, still becomes faster and has lower tail latencies. &amp;#x200B; Beyond that, it was an intentional design choice that we should be able to create/register metrics by recording a value to them. That will always be one of the two ways to use \`metrics\` from the input side. The aforementioned PR adds new types, identical to the types you've sketched above, that allow direct access to update the underlying value: they can be locals, they can be embedded in your structs, etc. It's not directly updating a u64 in your own struct, merely an \`Arc&lt;Atomic\_&gt;&gt;\`, but it's still fast by reasonable measures. &amp;#x200B; I tossed and turned over whether or not there should be a way to explicitly register a metric, truth be told. This existed originally, and it was \_the\_ only way to register a metric. It made the "after init" metric output nice because the schema was fixed but it required code to register metrics ahead of time either in constructors or in some global initialize method and it just felt gross to me. I've not used a system that requires a particular metric schema lest it totally fail/explode, but I'd be curious to hear about any if they exist. &amp;#x200B; Beyond that, I also thought about the idea of registering your own types to be pulled in by the receiver, or to allow arbitrary pushing, but my conclusion always seemed to be the same: \- no matter what, if you want multiple threads pushing to the same output, you're going to have to synchronize somewhere \- you might be able to store metrics internally much faster than \`metrics\` can take them off your hands, but you still have to eventually send them, or allow them to be pulled, and you're back to synchronizing &amp;#x200B; You can obviously structure the synchronization such that it does its best to avoid the critical path -- maybe you flush your local metrics during dtor -- but when does that dtor run? Is it still in the critical path for serving a request? Do you now try to flush it to TLS? Who cleans up that TLS for you? Is it the next flush? How long will it be until that happens? Are you OK with metrics sitting around waiting to be purged? &amp;#x200B; Ultimately, I'm fairly comfortable with the performance profile of \`metrics\` (once the atomics PR lands) for 99.9% of the things that may use it. If a user has super tight loops, they can always aggregate counters/track gauges locally and push them as a single update. It won't beat memory access speed like updating a struct field, but the ergonomics around supporting \_that\_ architecture don't seem worth it.
Are there plans for Pathfinder to be ported to something like gfx-rs? With Apple's deprecation of OpenGL, it seems like that's the likely way forward?
Heck yeah! Alex was very positive/quick to transfer ownership when we told him that we wanted to revive the crate, so hopefully we can do right by him with our work. Definitely stop by our issues page if there's something you don't see and wish it had. This crate is all about helping you make the leap from "the software works" to "the software works *well*, and we can show you how we know". :)
Yeah I was considering it, but then the consumer of the API would have to make a unit struct for each arena, which is not a big annoyance, but I was hoping there was a elegant way around it. Using const generics would probably be my best bet, thanks for the tip!
From my original glance, it looks like their concept of vendor neutrality is that they can output to multiple backend systems, which `metrics` is definitely capable of doing. I assume their value proposition is a little more nuanced than that, so I'll have to take a deeper loop. Thanks for the pointer!
I always forget how notifications work for comment threads, but I responded to the parent.
Inspired by your original comment, I created a PR to hopefully explain more simply and succinctly what problems the library intends to help users solve: [https://github.com/metrics-rs/metrics/pull/17](https://github.com/metrics-rs/metrics/pull/17)
It was just about two weeks between syntax committed to the repository and final decision is made. This doesn’t look like proper decision making. We could start using await with macro and do syntax later.
What you're basically describing is a [slowloris](https://en.wikipedia.org/wiki/Slowloris_\(computer_security\)) attack. As soon as you use `epoll`/`select`/`IOCP` you're basically vulnerable to this type of attack. It doesn't really have anything to do with `await`.
Have you seen this? https://fullyfaithful.eu/bounds-check-elision-rust/ It sounds very similar to what you're trying to do.
I don’t recall “null and void” style arguments in threads, maybe only a few. Advocates of all syntaxes presented mostly good arguments..
See my `qcell` crate where I tried all the options I was aware of (after also asking here). If you come up with a new way, I'll try and incorporate it, but I think those are the choices: numeric ID, marker type or unique lifetime. Lifetime has good properties but means lifetime annotations everywhere.
The first step would be [https://bugzilla.mozilla.org/show\_bug.cgi?id=1543981](https://bugzilla.mozilla.org/show_bug.cgi?id=1543981) and then see where it goes from there. What I want most is to move a subset of SVG out of the current CPU fallback and use pathfinder instead. &amp;#x200B; We don't have immediate plans to use pathfinder for text rendering in Firefox although we'll be happy to eventually (it is just not at the of the priority list right now). Whenever we do that, Fontkit will replace a bunch of the text rendering code currently in WebRender.
There are plans although I don't know how soon that will happen.
I remember reading that the `failure` crate was waiting to find out what the future would be for the standard `Error` trait.
This [issue](https://github.com/rust-lang/rust/issues/50547) was opened a year ago, and the very first comment is about the await syntax with a link to even earlier discussion.
Thank you so much! I think that pointed me in the right direction, because closures always have unique types even though they are visually equal in the sourcecode. I will try to use that fact to make this work.
It's a fairly easy task to roll your own. So that's what I did.
Since I'm sure we'll be getting some comments here, a Reddit thread already exists for the update on `await` syntax: https://redd.it/bu0r5n
This [issue](https://github.com/rust-lang/rust/issues/50547) was opened a year ago, and the very first comment is about resolving the await syntax with a link to even earlier discussion. There's still ~2 months of experimentation with the final syntax before it's actually released. I feel awful for the language team having to endure accusations of "improper decision making" after all the care they've taken with this.
Thank you for the insightful reply
Then you would have reinvented err-derive.
I keep trying them, but I always end up being happiest with some simple `impl_display!` and `impl_error!` macros that cover the common cases, and implement things manually for the more complex ones.
I will take a closer look at this. Thank you for the suggestion! Have you thought about using closures as shown in the article linked by /u/shadow31?
Yes, but rust is not designed for it. The method is documented in [this thesis](https://raw.githubusercontent.com/Gankro/thesis/master/thesis.pdf) (Chapter 6.3). I had a similar discussion about this topic earlier. Rust's monomorphisation rules prevent you from creating this structure from real types, however, the `for` keyword in closure type signatures can be used to create a new lifetime on demand. Here's a summary of some of the key points. * The `id` datatype is as follows: `type Id&lt;’id&gt; = PhantomData&lt;Cell&lt;&amp;’id u8&gt;&gt;;` Putting your lifetime in a `Cell` prevents the compiler from coercing one lifetime into another, keeping them unique. * Use the closure signature `F: for&lt;’id&gt; FnOnce(SomeData&lt;’a, ’id&gt;)` to create an existential type. This is essentially the same as Haskell's `ST` monad. * Even with all of this, you have to keep the indices and the parent in a closure. It requires an enormous amount of boilerplate, and when you get things wrong, the compiler gives nonsensical errors. In short: don't do it. It's verbose and you're more likely to create an error in the lifetime code than actually mis-indexing something. If it were me, I would probably create a struct which owns the arena and anything that has an index to it. The public interface to this object should not return anything that could be potentially mis-indexed. That way, as long as the code inside the module is correct, and we only use the public interface outside of it, then we shouldn't be able to mis-index anything. Alternatively, you could create a unique ID at runtime for each arena, and then check at runtime if the index and arena are the same.
Ah great! That reads perfectly clear to me.
The same closure in the same place in the source code always has the same type. See the errata. So the closure type trick is exactly the same as using a marker type (i.e. you might as well just put `struct Marker;` inside the function.
It is equivalent to using a marker type, because the same closure at the same point in the source code always has the same type. (Unfortunately!!!)
Exactly 👍 I feel the language team has handled it very well and I’m excited to use async/await once it is stable.
What if you hide the creation of the closure behind a macro and make the constructor unsafe? Cause then the macro will be expanded and the closure would be unique (in theory at least?). I'm going to try it out when I'm back infront of the PC.
That's awesome to hear, thanks for reaching out! As per a subreddit, I actually don't think that's a bad idea. The discourse forums I linked in my post are currently only populated by people directly contributing to the vst/lv2 crates, so it's rather limited. It's great for keeping track of development but outside of that I'm not sure. Reddit could be a more open and general community. As per contribution, I think the most valuable resource at the moment is information. Across Discord &amp; Reddit, I see a lot of architectural and design questions about audio topics go unanswered. This is understandable since audio development seems to be less "open" than a lot of other development subjects. That's why we wanted to start up rust.audio and areweaudioyet. However, going back to a forum or subreddit, I think that could be an excellent place to share resources too. I'm really open to anything that'll help unify and open up audio dev, especially for Rust. If you're so inclined (and don't mind installing yet another chat app) we have a telegram chat here - https://t.me/joinchat/CHRaiE64AfwG0uBp5hirLg - it was created for the purpose of developing the `vst` crate, but since it's evolved to include some of the other previously mentioned topics. I'm pretty nooby at DSP/Audio Dev in general, but there's a lot of experienced audio industry peeps in there. Thanks so much for the message.
The future of `std::error::Error` was kinda decided in [RFC 2504](https://github.com/rust-lang/rfcs/blob/master/text/2504-fix-error.md)
&gt; This issue was opened a year ago, But there were only three weeks before prototype committed to the repository and final decision made. People (even language developers I think) did not have not enough time to play with the new syntax. &gt; There's still ~2 months of experimentation with the final syntax before it's actually released. Doesn't matter, because the new syntax is announced as final. &gt; I feel awful for the language team having to endure accusations of "improper decision making" after all the care they've taken with this. Well, sometimes people do a lot of thinking, but the resulting decision is not good enough. For example, Java had anonymous classes instead of lambdas for over 10 years, and when anonymous classes were introduced, there were explanations why functions as wrong and classes are right. Or C++11 authors spend a lot of times designing rvalue references, but the result is meh. I suspect in both of these examples decisions were made by thinking without prototyping and testing.
Macros always expand to code and nothing else. If you can't do it with pure code, you can't do it with a macro.
As far as I can see, it's the same. After macro expansion you have a finite amount of source code, producing a finite number of types. Whereas what you really need is something that could create an infinite number of types at runtime (since your arena or my cell-owner might be created and destroyed an infinite number of times). So I think type markers are not going to work, except in the case where you are only ever going to need one instance of the arena at a time.
None. For each method that can fail, I make an enum with the possible causes of why it can fail. Not one giant enum for all the functions in your entire library. If you have a few similar functions that can all fail in the same way then you can perhaps reuse some of the enums but abominations like [`std::io::ErrorKind`](https://doc.rust-lang.org/std/io/enum.ErrorKind.html) are (in my opinion) a mistake. These small error enums are actually quite nice to work with, you just use `Result`'s [`map_err`](https://doc.rust-lang.org/std/result/enum.Result.html#method.map_err) and `Option`'s [`ok_or_else`](https://doc.rust-lang.org/std/option/enum.Option.html#method.ok_or_else) in combination with the `?` operator and it all works very nicely. When you first start out it seems like it's pretty verbose, but I've found that you're not so much being verbose, you're being explicit, and that's actually a good thing in this case. Too many people just use the `?` operator as an "I don't want to care about this error" operator. But not thinking about the errors doesn't make them go away. They need to be handled properly, and `map_err` forces you to do that.
Most much less important features a prototyped and feature gated for months. E. g. [this](https://github.com/rust-lang/rust/issues/48075) (`?` operator in macros) was stabilized after more than half a year after it was added to nightly. This syntax was announced as final just three weeks after it is implemented. I call it rushed process.
Yeah that makes it break down. Here is another example where it works when the types are made at different points in the source code, but it breaks down as soon as you add more complex ways of instantiating (using e.g `repeat_with`). https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=c4fdc6e8fcf31a11f83290a8b7da734c
Thank you very much for the detailed answer. After some experimenting I definitly agree with your sentiment that it's not worth the complexity. It's not a particular requirement neither so the only reason I asked the question was to explore the possibilities of the language. The thesis you linked looks really interesting as it dives deeper into other parts of the language aswell.
You can _do_ it with pure code, it's just verbose and errorprone. However using a macro is also error prone because it doesn't work in all contexts ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=c4fdc6e8fcf31a11f83290a8b7da734c)).
`Error::source` has been implemented for quite a while now, I've been using it to replace `quick-error`.
Doesn't seem so. err-derive was published in Dec 2018 where as derive_more was published March 2016. err-derive appears to be a functional subset of derive_more
Yes, it's unfortunate that there doesn't seem to be a nice way to do this in Rust that is both zero-overhead at runtime and is also clean in the source code. For `QCell`, I could consider that the ID is currently `u32`, and test like that in debug mode, and then when I'm confident that the code is correct (i.e. there are no code errors where the wrong owner is being used to access a cell), remove the `u32` ID in release mode to reduce the overheads to zero. (Note: This isn't implemented yet.) For code that doesn't try to actively exploit this, it would be safe, i.e. if it's all your own code. However it does depend on good code coverage of all accesses to the cells in testing. I'm not sure this would be accepted as a solution in a public crate, because it depends on good testing for soundness (at least it would in the qcell case).
font-kit is a cross-platform abstraction that provides access to system fonts, and extracts glyph outlines (made of bézier curves) and other data out of those as well as user fonts. It also exposes the platfom’s glyph rasterizer which is typically CPU-based. Pathfinder can be an alternative to the latter, doing rasterization of these outlines on the GPU.
Rust generally doesn’t do this. It’s std::io::Result, not std::io::IoResult. The type system already will make sure you do the right thing, and if you don’t, the error will explain.
Finally, I worked **not** on [rs-pbrt](https://www.rs-pbrt.org/about/) for a couple of days: https://www.janwalter.org/jekyll/blender/rust/blendinfo/2019/05/28/blend_info.html Some **Rust** code to explore binary [Blender](https://www.blender.org) files ...
❤️ 🦀
For whatever it's worth (and cc /u/fgilcher and /u/colelawr), I'm happy to make some time to talk about mechanics and production and so on for anyone giving this a go! I'd love it if the result of New Rustacean ending was that there were multiple new, great podcasts going!
That’s what I suspected. Thanks for giving it a shot!
That’s just the thing; I’m not making a blanket implementation, I’m exposing a trait for downstream users to implement and just want to let it take `Copy` types by-value and non-`Copy` types by reference. It doesn’t look like it’s currently possible, though, so I’ll just have it take a reference.
If you can't do it without errors, you can't do it. Anywhere you call your constructor inside a loop or a reusable function, which is pretty much anywhere, you have broken your guarantees that each arena has a unique type. That's not a technique that "doesn't work in some contexts" that is a technique that "doesn't work in *any* context". The number of unique closure types is the number of times you write it in your code after you expand macros. This is guaranteed by the monomorphisation process which makes all types concrete. If there are 5 instances of `arena!` in your code, and 10 arenas at runtime, then by the pigeonhole principle, you know that at least 5 of those arenas have indexes that can be used on another arena. This is not going to be a rare occurrence, or an edge case. You need existential types: types that cannot be monomorphised.
Ok, the issue was that I was not depending on the num crate. Thanks!
Ok, the issue was that I was not depending on the num crate. Thanks!
I'm pretty sure that if you endeavored into building a podcast, it would be pretty freaking awesome.
Yes, the compiler already warns when you ignore a returned Future.
That [Mockatio](https://github.com/myelin-ai/mockiato) crate looks really cool!
Unlike C/C++/C#/etc, you can actually have to functions with the same name but different return types on the same struct/enum, by splitting them into different traits. `trait Into&lt;_&gt;` is a very common example of one.
Yep. bzm3r is working on a Vulkan port right now :)
While that's cool, it unfortunately is still fairly limiting in that it won't work on a Mac without MoltenVK or gfx-portability which includes a performance penalty, and similarly doesn't work in UWP apps.
I didn't mean to imply that there won't be other backends too, such as gfx-rs.
Hi there, I have been using your `name_of!` and `name_of_type!` macros for quite some time now, they are incredibly useful when logging with `slog`: ```rust match result { Err(err) =&gt; { debug!( self.logger, "name_of is useful"; name_of!(err) =&gt; err.to_string(), ); } // ... } ``` The only feature I'm missing sometimes is to be able to refer to some field inside `name_of!`: `name_of!(self.some_field) == "some_field"`. One other this I'm using them for is when a struct has a worker thread, I use `name_of_type!` to give the thread the same name as the struct, then when it panics, I know immediately where to start looking. Thank you.
My understanding was that `epoll` et al. are more resistant to slowloris attacks than servers which spawn a thread per client. Am I wrong?
Something to note is that std::error isn't in core despite having no host system dependencies. I think it had something to do with not encouraging it too much?
Supposedly, `gfx-hal` will be pretty much the holy grail of portability in graphics. It has an API that's similar to Vulkan, and supports Vulkan, DirectX 11 and 12, Metal and OpenGL. So using that, you would no longer need to implement other backends. Nb. I don't know how "done" it is, and what the shader story looks like.
Interesting, I haven't heard of anything like that, and a quick googling fails me. Do you know where I could read more?
None, they are pointless
Let's put it that way: WebRender plans to move to gfx-hal and to use pathfinder, so mechanically we'll need a gfx port of pathfinder.
I've written a port of \`Hyperminhash\`, a fast, constant-memory cardinality approximator, for Rust as a [\`sqlite3\`-extension](https://github.com/lukaslueg/sqlite3_hyperminhash/). You can use it for fast \`COUNT(DISTINCT ...)\` queries, including union &amp; intersection operations.
I thought the plan was to have `gfx-hal` as an additional backend to WebRender, so what I said above still applies. But is someone still working on that port? I haven't seen any progress lately.
In the callers' mappings, do you match with a `_` arm? or do you want to see a `non-exhaustive patterns` error in each caller when you add a new failure mode, forcing you to re-evaluate the caller's error logic? Do you do this across crate boundaries? Adding a new failure mode would force a major version bump, unless you force the `_` approach via `#[non_exhaustive]` or similar.
I think that libraries should just hand roll their own errors. Binaries can do whatever they want - I use failure.
Patapé &gt;&lt;
Oh neat. It sounds like the backtrace features are still in progress though.
In the past I've had people ask whether an event will be recorded; note that in this case that won't be possible because the venue does not allow photography. So if you want to see robotic mastermind Jon Soo's creations, you'll just need to be there! :)
wtf when did it become "Pre Pit Your Peaches" ??
Have you published any libraries that rigorously follow this advice and avoid "abominations" such as `io::ErrorKind`? If so, have you collected any user feedback on how well this works in practice?
Can't C++ and C# both achieve this? C++ could use different base classes, C# could use different interfaces? Both would achieve functions with the same name on the same type but different return types.
[fixed link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;code=%0Afn main(\) {%0A stringify!(b\)%3B%0A})
You're not wrong. Slowloris on a non-blocking socket usually just wastes a file descriptor, slowloris on a blocking socket can waste a thread (and thus its stack) in the best case, in the worst case the whole application if that was the only thread.
https://internals.rust-lang.org/t/need-for-operator-for-unsafe-code-guidelines/10022
It's pretty unfortunate `std::intrinsics::type_name` is an "unsafe" intrinsic that only works on nightly IMO. Even if it was still unsafe, but worked on stable, you'd be able to trivially make a safe wrapper: #![feature(core_intrinsics)] use std::intrinsics::type_name; pub fn nameof&lt;T&gt;(value: T) -&gt; &amp;'static str { unsafe { type_name::&lt;T&gt;() } }
Yeah it is unfortunate, however perhaps existential types will enable this if/when it's implemented. `qcell` is really cool and shows of some nice intricaties of the type system.
When submitting content not obviously relevant for r/rust, that is, content about Rust or mentioning Rust, but whose title for some reason is not Rust specific, do make sure to explain the relevance.
Maybe, but they don’t use that convention. It’s also pretty awkward to define in the code, since you have to explicitly specify it with every call.
Good point, the module option seems to be the most idiomatic.
In an application (i.e. where I don't publish a stable API), I'm using `failure` for now. There's an approach I haven't seen in any of these crates. At work, newer code uses a [https://github.com/grpc/grpc/blob/0e00c430827e81d61e1e7164ef04ca21ccbfaa77/include/grpcpp/impl/codegen/status_code_enum.h](general-purpose error space) across the whole company's codebase, rather than library-specific error types. It's useful to have this common vocabulary - less to explain about each library's error handling, ability to aggregate more meaningfully in monitoring, etc. Occasionally you need to attach more specific information, so arbitrary payloads can be attached to the error type as well (which has pluses and minuses of course). Often errors are just propagated up the stack with their original type (often the message is augmented with context) until they get to something that knows what to do. Some types change a bit, like "invalid argument" becoming "internal" when the caller thinks it's impossible. I've played a little with doing that in Rust [here](https://github.com/scottlamb/moonfire-nvr/blob/master/base/error.rs), based on `failure`.
Can you provide an example of this?
Most of the time I do neither. Occasionally I'll use `_` in match arms, but I believe that error enums *should* be exhaustive. Adding a new error cause or a new potential way in which a function can fail *should be a breaking change*! I mean think about it: people that only care about some of the possible error causes can use `_`, and if you add an error cause, their code won't break. But people who match on all your error causes most definitely will *want* their code to break if you add a new one!
Yes, the macro is not working, however without it, the code does what it's meant to. By marking the constructor `unsafe` one could "enforce" that the user of the API ensures that the given type is unique. I would probably not use such an API myself, and I doubt that it would be worth the trade-off, however I would not say that one **cannot** do it. Isn't that one of the main purposes of `unsafe`, to enable code which cannot be verified by the compiler to be considered safe because the programmer promises to uphold the needed guarentees?
The reason the intrinsic is unsafe is that all intrinsics are unsafe. If it were to have a stable interface, it would be a wrapper around the intrinsic with the appropriate stability. That said, there's some pushback on exposing a stable version of type_name because it could then be used to violate parametricity without going through `Any`.
Hi. I'm working on the port here: https://github.com/bzm3r/pathfinder/tree/pf3-gfx-hal This is my first rodeo on a project of this scale, so my progress hasn't been as fast as I had imagined/would have liked. Most difficult are the many design decisions that have to be made when going from "OpenGL-style" to "modern graphics API styles". So far I am choosing to be lazy and drifting quite far from Pathfinder 3's GL based design. Tiger is still not rendering, but the main tasks remaining right now are not too many, and I list them here along with a note regarding their difficulty: * confirm design of synchronization/command submission, and make necessary design changes to fix synchronization issues (medium to hard) * fill out explicit layout qualifications for shaders, given current choice of descriptor sets/vertex attributes (easy) * finish rewriting `renderer/src/gpu/renderer.rs` to use new gfx-hal structure (easy to medium; already partially on its way there) * supply vertex data using Triangle Strips rather than Triangle Fans (deprecated in modern graphics APIs, and gfx-hal) (medium to hard) Note that the code does not yet build. I occasionally get it building, and then write a lot of breaking changes, then rinse and repeat. Currently I am in the breaking changes phase. Finally, regarding gfx-hal: its OpenGL backend is still a far cry from completion, because it is difficult to fit OpenGL concepts under modern graphics API concepts. Therefore, at the moment, it makes a lot of sense to use gfx-hal for Vulkan/DirectX/Metal portability support, while maintaining a separate OpenGL backend. Also, there is a project to write a gfx-hal backend for WebRender: https://github.com/szeged/webrender It is instructive because it shows that the current OpenGL backend for WebRender is highly optimized: writing a gfx-hal backend isn't a straightforward exercise, as it must take into account all these optimizations in order to effectively compete with the OpenGL implementation.
OK. I compiled these myself on Fedora, using `COMPILER=clang` for OpenMP so they're using the same LLVM backend. (i.e. different language frontends, but the exact same codegen backend.) rust-1.35.0-1.fc30.x86_64 clang-8.0.0-1.fc30.x86_64 llvm-8.0.0-6.fc30.x86_64 On an i7-7700K (4 cores, 8 logical threads), I get comparable results: BabelStream Version: 3.4 Implementation: OpenMP Running kernels 100 times Precision: double Array size: 268.4 MB (=0.3 GB) Total size: 805.3 MB (=0.8 GB) Function MBytes/sec Min (sec) Max Average Copy 23411.772 0.02293 0.02689 0.02321 Mul 23480.295 0.02286 0.02658 0.02307 Add 26388.664 0.03052 0.03562 0.03078 Triad 26415.607 0.03049 0.03645 0.03078 Dot 37978.583 0.01414 0.01640 0.01433 BabelStream Version: 0.5 Implmentation: Rust Running kernels 100 times Precision: double Array size: 268.4 MB (=0.3 GB) Total size: 805.3 MB (=0.8 GB) Function Mbytes/sec Min (sec) Max Average Copy 28743.490 0.01868 0.02141 0.01894 Mul 23094.202 0.02325 0.02926 0.02356 Add 25698.260 0.03134 0.03316 0.03158 Triad 25599.414 0.03146 0.03310 0.03167 Dot 36870.470 0.01456 0.01625 0.01474 I do have access to some larger Xeon machines with NUMA, but they're busy right now, so I'll try later if I find them idle.
To add to what Nical said, another reason for fixed-size tiles is so that we can pack data efficiently for the GPU. Right now, with 16x16 fixed tiles, edge coordinates are stored in 12-bit 4.8 fixed point, 4 bits for the whole number and 8 bits for the fraction. This means that each edge fits in 64 bits, the size of a CPU register. I'd probably have to switch to a different format if tiles were variable-size, which would bloat the size of edges. Given that memory bandwidth and I/O bus transfer is so important to Pathfinder's performance, it might end up being counterproductive to do so.
Dot await (and anything involving a dot) has been a big no from me since the beginning because that syntax is for referencing a field value or method. In this post, he claims that "the dot operator is already overloaded." Is that in reference to the two access modes I I referenced before, or is there another usage with an even bigger difference? Those two accesses are so close I'd hardly call it overloading, and awaiting as an operation is not at all similar to either one.
&gt; That said, there's some pushback on exposing a stable version of type_name because it could then be used to violate parametricity without going through Any. What exactly is the specific concern there? I have a hard time imagining anything particularly terrible that could be done with an `&amp;'static str` that just (correctly) names something.
Right, I should have said "plans to have a gfx-hal backend". I didn't mean to contradict you, I only meant that it's very likely that there will be a gfx-hall port pathfinder, if only because of WebRender's needs. I agree that gfx is (the closest thing to) the holy grail of portability although OpenGL means a lot of things and I don't know how far back their plan is regarding older GL versions which represent an unfortunate amount of android users. In the mean time, if WebRender can get away with a low-spec-GL backend and a gfx backend, it's not too bad compared to having to deal with all APIs. To my knowledge webrender's gfx port is still being worked on and it looks like the szeged repository where the work is happening is still active https://github.com/szeged/webrender/
This is what is used in `qcell` for the `LCell` type. The fact that Rust can prove this to itself (although with a huge amount of help in the form of lifetime annotations) means that there might be some hope that one day some way to describe this might be formulated that Rust can prove without the annotations. For example something that says "Rust, dive down the callstack to prove that this is okay". So this produces a kind of viral requirement on callers that is not specified in the API. This isn't the "Rust way", I know, but it would solve the problem. Also whilst slower than the normal proof checks, it's not something unbounded -- it either passes or fails.
The recent extension for futures is nice. let apple = api::fetch_page("apple").context(UnableToLoadAppleStock); let google = api::fetch_page("google").context(UnableToLoadGoogleStock); let (apple, google) = future::try_join(apple, google).await?; [https://github.com/shepmaster/snafu/pull/86](https://github.com/shepmaster/snafu/pull/86)
We did discuss a lot of objection to .* relating to its orthogonality, and I hardly saw any of those points addressed by team members, which disappoints me and makes me feel like it didn't matter anyway. I did stop following the discussion for quite some time, so if theres something I missed, please let me know.
 fn bad&lt;T&gt;(t: T) -&gt; T { if type_name::&lt;T&gt;() == "i32" { panic!("no i32!") } else { t } } Like this, except more complex functions can do more interesting things.
Interestingly, it looks like the nightly build of clippy is broken right now, but hopefully they will get it fixed quickly. [https://rust-lang-nursery.github.io/rust-toolstate/](https://rust-lang-nursery.github.io/rust-toolstate/)
According to the README it seems to be possible to access inner fields with `name_of!(some_field in YourType)`. It may even be possible when using `Self` as your type.
Part of it depends on how much you care about [parametricity](https://wikipedia.org/wiki/Parametricity) (TL;DR: all versions of a generic function do "the same thing"). `type_name::&lt;TParam&gt;()` doesn't return `"TParam"`, it returns the concrete path to the type the caller provided. This allows recreating [`Any`](https://doc.rust-lang.org/stable/std/any/trait.Any.html)/[`TypeId`](https://doc.rust-lang.org/stable/std/any/struct.TypeId.html) style behaviors that violate parametricity like the following: fn greet&lt;U&gt;(u: U) -&gt; &amp;'static str { match type_of!(u) { "animals::Cat" =&gt; "meow meow", "animals::Dog" =&gt; "woof woof", x if x == NUCLEAR_LAUNCH_CODE =&gt; uhoh(), _ =&gt; "socially appropriate greeting", } } You could even do safe-looking but subtly unsound downcasting to known type names. The problem is that allowing code to know the real type of a value through abstraction layers is great for the end usage debugging, but also means that use locations can make choices based on it. Renaming a type could change the behavior of a function that used the type name! Sure, this is "bad, abusive code" that would cause problems with `type_name`. But programmers love to abuse all the tools they're given to get the problem solved ("it has to do something minorly different for just this one type"), and we don't have this hole available right now. Can we justify adding it? I don't know.
All the sigils!!!! /s
Iterator::collect is a good example
A simple solution would be to return the generic variable name in a generic function. If in a generic function you are not supposed to know more about the type then don't give out that information?:) This shouldn't ruin the fun for non-generic usecases.
I will be using [SNAFU](https://docs.rs/snafu/0.4.1/snafu/). I'm a big fan of the author.
I didn't know that this syntax is also allowed, thanks. I do know that name_of_type!(X) can also be written as name_of!(type X)
So, here's a status update for you. USB is rather difficult to use with Rust. Rust assumes that things *exist*, but with USB, they might *not* exist, because they got unplugged. My (fifth) iteration of the design involves giving somebody an ID rather than the actual device itself, and then having the API revolve around querying the library for things using this handle. Will it work in practice? Not sure as of yet; I'm trying to build a minimum viable product for people to use.
https://github.com/tailhook/quick-error/pull/45
There's nothing more to invent. I can use the crate as it is to do as needed.
&gt; The key advantage, IMO, of a error handling helper crate is backtrace support Except that backtrace support is only as good as the library that generates the original error. If `ripgrep` (binary) calls `ripgrep-core` (library) calls `regex` (library) and `regex` reports an error *without* a backtrace, then the best that `ripgrep` can do is print out the backtrace up to the point where it calls `ripgrep-core`. All the context inside of `ripgrep-core` and `regex` is missing. (I feel like we've discussed this before, so apologies if I'm repeating something you already know — hopefully some other reader will be illuminated)
Of note is that there's support for both [futures 0.1](https://docs.rs/snafu/0.4.1/snafu/futures01/index.html) as well as [standard library futures](https://docs.rs/snafu/0.4.1/snafu/futures/index.html).
I'm working [Rosy](https://github.com/oceanpkg/rosy), a new library for calling into Ruby from Rust and vice-versa. It's designed to: - Leverage Rust's strong type system. - Make the user aware of any and all possible `unsafe`ty when interacting with Ruby objects. - Allow for writing code that's as fast as using the C bindings directly I have a [`CHANGELOG.md`](https://github.com/oceanpkg/rosy/blob/master/CHANGELOG.md) that details everything that's happened since 0.0.1. Currently, the main things missing are: - Linking to Ruby on Windows with MinGW or MSVC toolchains. I'm using [aloxide](https://github.com/oceanpkg/aloxide) for linking Ruby and so I need to make such changes there and then update Rosy. - Convenience macros for wrapping Rust data to make usable as a Ruby object. This is currently a bit more involved than I'd like it to be. See [here](https://docs.rs/rosy/0.0.9/rosy/trait.Rosy.html) for more info. I'll be posting about this project in the upcoming month so stay tuned! I hope to release 0.1 in a few weeks. :)
&gt; Most difficult are the many design decisions that have to be made when going from "OpenGL-style" to "modern graphics API styles". So far I am choosing to be lazy and drifting quite far from Pathfinder 3's GL based design. Pathfinder 4 incoming? lol
Nah, I always figured the graphics API would have to be changed.
Hijacking this comment for visibility. Most people here agree that no library is fine. However, it took me quite a while to figure out the quick and dirty way for descriptive errors: fn test(x : u32) -&gt; Result&lt;u32,Box&lt;::std::error::Error&gt;&gt; { if x &lt; 10 { Err("Less than 10")? } if x &gt; 20 { Err("More than 20")? } Ok(x) }
hard pass. that's what the type system is for. in the same way that we don't suffix function names with their return types, like: fn do_something_i8() -&gt; i8 { ... } or the names of variables, like: let foo_usize: usize = ...; and we don't write the type constraints on generic arguments in the new type's name, like: struct ListOfSync&lt;T: Sync&gt; { head: T, tail: Arc&lt;ListOfSync&lt;T&gt;&gt; } we shouldn't put information about a function's `async` status in it's name because other language features already encode that information, in ways with lexical/syntactic and compiler/tooling support. human-readable-only parts of your code, like function, variable, and type names, should only hold information that's irrelevant to the compiler, because you have to write it somewhere else anyway, and maintaining both real compiler annotations and fake function/variable naming annotations is a recipe for duplication of work, programmer error, and sticky refactoring challenges.
Thanks!
Yeah I know. In general, I think that's okay, because it's unlikely that your average joe is going to find a backtrace into the bowels of regex useful anyway. I think that's probably true for a lot of libraries, but maybe not all. If your "library" is really just support for an application---i.e., a way to break it apart---then I think using an error handling helper library (for backtraces) is fine in that case, since I don't really see `ripgrep-core` in your example as a "library" per se. But I'm playing fast and loose with my vocabulary here. :-) I guess a better way to phrase it might be that the deeper into the dag your crate is, the more unlikely I'd find myself adding dependencies for helping with errors.
Double hijacking this comment for extra visibility. Don't use strings as Err values, Please. No one wants to have to start comparing strings to figure out what error occurred and in what context. Use enums with semantic variants and rich contextual information attached to each one. Whether you're writing a library or a module in a bigger application, users of your code should be able to match on the error conditions and easily determine what error conditions can occur.
I think strings are fine when qualifying it with "quick and dirty," as the GP did in this case. :-)
I'm very excited for this project! I have a question regarding performance: When I compiled the examples some weeks ago I noticed the moire pattern example was running consistently at about 10% CPU cpu usage. Similar animations implemented in the browser using canvas API don't use anywhere near that much CPU in my pc. Is cpu usage expected to improve in the future? When compiling the examples I used: RUSTFLAGS="-C target-cpu=native" cargo run --release
and they are changing with upcoming WebGPU, but that still requires PF3 changes similar in spirit to what /u/bzm3r is doing
Can’t you just use ‘cargo install cross’ and then ‘cross test —target=...’ ?
Yup. Get things working correctly. Add quick and dirty text errors to figure out when things are going wrong. Go back and optimize and track as needed. Fill in application/library beyond just 'core' getting things working. Added actual factual error handling. This is usually my method and it seems to work well enough.
To add to what other folks are saying, consider what it means to have these two lines: let a = ... something ... ; let b = a; What operation takes place on the second line? We often call it "moving" `a`, but what it is it really? The answer is that the memory on the stack referred to by `b` is set to a bitwise copy of the memory referred to by `a`, exactly as if you'd done a (C style) `memcpy(&amp;b, &amp;a, 1)` or a (Rust style) `std::mem::copy_nonoverlapping(&amp;a, &amp;mut b, 1)`. The compiler is free to optimize away this copy if it can find a way, but semantically what's happening is always equivalent to this. Note that I haven't even mentioned the `Copy` trait yet. The above is what happens when you move _any_ type, `Copy` or not. Now the next question is, what happens to `a`? Of course reading its bits doesn't really _do_ anything to `a` -- it's still sitting where it was. And if `a` is a "plain old data" type like a `u64` or a `bool`, then that's really all there is to it: `b` is now just a copy of `a`. But sometimes we're not allowed to use `a` anymore. For example if `a` is (or contains) a `Vec`, that means `a` and `b` now both contain pointers to the same heap allocation, and allowing you to use both might lead to aliasing or double-freeing, which are the sort of unsafe behaviors that Rust is designed to prevent. This is where the `Copy` type finally comes in. "Plain old data" types are those that implement the `Copy` trait, and the compiler understands that when you move one of those, you're still allowed to use the original moved-from value. But when you move a type that doesn't implement `Copy`, you're not allowed to use the moved-from value anymore. The actual move operation itself is no different; the only difference is whether you "still have" the old value after you move from it. This is also why the `Copy` trait has no methods; the moving operation is a built-in part of the language, and it's done the same way for all types. That makes `Copy` what we call a "marker trait" (like `Send` or `Eq`); it describes a property of a type, rather than the implementation of some operation on that type. The `Clone` trait is different. It does include an implementation, the `clone` method. This behavior of that method varies from type to type, and in general it's whatever needs to happen to make a duplicate without "consuming" the original. So for example with a `Vec`, it's going to make another allocation of the same size, `clone()` all the contents of the source `Vec` (those contents have to be `Clone` themselves), and return a new `Vec` with a pointer to that new allocation. Now there's no risk of aliasing or double-freeing, because the old `Vec` and the new one refer to totally different memory. But of course this operation is more expensive than moving the `Vec`, which only has to copy the pointer/length/capacity triple on the stack. Generally all `Copy` types derive a trivial implementation of `Clone` that's equivalent to a regular bitwise move. I suppose it's possible for a `Copy` type to implement `Clone` in a way that does something more interesting, but I can't think of any reason anyone would ever do that. So to your specific questions: &gt; How does `Copy` work if it does not just duplicate the object? As we discussed above, `Copy` doesn't really do any "work". All moving is just moving. But `Copy` says that you're allowed to keep using the original after you move it. &gt; What makes it cheap? Again, moving is moving regardless, so it's equally cheap for all types of the same size. But moving is usually cheaper than _cloning_, because types that need to be cloned often own some heap allocation that needs to be duplicated also, and those heap allocations might be very big. &gt; Can you use `Copy` in `no_std`? Again you don't really "use" `Copy`, because it's marker trait with no methods. But yes, it's a fundamental part of the language, and it functions the same way with or without libstd.
I'm in a similar boat: self-taught JavaScript dev, no CS background. I haven't gotten very far in my Rust journey but I did finish a little wasm tutorial and I've read most of The Book. Something I've found really interesting is that Rust forces you to think about things that you haven't had to consider when working with higher level languages. Just by learning Rust, you'll learn some CS principles. However, I do think you need at least some basic CS knowledge. If you don't know *anything* about how memory works -- for example if you've never heard of a pointer -- you'll be able to go through the motions of doing things the Rust way, but you probably won't appreciate why you need to jump through so many hoops just to, like, pass a variable to another function -- and why those hoops are actually brilliant language design! I'd say, just dive right in and start learning Rust and see how you like it. If you have the time, consider doing an intro CS course like Harvard CS50 in parallel.
[Here's some example code](https://github.com/anlumo/syncasync). Note that the way to call the trait implementations is a bit awkward, because it's all in one module. Usually you'd only `use` the sync or async trait, so the compiler would know which one to pick.
We could calculate grams per square meter per kitten? Now this is why I started my interest on rust! The real problem solving begins.
Sorry Just a drive-by comment: look at the examples in the https://github.com/rustasync repos
Check out [Rust in Action](https://www.manning.com/books/rust-in-action), it's meant to teach you Rust and those concepts at the same time.
Thanks for the kind words :) If you have any feature requests or bug reports, make sure to post something on the project page!
When we need to store a mutable reference-counted object we have to put an object inside a `(Ref)Cell` and then inside an `Rc` Doesn't this introduce an unnecessary level of indirection (compared to something specialized)? [Example](https://doc.rust-lang.org/std/cell/index.html#introducing-mutability-inside-of-something-immutable)
To be quite honest I see absolutely nothing wrong with your `greet` example, and I was unaware anyone had the opinion that stuff along those lines was universally bad. To me it seems like more of a "what kind of function is it" / "use common sense" sort of thing. Like, obviously it would be very strange to have a generic math function that did different things depending on the type, but for something like `greet`, I don't see what the problem is or how else you'd really achieve the same kind of end result in an equally ergonomic, generic way.
looks pretty cool but is alrdy 2 years old aswell :/
Full disclosure - I'm self taught so I get that part but I didn't come to rust until well through my career as a professional programmer including some time with C. As such my perspective isn't a good comparison on many levels. That said I think you can learn Rust without the cs fundamentals. There are so many resources available online to learn any topic you stumble into or buy a book or two where needed that you shouldn't consider that a blocker. Will it be easy, perhaps not. The rust materials and just works tooling are great and make it easy to learn and explore. There really isn't any reason not to try it out going through the "book" and see how it goes. You will need to be patient and not set your expectations to high to start. :-) If your goal is wasm more than rust I think you may find that a really big hill to climb to get to the part you want. There is a book on Rust and WebAssembly at Pragmatic Programmers ( [https://pragprog.com/book/khrust/programming-webassembly-with-rust](https://pragprog.com/book/khrust/programming-webassembly-with-rust) ) you might want to check out. As for concepts the most critical is memory management as you really can't avoid that in even basic rust. The more complex parts you can leave to learning later but right up front you are going to run into the memory features and restrictions that help make rust what it is. At the end of the day only you will have a good feel for what will work best for you - classes first, then learn or go straight into it. Either way be open to adjust your plan and work at the learning and you will get there!
I don't see anything "bad" about that. If I'm calling a generic function without knowing anything about how it behaves, I'd say I need to go ahead and read the docs for it. If there aren't any, I'd say I need to use a better crate.
No, it's not even out yet. If you get it now, you will get regular in-progress versions until it is released.
What if you have both sync and async variants of the function? Rust won't let you overload the name...
It violates parametricity, which is regarded as a bad thing. The case that I provided seems weird and antithetical to Rust's philosophy of pushing all possible checks to compile time. And this is the same for most cases of breaking parametricity. Think of it this way, it would seem weird if `Iterator::collect::&lt;Vec&lt;i32&gt;&gt;` did something wildly different from `Iterator::collect::&lt;Vec&lt;u32&gt;&gt;`, right? Well, if you could break parametricity, you could make `Vec&lt;i32&gt;` collect normally, and `Vec&lt;u32&gt;` skip the first element. Overall, breaking parametricity tends to lead to buggier code, so it is best to avoid it.
[https://www.rust-lang.org/learn](https://www.rust-lang.org/learn) Read the book; do the Rustlings course. That should get you pretty familiar.
&gt;Well, if you could break parametricity, you could make Vec&lt;i32&gt; collect normally, and Vec&lt;u32&gt; skip the first element. Right, but why would you? I don't see a realistic scenario where someone writes a crate which behaves in a blatantly bizarre way, and yet people continue to want to use it for any length of time. It's the kind of thing where the results of "abusing" it are as unlikely to be desirable to the person *writing* the code as they are to anyone later *using* the code.
It may not be this obvious that something different is happening in more complex code, so these bugs may surface too late making it hard to deal with. I was giving a blatantly bad example to highlight the issue.
In the case above, a boxed error is being used so I would only expect the error to be used for logging and to display to the user. In many cases, how an error will be handled will be the same for all errors. In such cases, I don't think strings are quite as bad since the caller won't be matching on the error. Admittedly, I'm not writing a library for general use but I find myself preferring to return boxed errors in many cases. However, I do use an internal enumeration for errors that originate in my code so that identical errors have identical string representations via the Display implementation.
See #2 and #3. Within the docker dind (Alpine Linux) container, it would download the dependencies and then fail before starting the container because the default toolchain I used (stable-unknown-linux-musl) doesn’t allow shared libs. The specific error was about proc-macro not being supported on the target toolchain, but that happened for every toolchain (even linux-gnu) and I found a ticket referencing that kind of error being cause by the host toolchain even though it isn’t mentioned in the error message. This error message disappeared with the same targets when I tried under Ubuntu (which uses a linux-gnu host toolchain). When I tried with the system toolchain (1.34.2-alpine-linux-musl) which I believe does allow shared libs, it would fail because I couldn’t add components to the system toolchain. Alpine doesn’t provide glibc, and I tried enabling xargo, but the latter didn’t seem to have an effect. I thought about troubleshooting that last thing, but then I realized it probably wouldn’t be able to add rust-src as a component either. With Ubuntu I was able to get it to start the container, but then cross’s musl container failed due to a missing musl-g++ and pc-windows-gnu failed due to a missing std::thread. Even after running using the latest master of cross, I still had the same problem with musl.
if you want to say "this function is like `foo`, but `async`," then i think `foo_async` is a perfectly reasonable naming convention. the point is that it's still only intended to be a human-readable name, and the suffix "async" serves to quickly highlight the key difference between `foo` and `foo_async`; it's not assigned any significance by the compiler, and you wouldn't name the async function `foo_async` unless you had already written `foo`.
Would it be possible to use gfx-rs from a C++ project?
That's fair. For context, in my (admittedly limited) experience with C#, this is often what the `fooAsync` functions are. Async variants of existing `foo` functions.
... no, that's a generic return type which is not the same thing.
Hey, I would be happy to help / alter the `metered` crate so we can build an inter-operable metrics ecosystem. Don't hesitate to ping me or open issues on GitHub.
The answer is that in Rust, all dispatch is supposed to go through traits. Instead of a free function which magics up a sort of dispatch based on the string representation of types, you use the type system for what it's designed for. Here, you'd probably `trait Greet { fn greet(&amp;self) -&gt; &amp;'static str; }` and `impl Greet` for whatever types you care about. And then you wouldn't have to have the "socially appropriate greeting" fallback, you could just impl for the cat, dog, and nuclear launch codes. This is a poor-ish example since it violates parametricity for the "right" reasons. The problem is that `instanceof` (which `type_name` is basically a weak version of and will be abused for) is a horrible idea for code maintainability. Ultimately, I think we'll get this, but with a big "for debugging purposes only, please" note. [See the related GitHub issue thread.](https://github.com/rust-lang/rfcs/issues/1428#issuecomment-484525456)
Then you use all the actual utility of the intrinsic for debugging. "``Unwrapped a `None` of an `Option&lt;T&gt;` ``" is fairly useless of an error, "``Unwrapped a `None` of an `Option&lt;FooBar&gt;` ``" is somewhat helpful.
Experience from Java and `instanceof` is that nice parametricity-violating functionality will be used. People write APIs that take a `Animal` and do different things based on what animal it is, without actually doing dispatch on `Animal`, but rather `instanceof`-guarded downcasting. Reflection and other violations of parametricity are very very powerful, but also very very footgun-y. Typically Rust tries to avoid those footguns.
A resource I found super helpful is Learning Rust with too many linked lists https://rust-unofficial.github.io/too-many-lists/
programming hasn't changed much since the 1950s youll be ok.
Pathfinder is more multithreaded than Skia and other vector graphics renderers used by browsers typically are. So it will generally use more CPU. You can avoid this by running in single-threaded mode, but of course you can lose some FPS doing that. For me, Pathfinder gives me much better FPS for the same kind of canvas scenes in Firefox. With GPU rasterization turned on in Chrome, it depends on the scene, but Pathfinder is usually competitive, and there's tons of room for further optimization. For example, the moire demo would really benefit from specialized handling of strokes. It's actually quite encouraging that Pathfinder is so competitive with Skia already on that demo, given that what Pathfinder is doing to render circles is really quite dumb at the moment :) On my MacBook Pro, every browser uses at least 50% CPU constantly rendering the moire demo, except for Safari, which uses only 25% CPU but only hits about 20 FPS. That demo is actually quite taxing on vector graphics renderers!
&gt; Doesn't this introduce an unnecessary level of indirection (compared to something specialized)? No, it doesn't. As you can see in [source code](https://doc.rust-lang.org/src/alloc/rc.rs.html#254-276), `Rc` is an intrusive pointer.
When you resolve and adoptize rust module system issues for newbies from other languages like Java, C# ? Now - using module and submodules for projecting big tree structure is hell !!! Business never use Rust if current submodule hierarhy rules keep current
Any reason why you're not just using a slice which behaves exactly the same without allocating?
You can take a look at my crate that's a heavy work in progress but uses futures 0.3, async/await and tokio: https://github.com/gngeorgiev/futures-conn-pool/tree/async-await You can start by looking at the tests in `src/lib.rs`.
I would like to add that no matter what one's position on error crates is, snafu's [documentation and user guide](https://docs.rs/snafu/0.4.1/snafu/guide/index.html) is inspiringly good. Expressing chapter structure via modules and making it all available in a single place is *fantastic* and I wish my crates were half as well documented.
You don't have to cast to f32 and back to usize for division. Integer division in rust (like in most languages) automatically floors the result.
'Katas' are good for learning a new language. Pick a small but non-trivial project you've done already in another language - Sudoku solver, static HTTP server, something like that - and re-do it in Rust.
I prefer [this website](https://rust-lang.github.io/rustup-components-history/index.html), as it shows a week of history and also shows when each tool was last available.
Recently I published [webdav-handler](https://crates.io/crates/webdav-handler), a crate that can be used with a HTTP server like `hyper` to serve the `webdav` protocol (iow, a file server). It uses futures 0.3 and async/await internally. I've just updated it from the old `await!()` syntax to `.await`. Not published on crates.io yet, but it's in the [`webdav-handler-rs` github repo](https://github.com/miquels/webdav-handler-rs).
Java type-erases generics, for one thing. Rust doesn't. `instanceof` can't tell you the exact type of any generic without any chance of failure the way `type_name` can, because it doesn't work on generics at all, because it can't. More generally, to me, it seems pretty obvious that the crate this thread is about *would* provide the exact functionality the nightly-only code in [the example I gave here](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=a278f22f3d1c150c0f9834460d89d262) does, if it could. Does that make it "bad"? I don't think so, personally.
I found zigbee library, minimal but working: https://github.com/qm3ster/zigbeer-rs
How do you access your main program's functions from criterion? I'm using my\_program::doit::parse() but it reports that my\_program and doit and not valid modules... &amp;#x200B; I've tried use and mod, but no luck. I've got my code split into several modules and have no issues in the src/ directory using them.
I'm learning rust without a cs degree. Still learning though. Loot at Brooks Builds on YouTube. He goes through the entire rust book. I think his background is webdev as well.
Also don't use `.push` to add items one by one, instead prefer `.extend_from_slice` or iterator methods to copy entire slices at once.
What blocking loop are you talking about? After compiler is done with `.await` there should be no loops, just futures.
What are linked list? I’m learning rust. Thanks!
Everything you need to know is in the link. Follow the tutorial and actually write the code with your own fingers. It is worth the time.
Dive in headfirst, work it out as you go along. Seriously.
Hey I was looking at [wiki.mozilla.org/Areweyet](https://wiki.mozilla.org/Areweyet) and I was wondering about a few of the dead projects and if anyone knows why they died? They are: Are we private yet? Are we concurrent yet? Are we radical(participation) yet?
&gt;dev.to/kdrako... No, but that's a great idea! cheers I wrote this after recently learning rust last year, and only in the last few months have I been getting into the habit of attempting to use slices that can be made from heap-allocated vectors.
On mobile, and probably not understanding the issue, but we use cross on Linux to cross compile for lots of architectures, including Alpine/musl. We don't use docker, but I'm not sure if that matters. Do you need docker? Check out https://github.com/IronCoreLabs/recrypt-rs The Travis scripts might give you some insight. Hope this helps...
Personally, I would spend a week or two on c or c++. Not to become good at them, but because they teach you the basics of computer memory. A lot of rust resources touch on this, but I found it really helpful to have exposure first. I think you could move on once you understand pointer arithmetic. Just my two cents. I’m sure other disagree
https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo Watch the series crash course computer science. It’s a great overview of topics taught in a CS degree. Highly recommended!
There's no garbage collection in rust. So there's one off the list :p
https://github.com/withoutboats/romio/blob/master/README.md#examples
Okay I have another silly question. I can't find a good, concise answer to the question on how one can check a type in Rust. For instance in Python one can type: print(type(x)) How is this done in Rust?
Write a linked list in C. You will have a better understanding of the memory management and how references work.
My concern is when the attacker __starts__ to outpace the program, a push-based implementation will only __start__ to have slowly degrading performance, but in rust it takes down an entire thread. You have to determine whether this is a problem for each loop, and most crate today I saw didn't consider this. Will that make rust more vulnerable?
I believe poll- and push-based implementation slightly differ in this issue. See my edit.
Thanks for your answer! To be able to make a fair comparison I tried replicating the moire demo in javascript: http://jsfiddle.net/t0Lcoqn9/2/ Cpu usage in chrome is around 4% and GPU usage is 12% while for the pathfinder demo it's CPU is 10% and GPU usage is 30%. I think both are reaching a steady 60 fps without dropping any frames. I'm running this on an i7 7700 and geforce 970gtx windows pc.
What you describe isn't slowloris (I realize you didn't claim it did, but to clarify for others tuning in). Being able to get bytes into the recv buffer faster than an application will handle the data wouldn't happen often. But part of what you say is true, Rust's `Future` design is cooperative, so loops may wish to include some sort of `cnt_since_i_last_yielded` mechanism to force a yield when underlying futures are otherwise always ready. This isn't unique to Rust, though.
Depending on async library's implementation, a slowloris attack on rust might just block the thread. In push-based future (go etc.) the event loop still has a chance to run and give some CPU time to another future. In rust you can't even get back to the event loop.
[wgpu](https://github.com/gfx-rs/wgpu) and [portability](https://github.com/gfx-rs/portability) are wrappers for gfx that provide WebGPU and Vulkan C APIs. There isn't a C wrapper for gfx, but the gfx API is rather similar to Vulkan, so I'd recommend just using that.
According to [cross](https://github.com/rust-embedded/cross)'s support table on README, x86_64-unknown-linux-musl target has no C++ support, so the issue is known, is a missing feature, and not a matter of maintenance. cross's developers would gladly accept C++ support for x86_64-unknown-linux-musl target.
As far as I can tell, issue 2 and 3 are Docker-only issues and unrelated to cross.
c# can do this with explicit interface implementation, but you'd wind up having to specify which you wanted anyway by casting to the desired interface, so it's not a useful way to solve this issue.
Do you a use &lt;module_name&gt;; declared in the file you’re trying to use the functions? Also are the other functions your trying to use declared public? (pub fn function())
Pick a project idea and build it. &amp;#x200B; I work around RDBMS, so my first project is build pseudo-orms. I made one for each language, sometimes multiple times. Because I already know it by heart, is a good way to see how things are in a new environment. &amp;#x200B; So, I advice to do in rust something you have already do somewhere else. Not need to make it fully, just a MVP to see how it feel. &amp;#x200B; And how that help with garbage collection, system language, memory safety etc? Very simple: You will hit, often at first, problem after problem. Learning about how solve it , will teach you. &amp;#x200B; Probably the first month will be very hard. But after, will be just hard :)
Thanks for responding. I ended up using valgrind w/ callgrind and found my problem. I did have a use mymod and it kept saying not found. I'll try to give it another shot and maybe some example code and put it back up. Trying to find some github repo's where they are doing it to see what I'm doing wrong (I like learning from examples too).
Agreed. OP: Rust in Action is very good, I liken it to the Stevens book for C/UNIX, but in my opinion not for a beginner. Start going through the [Rust book](https://doc.rust-lang.org/book/), write stuff, change example to see if it really works the way you think it does, don't hesitate to go on a tangent whenever something picks your interest. The think about Rust though is that it is a "big language", so dont' worry too much if it seems a lot and you quickly forget stuff. Once you're done through the book, go through it again. I've been through it a few times, every time I picked more things, and the language felt less intimidating, less "big".
&gt; Java type-erases generics, for one thing. Rust doesn't. instanceof can't tell you the exact type of any generic without any chance of failure the way type_name can, because it doesn't work on generics at all, because it can't. In Java you can do `object.getClass().getName()` to do the same as Rust's intrinsic `type_name`, so it's not like you can't do it (but how often do you see that used in Java?). --- If a weaker form of breaking parametricity is footgun-y, then a more effective version is also going to be footgun-y. As /u/CAD1997 said, Rust tries to avoid these footguns.
Awesome! It's not flashy, but utilities like this are core of the ecosystem and people working on them are unsung heroes.
Thanks for doing this! I was a contributor to the older repository, but not too active of one. If you need any help with the new one, feel free. With 1.0, I'd definitely recommend going at least one or two releases before committing to it. There were some other things, like loading multiple .env files and merging them, which I think would be best to include before 1.0, and it doesn't seem easy to think of the "best" API on the first go for doing that.
Is anyone else having problems with `cargo publish` not working? I have code that builds and compiles fine when using `cargo build` and `cargo run`, but when I try using `cargo publish`, it suddenly finds cryptic errors - and not in documentation. Anyone have any thoughts?
`let () = x;`
`RefCell&lt;T&gt;` contains a `T` directly and `Rc&lt;T&gt;` is a pointer, so `Rc&lt;RefCell&lt;T&gt;&gt;` is, in terms of indirection, just a pointer to `T` (although the layout will be different). You have to have that one level of indirection to achieve reference-counting by definition, so `Rc&lt;RefCell&lt;T&gt;&gt;` has as few levels of indirection as possible.
Ah yes! Merging would be a great thing. Since the original dotenv for ruby doesn't provide that feature that does open up some unexplored territory in the API space.
Thanks for taking control. This is a great tool.
&gt; multi-line values Just ran into this earlier this week! This feature would be huge... I'll keep an eye on the repo for opportunities to contribute something towards this feature. And thanks for jumping in as maintainer!
I wasn't aware of this project before. Just looking at the README, this stood out: dotenv().ok(); That's... odd. I thought at first maybe that wasn't just the standard [`Result::ok`] method, but looking at [the docs], it does indeed appear to be. Is this just to silence the unused result warning? I thought standard way to ignore errors was `let _ = &lt;expr&gt;;`? Using `ok` seems kinda arbitrary. [`Result::ok`]: https://doc.rust-lang.org/std/result/enum.Result.html#method.ok [the docs]: https://docs.rs/dotenv/0.14.1/dotenv/fn.dotenv.html
Yeah, its just to silence possible errors. You are right that the idiomatic way to ignore this would be `let _ = &lt;expr&gt;`.
I thought I had seen this before, but I can't remember where. Do you have plans to add the ability to parse the env file directly into a struct, like structopt does for cli arguments?
I've got a super simple example of fetching a web request at https://github.com/daboross/futures-example-2019. I created as part of my figuring out how to use everything for my main project, a TUI game client with a `tokio` networking backend. That's here: https://github.com/daboross/srvc if you want another example using `hyper` and `websocket-rs`.
As of right now parsing the .env file into a struct is explicitly a non-goal of the project. `dotenv` is meant for combining these files with your current environment and variables are meant to be accessed that way. For parsing into structs you should probably use something like `serde_toml`.
I think one big difference is that in rust, it's not often practical to have a library supporting both sync and async operations in one place. Rust async requires such a different paradigm of thinking about data that I'd expect anyone exporting both kinds of functions to be providing one "main interface" using either sync or async, then a sub-interface with different structs and functions as a module, or separate crate.
Perhaps it's worth explaining in the readme or docs when the user should catch/handle that error instead of ignoring it? Also, what is the reasoning for the \`\_\_Nonexhaustive\` error enum variant?
The `__Noneexhaustive` should be hidden from the docs. This is a common pattern to force users to use a default branch when matching error variants, it allows adding new variants without breaking users of the library.
Trying to wrap up an [Ant Colony Optimization](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms) library and binary along with a [schedule generation scheme](http://www.pmknowledgecenter.be/dynamic_scheduling/baseline/optimizing-regular-scheduling-objectives-schedule-generation-schemes). This is for two distinct projects, one for work and another for the [GTOC X](https://gtocx.jpl.nasa.gov/) astrodynamics competition!
For me, it definitely doesn't hit 60 FPS in Firefox if I modify your demo to use HiDPI. Anyway, like I said, Pathfinder is doing a lot of dumb things right now on that particular workload. In my head-to-head benchmarks, in the general case, Pathfinder is quite competitive, and often faster than, Skia. I wouldn't be surprised if, for example, Skia is actually just caching the circles and not rerendering them every frame. Pathfinder doesn't do anything like that yet. I just pushed a fix to reduce the CPU usage a bit, by the way.
What cryptic errors in particular? One thing to keep in mind is that `cargo publish` will copy _only files set to be archived in the project_ before trying to build. If you have anything in `excluded` in `Cargo.toml`, or excluded in `.gitignore`, then `cargo publish` will exclude those files when building. If you aren't manually excluding anything, then one good step to try is to make a fresh `git clone` of your repository and see if that builds well. If that doesn't work, I'd recommend posting the error here and seeing if any of us have ideas about the particular problem?
As others have said, it has an extremely minimal layout. If we were to take away everything that only matters at compile time, this is what your data looks like: struct Data { strong_references: usize, weak_references: usize, borrow_status: isize, data: YourInnerData, } struct OuterHolder(*const Data); It's 3 ptr-sized bits of data attached to your data inline, and then one pointer of indirection towards data. If you don't use weak references, then you could probably get away with a specialized `Rc` which excludes those. But still, you're only saving 64 bits in heap memory! The weak reference adds no runtime time overhead, only memory overhead. All the other data I would count as necessary. The borrow status is necessary to prevent accidentally having two mutable accesses to the data, and the reference count is necessary to free the data at the right time. It's all stored inline, so your access is literally just 1 pointer indirection, then checking the borrow status.
No, there is no compatibility between IOCP and async. Both Java (with NIO2) and .NET do it just fine. And they also need to think about the lifetime of the utilized buffer for the async operation: They need to make sure that this buffer (which is for the context of the operation also just a pointer/length pair) is also pinned in memory and doesn't get touched by the GC until the operation completes.
&gt;So it's fundamentally incompatible with asynchronous procedures. If a loan starts with one function call and ends with another, that's not supported. The first half isn't true, the second half is. What you just need to make sure is that whatever is loaned outlives the duration of the function - it doesn't even matter if it's sync or async. E.g. in the synchronous world we still have scoped threads, and could provide a reference to data on the first thread to another thread which interacts with it. The other thread could even store a reference to that data and manipulate it later on, it knows for sure that the first thread is e.g. blocked while it does that. This approach is btw also used for all blocking syscalls. The user thread thread is blocked, while the kernel can keep address pointers into userspace around and manipulate those (e.g. for delivering results), set results, and then unblock the userspace later on. In the async fn world one async fn can borrow something to an IO engine (which provides an async\_read() function) or the kernel, as long as the implementation of those makes sure to return the result to that function before it exits - which it shouldn't be able if it is (asynchronously) blocked on the IO engine. But the ability to cancel async fns at any point of time invalidates this property.
That's pretty clever! Thank you! I'll have to figure out if that can be utilized with an error statement
Cargo build on my project produces a successful target application, and I can run the example against the library. The moment I try using `cargo publish`, I get the following errors (which I have tried ALL NIGHT to fix): error[E0053]: method `draw` has an incompatible type for trait --&gt; src/widget/text_widget.rs:137:62 | 137 | fn draw(&amp;mut self, c: Context, g: &amp;mut GlGraphics, clip: &amp;DrawState) { | ^^^^^^^^^^ expected struct `graphics::draw_state::DrawState`, found struct `widget::text_widget::graphics::DrawState` | ::: src/widget/widget.rs:116:62 | 116 | fn draw(&amp;mut self, c: Context, g: &amp;mut GlGraphics, clip: &amp;DrawState) { | ---------- type in trait | = note: expected type `fn(&amp;mut widget::text_widget::TextWidget, graphics::context::Context, &amp;mut opengl_graphics::back_end::GlGraphics, &amp;graphics::draw_state::DrawState)` found type `fn(&amp;mut widget::text_widget::TextWidget, graphics::context::Context, &amp;mut opengl_graphics::back_end::GlGraphics, &amp;widget::text_widget::graphics::DrawState)` note: Perhaps two different versions of crate `graphics` are being used? --&gt; src/widget/text_widget.rs:137:62 | 137 | fn draw(&amp;mut self, c: Context, g: &amp;mut GlGraphics, clip: &amp;DrawState) { | ^^^^^^^^^^ error: aborting due to previous error For more information about this error, try `rustc --explain E0053`. error: failed to verify package tarball Caused by: Could not compile `rust-pushrod`. This compiles perfectly out of the box, but the publish does not. I have specifically pulled in the following imports: use piston_window::{TextureSettings, Context}; use opengl_graphics::{GlGraphics, GlyphCache}; use graphics::character::CharacterCache; use graphics::draw_state::DrawState; use graphics::text; use graphics::Transformed; And again, this compiles 100% without warnings or errors. I am really confused. rustup version: 1.18.3 cargo version: 1.35.0 rustc version: 1.35.0 Not sure where to go with this. I'm ready to publish a 0.3.0 release of my lib, but my hands are literally tied right now.
Ah, I figured it out. I was indeed using two different versions of the graphics library. D'oh!
I've actually used it! Now I was looking for something more web-server like. :)
I pulled the changes and indeed the CPU usage is now around 7-8%. Great improvement! Thanks again for your reply!
&gt; The official Sublime Text 3 package for the Rust Programming Language
looks like I can't read,my bad
Look into [envy](https://crates.io/crates/envy) for that sort of functionality.
but Frameworks, tooling, and language features have.
I have found serde_ini works really better for this, it doesn't need quotes around values for a start.
It's not an easy language to learn, as it's intended to solve hard problems, so if you want to understand what problems it's solving you need to know a bit of C++, and to understand the core functional principles it uses to solve those problems you need to know a little haskell (which elm is a great introduction to, so you're already in a good starting place). But it's its own mix of concepts, so expect to take a while to absorb all concepts and keep trying, eventually it'll all make sense. Having no development background actually helps you a little, since you won't have as much of a preconception of what the language "should look like", and you can start learning it without vices brought from other languages.
It might be worth it to hand roll a parser since the syntax is relatively easy. A tokenizer into validator/reducer is decently quick to write, and I may be an exception, but I enjoy it. This would avoid the increased compilation/size overhead with an additional dependency.
Ah cool, glad you got it!
Except it isn't because it assumes that you already know what a linked list is.
I'm aware. Into::into is also a generic return type, and is what they were referring to
https://www.cs.cmu.edu/~adamchik/15-121/lectures/Linked%20Lists/linked%20lists.html
Thank you for describing the entire point of Rusts error handling compared to (unchecked) exceptions! Compile time errors are not "breakage", it's a successfully avoided run-time bug. I think the idea that compile time errors are breakage comes from dynamic languages where everything including syntax errors are run-time errors.
There are issues with the `_` approach as well if you need to ignore the result but ascertain that no errors occurred. We released `ignore` for this reason: https://neosmart.net/blog/2018/rust-ignore-result/
Hi! Let me divide your question in 2 sub question: 1. Basic cs stuff 2. Rust stuff For 1 point I would suggest you to start with Computer Science Distilled. It is a brief, NOT comprehensive overview of CS fundamentals. It will give some basic concepts. Then you can proceed with more serious literature like Tanenbaum and etc. For 2 point - The Rust Programming Language book. It is an excellent starting point for learning Rust. I'm currently reading it and really enjoy. This book is very good written, have a lot of examples, and also provides explanations on certain language design decisions. You can just follow book examples and it will provide a good "feeling" of the language at the start.
Procedural macros are just rust code that generates other rust code. In many crates they're used to reduce the amount of code you need to write to do common or simple things (such as creating a get mapping for a web service in rocket). I don't think you need to dig too deep into how procedural macros work, because the implementation details of each macro you're using will be very different. If you're curious though, you could try making one yourself just to see what is meant by "Rust code that generates other Rust code".
You don't need to understand macros on order to use it, especially procedural ones.
Thank you, I was wondering.
Just out of curiosity, how many hours of work do you expect this project to take the developer?
Macros are a last resort when the language itself is not expressive enough to reduce copy/paste.
multiple dotenv file support is critical for one of the projects we have. Whether this is suitable or not, since the files would have to be dynamicly loaded, not on runtime, but on a per-request basis, but we had to build our own dotenv parser to handle our use case, it would be great if we could fall back to a dependency instead.
Wooho! No more \`print!\` snippet :)
I have a significant interest in USB, so this project also interests me. I theoretically might be interested in such a thing as well (though no promises!!)
Just as a side note: Rust has a strong focus on long term stability which might be in contrast to what you are used to from web development. The most important libraries and tools had very little to no breaking changes in the last 4 years.
What are you trying to do? I can't see how this would be useful since putting a variant in the signature like in your example means there's only ever one thing that can be passed to the function.
 &lt;content&gt;&lt;![CDATA[print!("${1:\{${2::?}\}}"${1/([^\{])*(\{.*\})?.*/(?2:, :\);)/}$3${1/([^\{])*(\{.*\})?.*/(?2:\);)/}]]&gt;&lt;/content&gt; I.. um.. what..
Have an enum of possible types. Have structs that can use all or only specific variants of those types. That way those structs that can only take specific variants, don't need some redundant match arm and even better, don't need to worry about a bug where you misuse that match expression &amp;#x200B; fn petDog(a: Animal) { match a { Animal::Whale =&gt; [self.pet](https://self.pet)(....) // bug: youre supposed to be petting the dog not the whale silly Animal::Dog =&gt; () } &amp;#x200B; So things like that would need to have test cases to back them up. Whereas if I simply defined the variant in the signature of the function, I would get an excellent static compile time check. I can delete the tests. I can delete the match arm. I can sleep more soundly at night.
I released my rust library which is able to parse natural language date/time and convert it into chrono `DateTime`. It can be found here: https://github.com/risboo6909/when Now I'm going to improve code readability and fix bugs.
This crate is super helpful when implementing `Debug` by hand: ``` impl Debug for MessageSender { fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result { f.debug_struct(name_of!(type MessageSender)).finish() } } ```
Okay that makes sense. How about, instead of these being variants of Animal, you make Animal a trait, and each implementor (Dog, Whale) are types that may or may not implement the Pettable trait. I'm guessing you have them as an enum for some other reason as well, like putting them into a match. For that, you can turn the match into a method of Animal, and move the match arms into each type's implementation.
I don't agree that macros are the last resort. Macros can help a lot in reducing boilerplate. That said, procedural macros can generate _anything_. So good macros are those that emit easy to understand, predictable code based on the input. For example, the `derive` macros for standard traits all emit a trait impl for the input type, using type's contents to implement the functionality in the obvious way (e.g., clone the contents of the structure in the impl of `Clone`); if the type has generic type parameters, all of them are bound with the same trait for the implementation. This can be a gotcha if the impl does not really need the bounds on all of the parameters; for example in `struct Foo&lt;S: Future&gt;(S::Output)` you need the bound on the `S::Output` type, but the `derive` macro can't see it.
https://github.com/rust-lang/rfcs/pull/1450
Animal has no behavior, they are just objects of similar data. So I have no reason for putting them under a trait. And I don't want them paying dynamic dispatch costs. The behavior of petting them isn't the same for each type either. For example, having one pet() on Dog wouldn't work for me. There's many ways to pet a dog, so it wouldn't work for me anyway.
I'm willing to try out Snafu, but I'm having lots of issues with it, seeing as the guide leaves thousands of questions up to the user to figure out. Once you do however figure it out, it just seems like one more reason to continue using failure/error-chain/a good ol' Error enum with own impls for the std error trait.
&gt;Hmm, I do not agree with this. My sense is that I would like be doing **less** "off-roadmap" work, not more, and I say that both from the perspective of the compiler team **and** the lang team. The problem is that work like this is never "contained". There is a need not only to implement the feature, but to document it, to support it, etc. &gt; &gt;I agree that some sort of approach to variant types would be great. This RFC might even be advocating for the right approach -- I haven't had time to read it, so that's a bit hard for me to say! But then, maybe that's the point. I feel like if we are going to try to do "variant types", it would probably behoove us to also spent more time digging into the design space. (I feel this touches on a couple of related issues, such as closed traits, the relationship between enums and traits, the need for nested enums.) &gt; &gt;Don't get me wrong, I want this work to be done, and I always feel terrible discouraging people from doing work they want to do. But I would definitely want to have a team-wide conversation where we consider all the possible work to be done and choose carefully where to spend our energy. &amp;#x200B; Aww. The RFC and the discussion is actually pretty decent. I think most Rust newbies at one point or another have attempted to use an enum variant in a type signature. So I guess we're waiting until async/await and const generics get stabilized before taking on anymore polish to the type system?
New Rustacean is pretty good, and has been on for some time.
The following approach is often useful struct Dog {} struct Whale {} enum Animal { Dog(Dog), Whale(Whale), } See also the crates named [derive_more](https://jeltef.github.io/derive_more/derive_more/) and [enum_dispatch](https://gitlab.com/antonok/enum_dispatch)
Hint for anyone attempting this: you may want to look at [RoutingKit](https://github.com/RoutingKit/RoutingKit) if you find GraphHopper and OSRM too complex.
This may be clarifying: did you need to understand procedural macros to use built-in println! macro? The answer is probably no. It's same with those in external crates.
It's also probably worth mentioning that you're trying to produce a shared library, which even further complicates things because it would mean that applications linking to your lib would effectively be linking *two* libc's. I spent quite some time trying to get it building with musl before settling on the present solution, and the conclusion that I came to was that it would be easier to statically link everything *but" libc, and shoot for linking against the earliest version of that possible to get the best Linux compatibility coverage. If you're dead set on ditching the custom toolchain, you might take a look at [holy build box](https://github.com/phusion/holy-build-box). I looked at it briefly a while back, but the existing solution worked well enough and it didn't seem worth the hassle. On the windows front: you have windows build servers, why not just continue using those?
You disagree with GP's claim at the outset but the body of your comment doesn't really provide evidence contradictory to GP's claim. It can be simultaneously true that (a) macros cover up lack of expressiveness in the core language and (b) they are used for generating different kinds of boilerplate-y code. For example, Purescript has a notion of row types which means you get libraries like purescript-variant, which can be handy for error handling. In contrast, Rust doesn't support row types at the moment, so you get error handling libraries which often use macros to avoid making the user write a lot of boilerplatey code. (Not saying I necessarily agree with GP. Just pointing out that the two of you are not in opposition.)
Or you just assign it to a variable of the interface type, e.g. \`IAsyncHttpClient\`, but then this would involve dynamic dispatch.
That’s true, but programming is not the same thing as reading about a language and its community. Making efforts to help people step further is always a good thing. I don’t use French _at all_ in any programming communities (besides some very specific places) but I think it’s important, still.
I remember Niko had a proposal for [Extended Enums](https://smallcultfollowing.com/babysteps/blog/2015/08/20/virtual-structs-part-3-bringing-enums-and-structs-together/) with a type hierarchy that made enums powerful enough to let you talk about how variants can interact with other data.
yakshave.fm isn't strictly about Rust, but tends to have a lot of Rust content.
If they just hold similar data then an enum doesn’t seem that suitable for me, as you still need to write the data structure for each of them. I’d think composition would make sense here. So you’d have an animal Struct which contains the base data, and then each animal type had that as a member and implements their own interface over it.
It isn't mandatory to understand how procedural macros work because like APIS that just provide some interface for some work to get done. However, understanding them provides insight into how external tools generate types to be used within your system (I think Diesel does this). This also provides compile time modularity (although rust provides many different ways of doing this). &amp;#x200B; I don't particularly agree that procedural macros are common in external crates. I may agree with the notion that it can make code obscure to the user but rust has a solid compiler that typically breaks expanded code at compile time. You can extract the expanded code by using the cargo-expand command (you will need to install it).
That's what I'd do too.
I'm having trouble understanding why someone would use dotenv files. The bkeepers project seems to state this as the primary motivation: &gt; But it is not always practical to set environment variables on development machines or continuous integration servers where multiple projects are run. I'm having a hard time coming up with cases that aren't "practical." At least on Unix, a shell environment is really pretty flexible. I use at least two different approaches to this. First, if you have your env vars declared in a file, then you can just source them: . path/to/env And maybe do some other things that setup the environment. And maybe add something to your prompt (like Python's virtualenv does). The other approach I use, in cases where it's more of a one-off, is to just write a simple wrapper script: #!/bin/sh export MY_ENV_VAR="whatever" "$@" And then invoke it with `my-wrapper the-actual-command ...`. What am I missing?
&gt; Macros **are** a last resort when the language itself is not expressive enough to reduce copy/paste. should be…
No, you're overstating your case. There are tradeoffs here that both you and the GP are completely neglecting. If your error enum is intended to be exhaustive, then adding a new variant to that enum is a breaking change and requires a semver major version bump. For the standard library, this is effectively impossible, since we have a policy of not breaking code and intentionally no avenue for releasing intentional API breaking changes like this to std. You might be able to get away with this in a crate, but you _still_ need to balance what you gain (better compile time checking) with what you give up (more semver breaking change releases). A semver breaking change release doesn't always have to be painful, but: 1) if your crate is used as a public dependency, then it will be painful, and 2) personally, I'd view a lot of breaking change releases in a short time span as a _sign_ of immaturity. The comparison with "unchecked" exceptions here is completely off base. Unchecked exceptions can happen anywhere at any time in your code. Having a non-exhaustive error enum isn't even remotely the same thing. The non-exhaustive error enum only becomes a pain point when users of your crate are regularly inspecting errors. In my experience, this very rarely occurs. And even when it does, it's almost always in the form of a non-exhaustive match anyway. i.e., "Is this I/O error a pipe error? If so, do this, but otherwise, pass it on up as a normal error." Adding a new variant to `io::ErrorKind` wouldn't change that logic. So I'd say if you're in a particular domain where the full universe of all possible errors is known _and_ it's common to do exhaustive matching on all possible errors, then it might be worth adopting a more fine grained strategy for error handling. But this is almost certainly not the common case and I wouldn't generally advise people to do this without good reason. Like, this doesn't have to be a black or white issue. Yes, the standard library has `io::ErrorKind`, but it also has a number of specific error types for specific functions, e.g., `Utf8Error`, `AllocErr`, `LayoutErr`, `BorrowError`, `NulError`, `ParseBoolError` and so on. So the standard library uses precise error types in some cases and uses more general error types in others. The important thing is that we don't follow some dogma about what error handling should look like in every case based on some crap about "people coming from dynamic languages," and instead evaluate each situation on their own and balance the trade offs.
Can you put some of those questions on to the issue tracker? The maintainer seems pretty responsive and seems to care about improving the guide. &gt; it just seems like one more reason to continue using failure/error-chain/a good ol' Error enum with own impls for the std error trait. Snafu uses an error enum too...
Unholy
The nightly tikv rust client does after this patch from nrc: [https://github.com/tikv/client-rust/pull/41/files](https://github.com/tikv/client-rust/pull/41/files) &amp;#x200B; The patch is pretty instructive of the differences between futures 0.1 and 0.3 + async/await
He switched to pure C?
Dunno, ask /r/playrust
Combine this with [JEP](https://github.com/ninia/jep), and we can have Rust calling Java calling Python.
It's not a question of necessity or objective practicality to me, dotenv just meshes better with how I prefer to work. For me, it's more fiddly to add more bash scripts. I personally prefer my binary to be more self-contained when possible. I also don't like the approach of setting up a shell environment in the virtualenv sense, because not infrequently I'll have multiple shells running when I'm working on something, and I don't like having to source a file every time I open a new one.
Dependent typing when
except they've been silent for so long now 😭
```rust #[derive(Debug)] struct A; fn main() { let a = &amp;mut A; foo(a); foo(a); } fn foo(x: &amp;mut A) { println!("{:?}", x); } ``` `A` isn't `Copy`, and `&amp;mut A` can pass twice too.
Can someone help me to understand in the following example using simple non-blocking TcpListener server: &amp;#x200B; [https://github.com/tensor-programming/Rust\_client-server\_chat/blob/master/chat/server/src/main.rs](https://github.com/tensor-programming/Rust_client-server_chat/blob/master/chat/server/src/main.rs) &amp;#x200B; 1) TcpStream can be cloned? Can I have 2 mutable instances of sample sockets? Can 2 threads read/write from same sockets? Maybe it does its own locks inside? &amp;#x200B; line 24: socket.try\_clone() &amp;#x200B; 2) It is using read\_exact that should only returns Ok(size) when its able to read the 32 bytes. But running the code it is working like a read\_line. &amp;#x200B; line 29: socket.read\_exact(&amp;mut buff) &amp;#x200B; 3) if you try to iterate in clients collection with a "for c in &lt;- clients" or even remove the left reassignment "client = " you receives "move occurs.... does not implement Copy trait". &amp;#x200B; line 49: clients = clients.into\_iter()
Interesting, okay. So I guess it's just a preferred workflow thing then. I got confused with other people in this thread mentioning things like this as "core" parts of the ecosystem.
This is what I assumed, but as a consumer of a library shouldn't I be the one to decide whether to handle a new error type? With `__Nonexhaustive`, if you ever do introduce a new error type that your users should be aware of, you'll have to let them know via a changelog or something like that. One of the big advantages of exhaustive matching requirements, to me, is that a library author can add a new variant and the compiler will let me (as a consumer of the library) know that I need to handle it. And if you think you will be adding error types which your users won't need to distinguish from existing error types, than perhaps you could adjust your errors such that you could fit the new error type into one of the existing categories. I really like this crate, and I'm glad you are stepping up to maintain it, so I don't mean to be overly negative here. Thanks for your work! If you are interested in re-evaluating error handling before 1.0, I'd be happy to contribute in whichever way would help.
I regularly use the `__Nonexhaustive` trick because 1) having to do a semver breaking change release when adding a new error variant is a major pain and 2) it is quite rare to need to exhaustively match on an error enum in downstream code. If you do need to inspect an error, it's usually a non-exhaustive match anyway. e.g., "Did this I/O operation fail with a pipe error? If so, do X, but otherwise, bubble the error on up like normal." Generally, the balance of trade offs falls in favor of `__Nonexhaustive` in my experience, but this may not be true in all circumstances.
Thanks for linking those awesome crates! enum_dispatch blew my mind. A bit off topic perhaps, but would you happen to know how enum_dispatch compares to "normal" static dispatch? Is it literally the same thing? Or does it incur a tiny performance hit? It _looks_ like it's static dispatch, with a tiny cost of one extra method call to proxy the trait request to the alternate implementations. So I imagine there is a cost, but very small. I've been looking at the benchmarks and I'm a bit confused not to see a comparison to static dispatch. I imagine I'm being dense here. Either way, this opened up ideas on my code, thanks a ton!
A few people mentioned patterns that can emulate this behavior, but after having used this via the enum/sub-type pattern in a fairly large project I would highly recommend finding another solution unless you know that almost all of your use cases are limited to functions that naturally have the right variance, where a function that expects an Animal is getting a specific one like Dog. Once you start dealing with covariant arguments, you lose a majority of the compile time analysis benefits and up generating really crazy amounts of boilerplate and line noise from run time conversions (that are partial functions which can fail). Working with collections becomes a huge pain with this pattern as well. In most cases you're going to save yourself a lot of work and get the same level of safety by just using assertions that check the enum's discriminant at run time. I really wish there was was support for this in Rust. I was surprised/disappointed to find out how little interest the lang team had in pursuing it.
You've posted to the wrong sub. This is the sub for the Rust programming language.
&gt; A bit off topic perhaps, but would you happen to know how `enum_dispatch` compares to "normal" static dispatch? It **is** ‘normal’ static dispatch. This crate just does the chore of implementing the trait for you automaticaly. Instead of doing enum Foo { A(A), B(B), } impl Bar for Foo { fn bar(&amp;self) { match self { A(a) =&gt; a.bar(), B(b) =&gt; b.bar(), } } } impl From&lt;A&gt; for Foo { fn from(a: A) -&gt; Foo { Foo::A(self) } } // and the same for B… You just write: #[enum_dispatch] enum Foo { A, B, } and annotate the trait itself with `#[enum_dispatch(Foo)]` and you get the former code implemented automatically by the procedural macro. The final compiled code is equivalent. It is exaplained [in the README](https://gitlab.com/antonok/enum_dispatch#enum-processing).
Seems to be in direct competition with the tree-sitter extension.
Don't forget that jep can run native extensions, so technically it could be Rust calling Java calling Python calling C (or even FORTRAN).
Loop back to Rust!! Rust calling Java calling Python Calling Rust ( so much win!!)
Its a bit messy yet, but I am working on rewriting my HTTP client chttp to use std futures and async/await (see the `next` branch): https://github.com/sagebind/chttp/tree/next. It might be a little better of an example once I finish the rewrite...
I agree. I used an older version of nom in one of my projects, and I gave up on it because having good custom error messages was difficult and undocumented.
Why stop there? Sounds like the best way to write recursive code. You get more stacks to use.
AV often creates more security vulnerabilities than it solves. Microsoft recommends only using Windows Defender.
If you have to rely on programmers to enforce invariants, you haven't actually enforced any invariants.
At the very least, generally, I don't get why functions wrapping or using `type_name` aren't allowed to be declared as `const fn`, even in nightly currently. Considering that all it takes to make it work is to add the following (adapted from Miri's `intrinsic.rs`) at roughly line 77 of `librustc_mir/interpret/intrinsics.rs`: "type_name" =&gt; { let ty = substs.type_at(0); let ty_name = ty.to_string(); let value = self.str_to_immediate(&amp;ty_name)?; self.write_immediate(value, dest)?; } it comes off like a rather arbitrary limitation if you're already using nightly (where non `const fn` wrappers for `type_name` work perfectly already, as I gave an example of in another comment.)
That's what unsafe is for; make the consumer of the API uphold invariants.
Yeah I'm also aware now of the alternatives but I'm not too thrilled about them. If I needed an alternative then the real alternative would be to just not do any of this at all. Sucks but my fallback is just to abandon any hope of trying to get cute with the type system and just go back to writing it all in procedures. At least now we're getting const generics, which is really cool. Hopefully more work on the type system is taken on, now that WebAssembly and Async and the module system rework are getting out of the way.
Can we please pretend this is written in l33tspeak and pronounce it as "jars". The wordplay is just too good.
If you have no id at all you're relying on other programmers to uphold your invariants. If you have an id, you're adding complexity, and you're *still* relying on other programmers to uphold invariants. Therefore you have gained exactly zero additional safety, while increasing complexity. It would be more idiomatic to have no id and mark `get` and `remove` as unsafe.
Added!
I agree regarding this not bringing a lot of benefit. I edited the original post describing this. However I would say it's a big difference relying on the programmer **once** at construction instead of all over the code.
Thanks for the suggestion! Listening to the latest episode it appears like this won't be continued in regular fashion.
&gt; Please note that the message makes it very clear that adjustments will be discussed and accepted. Just not wholesale "let's kick over the implementation people have worked on for 6 months in a rush" replacements. This was still during beta. Adjustments were rejected too. See https://github.com/rust-lang/www.rust-lang.org/issues/455#event-2376148562 which was closed yesterday. If you don't want outside feedback that's fine, but please don't waste everyones time asking for it.
Thanks for the suggestion! Topics look quite interesting to me, I'll give it a try!
Yeah, just heard that myself. Though I get it, he’s covered most everything already.
I second this. A lot of the awesome things in Rust can only be really appreciated after learning the horrifying ways of C/C++, after which you'll see how much nicer Rust is to use.
You have rust on your Java jar.
It uses JSON to convert values between JVM and Rust. I don't know the efficiency of that. 🤔
How about wrapping it in clojure and boom - Rust calling clojure calling java calling python
An additional reference implomantation is [https://pypi.org/project/pandana/](https://pypi.org/project/pandana/)
structopt does have a env var tag for fallback values. If I'm not mistaken it has a lower priority than command-line tho
Small nitpick, but "deploying maven artifacts" is confusing, because maven itself has a "deploy" target, which means to deploy an artifact to a remote repository. What this actually does is something similar to the "dependency:get" target in maven (using the maven dependency plugin).
Yes, that was me. The issue is old and as mentioned, sprawling. There's multiple issues with the same drift and I chose to keep those open that have the most direct and actionable feedback I am aware that you want your redesign to be taken, as you have posted it multiple times, I don't agree with them. A proposal of adjustments is not a guarantee for acceptance.
&gt; So I guess we're waiting until async/await and const generics get stabilized before taking on anymore polish/filling out of the type system? Anything with far reaching effects has impact on the language and should be done carefully. I took the pointed you quoted as more "we already have a lot of upcoming work of that kind that we planned for, let's focus on that". Not async await in particular but things that are in the road map in general.
I started learning Rust when I was still studying physics. At the time, I had some experience with different languages (MATLAB, C, and Java) but no strong background in any of them. I never really felt overwhelmed by Rust. The [book](https://doc.rust-lang.org/stable/book/) is a great resource, so is [Rust by Example](https://doc.rust-lang.org/stable/rust-by-example/). When you feel a bit comfortablenwith the language, start working on your own projects.
After finding the wonderful tool direnv that changes my env vars by directory using a .envrc script, I find myself not needing the support for loading env vars from .env files *by and inside the binary itself* at all. It seems to be all-around cleaner solution.
Why allocate on the heap when you can start new processes to allocate on their stack? *taps forehead*
Short on time but I wanted to mention I'm not the person who opened the issue, I just made this comment https://github.com/rust-lang/www.rust-lang.org/issues/455#issuecomment-443119925. I don't think I've posted it elsewhere.
Yeah, nom errors are quite bad honestly. That's why so far I've been leaning towards pest, it has excellent error messages.
I'm curious about your use case, can you explain more? It sounds like your writing a webserver and each incoming request might require a different set of environment variables.
I rather disagree with the "last resort" claim. Procedural macros are a very useful feature, they relieve the language from needing to address various application-specific requirements, and provide a rich ground for experimentation. If the language developers see a common pattern that makes people write similar macros, _then_ it might be a good time to think about extending the language.
*Mockiato
I don't have a great way to estimate this. It depends if the dev has some familiarity with contraction hierarchies, how comfortably they might port some other implementation, etc. That's why the compensation is very ballpark and negotiable.
Would you happen to know of any good resources on writing a bespoke parser? I haven't dabbled much in that area which is why I was planning on using something like pest.
Yeah, I’m both impressed and terrified
musl is always statically linkes in Rust - if you don’t want that you probably need to roll your own target from scratch
Thanks, this is significantly easier to read than the others I've found! [cc-routing](https://github.com/cc-routing/routing/wiki/Contraction-Hierarchies) has a good set of notes, and [this](https://gist.github.com/systemed/be2d6bb242d2fa497b5d93dcafe85f0c) is another list of implementations I found.
What's the timeline you're looking to have this done in? It looks really interesting, but I probably wouldn't have more than 6-8 hours/week for it...
I didn't know you could put test right there in the comments.I prefer to have my own testing file but that works too
An example (although just a regular macro) is \`try!\` which became so damn useful that postfix \`?\` was introduced. I totally agree that you cannot just encode every useful pattern in the language itself, without actual experimentation. &amp;#x200B; As for \_expressiveness\_, it is desirable but not an absolute good. There are more expressive languages than Rust, but I don't think they are \_better\_ languages because of this. There are trade-offs. And compile-time metaprogramming has an honourable history.
The same, I work on a audio card with Rust firmware [satin board](https://github.com/musitdev/satin_board) and I'd prefer a reddit forum than a Discourse. My interest is more on audio and DSP than VST or LV2 plugin. Sharing experience on audio dev with Rust will be cool. At the moment I'm more on the hardware part but soon, I hope, I'll start audio dev and to have a place exchange will be great.
Yes, definitely learn to at least read C and understand pointers and how memory is structured. &amp;#x200B; As for C++, I would say no; what you mostly learn when learning C++ is C++. The interesting things about modern C++ (generic programming, RAI and smart pointers) can be learned in Rust.
It might be worthwhile posting this as an issue on petgraph. I know that as a maintainer of open-source libraries it's always a joy to be able to improve / extend my open-source project for money.
I wrote a toy mqtt broker for learning purposes. Don't take it too serious! https://github.com/flxo/hogfold/tree/async_await
Dot await has been a big yes from me since right now, because that syntax is for referencing a field value, computed value, or awaited value. In either case, you have an expression and you intend to evaluate it and produce a result.
These intrinsics where made well before `const fn` was even a concept, and no one thought of making `type_name` a `const fn` after the fact. If you want it to be a `const fn`, then send a PR with the change.
I finished porting my python Dense Visual Odometry to Rust. It was part of my Master’s Thesis. The paper I based it on iis linked in the repo. In contrast to most academic source code I tried to keep naming and functions as intuitive as possible. &amp;#x200B; [https://github.com/geoeo/visual\_odometry](https://github.com/geoeo/visual_odometry)
But you see the type signature, and it's just `&lt;T&gt;` which is short for `&lt;T: Sized&gt;`. We have an expectation that the function knows nothing else about `T`, and any further information about it comes from explicit trait constraints. So the extra behaviour is really surprising.
It should be around 5. 5 Efficiency.
What is that?
Aww, no shout-out for [jni-rs](https://github.com/jni-rs/jni-rs)? Admittedly, it was originally written with the intention of being a way to call Rust from Java, but support for starting a Java VM and calling Java from Rust was added [at the end of 2017](https://github.com/jni-rs/jni-rs/pull/47).
Got the basics going (not to my problem though), but how do I make each instance of a struct have their own unique guid? Looking at the \`TokenStream\`, it seems that I get only what's in the struct definition itself, so I'd need a second macro for the \`fn new\`, to actually inject the guid generation into the newly created field. Is there any other way, with only one macro?
Got the basics going (not to my problem though), but how do I make each instance of a struct have their own unique guid? Looking at the \`TokenStream\`, it seems that I get only what's in the struct definition itself, so I'd need a second macro for the \`fn new\`, to actually inject the guid generation into the newly created field. Is there any other way, with only one macro?
Got the basics going (not to my problem though), but how do I make each instance of a struct have their own unique guid? Looking at the \`TokenStream\`, it seems that I get only what's in the struct definition itself, so I'd need a second macro for the \`fn new\`, to actually inject the guid generation into the newly created field. Is there any other way, with only one macro?
Perhaps we will see a full Android API binding eventually.
Well you either have to store it, or generate it from stored data. Your proc macro should just be a method that returns (or re-generates) that value.
I really don't think documentation tests should be used as a primary method of unit testing. I do think it is handy that your documentation examples function as tests. This helps ensure that your documentation is valid and up to date. But you should still have a place in the code for unit testing.
https://minecraft.gamepedia.com/Efficiency
I’d agree with that.
Good meme.
Warning, borderline joke post: // type equality witnesses mod type_eq { use std::marker::PhantomData; use std::mem::transmute_copy; pub struct TypeEq&lt;T: Sized, U: Sized&gt;(PhantomData&lt;(*const T, *const U)&gt;); pub fn refl&lt;T&gt;() -&gt; TypeEq&lt;T, T&gt; { TypeEq(PhantomData) } // we won't need these for this, but here's how you actually use the witness impl&lt;T: Sized, U: Sized&gt; TypeEq&lt;T, U&gt; { pub fn as_left(&amp;self, x: U) -&gt; T { unsafe { transmute_copy(&amp;x) } } pub fn as_right(&amp;self, x: T) -&gt; U { unsafe { transmute_copy(&amp;x) } } } } // we don't have const generics yet, so we'll use a marker trait to // "sort-of" emulate lifting an enum to the type level trait AnimalKind {} enum DogType {} impl AnimalKind for DogType {} enum WhaleType {} impl AnimalKind for WhaleType {} // emulate a GADT with explicit type equality witnesses enum Animal&lt;T: AnimalKind&gt; { Dog(type_eq::TypeEq&lt;T, DogType&gt;), Whale(type_eq::TypeEq&lt;T, WhaleType&gt;), } fn only_dogs(dog: &amp;Animal&lt;DogType&gt;) {} fn only_whales(whale: &amp;Animal&lt;WhaleType&gt;) {} fn main() { let dog = Animal::Dog(type_eq::refl()); let whale = Animal::Whale(type_eq::refl()); only_dogs(&amp;dog); only_whales(&amp;whale); // only_dogs(&amp;whale); /* error[E0308]: mismatched types --&gt; animals.rs:40:15 | 40 | only_dogs(&amp;whale); | ^^^^^^ expected enum `DogType`, found enum `WhaleType` | = note: expected type `&amp;Animal&lt;DogType&gt;` found type `&amp;Animal&lt;WhaleType&gt;` */ } this meme was made by GADT gang
Your probably get a better response if you ask on internals.rust-lang.org and https://rust-lang.zulipchat.com/#narrow/stream/131828-t-compiler
Indeed this seems interesting. How to yield? Let's say you are implementing the future yourself, if you return Not Ready while the underlying future (e. g. socket) is in Ready state - you future won't be polled again, right? Without using Task api. Or is there a high lvl way of doing it?
Two out of the three have nothing to do with Rust. I never heard of "are we concurrent yet", but the wayback machine has a page: [https://web.archive.org/web/20160120200403/http://areweconcurrentyet.com/](https://web.archive.org/web/20160120200403/http://areweconcurrentyet.com/)
That's a funny timing, I literally just posted an article about direnv (https://www.vincentprouillet.com/blog/direnv/), a tool which automatically source the variables in an `.envrc` file. I'm not entirely sure either what I am missing.
There are a few ways I could recommend! The simplest would be to pass the GUID to the macro - something like #[derive(HasId)] #[guid(40)] struct ... Second is also simple, but less so. If you need stable IDs at runtime but don't care about them being stable across different compilations, you could use the `rand` crate to come up with random IDs at compile time. If you want to ensure there are no collisions, you could write used IDs to a file in OUTPUT_DIR and read them in. Or just rely on long enough GUIDs to make collisions very unlikely. Another way I can think of would be to use a deterministic hash, like sha256, to hash the input TokenStream. You could then have IDs which remain the same for unchanging source code, and are also still unique. I could talk more about any of these if they seem most reasonable. But the best solution depends on what your particular constraints are, and what you're using the GUID for.
You can always learn as you go. try reading through the rust book and whatever you don't understand just google. I am very much like you, no degree. I'm currently an employed software engineer (who occsaiouslly uses rust for work) and taught myself pretty much everything, google can be your friend. If you want, you can purchase some rust books (or pm me, I have some) which might give a better description of some quirks but IMO the official book is the best place to start.
i've found it to be pretty problematic, increasing compile times and slowing down tooling. in my opinion, with a type system as strong as rust's it shouldn't be necessary to lean on macros so heavily. i've seen them used to generate named fresh ids, which was particularly egregious (looking at conrod). as much as people seem super resistant to it, it does feel like monadic APIs would make a few rust projects quite a bit simpler (&amp; faster) to work with.
Also, last I checked, neither syntax highlighting nor code coverage solutions supported doctests. (That said, so far, I've only written `--bin` crates using Vim, so I haven't keep abreast of the state of the art on that front.)
You could just notify the task before returning `NotReady`. Most executors will push it to the back of their queue of tasks to execute.
That pun.
Thank you for the thorough reply. I agree with all or most (?) of what you are saying. My point was not very clear to be honest, at all, here goes another attempt. No disagreement on the SemVer part, a SemVer major version bump is needed on compile time errors. For the standard library it makes perfect sense to never break anything, unless it needs to be done in the name of soundness. &gt; Compile time errors are not "breakage", it's a successfully avoided run-time bug. I can see that my comment suggests that a SemVer major bump isn't needed, that's actually not at all what I meant :) I would like to rephrase it as: 'In the context of a major SemVer version bump of a public rust library on crates.io, compile time errors are not "breakage", it's a successfully avoided run-time bug.' In other words, when updating dependencies to the latest major version, one should expect to spend some time on fixing compile time errors, which in more dynamic languages would be more likely to result in runtime errors instead. &gt; I'd view a lot of breaking change releases in a short time span as a sign of immaturity This I also agree on. And I also think being honest about the immaturity of a project by doing the needed breaking changes with their SemVer major bumps is better than trying to hide the immaturity by avoiding breaking changes. I see some Rust library projects brutally sacrificing progress as a result of SemVer major version bump anxiety, which to me is a bit sad, it's just a library version number. The most recent example I can think of is the gloo project seems stuck in some kind of "let's-talk-about-theoretical-perfect-API-designs-forever-in-order-to-avoid-breaking-changes mode".
Hail satan, write java.
Thanks for the reply! Yes definitely looking at the `Num` library, the issue becomes finding good examples on how to properly use it... I personally haven't gained much insight from the docs. The existing numerical crates look nice, and I'll probably end up using one of them. I do try to implement code myself first, or at least have some "theoretical" background, before jumping to a library. Personally, I see great value in understanding what's going on under the hood, even at a high-level. Hence my struggle with generalizing numerical types, and this post. &amp;#x200B; I did look at the `num` trait, since as you said it is the main trait. My hangup with that is, it's only implemented for 0 and 1. How is it intended to be used for any input number of integer, floating-point, or complex type? &amp;#x200B; I think #3 was somewhat ill-posed on my part, to keep it brief. As you said, `std` should absolutely be kept stable and backwards-compatible. I'm mostly curious as to why complex types were not built-in, considering it would be highly unlikely to break the standard library, and more intuitive from a user standpoint. Functions for numerical types are already seem very compartmentalized, so adding extra functions for manipulating or retrieving the real / imag parts doesn't seem too outrageous. But I'm also not a core dev- Different people, different priorities.
That's basically correct. It's a static asset server for a multi-tenant app. Each of our customers have a `.env` file in a specific folder eg `&lt;customer id&gt;/.env`, and that file contains some customer specific details (such as where their files are stored). Each request comes in with an X-Customer-Id header which we currently use to open their file and the global .env file. The global is loaded and checked first, and then the customer specific one is opened and checked, with any duplicate details overwriting the global details. This then makes the crux of our config for each request.
and the documentation looks great (for a niche project)
Are the .env files overriding environment variables in this scenario or are you just using them as config files? If you're just using them as config files it might make more sense to use ini or toml
Ah, yes, they are just config files. Unfortunately, changing to ini or toml isn't going to be likely, due to how the rest of our system works with php dotenv projects. Calling it dotenv is potentially problematic, other than the files are literally `.env`, they could I guess be called `.ini` and that'd be effectively identical. We do not modify any environment variables for the server itself.
I'm really sorry, I confused you with someone else, who posted very similar designs in another issue.
Sure, but there's an orthogonality issue here still. `.await` looks exactly like a field access even though it's nothing like one, and that's what my mind thinks when I'm skimming across unhighlighted code snippets. I understand that it gets better with keyword highlighting and that people can read about it and learn what it really does, but the issue still stands that it is completely invisible when skimming code. I literally had to look at a block of async code for multiple seconds asking myself "where is the await call?????" because my mind couldn't pick it up unless I searched it word-by-word.
I am actually working on implementing customizable contraction hierarchies. CCH is slightly slower than CH but after preprocessing the graph topology it can be refit to new edge weights very cheaply. I also have a sort of half-assed CH implementation, but I switched to CCH since having preprocessing independent from edge weights requires less tuning, fewer heuristics, and can customize the graph at runtime. I will probably publish a WIP project once I get over the first hurdle (and most computationally expensive part), which is the nested-dissection contraction order. If you want to support this, I would prefer we discuss it after that.
This honestly looks a lot simpler than the jni crate. However, I'm not seeing a way to register methods for native classes.
/r/PlayRust This subreddit is for the programming language
Oh sorry lol
Things like this are why the `unsafe` keyword was created.
You may want to use an associated type instead of a generic type. The issue is that generic types allow for more than a single implementation, the following is allowed by Rust: impl GenericTrait&lt;bool&gt; for MyStruct {} impl GenericTrait&lt;i32&gt; for MyStruct {} So, instead, we can do make use of associated types which guarantee not having multiple implementations varying by the associated type only: trait GenericTrait { type Associated; } impl GenericTrait for MyStruct { type Associated = bool; } And create a non-generic trait over that. trait NonGenericTrait {} impl&lt;T&gt; NonGenericTrait for T where T: GenericTrait {}
Thank you for the reply! I have actually thought about using associated types but I dismissed it because I didn't want to constrain each struct to one implementation only. In any case your example seems to work so I will probably just go with that to save myself some headaches. I might revisit the problem later, with more experience, I just want this thing to work for now. &amp;#x200B; Again, thank you!
1. Yes, both `TcpStream`s will be wrapping the same underlying socket. (I.e. on Unix systems it ultimately [duplicate's the file descriptor](https://github.com/rust-lang/rust/blob/0bfbaa6e8dfb509b453020740fd37c7a22882c87/src/libstd/sys/unix/fd.rs#L227). As mentioned by the [documentation for `try_clone`](https://doc.rust-lang.org/std/net/struct.TcpStream.html#method.try_clone), both of the resulting `TcpStream`s will read and write from the same stream of data, and otherwise act like the same socket. `TcpStream` [implements `Send`](https://doc.rust-lang.org/std/net/struct.TcpStream.html#impl-Send), so you can `try_clone()` it, and send the duplicate to another thread (as is happening here; we push the clone into the `clients` vec, while the original gets moved into the thread closure). 2. `read_exact` returns `Ok(())` if it reads the necessary number of bytes. Otherwise, it will return an `Err`. In the client, it's using `read_line`, but it truncates (or extends) the string to 32 bytes before sending it to the server. This means that if you send `Hello world`, the server will receive `Hello world\0\0\0\0\0\0...` (and then [discards everything after the first NUL byte](https://github.com/tensor-programming/Rust_client-server_chat/blob/4158c0daa5191f82e4bec6a3e1b9963e570e5719/chat/server/src/main.rs#L31)), while if you send `This is a longer message that should be truncated`, the server only receives `This is a longer message that sh`. This is fine for a toy server like this, but in real life runs the risk of breaking UTF-8 characters: if you send the server the string `汉字汉字汉字汉字汉字汉字`, the server's listener thread will panic. 3. `clients.into_iter()` will consume `clients`, preventing it from being used again after if nothing else is done. We then call `filter_map` on the resulting `Iterator`. Most of the contents of the closure passed to `filter_map` is just sending the message to each of the clients, but for this question the relevant portion is `.map(|_| client).ok()`. If sending the message to the client was successful, it will return `Some(client)`. Otherwise, it will return `None`. `filter_map` will keep only the `Some(client)`s (turning them into `client`s) and discards the `None`s. We then `collect` into a `Vec`, and reassign back to `clients`, so the variable will still have a value for the next run through the loop. If you remove the `clients =` then `clients` is consumed, and no value is provided to replace it. If you write `for client in clients`, Rust will call `into_iter()` on `clients` (this is typical behaviour for `for` loops), and `clients` is consumed with nothing to replace it. If you wanted to use a `for` loop, you could probably rewrite it as something like: let mut new_clients = vec![]; for client in clients { let mut buff = msg.clone().into_bytes(); buff.resize(MSG_SIZE, 0); if let Ok(_) = client.write_all(&amp;buff) { new_clients.push(client); } } clients = new_clients; This code is identical to the original.
For someone who doesn’t know, how does cargo’s semver rules differ from traditional semver rules? It was my understanding that in semver 0.1.0 and 0.2.0 are considered API incompatible.
Anyway you can send me a message? Je suis interessé par un dev Rust pour notre boite.
You can use slices to make this more generic, and you don't need to allocate at all. You can then compute the correct indices yourself instead of relying on the chunks method, this makes the page method O(1). use std::{cmp::min, marker::PhantomData}; pub struct Paged&lt;'a, T, V&gt; { vec: &amp;'a V, page_length: usize, phantom: PhantomData&lt;&amp;'a T&gt;, } impl&lt;'a, T, V&gt; Paged&lt;'a, T, V&gt; where V: AsRef&lt;[T]&gt;, { pub fn new(vec: &amp;'a V, page_length: usize) -&gt; Paged&lt;'a, T, V&gt; { Paged { vec, page_length, phantom: PhantomData, } } pub fn page(&amp;self, index: usize) -&gt; Option&lt;(usize, &amp;'a [T])&gt; { let slice = self.vec.as_ref(); let len = slice.len(); if index &lt; len { let page_index = index % self.page_length; let start = index - page_index; let end = min(len, start + self.page_length); slice.get(start..end).map(|s| (page_index, s)) } else { None } } } fn main() { let v = vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; let paged = Paged::new(&amp;v, 3); println!("{:?}", paged.page(0)); println!("{:?}", paged.page(1)); println!("{:?}", paged.page(2)); println!("{:?}", paged.page(3)); println!("{:?}", paged.page(4)); println!("{:?}", paged.page(5)); println!("{:?}", paged.page(6)); println!("{:?}", paged.page(7)); println!("{:?}", paged.page(8)); println!("{:?}", paged.page(9)); println!("{:?}", paged.page(10)); } [Playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=65628184fc459fad10832ee4cf83f5fe) [Playground link to version with mutable pages](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3591b224b4f76a1d189fa0cae276bdfa)
Calling your library JEP when [JEP is already a thing in the Java world](https://en.wikipedia.org/wiki/JDK_Enhancement_Proposal) is kinda confusing...
Semver explicitly states that \`0.x.y\` is unstable and not "constrained" as you call it. Specifically, "anything may change at any time". [https://semver.org/#spec-item-4](https://semver.org/#spec-item-4) Personally, I prefer starting at 0.1.0 (I always do this for my packages in most languages), and semver seems to explicitly not care before you put a 1 (or higher) in the major version number.
Semver just says that *everything* under 1.0 is allowed to break compatibility (that is, including 0.0.1 -&gt; 0.0.2) but Cargo loosens that to treat changes to the third number as compatible.
Note that cargo uses slightly different rules (more useful/expressive): &gt; This compatibility convention is different from SemVer in the way it treats versions before 1.0.0. While SemVer says there is no compatibility before 1.0.0, Cargo considers 0.x.y to be compatible with 0.x.z, where y ≥ z and x &gt; 0. https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html#caret-requirements
`type_id` is allowed in `const fn`, though (meaning, someone intentionally wrote an arm in the long match statement in `librustc_mir/interpret/intrinsics.rs` for it, but not one for `type_name`.)
Ok, again if you want `type_name` to be a `const fn` make an issue or a PR.
It's also incorrect to use f32: [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d3a2837a05fb5e634e4db7fda574c496).
Right. Those are semver ranges though, which I don't know where/if are specced. I immagine they have to be though, cause I think node/npm does the same thing.
It looks like you mean for different architectures, but if you happen to mean Linux -&gt; macOS, I have a few build scripts that compile to macOS that work without any hassle. [gitlab.com/darrieng/terminal-typeracer](https://gitlab.com/darrieng/terminal-typeracer)
I don't think that's quite right. We're both talking about compatibility: the spec says version `0.4.8` is not compatible with `0.4.2`, but cargo considers that it is. That is, given a requirement like `^0.4.2`, strict semver treats that the same as `=0.4.2` and won't allow using `0.4.8`, but cargo's variant does allow using that new version.
If you want to force the user to change the version, why not set it to something explicitly illegal? version = "changeme"
I believe the guiding idea has been to offer tools such as macros and procedural macros, and then gradually promote common and generally useful uses of them to integrated language features. `try!` and `?` are an example of this. It's probably a good decision, because something like reflection is very difficult to get right for every use case. I'm hoping for higher-kinded types as well, though.
Not quite right: - 0.0.z is incompatible for different z, same as traditional semver. - 0.y.z are incompatible only for different y, z does not matter. This is the case where cargo differs from traditional semver rules, which considers those always as incompatible. - x.y.z are incompatible only for different x, same as traditional semver.
Without WebRender there are cumulatively ~2 seconds of layout runtime on my machine.
 &lt;content&gt;&lt;![CDATA[ XML CDATA disables XML processing until a trailing `]]&gt;`. print!(" Snippet starts with a literal `print!("`. ${1 First field, where the cursor lands after the user hits tab. : This contains default placeholder text, which is automatically selected in the editor. "\{${2::?}\}}" The placeholder is `{`, followed by another, inner-field (for the next time tab is pressed), containing `:?`, followed by `}"`. This of course gives `print!("{:?}"`, with the `{:?}` highlighted first, and then if you hit tab again, `:?` is highlighted. ${1/([^\{])*(\{.*\})?.*/(?2:, :\);)/} A substitution, `${&lt;field&gt;/&lt;pattern&gt;/&lt;format string&gt;/}`. These take the contents of the named field, perform the match and then generate text based on the [format string](https://www.boost.org/doc/libs/1_56_0/libs/regex/doc/html/boost_regex/format/boost_format_syntax.html). So, for the first field, try to match: ([^\{])* In capture group 1, any amount of text that isn't `{`. (\{.*\})?.* In optional capture group 2, any text between `{` and `}`, followed by any trailing text. This then gets formatted: (?2:, :\);) If capture group 2 matched anything, generate `, `, otherwise generate `);`. In other words, if you have any `{}` bits to interpolate in the format string, there needs to be an argument list next. Otherwise just close the macro. $3 Move the cursor after that if the user hits tab again. ${1/([^\{])*(\{.*\})?.*/(?2:\);)/} Same match as before, only this time print the missing `);` if we didn't add one last time (there was an argument list to type first). ]]&gt;&lt;/content&gt; And now you know at least as much about snippets as I do.
Interestingly, npm went the opposite way: they generate 1.0.0.
So what's fun about this discussion is that ranges aren't specified in semver at all; so while semver \*does\* say what you're saying, semver says nothing about \`\^0.4.2\` at all. Implementations tend to follow the same thing we do. One of my goals with the next version of the semver spec is to actually encode this stuff.
For the most part, `std` contains 2 types of code: code to support language features (why `futures` was recently added), and code used in the vast majority of software. Complex numbers are useful if you need them, but most people don't and they're not a part of any language features. There's also code that had been found to not be as necessary as initially thought, and they can be hard to get rid of after already stabilized. As for conversions, as I mentioned, `NumCast`, `FromPrimitive`, `ToPrimitive`, and `AsPrimitive` are the main traits. You didn't have the issue earlier because they're included as dependencies for `PrimInt` and `Float`.
As far as I know (and I'm a bit on shaky ground here, going off memory), `^0.4.2` is not a semver, but a semver range. Or rather, it's a (cargo-specific-esk) encoding of the semver range `&gt;=0.4.2 &lt;0.5.0`. The fact that cargo chooses to interpret `^0.4.2` as "anything greater than or including 0.4.2 and less than 0.5.0" is totally fine, and as far as I can tell not at odds with the semver spec at all. Cause the semver spec does not state how to parse a semver range. It's perfectly valid as a requirement to state that I need package X &gt; 2.2.5 and &lt; 5.2.1, even though there are no semver restrictions on these versions with regards to compatibility. Also, as stated, anything bellow 1.0.0 is according to semver basically undefined behaviour, and as I recently learnt from https://gankro.github.io/blah/initialize-me-maybe/, when dealing with undefined behaviour, it's fine to do whatever, therefore what cargo is doing is perfectly reasonable :p
Great idea! https://github.com/bluss/petgraph/issues/252
This is exactly what I was trying to say. With way less certainty and clarity :p
I'm not in a great rush; sometime in the next month would be ideal though.
Awesome to hear! Keep me posted please.
We're saying the same things here.
because it's probably best if the crate compiles out of the gate
oh my god this is amazing
... assuming the macros are well designed.
Your original post makes it seem like Cargo treats `0.0.2` and `0.0.3` as compatible- but cargo and semver agree that those are incompatible versions and `0.0.3` does denote a breaking change in Cargo. Perhaps it'd make more sense to reference a transition `0.1.2` to `0.1.3`, which semver treats as breaking but Cargo doesn't?
Your username makes me hungry
Still slow as shit on my chromebook, so probably webrender?
Thank you it's a direct translation from french: https://fr.wikipedia.org/wiki/Porc-%C3%A9pic
Thanks for the explanation.
Vim has syntax highlighting on doctests if you use the latest syntax from rust.vim
On my Linux machine which definitely does not have WebRender enabled, it still takes a while to render.
Under 1.0.0, everything is shifted to the right one.
Just skip all the fuss and call all other rust functions only through the🤙FFI 🤙
&gt; There are issues with the _ approach as well if you need to ignore the result **but ascertain that no errors occurred**.
God damn all these Rust docs pages are slow as hell
&gt; There are issues with the _ approach as well if you need to ignore the result **but ascertain that no errors occurred**. The [ignore function] that the [article] you linked talks about does not ascertain that no errors occurred. It ignores all results, both `Ok` and `Error`. This means free_foo(foo).ignore(); is *exactly equivalent* to let _ = free_foo(foo); So, that doesn't seem to explain any issue with the `_` approach. It seems like you're just using this as an opportunity to plug an article and a crate that, if usernames are anything to go by, you wrote. [ignore function]: https://github.com/neosmart/ignore-result/blob/6a85ef7ff203fd6dbeed4a1449e72a06116b3f13/src/lib.rs [article]: https://neosmart.net/blog/2018/rust-ignore-result/
This has been answered in various other comments, but just to make it clear, this is *not* how it usually works with Cargo, at least if your dependencies use ("caret requirements")[https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html#caret-requirements], which is the default, i.e. "0.1.2" means "^0.1.2" which means 0.1.x, with x &gt;= 2, which are considered compatible: &gt; This compatibility convention is different from SemVer in the way it treats versions before 1.0.0. While SemVer says there is no compatibility before 1.0.0, Cargo considers 0.x.y to be compatible with 0.x.z, where y ≥ z and x &gt; 0. If you break that convention, you force your users to use non-default (i.e. non-caret) requirements to avoid their code from breaking, e.g. "=0.1.0", preventing upgrades. This is not /that/ big a deal, but if you look at the Rust crate ecosystem, you see that this convention is largely adhered to, and you can generally expect "0.x" versions to avoid breaking API.
For my binary application, I've been enjoying [`err_ctx`](https://docs.rs/err-ctx). It's super minimal, providing much less than most other crates, and for that reason it works quite well. For library crates I'm still going with [`failure`] or just plain enums, but that's overkill for things where you don't need to consume the error as a programmer. `err-ctx` gives me just enough to make errors nice without any more conceptual or runtime overhead than I need.
The `Iterator` docs are really the only ones I've had problems with. Which other ones stand out to you as slow?
Wrong sub my dude. You’re looking for /r/playrust . This one is about the Rust programming language.
Even in Rust, I don't believe this is true. When you're waiting on any IO event, even waiting on "who is calling connect next", there's a chance for another future to run. Unless you're doing CPU-bound computations, other futures can run (and you shouldn't be doing CPU-bound computations in a future). How would this be different in a pull-based rather than push-based implementation? Both of them have event loops which consider running other things when IO operations would block.
Maybe I'm misremembering, but I swear that I'd found quite a few of the docs pages to have a very slow load time. Going back through my history though (haven't been using Rust that long) I can't actually find any others that stand out. Iterator definitely is though!
Still quite slow on my laptop (it's a 1.7 ghz i3 though) takes like 10 seconds to fully render and i have webrenderer enabled.
Whoops. Edited.
You can force it enabled if you're brave.
Better not 😅
TL;DR: you can't. Sounds like the solution to your problem is going to involve heap allocation and dynamic dispatch. First off, I agree with /u/MysteryManEusine that you should be using an associated type rather than a generic parameter --- otherwise, it will be nigh-impossible for the compiler to reason about your code. As to building a polymorphic `Vec`, a minimal version of this code might look like: trait MyTrait { fn do_something(&amp;self) -&gt; Box&lt;dyn MyTraitResult&gt;; } trait MyTraitResult {} impl MyTraitResult for f32 {} impl MyTraitResult for f64 {} struct VecOfMyTrait(Vec&lt;Box&lt;dyn MyTrait&gt;&gt;); impl VecOfMyTrait { fn do_something_to_all(&amp;self) -&gt; Vec&lt;Box&lt;dyn MyTraitResult&gt;&gt; { let mut v = Vec::with_capacity(self.0.len()); for thing in &amp;self.0 { v.push(thing.do_something()); } v } } struct ThingA {} impl MyTrait for ThingA { fn do_something(&amp;self) -&gt; Box&lt;dyn MyTraitResult&gt; { Box::new(0.0f32) } } struct ThingB {} impl MyTrait for ThingB { fn do_something(&amp;self) -&gt; Box&lt;dyn MyTraitResult&gt; { Box::new(1.0f64) } } fn main() { let my_vec = VecOfMyTrait(vec![Box::new(ThingA {}), Box::new(ThingB {})]); let result_vec = my_vec.do_something_to_all(); } Note the use of `Box&lt;dyn MyTrait&gt;`, which represents a dynamically-dispatched implementer of `MyTrait` stored on the heap --- the same way "objects" which implement a "class" are typically represented in object-oriented languages like Java. If it's appropriate to your needs, you might replace `Box&lt;dyn MyTrait&gt;` with either `Rc&lt;dyn MyTrait&gt;` or `Arc&lt;dyn MyTrait&gt;`, which will allow you to share immutable objects. `Arc&lt;dyn MyTrait&gt;` uses atomic reference counting, which is the mechanism e.g. Swift (implicitly) uses to share data between threads. Either one will involve heap allocation --- trait objects are unsized, which means they (basically) always have to go on the heap, so you need a container of some kind.
also, if you're going to do this, please name the blocking version `foo_blocking` for consistency.
I think keeping the separate testing file in the test directory is preferred for integration test.
the page size is 13.3 MB !!!
You really shouldn't post to subs before reading them. This is the subreddit for the Rust programming language. You want to post to /r/playrust But I agree with you. Rust (the game) does suck.
Are you using the online or rustup version of the docs? I use the local one because my internet connection is… not the best.
Nice, that's really creative
The online version, but rustup's version is equally slow.
Fuck
This makes so much sense
 [https://github.com/rust-lang/rust/issues/55900](https://github.com/rust-lang/rust/issues/55900)
Tarpaulin: https://github.com/xd009642/tarpaulin
That page doesn't look too heavy on the painting side of things. It's mostly blitting glyphs, and as a result WebRender isn't going to *especially* help performance on that page. Of course, it won't hurt, as GPUs are fast at blitting. I suspect that page is DOM and layout bound. If any Rust code is helping, it's probably Stylo. But the improved performance on that page is likely primarily due to the tireless work of Gecko engineers. Even if their work isn't as high-profile as the flagship Quantum pieces, it's very important :)
Major release. Service pack. Patch so I think 0.1.0 makes sense
Are you sure it would silently overflow? I thought rust had panics for under and overflows.
Thanks for that, You are right! Deploy indeed seems confusing. I was thinking like deploying for the Rust application but... The method should be renamed. Something like add_maven_dependency maybe?
Hey, I can't comment on the process for getting things to Rust's standard library - I'm not involved with that. But I'm curious why you feel that the solution to a supply chain attack (I'm interpreting this to mean the loss of secure access to crates.io) would be best solved by pulling more functionality into the standard library. And wouldn't the distribution of the standard library itself be susceptible to that same kind of attack? Also, you might be interested to know about Cargo recently adding support for private registries which (I think) mitigates this. References: https://github.com/rust-lang/rfcs/pull/2141 https://blog.rust-lang.org/2019/04/11/Rust-1.34.0.html#alternative-cargo-registries https://blog.cloudsmith.io/2019/05/01/worlds-first-private-cargo-registry/
It is not possible to create a Java classes and methods from Rust code in j4rs. However, users are able to create their own jars or classes and add them in the classpath during the Jvm creation. Regarding a Java "Hello World" example, when static methods chaining will be supported in j4rs (this should be in the next release), it will be something like: ``` jvm.chain_static("java.lang.System")? .invoke("out", &amp;[])? .invoke("println", &amp;vec![InvocationArg::from("Hello World")])?; ```
the standard library is not maintained by one person. In the case of the nodejs dependency someone found a library which was used by another dependency that had a single maintainer. The maintainer gave control of the package to a malicious person. here is an article: [https://www.kaspersky.com/blog/copay-supply-chain-attack/24786/](https://www.kaspersky.com/blog/copay-supply-chain-attack/24786/) If I use something from the standard library - i dont need to worry. It's a basically apart of the language. There is significantly lower chance of something like that happening.
Something that tends to help with performance slightly for me is opening the local copy of the documentation with `rustup doc`
It could be argued that when you're depending on `^0.x.z` you're opting in to significant breakage, when you're depending on `0.x.*` you're opting in to less breakage, with the expectation that while nothing is guaranteed, the upstream author probably will still at least *roughly* distinguish between patch and minor versions. A thing worth considering might be to have cargo at least warn if you want to release a package with major version `&gt;0` which depends on packages `=0`, as breakage tends to be transitive.
There are several measures in crates.io which prevent _exactly_ what happened to node.js. In particular, it's impossible to delete packages, and thus impossible to "replace" them through credential attacks. Once a program version is uploaded, it's final. This isn't by accident, either: these decisions were made explicitly keeping in mind node.js's crisis. With that said, that just prevents the type of attack the majority on node.js happened to be. It's still possible to take over a user account and publish _new releases_. These new releases _won't be used by existing projects immediately_, since Cargo.lock locks versions, but they could be pulled in when upgrading. The only defense against this is having trustworthy authors the protect their GitHub accounts with 2FA, etc. --- With std, it's true that it's much harder to execute an attack on std. But having things in the standard library has many other downsides, like forcing permanent API stabilization, and having crates owned by particular trustworthy team members accomplishes largely the same thing. All in all, I would say that this can be a concern, but it's much less of one than for node.js.
The intention is to give the ability for the user to communicate with Java custom structs/classes. Serialization is done only when needed. E.g., when creating `InvocationArg`s or when calling the `to_rust` method. In other cases where the `Instances` are already created, they are used as is, without serialization. I was thinking in the future to use protocol buffers for faster serialization. Still not perfect but should be faster.
i think it got worse recently, for some reason it also seems like it lazy-loads parts of the page now, too
I tend to reserve patch/z for bug fixes, so I'll start at 0.1.0 and go to 0.2.0. That way, I can go back and fix 0.1.0 with 0.1.1 if I need to.
Been using this pong game as an example case for a few presentations on WebAssembly. The latest iteration employing wasm-bindgen, web-sys and js-sys actually came out quite decent. There's a couple of warts I'd like to iron out though: - Handling of the global state - requestAnimationFrame loop handling - Event handlers I'd use a Mutex around the main state, but web-sys stuff isn't Send. I shouldn't need Closures to have event callbacks if my handlers are plain functions, shouldn I? Couldn't figure out how to pass a plain function to `set_onkeydown` and friends.
It is possible to do that with generic types, but this is more complicated, as you end up needing to use wrapper types to store a type. use std::marker::PhantomData; struct NonGeneric&lt;T, U&gt; { value: T, _phantom: PhantomData&lt;fn() -&gt; U&gt;, } impl&lt;T, U&gt; NonGeneric&lt;T, U&gt; { fn new(value: T) -&gt; Self { Self { value, _phantom: PhantomData, } } } impl&lt;T, U&gt; NonGenericTrait for NonGeneric&lt;T, U&gt; where T: GenericTrait&lt;U&gt; {} Then it can be used as such. function(Box::new(NonGeneric::new(MyStruct {})));
My point is that when you write `version = "0.2.4"` in cargo, that's a (valid) semver. However, when you write `cratename = "0.2.4"` or `cratename = "0.2"` or any other dependency version specifier, this is not a semver, but a cargo specific semver range, which is not specified at all (except for in the cargo documentation). The string `^0.1.2` is not a valid semver, and the spec says nothing about it.
Okay, yeah - I can see the value in having a more robust standard library. I think the sentiment I've seen is that being more conservative with what makes it into the stdlib will allow more experimentation and prevent more (not all) situations where an outdated module is being maintained because it has been essentially "blessed" by being included in std. You make a good point. I think it's fair in this situation to play the reactionary hand and respond to security situations as they become clear and present. So, maybe it would make sense to have a second layer around the standard library where this kind of auditing is mandatory, but doesn't carry the maintenance burden. Maybe analogous to Nighty's relationship to Stable.
Haven't tried this myself yet, but https://github.com/codecov/example-rust
&gt; Tarpaulin only supports x86_64 processors running Linux.
My MacBook Air doesn't seem to have any issues with it forced on, honestly.
Speaking of academic papers, there is [RAPTOR for public transport routing](https://www.microsoft.com/en-us/research/publication/round-based-public-transit-routing/). We successfully use combination of that and our own in-house OSM router to route between public transport nodes.
I few a idiot after reading your reply. Thanks very much!
If you don't have any platform-specific code, then you can run it on a CI platform such as Gitlab or Travis, and the coverage will be the same as everywhere else.
&gt; Rust async requires such a different paradigm of thinking about data that I'd expect anyone exporting both kinds of functions to be providing one "main interface" using either sync or async Hmm... I probably wouldn't expect the same application to use both sync or async versions of a function. But I don't see why a lib couldn't provide them quite easily. The async version just returns a future rather than data. I guess it depends on the lib though. Client libs are probably a lot easier to do with both options than server libs...
[cargo-make](https://crates.io/crates/cargo-make) includes a task which can calculate coverage using kcov and my [rust-cli-boilerplate](https://github.com/ssokolow/rust-cli-boilerplate) includes an equivalent tasks for [just](https://github.com/casey/just). There's also [this blog post](https://sunjay.dev/2016/07/25/rust-code-coverage) which explains how to do it manually.
&gt; initial iterations on a new library will almost certainly break API Maybe. Almost. But if current version is 0.0.x and the next one happens to be compatible, it is simply impossible to express that. Cargo will consider any other version number to be incompatible with 0.0.x. However if the default is 0.1.0, you get to choose. Maybe the next three versions will be 0.2.0, 0.3.0, and 0.4.0. But after that you can make 0.4.1 for example. Integers are not in short supply. 0.1.0 is a better default.
There may be still per-target differences: `usize`/`isize`, floating point implementation, and myriad other external things that may behave differently.
How about [https://adriann.github.io/rust\_parser.html](https://adriann.github.io/rust_parser.html)
No sorry
I would assume that the proc macro needs to emit an `impl` block containing the method definition, not just the function definition. :-)
Note that `LockGuard` from `std::sync::mutex` does not implement `Send`, so you wouldn't normally be able to yield while holding the lock. (Disclaimer: Still new to futures/async/await, it might be possible to write futures that don't have to implement `Send`.)
I don't even like this convention in C# and don't follow it, unless I write a matched pair of sync/async functions.
Fair point.
15.36s for all 23 requests to retrieve and rendering to complete. i7-3820 - AMD RX590 - Windows 10
On Arch, WebRender has caused 0 issues for me, been using it for about 2 weeks now.
DOM/layout/paint perf. is still a bit of a sore point in Firefox (even with webrender, although it definitely helps!). I've done some stuff with "infinite scrolling" data grids (i.e. showing a view into a v. large array with a fixed number of recycled elements), and Firefox really seemed to struggle (notably delayed rendering that caused (brief) content flashes) with anything more complicated than a single div with a text node. Both Chrome and Safari seemed to handle such cases flawlessly. Are there any fundamental architecture issues that are blocking better perf here, or is it just a case of "not quite as well optimised". Firefox now seems to log a console warning if you use scroll handlers to manipulate the DOM, which is a it worrying as IMO this is the only way to implement components which handle v. large data sets on the web. And this is a pretty common use case to not support!
https://github.com/mozilla/grcov README has an example you can copy and paste.
I can confirm though that it does work in Windows Subsystem for Linux.
Sounds good. I'll give it a shot.
I suggest hosting the game somewhere or as least use a GIF in README, for lazy people (like me).
I've been running Webrender on Firefox Nightly for about a year and haven't had a noticeable issue never mind a crash for probably 6 months, so ya there should be no problem running it.
Coming up at [https://bzar.github.io/wasm-pong-rs/](https://bzar.github.io/wasm-pong-rs/) as soon as github processes it.
Great!
&gt; For the standard library it makes perfect sense to never break anything, unless it needs to be done in the name of soundness. Okay. But just to be clear, the person you were responding to called io::ErrorKind an "abomination." It sounded like you agreed with that. But thanks for the clarification.
I believe you can do: match token_result { Some(token @ Token::Name(..)) =&gt; Some(token), // &lt;-- only match when type of token is T Some(token) =&gt; None, // &lt;-- match on Some where token is not T None =&gt; None }
Nice, that brings me closer to my goal, but I believe I would still need to get rid of the enum and replace it with structs (i.e. `consume::&lt;Token::ClosingBrace&gt;()`) won't work. Is there a way to avoid that?
Same here, I came from Python and tried my first lines of Rust yesterday. Documentation is veeery good and I haven't had big problems on the transition. Also the book But How To Do It Know is amazing if you are willing to have a better understand of memory management stuff.
Ah, I didn't notice the genericness of the function. You could look into `https://doc.rust-lang.org/std/mem/fn.discriminant.html`. Otherwise you might be able to implement a similar Trait for your enum (or just add a method to your enum that does the comparison manually)
Up now at [https://bzar.github.io/wasm-pong-rs/index.html](https://bzar.github.io/wasm-pong-rs/index.html). Apparently github-pages [can take a while](https://stackoverflow.com/questions/45362628/github-pages-site-not-detecting-index-html) to notice the index.html file.
It is, but the author was willing to enhance Rust support. The other one prefers code contribution but I don't know anything about Typescript.
These blueish colours are the default from this extension :)
This is apparently in production at Zhihu. Zhihu is an extremely large Q&amp;A site, more on https://en.wikipedia.org/wiki/Zhihu.
With the pace Rust is progressing on, any dead tree book is going to be outdated the moment it's coming out. However, the core principles haven't changed in a long time.
I would just write `str::from_utf8(&amp;[ch as u8])` inline. Is this conversion needed so frequently that it needs to be a inherent method?
A `char` is 32 bits, so `&amp;char` can transmuted into a `&amp;[u8; 4]`... Then you can make a slice and call `str::from_utf8`, and trim away the 0 codepoints. Shame on you :-)
[https://github.com/jesusprubio/online/blob/master/src/lib.rs#L47](https://github.com/jesusprubio/online/blob/master/src/lib.rs#L47) You should not be comparing the string representations of an error to determine the cause. Remove the simple\_error dependency and make the connect function return Result&lt;bool, std::io::Error&gt; and then compare its kind.
Does `ch as u8` really do what one wants here? How would one find out? I've always wondered where the behavior of `as` would be documented, but I did not find out yet.
I'd recommend basing your solution around [`char::encode_utf8`](https://doc.rust-lang.org/std/primitive.char.html#method.encode_utf8). Since an ASCII `char` encoded to utf-8 can be stored in a single byte, you would only need a 1-byte array. I believe you'd only need a maximum of 4-byte array to store any rust `char`. [Example using a 1-byte, stack-allocated array.](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=3b7e962fd193bf97107bcfb3cfd8ee2a)
Sure, but code coverage is always more of an indicative measure anyway. For example: fn foo(a: bool, b: bool) -&gt; String { let mut result = "".to_string(); if a { result.push('a'); } if b { result.pop().unwrap(); result.push('b'); } result } Call with `foo(true, true)` and a code coverage tool will indicate 100% code coverage, obviously missing the fact that `foo(false, true)` would panic.
Feels like you don’t need generics here at all.
Then how can I pass the specific variant of the enum to the function? What I want is a function like \`next\` (which can return any variant of \`Token\`), but will return \`None\`/\`Err\` if the variant returned by next is not the one that caller wants.
As ASCII only has 128 characters, you may store a static array `static ASCII_CHARS: [&amp;'static str; 128] = [ "\x00", "\x01", ... ]`. In this way, you don't worry about `unsafe` or lifetimes.
Sure, thank you :)
MOZILLA and REDDIT ARE BULLSHITE! Bloody Enemies of FREEDOM
If the project is already using a \`.env\`, you can also use the \`dotenv\` command in the \`.envrc\` to automatically load that file. It's part of the direnv stdlib function set.
The biggest issue is that most projects we work on are or include proprietary SDKs, which don’t seem to fit the Rust way well. If rust had a stable ABI, this would be solved, but we’d have to come up with different solutions to fit our projects into a Rust world. Other headaches include Pending Rust features, but none are show stoppers. Off the top of my head, const genetics and proc macros being complete would be great. Variadic genetics are something we rely on quite a bit, but an analog doesn’t really exist yet. A typeof operator could be nice for macros to make some C++ like template patterns easier. Specialization would be nice, if only because it’s a pretty common solution in C++. CMake handles things like installing all outputs into a directory layout and even includes tools like Cpack to do packaging. I’ll like to see Cargo and the ecosystem evolve a bit here. I’d also like to see cargo grow more support for censoring dependencies and working offline. On the library side of things, Rust has no analog to Qt which is used heavily. One good thing though is because a lot of basic things aren’t covered well by the C++ standard library, historically a lot of our code base uses Qt as a platform abstraction for things like: plugin loading, file system operations, ini reading/writing, network programming, and other non GUI tasks. I think Rust has comparable or even much better solutions (by virtue of everything speaking standard types and container types). Qt has a very nasty habit of infecting your APIs since every Qt API needs to speak Qt types, so using them requires you to use Qt types. Rust isn’t all bad though, there’s plenty of things I’ jealous of everyday. - Cargo is a big one - starting a new project or adding dependencies involves writing a CMakeLists, which is never as nice. - Modules are really nice, headers not so much. - Our applications are cross-platform, but So much of or code isn’t cross-platform, not because of logic bugs, but because compilers have different interpretations or strictnesses about types or syntax. Builds are constantly broken since most devs only have on machine and couldn’t even test across platforms if they wanted. - Testing is easier in rust and I hope it keeps getting easier. Also benchmarking becoming stable would be fantastic. - We recently ran into a problem where QT’s 64 integer typedef doesn’t match the standard library’s 64 bit integer typedefs. That one pains me greatly, but it’s something you would never think about with Rust’s u64 and i64 standard types that have existed since 1.0.
Why do you have a Reddit account, then?
You can probably (ab)use bits of rustdoc for this. But also, what's the end goal here?
Not necessarily - your example is recognized by other types of coverage measurements like Condition/Decision coverage. Of course it's still only a sort of a sanity check. You can have the same problems with (M)CD/C- but it is much less likely.
About io::ErrorKind, I think I get why it's there and that it lowers friction for many cases and I would probably say that it's the pragmatic solution for IO errors in `std`, so many things can go wrong with IO. However, I would also say that io::ErrorKind is possibly one of the worst examples to use for the general case of how to do error handling in Rust and that it might even be a special case that could be unique to IO in the standard library, a library that can never change in combination with errors that are unusually unpredictable. On the other hand it's probably a perfect example of how to handle that combination of constraints, when and if they do appear outside of the standard library. So "abomination" might be a bit harsh and uninformative, but I think it's a good idea to highlight that io::ErrorKind is not an example normal error handling in Rust and that it should be avoided unless that pattern is actually needed.
Related: https://github.com/tantivy-search/tantivy
I think this is probably the best we've got for now: https://doc.rust-lang.org/reference/expressions/operator-expr.html#type-cast-expressions `ch as u8` is effectively going to behave just like any other integer cast, as if `ch` were a 32 bit unsigned integer (i.e., the cast will truncate). So in this case, `ch as u8` will do what you want, assuming you guard it with a `ch.is_ascii()` check first.
To practise his freedom of speech against the very platform that he is practising his freedom of speech on.
the problem is rust guys are here, you block and ban them and then let me know where they go , i will follow them.
You'll know once I publish my crate :) I'll make a post here.
Yes, that should be possible.
Thanks for that :) Is it understood that "truncate" means chopping of the least significant byte first? The OP's code explicitely handles endianess, but the reference only makes sense if one understands what "truncate" means.
oh and mr Light123 you JACK ASS my accounts are banned here EVERYDAY YOU MOTHER FUCKER SHEEP SLAVE OF ILLUMINATI HAW DARE YOU ASK THAT YOU GARBAGE SHIT REMAINS OF HUMAN EVOLUTION. Bloody Brainless junk among sentient humans. NOW bend over for the tyrants to JACK YOU UP
No, `io::ErrorKind` and the like are very common in the ecosystem, precisely because folks want the flexibility of adding new error variants without releasing breaking changes, _and_ because exhaustiveness checking on errors is not used often. So the exhaustiveness checks that you get end up not being worth a major version bump every time a new error is introduced. Like, there will always be examples of people who are too conservative of major version bumps. But that doesn't mean every form of advocacy in favor of keeping APIs conservative with room to grow necessarily gets lumped in with people who are scared to do major version bumps. Major version bumps have a cost, and sometimes it's warranted and other times it would be better to slow the pace, especially if you're trying to maintain a core mature library. But now I just feel like we're going in circles?
No I mean without the extension enabled, I'm getting syntax highlight, so I was wondering from whom
None, just `Result&lt;..., SomeEnum&gt;` because ot's fast and requires no dynamic allocation, unlike a boxdd trait. As a bonus, everyone knows `Result` and `enum`.
Why does Rust need `ldl`?
seriously what time and era is this, why in the world is anyone still using testing anything regarding coding ?
It's from VSCode itself :) if you type `@builtin rust` in the extension section, you'll see it
No, truncate means _keeping_ the least significant bits, and chopping off anything else that doesn't fit into the destination type. Example: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6071baf2739a73960114abd11e0884e1 Endianness does not matter here. Endianness only matters when converting between integers and bytes. Truncating a u32 (or a char) to a u8 does not require first converting the u32 to bytes. The OP is using endianness because it's trying to get at the raw byte representation of the given `&amp;char`, so that it can get a pointer to the correct byte in order to build the slice. (But `encode_utf8` is a much nicer way of doing this that's already available in the standard library.)
Great solution :-)
This sounds really promising. I would love to see a Kibana alternative based on it. The ELK stack is such a resource hungry bunch of applications and the slow startup times don't make it any better.
There needs to be a pattern for dealing with the allocator context problem because there are definitely other C libraries that do that. You might have trouble finding open source code taking advantage but almost certainly there are people on embedded using allocator customization for their C libraries.
Do you mean Tarpaulin doesn't have branch coverage?
&gt; No, truncate means keeping the least significant bits Ah I think I simply switched words, not a native speaker. Your example shows what I wanted to say :) &gt; Endianness does not matter here. But it matters "under the hood", right? The compiler has to know which is the right byte to keep, and that depends on endianess.
I am not sure of Tarpaulin exact feature set. However, even branch coverage is not perfect if intra-procedural. For example, if the original `String` is passed as an argument, you would need inter-procedural checks to guarantee that the `String` always contains at least 1 character whenever `b` is true. I don't remember seeing any code coverage tool with inter-procedural branch coverage.
Comparing Chrome to webrender-Firefox on my computer, yes, it is indeed much slower on Chrome.
You mention not knowing how bzip2 works internally. I know a bit about that, it uses a weird and wonderful thing called the Burrows-Wheeler transform. Fairly recently there have been found faster ways to calculate that transform, thanks to linear time suffix array algorithms. The first linear time suffix array construction algorithm was first invented in 2009, and bzip2 was last updated in 2010, so it's possible it can be asymptotically speeded up... then again, maybe it doesn't matter on the small block sized bzip2 uses. Andrew Gallant (BurntSushi, the ripgrep author) has a fast suffix array implementation in Rust which you may be able to use.
How about a `dirty` keyword?
Please feel free to create an issue asking for help in the [`ndarray-linalg` repository](https://github.com/rust-ndarray/ndarray-linalg). Please provide any `Cargo.toml` you've tried and any error messages you're seeing.
Yes, in a similar way to accessing members on a struct, under the hood the compiler needs to know where to get the data, and sometimes it is useful to deal with bytes directly, but generally the exact memory layout wouldn't be important enough to deal with directly.
It seems we agree on most things except the io::ErrorKind pattern then. Nice talking to you, I feel like I'm done in this thread if you are :)
Following tantivy for a while. Can someone tell me how the [rucene](https://github.com/zhihu/rucene) project compare with tantivy?
Ok, thanks, just checking my mental model :)
rucene looks like a code transliteration of Lucene. In particular, while the README is sparse, the code suggests that it uses the same on disk format as Lucene. That means you should be able to use rucene to read/write an existing Lucene database. Tantivy is not a port of Lucene. It is its own thing. Beyond that, I think you're on your own with respect to a comparison.
Maybe using a macro? macro_rules! consume { ($p:pat) =&gt; { let e = next(); if let Some($p) = e { Some(e) } else { None } } Use like: consume!(Token::Name(_)) I realise it isn’t pretty though.
Agreed, although to be nitty I think you mean an ELK alternative. There was some project to make a copy of Elasticsearch based on tantivy. What would be interesting is if the APIs were kept the same so that Kibana could just drop in place.
My i3 laptop from 7 years ago with onboard intel graphics works just fine with webrender. Features Compositing WebRender
The online version is ahead of the printed version. We're actually working on finishing up editing for the next printed version. That said, Rust is also about stability. You will see some sections where it's like "Rust will throw an error here" where it now compiles, but you shouldn't see anything that's \*incorrect\*. There are some newer features you may not see. But the foundations are still the same.
https://github.com/mehcode/actix-web-async-await
Reading about PhantomData took me down a rabbit hole about subtyping and variance and I will be trying to understand it all in the upcoming days. I suppose putting a `fn() -&gt; U` into PhantomData instead of just `U` has to do something with avoiding drop checking?
The enum is a type, the variants are not individual types. The variants are instances of the type.
I am not 100% sure, but embrio-rs could be doing something like what you describe to hack around the thread local requirements on async / futures. ( This is needed on embedded )
Indeed I will definitely need dynamic dispatch. I was trying to minimize boilerplating but I might have to just go for it. And if that's the case I will look into macros to maybe create a custom derive. Thank you for the example, I haven't thought of using traits for return values.
Let's do a compromise and agree on the E\_K part :D
It's impossible to debug that from your description, unfortunately. I'm sure if you have a test case, though, that Gecko folks would appreciate a bug report. I will say that in general all the browsers use broadly the same techniques for DOM, layout, and paint—except for WebRender, which is fairly different.
I started on a bzip implementation a while ago (mostly because the algorithms fascinated me and there wasn't a Rust implementation) but didn't get to far. I've put up what I had [here](https://github.com/BurntPizza/bzip2-rs) if you want to take inspiration. Good luck!
There's Docker images so you can run it anywhere that has Docker, see [instructions in readme](https://github.com/xd009642/tarpaulin/blob/master/README.md#docker).
That would probably model well as a stream, something consuming that stream, and something consuming the output from that. Effectively a stream transformer.
While I agree that dotenv probably isn't the best format for this use case, perhaps exposing the parser separately from the part that sets environment variables would make a lot of sense?
Can I extract the syntax highlight provided by the builtin extension and port it to this new one?
The Rust community is on the cusp of having a Logstash alternative. Is anyone working on a Grafana / Kibana alternative yet? It's not clear whether anyone even should?
I don't understand quite how this solution improves on things? Wouldn't it require a bunch of instructions to "trim away the 0 codepoints"? Three conditional branches, presumably?
You know that /r/rust is the subreddit for the Rust programming language, and not the game Rust, right? You should really read the subreddits before you post to them. You want /r/playrust
I think this or `unsafe{std::str::from_utf8_unchecked(&amp;[ch as u8])}` is a great answer to the student's original question. Thanks much!
Interesting idea. Would work. Would add 1KB to the binary size and cache use in the worst case. In student's particular case, would only need the chars '0'…'9', so would be less expensive.
Thanks much to everybody who provided alternate solutions! Does anybody see any safety problems with my code? I am really curious whether I have a bug here.
I keep wondering where doc test stop and things like Jupiter notebooks begin. I hear netflix has a large stash of notebooks as tests. As a testing device and as a communication tool I think jupiter notebooks are a very exciting direction for testing / literate programming to go in.
For the past month or so, I've been making a scripting language with an interpreter written in Rust/C. It's is intended to replace Python/sh as a quick-and-dirty prototyping language, and (maybe in the future) a good embedding language for Rust programs. Let me know what you think!
Hmm is it repeatable? Meaning that if you run the same simulation multiple times, it's guaranteed to have the same outcome?
/u/fulmicoton do you know about this?
That's really cool! How could you invoke another process and get the stdout from it?
k
Thanks! The standard library comes with a Cmd object for command objects. To spawn a command like `date` you could do Cmd("date") which would create a Command that would spawn `sh -c "date"` when invoked, or Cmd(["date"]) which would directly execute the "date" command. To get its stdout do something like: Cmd("date").out() which should spawn the process, wait until it finishes and gets its output as a UTF-8 encoded string.
You seem to be using the str type, so I'm gonna base my assumptions on that, but the split_whitespace function borrows the str. When you do con.unwrap().split_whitespace(), the con variable is essentially dropped as it has gone out of scope. The way to solve this is to create a new variable con where you unwrap the value and then call split_whitespace() on it. This should ensure that the str lives long enough that it can actually be borrowed The simplest solution would be to create a new variable let con = con.unwrap()
You can use `syn` to find all "doc comments", but as I know `syn` skip comments if it is not "doc comment". There is also rust-analyzer, I suppose parser's crate that used by their team can find comments also.
Will the new book include async?
&gt; I'm clearly using the words variable which should live until the end of the block. Please note that the too-short-living value is not `words`, but `con.unwrap()` (please see the `^^^` underline). (so you should follow u/SethDusek5 suggestion). I think I have seen this kind of misunderstanding on what's not living long enough, and I wonder, how can we improve the compiler's message in this case, so it's more obvious what's happening.
Wow, it worked! I don't know why though.. So for my understanding: con is just a simple `Option&lt;String&gt;` (sry for not mentioning) which will be then owned by the `words` iterator ( I think as a `&amp;str`). The "`con`" in memory doesn't change whatsoever. It just gets owned by the iterator. Then why do I need it to be 'two?' variables working with the same memory? Anyways thanks for helping me out.
Only in debug mode
I know. That's why I'm looking for tricks to work around that (or other ways to achieve what I want to achieve).
I think it's more because of the 1.4 megabyte "search_index.js" file that's being loaded - rustdoc currently puts the entire search index into a single file, which blocks page rendering. I have no idea why the people that wrote rustdoc thought it was a good idea to put the entire search index into a single file that loads on page load, instead of loading it when the search bar is being used, or loading the search index in chunks. For example, try to load https://doc.servo.org/servo/index.html with and without JS enabled, it's a huge difference.
This seems to be working - [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2b0a37db4b2a1c822d7d94fee7433062) I tried to use `match` guards aka `Some(ref val) if val == ... =&gt; val` but I couldn't get the returned val to dereference. Looks like there is an existing [issue](https://github.com/rust-lang/rust/issues/31287) that doesn't allow this for good reason.
The macro solution isn't bad, though I still have to do some unnecessary boilerplate: &gt;let token\_slide\_name = consume!(Token::String(\_), self.token\_stream.next())?; if let Token::*String*(slide\_name) = token\_slide\_name { *Ok*(Slide::*new*(slide\_name)) } else { unreachable!(); } Any ideas on how to work around having to add that if to match on the variant?
The con in memory gets consumed when you unwrap it. You are essentially in charge of keeping track of this unwrapped value, and once it goes out of scope it gets "cleaned up". What's happening is is that you're doing "con.unwrap().split_whitespace()", so the unwrapped value isn't getting "stored" anywhere, so it gets cleaned up. If rustc wasn't so awesome this could cause some horrible effects, as the underlying string has been deallocated, so you'd be trying to access memory that either doesn't belong to you anymore or has been overwritten with your splitted whitespace type. Unfortunately it's not something I can explain too well, it's just something that'll come to you as you write more and more Rust code
Yes, but rust-nursery should be downloadable through rustup and used by the playground. It should be the extra parts that didn't make it into the std, but otherwise work roughly the same. Except for being set in stone
This looks nice, but one problem I have with discriminants is that you have to instantiate the enum to get them. Is there a way around that? Example of my imagined syntax (which doesn't work or make sense with the fact that variants aren't types, I know): &gt;std::mem::discriminant::&lt;Token::Name&gt;()
Fwiw, hana is one in Korean and haru is one day. I saw the flower icons and realized it's Japanese
I'm confused, what do you want to be generic?
Is there any specific reason for being half-Rust/half-C?
Well, looks like I was a bit confused as to what I want on my initial approach. Basically I want that: if let Some(Token::Name(name)) = next() { if let Some(Token::String(block_name) = next() { return Ok(()); } else { return Err(ParsingError::UnexpectedToken); } } else { return Err(ParsingError::UnexpectedToken); } But without the nested ifs, so I would ideally want to look it similar to this: consume!(Token::Name(name))?; consume!(Token::String(block_name))?; println!("{} {}", name, block_name);
Hmm, I might need someone else to chime in on that. My gut says `Token::Name(String)` might be an issue whatever the solution but lets see.
Ok I guess it's kinda meme then. Thanks for your help! :)
Do you mean it's an issue as in "something's wrong with the design" (if so, please please be more specific :)), or as in "may be hard/impossible to deal with cleanly"?
As long as I stick to ndarray it all works good for me. But if you are trying to use openblas-src crate then it wont compile on windows, and this is a known issue in openblas-src repository. I am currently trying to use netlib-src crate instead of openblas-src. I need to restart my computer now. Will come back with results :D
The VM was originally built in C; I have ported much of the interpreter into Rust however Rust is missing a key performance feature, computed gotos which hinders me from fully pulling the Vm into Rust.
It seems that bzip2 uses the IEEE polynomial based CRC called CRC-32. For this polynomial, a simd accelerated version has been written: [crc32fast](https://github.com/srijs/rust-crc32fast). You should try it out.
If that were true, then it seems like that would impact every page in the std docs. But only some pages, like Iterator, are gratuitously slow. AFAIK, the only way to fix this issue is to either come up with a clever hack that uses fewer DOM nodes without changing functionality, or by changing the amount of information that's shown. &gt; I have no idea why the people that wrote rustdoc thought it was a good idea to put the entire search index into a single file that loads on page load I can think of several reasons. If you adopt a more charitable interpretation, I'm sure you can too.
From the article: &gt; There are many Rust crates to do CRC computations. I was hoping especially to be able to use crc32fast, which is SIMD-accelerated. &gt; &gt;I wrote a Rust version of the "CRC me a buffer" test from above to see if crc32fast produced the same values as the C code, and of course it didn't. Eventually, after asking on Mastodon, Kepstin figured out what variant of CRC32 is being used in the original code. &gt; &gt; It turns out that this is directly doable in Rust with the git version of the crc crate. This crate lets one configure the CRC32 polynomial and the mode of computation; there are many variants of CRC32 and I wasn't fully aware of them.
Currently getting my feet wet with Rust and thought it would be a good idea to port some C++ code. Right now I'm in the middle of a simple 2d vector class and a bit baffled by the way the std::ops::Add, std::ops::Sub are implemented since they take ownership, which seems to make them pretty useless. `let mut a = Vec2::new(1.2, 3.4);` `let mut b = Vec2::new(5.6, 7.8);` `let c = a + b;` `let d = a - b// this doesn't work since ownership transferred to Vec2::add() in the line before;` Am I missing something fundamental? Do I basically have to implement Copy trait to make this usable and wouldn't that be pretty inefficient compared to something that used references?
To be fair, I don't think computed gotos are standard C either ;) But indeed I can't think of a Rust construct for an efficient instruction dispatch loop.
Apologies, to clarify, I believe that variant may cause problems as its hold a heap allocated type. I'm not sure how you could get around not instantiating it. The `lazy_static` crate might worth consideration if you do decide to go ahead with this solution. &amp;#x200B; In your original message you mentioned you already know the solution using Traits and Structs. Can I ask why you have decided against using those?
An alternative to gotos is to use a trampoline, so long as you know all potential goto targets ahead of time. You can obviously give the `goto` variable target of the trampoline a dynamic computed value just as easily as a static one. It goes something like this: let mut goto = GoTo::Entry; loop { match goto { GoTo::Entry =&gt; { // code code code // `goto foo` goto = GoTo::Foo; continue; } GoTo::Foo =&gt; { // code code code // no goto goto = GoTo::Fallthrough; continue; } GoTo::Fallthrough =&gt; { // code code code // no goto, exit break; } } } It's obviously not as self evidently performant as the raw `goto` implementation, but LLVM actually does a really good job of optimizing this pattern into direct control flow edges rather than bouncing through the trampoline.
Okay, I guess it's one of those moments where you just have to explain your problem and you immediatly see the answer. `impl ops::Add&lt;&amp;Vec2&gt; for &amp;Vec2` and `fn add(self, v:&amp;Vec2) -&gt; Vec2` Is what I was looking for.
I wanted to avoid the trait/struct solution, just because it requires more boilerplate, and is IMO less readable, i.e.: enum Token { Name(String), OpeningBrace, ClosingBrace } turns into: trait Token {} struct TokenName(String); impl Token for TokenName {} struct OpeningBrace {}; impl Token for TokenName {} struct ClosingBrace {}; impl Token for ClosingBrace {} Though now I see that all the enum-solutions require some amount of hacking, maybe I'll reconsider, write a macro and live with that :)
Between the deprecation of webRequest blocking, webrender, and small things like this, I think I'm going to switch over to FF...
Unfortunately, I have been using the `syn` crate for quite a long time and I don't remember such functionality in it. I have also checked a few times today for this but without success. Also, the example does not work anyway: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3bfe67ae6b0e2becf48f4892cdbe1a68
Interesting, does it compile to a raw jump instruction?
This is actually a perfect use case for "true" structural logging as well! Structural logging is in its pure ideal, holding a contextualized logger handle on your context object and putting all structural logs through that. `slog_scope` makes things easier for stack frame organized programs by sticking the contextual logger in a thread local and push/popping state there alongside the call stack. This also is required for interop with code that calls to a global logger. But futures break the connection of code structure that you want to represent in your structural logs with the runtime stack frames that `slog_scope` piggybacks off of. Because of this, you need to explicitly carry around your logging context on your future state machine and give that logger to your subroutines however they expect it. In this case, this means instrumenting `Future::poll` to hook up the `slog_scope` (thread) global logger whenever we (re)start polling. (Please feel free to adapt this for your readme to help explain what this is all about!)
This looks like an XY problem. Why do you want such a function? Can you show it being used in a non-trivial example?
I've not actually stuck this in a compiler explorer or otherwise microoptimized beyond the use of the trampoline to avoid stack blowup personally. I just know experimentally that it performs equivalently to function calls at the level I care about. (That means there may be some stack manipulation that you see around a `call` of course, and you're introducing a `Drop` point at the end of the scope as well of course.) I think of a trampoline as a `call` that shares the same stack space. It may not be perfectly accurate, but it's good enough to get the job done.
It will not; it's based on Rust 1.31.
Alright guys, thanks for the information.
You [missed a `/crates`](https://crates.io/crates/slog-scope-futures) in your URL.
&gt;It is not possible to create a Java classes and methods from Rust code in j4rs. However, users are able to create their own jars or classes and add them in the classpath during the Jvm creation. That's not at all what I was asking about, I was asking about registering methods for native classes like this one: class Native { public native void callNativeCode(); public static void main(String[] args) { callNativeCode(); } } There needs to be a backing method in the native code in order for Java code to call `callNativeCode()` and it some sort of function pointer for that backing method (lambda or otherwise) needs to be registered if the JVM is being initialized from Native Code like it is in `j4rs`.
Whoops, thanks! All fixed now :)
Relevant https://github.com/valeriansaliou/sonic
Hana is flower in Japanese and Haru looks to be Spring (the season).
This isn't something you can do with Rust. There's no reflection that can "see through" a reference to see the name of the place it was borrowed from. If you want to name the borrowed place, you'll have to get the name of that borrowed place and track it to the point you need it. `stringify!` works directly on the token tree arguments you give it. It has no care whether what you entered is valid Rust code at all; it just turns your tokens into a string literal.
So basically I have some source of tokens (i.e. the `next()` function in my examples) and I want to perform recursive descent parsing on it. So, if the subsequent calls to next return `Token::Name("aaaa")`, `Token::String("bbb")`, `Token::OpeningBrace`, `Token::ClosingBrace`, then I want to use `"aaaa"` and `"bbb"` to do some further processing (in my case create some structs). But if there's an unexpected token, say `Token::Name("aaaa")`, `Token::Name("bbb")`, `Token::OpeningBrace`, `Token::ClosingBrace`, then I want to return an error.
ding ding! Haru also means [ha]na implementation in [ru]st which is totally not a meaning i made up just now
Yes, also it avoids adding unnecessary bounds for auto-traits. For instance with `PhantomData&lt;U&gt;`, there would be a requirement of `U: Send` for `Send` implementation for `NonGeneric&lt;T, U&gt;`.
12-word default seems a bit aggressive? I wrote [mkpass](https://github.com/Freaky/mkpass) for my purposes, based on an old Ruby script I wrote years ago. Targetting a number of bits of entropy rather than a strict size seems quite natural for this sort of use, since it automatically scales to the size of the dictionary. I also wrote a [toy one](https://github.com/Freaky/simplepass) for [an article](https://hur.st/blog/2018/08-25-password-generation-in-ruby-and-rust/).
I like it!
What does it do inside the future, though? It's not generally possible to turn the sync version of a function directly into an async one by wrapping it in a future, because then you're still performing blocking IO the main thread. And unlike C#, all the supporting libraries and abstraction are different for sync and async IO in rust - so it's not going to be easy maintaining two distinct versions of the function with different underlying IO libraries and types. Turning a sync function into as async one is generally impossible, is what I mean. The opposite is! But it's still complicated because to "block on" as async function, you have to run some whole async runtime on the thread, like tokio. Some libraries do do this - it just takes extra abstraction. For instance, take reqwest: it has an `anync::Client` asynchronous client, implemention methods returning Futures. Then the sync `reqwest::Client` is a wrapper, and internally it just stores a handle to a message stream communicating with the async client running on a thread pool. It works, but it's not as simple as just transforming each method to operate both ways.
Got it! You're making a recursive descent parser where your valid tokens are enum variants.
Thanks! Coming from Java this would be easy, I see how this would be impossible for Rust. Tuple time!
You win the thread. Thank you!
yep
Yeah. I've been thinking about changing the default to something that is better optimized for memorability. And I agree about your point regarding the bits of entropy, been thinking about providing an option for providing the minimum bits of entropy desired.
&gt; https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3bfe67ae6b0e2becf48f4892cdbe1a68 fn main() { use proc_macro2::TokenStream; let t: TokenStream = syn::parse_str("/// hello doc comment").unwrap(); println!("{:#?}", t); } such code prints: Ident { sym: doc, }, Punct { op: '=', spacing: Alone, }, Literal { lit: " hello doc comment", },
I don't believe rust generators nor futures support passing in values into a "yield" at the moment - so nothing _this concise_ will work. With that said, something like this would definitely be possible! With some explicit stream to pass values in, you could definitely do something like async fn callee(input: impl Stream&lt;i32&gt;) -&gt; i32 { let res = 0; while let Some(x) = input.next().await { // do something res += x; } res } Since rust futures generally expect to be run on a full runtime shared with many other futures, you'd probably want to use a `futures::channel::mpsc::unbounded` channel as the stream. You could then give callee the receiver, and give caller both the sender and a future for callee's completion.
I am in no position to say that piston is dead - as it is an umbrella over multiple projects it is not - but the game engine part is not as actively maintained as it was previously. &amp;#x200B; As I do not know any tutorial (apart from the ones on the [piston.rs](https://piston.rs) site), let me say this instead: Depending on what you want to work on (3d, 2d) there are multiple, actively maintained alternatives: You have [https://amethyst.rs/](https://amethyst.rs/), [https://ggez.rs/](https://ggez.rs/) and a lot more (see here: [http://arewegameyet.com/categories/engines/](http://arewegameyet.com/categories/engines/))
That still doesn't really explain why you need to encapsulate the `match` in a function though. Why not just do the match directly? match source.next() { Token::Name(name) =&gt; do_something(); _ =&gt; panic!(); } And if do_something is long, this can be "inverted": let name = match source.next() { Token::Name(name) =&gt; name, _ =&gt; panic!(); // or return Err(whatever) }; // do something with name here.
It's not all that great either. An increasingly high number of websites are targeting Chrome principally and end up with numerous bugs or atrocious performance on Firefox. It's infuriating, really.
I had the same need as /u/burntsushi , and I modified /u/shepmaster solution to this: I came up with https://github.com/shepmaster/snafu/issues/89#issuecomment-497803897 Maybe (potentially refined) it could be included in `snafu::ResultExt`?
Awesome!!
Rust has a difficult time optimising tail recursion, so you should avoid it where unnecessary; the binary search is confusing the compiler. I struggled to find a nice way of putting this into iterators (literal *edge* cases), but using a `for` loop results in LLVM [optimising out the function entirely](https://godbolt.org/z/1OHH0j).
Oh I see. I should dig a bit myself as well because crc32fast doesn't work for ogg either. It may be crc32b, too.
\*facepalm\* for some reason, I forgot that match is an expression, not a statement (which would lead to nasty nesting)... yes, that actually seems to solve all of my problems :)
Oh, I don't know how I could miss it! Thank you!
&gt;proto please consider flat buffers as well
I mean, I can do it with a statement too: let name; match source.next() { Token::Name(name2) =&gt; name = name2; _ =&gt; panic!(); }
I will check those out when i have time
Some kind of reference counted type could help you here. The problem is is that self-referencing structs are tricky, and that's why the Pin api also exists, but I think in this case reference counting could to the trick. It would require allocating quite a bit though (new allocation for each element)
Relevant: https://internals.rust-lang.org/t/idea-ambient-data-current-execution-context-was-provide-a-thread-local-scope-with-hooks-for-std-thread-spawn/8559
That's cool. I'm curious to see what you come up with. I like parsers but I haven't written one in rust yet.
At first glance it seems like you're doing the right thing. Could you provide some of your code?
&gt; There needs to be a pattern for dealing with the allocator context problem because there are definitely other C libraries that do that. Don't pretty much all non-trivial C libraries do that, really?
I'm wondering though, is it really that useful to work on bzip2 when it's pretty much entirely superseded by lzma / xz? The use case of bzip2 was that it provided better compression than gzip at higher cost, but xz pretty much always provides better compression at equivalent resources consumption.
I don't have much experience with parsers or with Rust, so don't set your expectations too high ;) &amp;#x200B; [https://github.com/Agares/przntr](https://github.com/Agares/przntr)
Hmmm interesting. Because according to Wikipedia, bzip2 does use the IEEE polynomial. But I got the same problem, my ogg crate in theory uses the IEEE polynomial too but does one of the other operations differently.
Interesting. The contents of that a little too advanced for me to follow completely, but it does seem clear that if Rust needs to add a C-like `-&gt;` deref sugar, that would indeed create the same ambiguity in `-&gt;await` as `.await`. So I guess I'm persuaded that there's no strong advantage to `-&gt;await`. Darn.
The word you're looking for is deterministic. Also, you could have it be determinstic if you use `World&lt;Fixed32&gt;` or smth
All criticism is welcome. The goal of this crate is to be the absolute fastest logger in terms of caller latency. That means the caller should spend as little time inside the crate's functions as possible.
That doesn't make sense if the whole reason you're not guaranteeing an order is because you don't want the performance impact of an ordering operation. In that case, if you're shuffling, you may as well be sorting instead.
The index seems like the simplest solution but also unidiomatic? Is there an accepted way to go about this kind of design? I image it isn't uncommon to have to self reference...
I think writing password generator is some sort of rite of passage for developers, feels like I see them every once in a while and they always seem to express the individual to certain degree. And of course I myself have done at least on of those myself, albeit in C which is not going to win favors around here https://github.com/zokier/pw/blob/master/pw.c.
Sounds like an async logger, which means it loses data or stops being async.
This is cool work but why would you consider using slog_scope unavoidable? A Logger can be explicitly passed through function calls just fine. Further, rather than simply cloning a logger before sending it into a Future, add some valuable context to a new child logger instance before moving it on. Every time context is added, a new child Logger is born. Referencing a parent logger can only last until more context is needed. Someone please correct me if I'm wrong..
It loses data currently. Do you think this should be configurable?
Lossy logging has limited applications. Cannot be used for tracing or warn/error diagnostics for example.
If you only pull in dependencies that use slog as their logging system, sure, it's avoidable. But I would hazard a guess that slog is a minority in the rust logging ecosystem, and that most things use the "standard" \`log\` crate. If you use \`slog-stdlog\` to connect those to a slog logger, then you're using \`slog-scope\`. You can either choose to ignore that fact and just let them all drain to the global context-free logger \*or\* you can sprinkle slog scopes around to add the context to your "std \`log\`s" from the slog loggers that you're already passing around as you suggest.
It prints a message on how many messages were lost, so if that's not present you can trust it. But your point still stands. If we were to have async+blocking logging we'd need to somehow set that at log creation, but I'm not sure how that can be done without virtual calls (1.5 ns + maybe an instruction cache miss vs 0.5 ns direct call on my machine).
It also happens to be the safest I'd imagine, so you should probably roll with it. Messing with pins and Rcs can be tricky and you have the perfect solution by just using an index.
I kind of follow what you're saying. Is there an example on github/gitlab you could reference, or maybe even just add an example to your crate?
I'm interested if it's going (if it happens) going to be more concise than python because rust will not have that weird 'continuations have to be 'started' because the first value they send has to be 'none'', which is super weird in python when you're doing this.
Almost no logging solutions are non-lossy when the system crashes. Not to mention that most log aggregators will drop data if they are overloaded. For a properly implemented long running daemon, async logging might add some risk, but the guarantees are mostly the same - assuming a well written logger(panic hook, functionality to wait on log flushing before termination, ...).
If I understand correctly, aren't python and rust exactly the same in this way? Neither Python's coroutines nor rust's futures execute immediately. In both languages, the runtime/event loop needs to be told about the future in order to drive it to completion. In python it's `await x;` to start it and wait on it, or `asyncio.create_task(x)` to start it on the event loop but not wait for it. In Rust it's `x.await;` to start it and wait on it, or `tokio::spawn(x)` to start it on the event loop but not wait for it.
hey, have another question and you possible can answer since you know rust and are making an Ecs framework. I'm trying to implement multiple storages for components but I'm having a hard time figuring out a way to store them in a HashTable. Tried to take apart spec but having issues understanding how they go about doing it. The idea in my head is Storage { stores:{ type\_id : sparse\_vec&lt;Pos&gt;, type\_id: dense\_vec&lt;Rot&gt;, } }
The issue is that your `generic_test` function only knows that `T` implements `Test` and that `Test` has a `Container` associated type. It doesn't know whether or not the `Container` implements `Default`. There are two things you can do, you can either make the `Container` associated type require `Default`: pub trait Test { type Container: Default; } or you can make your `generic_test` function only accept `T` types where the associated `Container` implements `Default`: pub fn generic_test&lt;T&gt;() where T: Test, T::Container: Default, { // ... } I've moved from the `&lt;T:Test&gt;` specification style to where clauses for easier illustration purposes. Which one of these is best depends on your situation.
I'm not sure that my crate is really the right place to answer the "why would I need `slog-scope`" question, though I'm not sure if any examples of the `slog-{scope,stdlog}/log` interactions exist anywhere. The docs for [slog-stdlog](https://docs.rs/slog-stdlog/3.0.4-pre/slog_stdlog/) and [slog-scope](https://docs.rs/slog-scope/4.1.1/slog_scope/) are probably the best places to start trying to figure out how all of the moving parts fit together (there are a lot of them).
Gotcha! The index does seem like the best solution. I wanted to ask however, is it possible to do this kind of self-reference if both the referenced and referencer have the same lifetime and the latter borrows from the former. ie. something like: struct Tetrinome&lt;'a&gt; { ... bones: Vec&lt;Bone&gt;, pivot: Option&lt;&amp;'a Bone&gt; } I attempted to do this but got a lot of errors and didn't follow through.
I don't remember where I read that computed gotos are not that performant anymore. Don't forget LLVM is very good at optimizing. &amp;#x200B; \--- &amp;#x200B; One thing I using for my own interpreter is to push into rust as much computing as possible. For example: [1, 2, 3] + 1 Can be more performant in a interpreter than: result = [] for num in [1, 2, 3]: result.push(num + 1) because I can instead put the looping IN rust: nums.iter().fold(....) instead of decode and all that stuff. I think with a good set of functional constructs you can get get some easy of use AND perf.
It's not the 'execute immediately that gets me annoyed. It's the 'you first have to send 'None' for the protocol to work' (that first send in the caller and first yield in the called). That's weird and a abstraction leak.