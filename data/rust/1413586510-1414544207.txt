Create a channel linking the tasks, and send a cancellation message along it when you want to kill it.
I think we need more code to isolate the root of your misunderstanding. What's your Element::new method doing? edit: 1. You're creating a Parser object which owns a String. 2. You're making a parser tree within this Parser object of `Element`s. Each Element has a reference to a slice within the Parser object's string. 3. Those slices cannot outlive Parser. Thus, the Element cannot outlive parser. Both will have a lifetime as a result, so that the compiler knows this is your intention.
I don't think it's unreasonable either. I'm just a little disheartened when there's a clean slate and somewhat of a hard-coded, half-measure is already introduced so early. I get the same feeling when I see some brand new language ignore generics or introduce nulls and wave it away as necessary for interop with other languages, or legacy reasons or familiarity reasons. We already know better so lets do better or hold off until we can. But, of course, my resistance might just be another case of great being the enemy of good enough.
Rust only has a solid use case if it provides great performance. It's designed for building safe, cost-free/cheap abstractions and that all depends on LLVM. Reliably ripping through layer upon layer of abstractions is not easy. There's a lot more to it than just having solid iterative inlining and ripping aggregates apart into scalars.
All fair points, and I really have no way of knowing precisely how much effort it is to develop something like LLVM, just that it's a whole lot :) Rust has a much harder performance target than GHC however. If there's *any* C or C++ program that runs faster, or uses less memory, or produces more compact code than a Rust program (allowing `unsafe`) optimizing for the same variable, that's a bug in Rust. I don't know if everyone agrees, but it's how I approach the language, anyway. (Of course many bugs are low-priority and never get fixed, but *in principle* we want to be at least as good as C on *every* program.) This is only a realistic goal because we get to use the same backend as a popular, high-quality C compiler :) I would love to have a small and fast direct-to-ASM backend. It could even be the default for non-optimizing builds, although it's kinda scary to have a different set of codegen bugs in debug and release.
You need those lifetimes because slices are *borrowed*. Lifetimes are a promise that you won't keep references around longer than the data in the block of memory that they point to. So you need lifetimes on the slices, and then a lifetime on `Element` because you have to promise that you won't keep the struct around longer than the slices can stay. Then a lifetime on `Parser` because you have to promise that it won't stay longer than the `Element` instances it owns. This is actually a nice layout because you only have one copy of the string in memory, but you have to make sure that any slices to `Parser.source` don't remain longer than it does. Not sure if `Parser` should own the `Element`, though. Its `parse` method should return a tuple with the root `Element` and the vec of warnings. Then `Parser` doesn't need a lifetime. // This says, "The `Element` in my return type should not live longer than I do." fn parse&lt;'a&gt;(&amp;'a self) -&gt; (Option&lt;Element&lt;'a&gt;&gt;, Vec&lt;String&gt;) { // parse here } You can eliminate the lifetimes completely by changing `Element` to hold owned `String`s instead, calling `.into_string()` on the slices. However, this will cause increased memory usage as it requires creating a copy of the contents of the slice. However, it's the only solution if you want the `Element` struct to outlive the `Parser` it came from. 
Nice. Did not know that.
Yeah, XOM has a lot more revenue. They're selling a commodity, the value of the company is how quickly they can pump money through the pipeline (more or less literally). I just looked and Apple has $160 billion cash on hand. They are literally making money faster than they know how to spend it.
Their description: &gt; **Rust** is a system programming language with modern affordances. It features a rich typing system, safe memory model and task-based concurrency. Compared to the Go language, Rust is more friendly to people who would like to write code in a functional style. (page 13, top right)
At first I totally read the title entirely wrong. Evaluate may be a better word for them to use...
&gt; Compared to the Go language (╯°□°）╯︵ ┻━┻
┬─┬ノ(ಠ_ಠノ)
&gt; But we'll never replace LLVM in the typical use case. Never say never. LLVM will never replace GCC, Rust will never replace C++, etc. In the long term it is possible to make an LLVM replacement written in Rust.
I don't know if it will compile anymore I haven't used the code for a few weeks but here you go. https://bitbucket.org/glademiller/blm-scrape-rust Sorry for any warts in the code. It was one of my first projects in Rust so I'm sure it could be greatly improved.
Another suggestion would be to use two types: struct List { link: Option&lt;Box&lt;Node&gt;&gt; } struct Node { value: uint, link: Option&lt;Box&lt;Node&gt;&gt; } This way, you can represent an empty list properly and can remove and replace the first node (which is impossible otherwise).
I don't think its a worthwhile goal anyway - LLVM makes it more viable for the world to diversify into languages that suit different domains &amp; even personal preferences better. C++ will live on alongside Rust since there are many popular projects still in use that demand improvements; and other domains can demand slightly different choices to Rust, plus there are straightforward user preferences. There is Swift on LLVM now aswell, inspired a little by Rust in some ways but driven by the demands of the Apple ecosystem (e.g. UI frameworks using objC interfaces), and designed for application rather than systems programming. So it makes perfect sense for Rust to continue to leverage this common resource, it gives people confidence that it can't "fall behind" or "fail to gain enough momentum" r.e. low level issues 
As an example, the recursive `insert_before()` can be converted to iterative this way: fn insert_before(&amp;mut self, element: uint, before: uint) -&gt; bool { // the reason `unsafe` is needed is because we need to dereference raw pointers. // Raw pointers are used because it's not possible to use Rust references (&amp; and &amp;mut) to iterate through the links due to issues with lifetimes. // One needs to make sure that the raw pointers are valid before dereferencing it. In this function, that is done by always converting from a `&amp;mut` (which is guaranteed to be not null) to a `*mut`, and not modifying the links while searching, so as to make sure that the raw pointers are not invalidated before it is dereferenced (as in getting dropped/freed before it's use). unsafe { let mut node = self as *mut List; loop { match (*node).link { Some(ref mut next) =&gt; { if next.value == before { // match `before` break; } node = &amp;mut **next as *mut List; }, // no more links. `before` not found. Explicitly return `false` None =&gt; return false, } } (*node).link = Some (box List { value: element, link: (*node).link.take() }); true } } 
The Go comparison is pretty natural, for a wide variety of reasons, even though Go takes a very different stance on things like correctness (tending towards heuristics/surface standards, rather than anything as involved as a borrow checker). For that matter, Go is often a better choice than Rust for production environments. The syntax is very stable and simple, there is a wide variety of third party infrastructure... it's just an order of magnitude more mature than Rust right now. The 1.0 release will be a big step towards closing the gap.
`fn parse&lt;'a&gt;(html: &amp;'a String) -&gt; Parser&lt;'a&gt;` could just be `fn parse&lt;'a&gt;(html: &amp;'a str) -&gt; Parser&lt;'a&gt;` I think.
About to go to bed, but someone gave some (lightning?) talk about a minimal LLVM IR compatible compiler focused on compilation speed at EuroLLVM 2014. I have a suspicion there were interesting questions too, so a video would be worth watching if one exists.
Still fairly early, but the process is actually extremely simple. Right now I'm simply going through all the C macros that postgres uses everywhere and getting everything represented in Rust (Lots and lots and lots of acking). Once I get a bit further, I'll probably write a blog post about the experience.
I'm not going to try to downplay the differences between Rust and Go (you bring up some really good specifics, btw). I think the reason they get compared so often, is because they belong to a new generation/class of languages, that just doesn't have enough members of the group for people *not* to group them. It's sorta like when first person shooters were all called "DOOM clones". Eventually there were enough wildly different games in the category, that the label had to be ditched for a new one. It's not that Go and Rust aren't different, they're just lacking good competitors in their field. I would probably define that field along the following criteria: * Task model with channels. * Compiled for near-C speed (in some cases, matching C) * Significantly reduced mental overhead for memory management. * API-based types. Very different implementations between the two, but mostly allowing the same results. * To some extent, language is designed with package management/Github in mind as a conceptual first class citizen. * Duality between panics and returned errors, with an emphasis on the latter. There are more similarities, and there's a heck of a lot of differences, but to me that defines the not-yet-named "class" of languages that, so far, only contains Rust and Go. Once we get more languages that more or less fit this criteria, people will start to treat them more individually, since there's less "common novelty", for lack of a better phrase coming to mind.
This is awesome. I love PostgreSQL (and I think it has one of the best codebases I've seen, C or no C) and would love to be able to write extensions for it in Rust. BTW, PostgreSQL's palloc contexts are basically memory regions, I bet you could expose an *amazing* safe interface to them :)
Rust uses basically all of LLVM and relies heavily on LLVM-specific semantics. For better or for worse, I think Rust is stuck with LLVM for the long haul.
Why would you need `if let` if you don't need to make new bindings. Simple equality comparison should be sufficient.
I think the larger point is that, other than being called systems languages (something Rob Pike says he now regrets), Rust and Go are *not* competitors. * Go does not and cannot reach C speeds, and certainly C predictability, in the vast majority of situations. Rust can and does (most of the cases where it doesn't currently are library issues, not issues with the language). Go does have some great programmers working on its optimizer but it will always be hamstrung by the hard requirement for fast compilation times. * "Significantly reduced mental overhead for memory management" is a comparative term. I would argue that Go has much reduced overhead for memory management compared to Rust. You would have to be comparing to only a very few languages in common use for this claim to make sense, I think. * I'm not sure what "API-based types" means (is this in reference to traits / interface?) but Go's type system is much less expressive than Rust's, and it doesn't "mostly allow the same results." I'm pretty confident Rust's iterators are more or less inexpressible in Go, for example. * On the other hand, Rust's tasks are just native OS threads wrapped in regular types, while Go's green threading is hugely central to the language (to the point that they do their own system calls to avoid C calling conventions!). Same with channels, they're just a regular language type. You could write Rust's interface in C++, or Java, or Scala (and people have) but you wouldn't be able to replicate what Go does. * Re: point about package management / Github, that's also true of node.js or Ruby--but nobody ever calls them competitors with Rust! * The panics / fail! thing are the closest the two languages come to overlapping of these points. I think there's *far* less overlap between the languages than you're giving them credit for.
You would be throwing away 40 years of research into code generation for C and C++ compiler backends. This is why, regardless of one might think of them, they will be around for many years to come. 
Honestly... I use if let wherever it makes sense (like all other feature gates), but I find that those times are basically never. I would be happy to see it gone from the language, I think it adds shockingly little to a match statement. Kinda feels like a "why bother?" feature. By contrast, I love love love tuple indexing and I'm warming to the slice syntax (even though it's a hack).
GHC exists since 1992, so they had quite a few years to improve their backend.
&gt; Go does not and cannot reach C speed in the vast majority of situations. Rust can and does (most of the cases where it doesn't currently are library issues, not issues with the language). But it can get much closer than most of the languages it's replacing - for example, just about anything interpreted. The exception would be where it competes with C/C++, but source sanity is still probably a big enough selling point to be worth it for most non-graphical projects. &gt; For example "significantly reduced mental overhead for memory management" is a comparative term. I would argue that Go has much reduced overhead for memory management compared to Rust. In both cases, this is vs. conventional systems languages. This is one of a few areas where D probably qualifies as well. &gt; For example, I'm not sure what "API-based types" means (is this in reference to traits / interface?) but Go's type system is much less expressive than Rust's. Yes, traits and interfaces. While Rust is more expressive, I much prefer the way Go interfaces work for things like errors, and I think Go is much more on an even footing with Rust than FP programmers would like to admit, even if there are still profound differences between each language's approach to interfaces/traits. &gt; On the other hand, Rust's tasks are just native OS threads wrapped in regualr types, while Go's green threading is hugely central to the language (to the point that they do their own system calls to avoid C calling conventions!). But in both cases, tasks are exposed as a first-class language primitive, which is the real differentiator. And Rust was originally intended to use greenthreading the same way, and may yet be able to do so performantly... someday. &gt; As for the point about package management / Github, that's also true of node.js--but I basically never hear anyone compare that to Rust! I agree, but none of these attributes are really meant to stand alone. It's more about the combination of attributes - very few languages pass every single one of those "tests", but there are plenty that would pass one or two. &gt; The panics / fail! thing are the closest the two languages come to overlapping of these points. Fair enough. &gt; I think there's far less overlap between the languages than you're giving them credit for. And yet, people keep comparing them, with the persistence of the systemd flame war :) My opinion is definitely subjective, I'm not demanding anyone agree with it. It's definitely closer to a user perspective than a language developer perspective. But I think it *is* somewhat substantiated by the old saying, *where there's smoke, there's usually fire.*
Unless I'm missing something, [you're violating memory safety](https://github.com/thehydroimpulse/postgres-extension.rs/blob/master/src/lib.rs#L467-L480): that function returns a pointer to something in its own stack frame. I got [an example on the playpen](http://play.rust-lang.org/?code=static%20mut%20n%3A%20int%20%3D%200%3B%0A%0Afn%20get_n%28%29%20-%3E%20%26%27static%20int%20{%0A%20%20%20%20%2F%2F%20This%20is%20safe%20because%2C%20even%20if%20we%27re%20called%20from%20two%20threads%20at%20the%20same%0A%20%20%20%20%2F%2F%20time%2C%20they%20will%20*both*%20write%20the%20same%20value.%0A%20%20%20%20unsafe%20{%0A%20%20%20%20%20%20%20%20n%20%3D%2042%3B%0A%20%20%20%20%20%20%20%20%26n%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20println!%28%22n%3A%20{}%22%2C%20*get_n%28%29%29%3B%0A}%0A) working, though there's probably better (and more idiomatic) ways of handling it.
There's no way that such a thing is feasible for just Rust. The only vaguely sensible option to achieve this is to develop a tool to imperfectly translate LLVM from C++ to Rust and convince the current LLVM developers to fix the result and adopt it. And this is not going to be successful until Rust is established as a language and has already replaced C++ for most projects that would have otherwise been written in C++. Anyway, it's probably an higher priority to rewrite the Linux kernel in Rust rather than LLVM, especially if you want to "completely eliminate C and C++ from an OS". 
Thank you. Now it's much more clear for me. You're quite correct that I intended to have only one copy of string in the memory. That seem to be a good idea especially since all the strings in `Element` are part of the `source` string. Though, I'm a bit surprised that (according to your description) compiler is intelligent enough to understand relations `Parser` and `Element`. It looks like lifetime constraint is based on the borrowing of string slices from `Parser.source` to `Element`. &gt; Then a lifetime on `Parser` because you have to promise that it won't stay longer than the `Element` instances it owns. By "it" you mean a parser instance? But isn't it always the case? Won't private owned field always live less than the struct itself? Now, regarding returning a tuple. I had this idea but I dismissed it because it doesn't seem to be a nice API. Say, if a user doesn't care about warnings they still would have to take care of them because parser would return them. I'd like to give an opportunity to access the warnings but not force it on everyone. But maybe I'm just used to write OO code with a lot of state.
Looks bad! Based on [the postgres definition](https://github.com/postgres/postgres/blob/60f8133dc95d8d55ac52186eb9988559816cac49/src/include/fmgr.h#L422-L427), does seem that a `static` is the right way to go.
Currently `Element::new()` does nothing. It only returns a stub instance. impl&lt;'e&gt; Element&lt;'e&gt; { pub fn new() -&gt; Element&lt;'e&gt; { Element { e_type: blank, // enum value value: "", attr: HashMap::new(), children: Vec::new() } } }
I don't think I agree. Rust also has a solid usecase as a language that uses mutability and data-sharing as primary concepts.
&gt; But in both cases, tasks are exposed as a first-class language primitive, which is the real differentiator. And Rust was originally intended to use greenthreading the same way, and may yet be able to do so performantly... someday. Tasks are not language primitives in Rust. They're purely implemented in the libraries. Same for channels. Rust aims to have a good concurrency story by building the power to expose a variety of safe concurrent interfaces into the type system, rather than baking a single model into the language. For example, Rust allows for performant shared memory without the risk of any [data races](https://software.intel.com/en-us/blogs/2013/01/06/benign-data-races-what-could-possibly-go-wrong) (e.g. if you wish to mutate the data, then a type like a `Mutex` is required); this is in addition to having safe message passing. Of course, having it built-in means that Go is good at its concurrency model, but it is presumably dramatically less good at other forms.
 use std::time::Duration; use std::io::timer; fn main() { let interval = Duration::seconds(10); let mut timer = std::io::Timer::new().unwrap(); let oneshot = timer.oneshot(interval); let (tx, rx) = std::comm::channel::&lt;&amp;str&gt;(); let (ctx, crx) = std::comm::channel::&lt;()&gt;(); spawn(proc() { select!{ () = oneshot.recv() =&gt; tx.send("It's a timer"), () = crx.recv() =&gt; tx.send("Cancelled") } }); spawn(proc() { timer::sleep(Duration::seconds(2)); ctx.send(()); }); println!("Received a message : {}", rx.recv()); } 
&gt; It looks like lifetime constraint is based on the borrowing of string slices from Parser.source to Element. Yeah, you're right. but the lifetime of the string is now tied to the `Parser`, so by putting the same lifetime on the `Parser` and the element, you're telling the compiler that the `Parser`, and thus `Parser.source`, will live at least as long as the `Element`. &gt; By "it" you mean a parser instance? But isn't it always the case? Won't private owned field always live less than the struct itself? Methods like this are allowed (and surprisingly common): // This function takes ownership of `self`. fn take_root(self) -&gt; Option&lt;Element&gt; { self.root } This method consumes the `Parser` instance and gives away ownership of the contained `Element`, discarding `warnings` and `source`. So the Parser is survived by the `Element`, but woops, `Element` has a slice to the string that the `Parser` died owning, which was destroyed when its owner was. Its slice now points to invalid memory. The compiler won't actually let this happen. &gt; if a user doesn't care about warnings they still would have to take care of them because parser would return them The user can easily ignore the `warnings` vector; they simply have to acknowledge that it exists: let (root, _) = parser.parse(); // Tuple destructuring kicks ass &gt; But maybe I'm just used to write OO code with a lot of state. I spent my formative years with PHP, Java, and JavaScript so I know how you feel. I think what you have to get used to is that, in Rust, there's no garbage collector to hold your hand. You can't pass references around willy-nilly unless you're willing to prove to the compiler that you're using them safely. It is a bit more to keep track of, but the compiler is wise and understanding; it'll let you know if you forgot something. That's what makes Rust so fast and reliable: the compiler checks damn near everything, so it doesn't have to be done at runtime. Addendum: If you think about it, if you create copies of the sub-strings for each `Element` to keep after you throw away the parser, your memory usage will be about the same because the original string will be discarded. It'll be temporarily double but on average about the same. Using slices is a microoptimization anyways, unless you're expecting a huge input string or avoiding heap allocations for some reason. But at that point it might be better to implement a streaming parser so you don't have to keep the whole input string in memory. 
"Datensparsamkeit" under "assess"? As in "well let's assess whether we'd like to follow the law"?
&gt; I think Goroutines are a huge differentiator for Go from nearly every other language this side of Erlang (however you feel about them, they're an impressive bit of engineering). I'll just note that Haskell has lightweight threads that are quite similar to Go's, and also [reasonably performant](http://benchmarksgame.alioth.debian.org/u32q/benchmark.php?test=threadring&amp;lang=all&amp;data=u32q).
&gt; if the inner data was dynamic/chosen at runtime [...] these are closures. True. Out of curiosity, does Rust perform closure conversion? &gt; The Servo people have thought of this but discarded it, as the cost of branching on the variant tag is still too high [...] a lot of branches will be mispredicted [...] I am not so sure. First of all, many branching constructs are still only two cases (e.g. `as_text_node`) and in general would the compiler not generate jump tables instead of a giant if-then-else construct? Again I would like to see benchmarks for this sort of discussion. Statements like "The Servo people have thought of this but discarded it" are not helpful as they appeal to authority instead of providing factual value. 
"Datensparsamkeit" is only law in certain jurisdictions. In others "collect it all and sift through later" is a valid model.
Given the space they had, I find that quite okay. The comparison with Go is good, because Go is the only other language in that rough patch (new, rather low-level) that they recommend.
&gt; True. Out of curiosity, does Rust perform closure conversion? I'm not really sure what you're trying to get at with this. If you're asking: does Rust have first class closure objects that store the captures? Yes. &gt; I am not so sure. First of all, many branching constructs are still only two cases (e.g. as_text_node) and in general would the compiler not generate jump tables instead of a giant if-then-else construct? It is still a mispredicted branch in either case, even a jump table. [There's actually two predictors at play](http://stackoverflow.com/q/21787457/1256624) (branch predictor and branch target predictor), an `if`s will stress the branch predictor, jump tables will stress the branch target predictor. Branches also get in the way of out-of-order execution that CPUs do, and make SIMD algorithms harder. You could ask in #servo (link in the sidebar) if they have some benchmark numbers available. &gt; Statements like [...] are not helpful as they appeal to authority instead of providing factual value. So are statements like "According to /r/cpp inheritance is not faster than variants", fwiw. BTW, it would've been nice to back that one up with some context, since presumably there's some post on /r/cpp about it and so linking to it is trivial (I asked on #servo for the assertion I gave you). Either way, the Servo people are not stupid, and they are not using the obvious and 'easiest' (no language changes) route of normal `enum`s. Maybe they just had a mental lapse, but it seems that the background probability that Rust is diving into this hole on a whim is rather small (especially given the frothing attacks it receives from the community occasionally).
Where "certain jurisdictions" includes the biggest economy in the world, aka the EU. Which means that as soon as you're international you'll have to deal with it no matter where you're from. Which means you need to deal with it from the beginning, or pile up enormous data-protection debt that's going to hold you back later. Now, if you're doing web design for plumbers in the US you might not need to care -- legally speaking. Still, even then "assessing" is a strange way to put it: You should already be doing it as any data you don't have is data you can't be sued, or just shitstormed, over when it gets leaked, and plumbers don't generally do Big Data, I think.
Well, LLVM will never replace GCC, because they're different things ;) (now clang, on the other hand...)
GCC isn't just a C/C++ compiler.
I feel like Rust should probably expose a green thread system eventually...
Hahah yeah, that's true! Even my improved comparison is a poor one!
Thanks, this is exactly the type of response I was looking for. I really appreciate you clearing up exactly why it's a bad idea.
&gt; I'm not really sure what you're trying to get at with this. If you're asking: does Rust have first class closure objects that store the captures? Yes. I am trying to figure out what kind of overhead closures have in Rust. With closure conversion, one turns closures into ordinary top-level functions, passing the free variables as additional parameters. Depending on the lifetime of bindings at the call site, the overhead should not be much higher than a direct function call. &gt; it would've been nice to back that one up with some context, since presumably there's some post on /r/cpp about it and so linking to it is trivial My original post contains a link to a specific post in /r/cpp. Edit: It seems reddit performs some auto-linking, but the `/r/cpp` link in the top post leads to a specific post.
It's possible to replace and remove (my code does that (I've added this option)) but it's impossible to represent an empty list, which is kind of dumb... 
I just had a quick look at the n-body problem. Here, the C++ version computes the "potential" using SIMD instructions (and using single-precision square roots). Interestingly, by trivially caching self.bodies[i] and self.bodies[j] into local variables rather than accessing them over and over again in advance() one can already shove off about 15% of the runtime..
How would you remove the first node using `remove_node()`if the first node also represents the list?
I know a lot of Python people who are never leaving 2.x.
This is interesting, and something I've wanted to tackle for a while now. The C macros are a little too intimidating for me though. That being said, there is a huge potential for Rust in Postgres that doesn't exist with any other PL/* language...specifically, it is native compiled, not garbage collected, and safe by default. And I believe that there is also some potential to create a PL/Rust libstd subset that is restricted enough that it could be considered a Trusted Language, allowing it to be used as an extension without superuser access (like PL/Python for example). 
nbody on 4700-HQ, gcc 4.9, rust 0.10 ian@U30Jc:[~]$ rustc --opt-level=3 nbody.rc ian@U30Jc:[~]$ g11 -O3 -march=native nbody.cpp ian@U30Jc:[~]$ time ./a.out 10000000 -0.169075164 -0.169077842 real 0m0.846s user 0m0.845s ian@U30Jc:[~]$ time ./nbody -0.169075164 -0.169077842 real 0m1.128s user 0m1.126s sys 0m0.003s Had to hard-code the args for rust since I'm on an older version. Have their been any major optimizations added to rustc since version 0.10?
Wrong subreddit.
I strongly suspect that improving the performance of the benchmarks game is going to be the result of optimizing the benchmark code itself and the Rust standard libraries, not `rustc`. `rustc` itself is very similar to `clang` in that it relies on LLVM to do most of the optimizations.
Damn, bounds checking hurts. Can this be rewritten to use iterators?
You ~~can~~ *can't* change the inner iterator to for j in planets_count.iter().skip(i+1) If you really want to go all out, you ~~could~~ *also can't* expand the outer iterator let mut planets_iter = self.bodies.iter(); loop { match planets_iter.next() { None =&gt; { break } Some(&amp;i) =&gt; { for j in planets_iter.clone() { INNER } } } } although I think that's a bit ugly. `while let Some(&amp;i) = planets_iter.next()` would mostly fix that: let mut planets_iter = self.bodies.iter(); while let Some(&amp;i) = planets_iter.next() { for j in planets_iter.clone() { INNER } }
We defitinely need to implement rust the game in rust programming language. Just like Hematite.
That won't work in practise though. The code is modifying entries of `self.bodies`. You'd need `.iter_mut()` at which point you get into lots of trouble with the borrow checker if you attempt this approach.
I was testing on the bottom block. :/ You can actually make the first suggestion work with a bit of tactful copying, but it ends up slower.
Cool! I'm currently using it with some CSS tweaks (reduced line height for code and using a not-so-white background color for code blocks)
I'd be interested in seeing your stylesheet! Maybe we can split `sliderust.css` into a core file and a `themes/` directory.
&gt; I am trying to figure out what kind of overhead closures have in Rust. With closure conversion, one turns closures into ordinary top-level functions, passing the free variables as additional parameters. Depending on the lifetime of bindings at the call site, the overhead should not be much higher than a direct function call. AFAICT, what you describe is actually lambda lifting. Closure conversion is putting (references to) the captured variables into an environment object that gets passed around with the function pointer and then is implicitly passed as an argument to the closure when it is called. As I described in detail above, inlining the indirect function pointer call to become a direct one is only possible in a limited set of circumstances, and, in general, once you store a closure in a data structure and put that data structure inside lists etc etc the compiler is going to have a very hard time following it through. &gt; My original post contains a link to a specific post in /r/cpp[2] . &gt; Edit: It seems reddit performs some auto-linking, but the /r/cpp link in the top post leads to a specific post. Oh, my apologies, I was just assuming it is the /r/... autolinking that reddit does. Ok, I looked over their responses: - The first one (/u/4fips) does not apply at all: heap allocation is needed for every part of the DOM, since they are shared and can each live for any length of time, disconnected from elements around it. - The second one (/u/vlovich) is better, although the benchmark is just testing a method call, and not a field access, and, the methods are all returning a global compile time constant (which isn't possible that often), so the statically dispatched one has a massive unrealistic advantage. - The reply to that (/u/twoodfin) is nonsense; with only two variants the compiler can completely remove the branches all together for the variant code, especially because the selection is immediately above the switch, so the compiler is actually forgoing reading the tag entirely. This clearly does not apply to the DOM situation. Basically: you are talking their responses to mean blanket "variants are faster than inheritance" while they're actually only looking at/measuring a few specific facets of it.
Num is a *trait* that is already built into Rust. You just have a name conflict.
I've just started to dive into the guts of Postgres and noticed how palloc uses contexts, which is super cool! My next step is to get some postgres data types implemented to where one could do: pub extern fn foobar(args: FunctionCallInfo) -&gt; Datum&lt;'static&gt; { PgText::new("foobar123").to_datum() } Still gotta think through (and do some testing on) the best way to work the lifetimes into the mix and how all these datums should work on Rust's end. I'd also like to abstract out the need to use `extern` functions. Perhaps a macro to work around it (it would generate the extern function underneath): #[pg_export] pub fn foobar(args: PgArgs) -&gt; PgDatum { // ... } I already know we can have an *amazing* API that is soooo much better than a bunch of the C macros that Postgres uses. 
The C macros used in Postgres aren't too bad, actually. The biggest challenge is going through and understanding how Postgres actually works with all the allocations, data types, function invocation, compatibility checks , etc... Dealing with the macros is just a lot of acking, reading, acking, reading, acking, reading... and some more acking.
Ahh thanks!
It is possible to do it without `unsafe`, for example `append` (other functions can probably be converted in similar way): fn append_iter(&amp;mut self, element: uint) { let mut current = self; loop { let cu = current; // needed to unconfuse borrowck current = match cu.link { Some(box ref mut next) =&gt; next, ref mut link =&gt; { *link = Some(box List::new(element)); break; } } } } 
&gt; And the second question is, my programm doesn't work if I append more then ~4500 nodes(stack overflow). As I understood, it's because the append function is recursive, then how can I make it non-recursive? Try compiling with `-O`. For example, compiler can see that `append` is tail-recursive and generates following loop: 1d0:┌─→mov %rax,%rbx │ mov 0x8(%rbx),%rax │ test %rax,%rax └──jne 1d0 You can usually rely on compiler when recursion involves only one function, and it is tail-recursive. But watch out for destructors – they can make function seem tail-recursive when it's really not. And check generated assembly to be sure (which sometimes is painful, I wish there was something like `#[assert_tail_recursion_elimination]`). There are also plans (probably after 1.0) to explicitely denote tail-rec with `become` keyword. You can also rewrite function in iterative way, as /u/rust-slacker suggested, but it's possible to do it also without unsafe (see my other comment). 
areweXyet style sites for Rust and Servo would be pretty awesome.
I remember there being a fuss about this a while back when the main curator of the shootout cut out most alternative implementations. Away went LuaJIT and PyPy, along with tendency towards fantastic benchmarks.
It would be great to have a combinations iterator in the standard library. Then you could do something like the following for (((p1,p2), distance_vec), magnitude) in particles.mut_combinations2() .zip(distances.iter()) .zip(magnitudes.iter()) { p1.velocity_vec -= distance_vec * p2.mass * magnitude; p2.velocity_vec += distance_vec * p1.mass * magnitude; } See also https://docs.python.org/2/library/itertools.html 
&gt; I actually used the html5ever to do some screen scraping as one of my first learning project with Rust. &lt;3 The fact that you're using the code I spent 8 months writing to win auctions for horses is just amazing &lt;3
And yet incorporating LLVM was a top priority for the ~year it took to do so.
If you really want supernatural performance from the backend, you can't stop at C and C++. you gotta take on FORTRAN. Rust has the strong pointer aliasing guarantees we need. It's just up to someone putting in the work in rustc (and maybe LLVM as well).
libgreen was built-in and it's now becoming an out-of-tree library. Servo needs it and we're going to maintain it. If you would like to contribute that is awesome :)
I fully agree. My point was about the general public, because when many developers compare new language X with C and C++ capabilities, they forget their performance capabilities are helped by the amount of money invested into improving their backends. You are also right about Fortran, which besides having semantics that allows for optimization algorithms not possible in C and C++, its compilers enjoyed even more money. This is one big reason why it is hard for newcomers to replace them. So the approach of picking an existing backend makes more sense than building one from scratch.
Not just backends but specific libraries as well, DNA is more or less a test of re2 vs the regex crate
While convenient, there's no way that can be an `std::iter::Iterator`, since it would lead to aliasing `&amp;mut`s for any slice that has more than 2 elements.
`T: Send` is not sufficient to make aliasing `&amp;mut T` references safe (e.g. `Vec&lt;uint&gt;` is `Send`, but aliasing `&amp;mut Vec&lt;uint&gt;`s allow for creating dangling pointers etc.). It would be possible to have a `&amp;` combinations iterator, it's just the `mut` that's problematic. (To be clear, imagine `v = [a, b, c]`, then `v.mut_combinations2().collect()` could be used to create a vector with contents [(&amp;mut a, &amp;mut b), (&amp;mut a, &amp;mut c), (&amp;mut b, &amp;mut c)] and not only does there *exist* aliasing `&amp;mut`s; every `&amp;mut` there is aliased!)
Shouldn't that be in favor for rust? The regex craze can precompile its expression. Wonder why it is more slow atm. 
It's only in favor if the produced function is better (or ends up better optimized), and therefore apart from "obvious" regular expressions will tend to favor a more mature implementation producing a better state-machine.
That's just a problem with the Iterator API. You can still write an Iterator-alike that works that way, it just won't work in for loops.
Yes, that's exactly why I qualified `Iterator` so specifically above. :)
I do like where Rust is headed. I can't wait for 1.0.
Rust is being used in Skylight and by OpenDNS. I'm afraid I don't have any repository links for you. Rust is reaching a point where the syntax and some of the standard library will settle down and you can start to build stuff on it... although you *can* build stuff on it already, if you don't mind keeping up with the breakage. ... which reminds me... I forgot to test against the latest nightly before I posted this. Which is what I did last time. Err... I'll be... right... back... *runs*
That's not possible with dSFMT as it generates random 64bit floats in IEEE 754 format. I'm planning to port SFMT, which instead generates 32/64bit unsigned integers, so I should be able to implement the traits.
&gt; the first thing I look at about a language is project structure and testing/build facilities. You want to read this: http://doc.crates.io/guide.html It's both.
That is a good point, I forgot that. I still think you can implement `SeedableRng` and `Rand`.
The native code generation gives a boost *relative to the underlying algorithm*, mostly because it removes most allocation and puts it on the stack. RE2 is a mature library with lots of very clever optimizations, like a parallel DFA. Rust's `regex` crate always uses a full NFA simulation. (A DFA will avoid repeating work.)
You don't, that's what the enum tags are for. But this enum can be expressed by the "anonymous" type (bool, int). (the logic here is that tagged enums represents in some sense a sum of types and tuples represent multiplication, and A + A = 2 * A) Or actually. One thing that Rust conflates is having a tagged enum (Bar int | Baz int) and needing to bind it to a name (Foo). Languages with [polymorphic variants](https://realworldocaml.org/v1/en/html/variants.html) like OCaml can have the former without the latter. If you added polymorphic variants you would have "anonymous", tagged enums, but it would cause some redundancy, unless you somehow joined it with current enums (that is, made enums to be an alias to an 'anonymous' variant), which would interact badly with type inference...
Yeah, RE2 is impressive. For those, who didn't used it - here's the link to several articles by Russ Cox (the author of RE2) describing the general approach and some special techniques used in the implementation - http://swtch.com/~rsc/regexp/
It would have been interesting to see Ada and ATS in that language table. Other than that very nice presentation.
In an ideal world, each function and method would have example code in their doc-comment.
My contract on docs includes API docs, but it's the last thing to do. My focus has been on the set of guides so far, though I've done _some_ API docs work. As each part of the API becomes stable, I've been trying to add some docs from there. I think the biggest weak spot currently is how traits are dealt with, but I'm unsure of the answer to doing it better.
That's an explicit goal of mine.
This is the first part of my latest experiment. I decided to make a wrapper library for [WiringPi](http://wiringpi.com/) and managed to find a relatively simple way to cross compile Rust code to the Raspberry Pi using Cargo. It's only the basic functionality so far, but it's enough to control the GPIO pins and have some fun with the electronics. 
I agree that the current state is not optimal. But I'm not sure how much that would help. You can just click on the trait to read the method description. I think it could get messy if you would just pull in the documentation of the trait. I would even go further saying that the documentation could be much more compact in this case. At the moment is is just two screens full of signatures, that does not really help. Maybe one should reduce the non-overridden methods to a simple comma-separated list of it's names? btw: The „class hierarchy“ in Rust is actually pretty low. TCPStream is just a single struct implementing some basic traits. There is no further hierarchy. 
Note after watching the talk: this talk is somewhat related to ruby. At least, many questions are approached from ruby point of view.
&gt; Basically: you are talking their responses to mean blanket "variants are faster than inheritance" while they're actually only looking at/measuring a few specific facets of it. Not quite. I concluded that "inheritance is not faster than variants". But anyway, thank your for your comments.
The calculator example uses the helper which I wish was in core: trait ZipOption&lt;L&gt; { fn zip&lt;R&gt;(self, other: Option&lt;R&gt;) -&gt; Option&lt;(L, R)&gt;; } impl&lt;L&gt; ZipOption&lt;L&gt; for Option&lt;L&gt; { fn zip&lt;R&gt;(self, other: Option&lt;R&gt;) -&gt; Option&lt;(L, R)&gt; { match (self, other) { (None, _) =&gt; None, (Some(_), None) =&gt; None, (Some(l), Some(r)) =&gt; Some((l, r)) } } }
Also, was there any thoughts about having these options enabled by default in Rust: #![feature(if_let)] #![feature(phase)] 
Thanks for taking the time to explain this. I get most of it, but I suspect I still need to brush up on a few concepts. My goal was to extract files in multiple spawns. Your are correct, Stack Overflow is probably a better place for this type of question.
The conference it was given at is a Ruby conference, so...
Yeah, that's totally OK. Even interesting in some ways. Just thought it may be worth noticing.
Either with trait objects: trait State { ... } struct Game { state: Box&lt;State&gt;, } or with enums and pattern matching: enum State { InitialState(...), ... } struct Game { state: State } impl Game { fn input(&amp;mut self, ...) { match self.state { InitialState(...) =&gt; ..., ... } } } 
The "Ru" in GoGaRuCo stands for Ruby, not Rust. 
Isn't this the same as https://www.reddit.com/r/rust/comments/2i8b69/wycats_at_the_golden_gate_ruby_conference_lets/ ?
I've encountered similar issues: * http://is.gd/KBHPV0 * https://github.com/tomaka/rust-hl-lua/issues/21 I don't think that anything should have changed about that, so it's most likely a bug. However I didn't manage to reproduce with a trivial code. This is the closest issue that I've found: https://github.com/rust-lang/rust/issues/17746 but I'm not sure if it's the same.
You can turn an `Option` into an `Iterator` and use the `zip` method defined on that type: match a.into_iter().zip(b.into_iter()).next() { None =&gt; ..., Some((a, b)) =&gt; ... }
Would this be a good way to start contributing to Rust?
I also thought a bit about how to conditionally cancel guards. To translate Boost's example: fn add_person(&amp;mut self, a_person: &amp;Person) { let mut commit = false; self.persons.push(a_person); cleanup! { if !commit { self.persons.pop(); } } // ... commit = true; } It would be great to instead write fn add_person(&amp;mut self, a_person: &amp;Person) { self.persons.push(a_person); let guard = cleanup! { self.persons.pop(); } // ... guard.cancel(); } This is simpler and can also be more efficient, I believe, because it avoids duplicating Rust's conditional drop flag machinery one level up. Attempting to actually implement this led me down some very strange paths such as impl&lt;F&gt; Guard&lt;F&gt; { fn cancel(self) { let _: F = unsafe { mem::transmute(self) }; } } which a) doesn't compile and b) is completely wrong in a world where drop flags are part of the struct (which, aiui, is how Rust today works). It's not correct simply to `forget` a guard because that will also leak any resources owned by the closure. Basically I want to move the closure out, drop it, and then `forget` the discarded shell of the `Guard`, but you can't move out of types with destructors — a necessary restriction in general, but how do I work around it when I'm about to `forget` the value anyway?
I haven't used it extensively, but I think the strength of the ScopeExit / `defer` style is that it puts cleanup code for each resource next to its initialization, which helps with readability, maintenance, and avoiding mistakes. With `try/finally` you have all the initialization and then (a screenful or two later) all the cleanup. Since they're both ways of handling task failure, the two idioms are fully compatible and different parts of a program can choose between them according to circumstance.
Thanks a lot for taking the time to look into the build. And thanks for the link to the Github issue :-)
Woh... that's neat!
`#[unsafe_destructor]` should hopefully go away at some point, see: https://github.com/rust-lang/rust/issues/8861 https://github.com/rust-lang/rust/pull/16154
Since a move is a `memcpy`, I can do my own moves and bypass all restrictions: unsafe fn as_buf&lt;'a, T&gt;(x: &amp;'a mut T) -&gt; &amp;'a mut [u8] { mem::transmute(raw::Slice { data: &amp;mut *x as *mut T as *const u8, len: mem::size_of::&lt;T&gt;() }) } impl&lt;F&gt; Guard&lt;F&gt; { fn cancel(mut self) { unsafe { { let Guard(ref mut field) = self; let mut closure: F = mem::uninit(); slice::bytes::copy_memory(as_buf(&amp;mut closure), as_buf(field)); drop(closure); } mem::forget(self); } } } This is pretty scary code!
the `uninit`, `copy_memory` pattern is also `std::ptr::read`, so this should work: fn cancel(mut self) { unsafe { let Guard(ref mut field) = self; ptr::read(field); mem::forget(self); } } edit: but see below i guess `let _f = ptr::read(field); mem::forget(self);` would work?
Yes, in the github repo.
i have 2 computers, one of them laptop have winxp, it is small win7 is heavy for him. I will buy win7 form my desktop pc, but still rust will not work in laptop, that is it. all others languages that i tested it works on WinXP.
That is a good writeup, but the link needs to be [www.aosabook.org/en/llvm**.html**](http://www.aosabook.org/en/llvm.html)
You could `replace` in `uninitialised`: let f = mem::replace(self.0, mem::uninitialised()); mem::forget(self); drop(f); You have to be careful to drop `v` after forgetting `self` or else a fail inside `f`s destructor can lead to undefined behaviour.
Thanks :) Trait objects didn't work right away, I got another tip and now it seems to work, I had to write it as Box&lt;State + 'static&gt;, not sure why though. Do you have a good explanation or a link I can read up at?
`State` as a trait means the compiler doesn't know *how* that trait is being implemented. As such, it cannot know whether the implementation contains any pointers or not. The `+ 'static` says "there is nothing in this `State` which outlives the lifetime `static`"; effectively, it cannot have any non-static reference in it. You can also say things like `+ 'a` which means the same thing, but for a lifetime parameter `a`.
This line of code, handles this situation if self.value == element { *self = *self.link.take().unwrap(); }
It feels weird taking a constant value like `0` by reference. Not that it's wrong, it just feels weird. Also to have a collection of primitives all work by reference and not by value also feels weird.
Currently, this syntax is used for trait objects right?
So are Rust traits, and the C++ concepts proposal has changed since it was first proposed.
It's just something to get used to (I find being able to do this rather convenient). As for collections, it's either this or having a separate set of collections for primitives (along with a separate set of iterators, option, documentations ... etc). It's the same with C++ collection classes no? Though I suppose it gets hidden due to the C++ reference syntax.
This is what I find 'ugly'. Probably because I'm used to high level languages
(To be clear: I like HKT, and definitely won't hate for it to be in the language.) All the functions you mentioned there are provided via [iterators](http://doc.rust-lang.org/nightly/std/iter/trait.Iterator.html). Find the appropriate sequence of elements and then use the iterator adaptors. These are lazy and automatically have stream fusion (that is, there's no intermediate vectors/allocated collection types). I don't think HKT helps with this particularly. I don't see how HKT allows us to better define the functions/functionality to go from a type like `Vec&lt;T&gt;` to an iterator over its elements. In any case, the iterator parts does not feel particularly random, it is mostly just a matter of realising it exists (a lot of people don't): container types provide (if possible) `iter`, `iter_mut` and `into_iter` for `&amp;`, `&amp;mut` and by-value iterators. All of these iterators provide the adaptor methods described in the link above, and, e.g. are [`DoubleEndedIterator`s](http://doc.rust-lang.org/nightly/std/iter/trait.DoubleEndedIterator.html) if possible. The names I mention above are the names used with all container types, so know the method name convention for one collection, and you know methods for all collections. (Similarly, know the API for a single iterator, and you know it for all, due to every iterator getting the methods via the trait.) For strings, there is at least 3 different ways to consume elements: as `bytes` (UTF-8 encoded), `chars` (code points) or `graphemes` ("visible" characters), and it doesn't necessarily make sense to default to any particular one. (In the best case, you're doing no iteration of strings at all, since processing arbitrary unicode while preserving the meaning is *hard*.) Maybe you're actually trying to question the lack of structure-preserving `fmap` and `&gt;&gt;=` etc? Well, yes, Rust does not currently have this in the general case, but it can be rather more expensive than iterators and various Rust-specific concerns (e.g. ownership, lack of purity, lack of a uniform representation for generic types) mean they won't necessarily be as nice as in Haskell. &gt; PS: what's up with the std::vec::retain function, how is it not a filter? I spent like half an hour looking for it. Note that it modifies the `Vec` it is called on (unlike a conventional `filter` in functional languages).
I prefer this myself: let i = match slice.head() { Some(&amp;x) =&gt; x, None =&gt; 0 };
This has almost nothing to do with HKT. This is merely the collection library being a mess. These are mainly artifacts from major conceptional changes during the evolution of Rust. HKT might have helped preventing this but probably not. HKT will most likely not come for 1.0. But the stdlib is supposed to stabilize with HKT in mind such that they can be added later without loosing backwards compatibility (as far I understood it).
Rustdoc does support running on [arbitrary markdown files](http://doc.rust-lang.org/nightly/rustdoc.html#standalone-markdown-files), it is actually rendering all the published docs on rust-lang.org, both API and [standalone](https://github.com/rust-lang/rust/tree/master/src/doc). However, I don't think `cargo doc` currently supports it (I could imagine it reading the `./doc` folder or some such). That said, the [Rust's index page is hand written](https://github.com/rust-lang/rust/blob/master/src/doc/index.md), but [#16103](https://github.com/rust-lang/rust/issues/16103) covers supporting creating an index page.
Oh wow, I knew there had to be something going on! Thanks!
I guess with `if let`, that could be written as: let i = if let Some(&amp;head) = slice.head() { head } else { 0 }; 
I think utf8 bytes for string encoding makes lots of sense as a default seeing how popular it is, are there many cases where it doesn't?
It's a fantastic way. It's actually how I got my start :)
utf8 never makes sense when trying to *manipulate text*, textual operations are defined either on codepoints or on graphemes never on code units. Manipulating text bytewise is a sure-fire way to break it. However UTF8 makes a lot of sense when trying to *exchange* or *store* text unless said text is mostly markup-less and covers very specific codepoint ranges (then dedicated encodings may make more sense)
Because if we have to wait for every wanted feature to be finished, Rust will never reach the "stable" status.
In what way has it changed?
It's just `enable_if` on steroids with support for concept-based overloading. The "modular type checking" feature of the original proposal is gone. It's possible to use the wrong constraints and have template instantiation errors explode in your face. So, the checking is only done at the call site, not the template code ... The template code can still use all kinds of operations that may or may not be available without the compiler complaining about not having seen the proper constraints for the template parameters. And in that respect, I agree, Rust traits are something more than "concepts lite". Whether this is a bad thing for C++, I don't know. We'll see ...
Like any other good abstraction, HKTs should shield your higher-level code from implementation changes, in the same way an Iterator interface protects you from a change in how iteration is accomplished.
That's kind of a half answer though, isn't it? Why can't we wait for this specific feature?
Well we could wait for this feature but the point is, that we don't have to wait for this feature. This is because HKT could be introduced at a later point without breaking much/anything. Therefore, also because it's a big feature, it is better to wait and get it done properly than to rush. You can have a peek at [the road to 1.0](http://blog.rust-lang.org/2014/09/15/Rust-1.0.html). In the linked RFCs is usually a description why we have to wait for these features and why they can't be implemented later.
`slice.iter().next().unwrap_or(&amp;0);`
After watching the video, yeah, this sugar is interesting. I'm wondering if something could be done to make generics (with traits) less verbose in Rust.
Thank you for taking the effort to list the version of Rust used in your posts! If only everyone could be so conscientious to future language-learners seeking up-to-date documentation. :)
&gt; find type-class instances **at runtime** why not at *type-checking* phase?
You're right, it doesn't need to be at runtime. It would still be quite a feature to implement, assuming you aren't expecting the caller to explicitly state which type-class instance to use.
&gt; HKT is only useful for generic programming please excuse my maybe stupid question, I'm not a Rust programmer (only thinking about getting my hands on in some time, after things settle down) and am only somewhat familiar with Haskell. so, still without HKT, can I create a *Functor* type-class, and be able to use *map* on all of the instances. Or even a *Monad* type-class? EDIT: oh, now I see. you don't mean *generic programming* as in ***data-type*** *generic*, but as in *generics* as in Java :D thank you guys the explanation anyway!
&gt; without breaking much/anything HKT could have big ramifications on how we write APIs in the future, and if we are not careful the standard libraries might not fit nicely once common abstractions are formalized, or might be better expressed in other ways. That said, I am confident that aturon has the foresight to prepare us for their eventual introduction.
&gt; without HKT, can I create a Functor type-class, and be able to use map on all of the instances. Or even a Monad type-class? No, you cannot. `Functor` is defined as something like: class Functor f where map : (a -&gt; b) -&gt; f a -&gt; f b Where `f` is of kind `Type -&gt; Type`. You need some way of instancing type classes on type constructors in order to properly realize these abstractions. 
I know that Haskell can do it. afaik, it's passign around tables with the right functions to apply. [or there *jhc* that doesn't do that at all.](http://repetae.net/computer/jhc/jhc.shtml) edit: rephrased it a bit
In Rust it might be like: trait Functor: &lt;_&gt; -&gt; _ { fn map&lt;T, U&gt;(x: Self&lt;T&gt;, f: |T| -&gt; U) -&gt; Self&lt;U&gt;; } impl Functor for Vec { fn map&lt;T, U&gt;(xs: Vec&lt;T&gt;, f: |T| -&gt; U) -&gt; Vec&lt;U&gt;; } It remains to be seen how useful functor would be in regards to Rust's collections though, due to performance reasons. I could however see HKT being useful for abstracting over smart pointers: fn foo&lt;P: SmartPtr = Box&gt;() -&gt; P&lt;Any&gt;;
Then one day you find an error in the docs, but wait, the docs are right, *it's the implementation that's wrong*. Next thing you know you're submitting patches to the compiler at 2am in your underwear on a saturday while you're writing an RFC to improve the ergonomics of a usecase and reviewing *some else's* PR to improve the docs. *Rust'd*
How does jhc handle the case where there are an unbounded number of potential instances at runtime? Currently, Rust will just give a monomorphization recursion limit error, but I assume jhc does something else.
I'm only wildly guessing, but since it does *whole-program* compilation, it should know of all instances, right? 
I am quite surprised to see that people are clamoring for HKTs here so much. I actually have the very strong feeling that HKTs would neither fit well into Rust's general style of predictability or code clarity. I would take iterators and *clear* abstractions over monads and functors any day. I played around with potential Rust APIs on paper quite a bit lately and I don't actually believe that HKTs would give us any real benefit. Instead we should be focusing on improving error handling which is where the highest API frustrations currently reside.
Please add yourselves! The data is maintained using GitHub - you add or modify your details using a PR to https://github.com/nick29581/rustaceans.org (a simple JSON struct, no fiddling with HTML etc.). It is all very easy and you won't need another username/password. Let me know if you have any problems/questions.
&gt;The only conclusion I can draw from this, is that you should probably not use Windows if you want performance from CPU-bound applications. Shots fired
From the repo it's not apparent how the timing was done so it's anyone's guess. I'd assume writing pngs to disk.
If it's any consolation, I was equally confused. And my last time writing Lua was weeks ago…
&gt; improving error handling And monads can help with it, can they?
In a program like this, what does "all of the instances" mean? f :: Eq a =&gt; Int -&gt; a -&gt; Bool f 0 x = x == x f n x = f (n-1) [x] main = do n &lt;- getLine; print (f (read n) ())
Makes me wonder if "fat pointers" should use a different sigil from "thin pointers". Then again, with DST, it doesn't help for this use case (since there can be DST structs that uses "fat pointers"). Maybe a trait object should use a different sigil? Like say `@Trait` ? Then `&amp;Trait` and `Trait` could be use to imply generics? Might end up being confusing (even from the Q&amp;A after the talk in this video, I could tell many were confused about this). Though I do think that it might be a good idea to use a different sigil for trait objects regardless.
&gt; …but alas clang does not seem to be included. What a wonderful opportunity for you to [take the program source code and the measurement scripts and publish your own measurements ](http://benchmarksgame.alioth.debian.org/play.html#languagex) with `rustc` and `clang++`. Better, identify specific functionality that better be fast and write test programs. Better still, figure out if [Stabilizer](http://plasma.cs.umass.edu/emery/stabilizer.html) will work for Rust as-is or if changes are needed.
I despise that syntax, It makes me remember php that I use everyday where I have no idea what I'm doing because everything is magic and I have no help or idea what methods I can use on something. Grrrr
If wish your explanation was in the guide. I don't understand the lifetimes guide but I did understand your single paragraph.
perhaps they can upgrade it to give warnings in the function definitions - "warning this method isn't part of the given concepts..". What I do like about the C++ concepts idea is they're optional, and can be bolted on top of ad-hoc overloading; I'm finding (compared to templates) that Rust generics create overhead micromanaging relationships between overlapping traits. I think my ideal system would be halfway between the two - define adhoc overloads, define traits/concepts, traits/concepts can improve error-checking for generic code, but traits/concepts are considered "implemented" if the required functions are found for a type. ("duck type traits") I want to be able to get things working quickly through rapid experimentation, *then* retrofit the emergent structures.
There are traits for generic operations over some types. Do you want to be generic over a collection? Too bad. This distinction even seems quite arbitrary to me. Not for the implementation obviously, but for a "we need this, we don't need that" argument. &gt; I would take iterators and clear abstractions over monads and functors any day. What would those look like?
As far as I can tell the linked thread is completely safe.
Can you bring a practical example of an API that suffers from the lack of HKT in Rust? Iterators seem like they do the trick for collections and early return for things such as working with results. Rust is a systems level language and too much abstraction hurts. It's already quite hard to understand the impact of a line of code.
 let i: u8 = if !slice.is_empty() { slice[0] } else { 0 }; Or let i: u8 = if slice.is_empty() { 0 } else { slice[0] }; This checks the size twice (`slice[0]` includes a bounds check), but presumably a Sufficiently Advanced Compiler can optimize this away.
So, I ran some simple benchmarks where 10^9 f64 random values in [0, 1) are generated and their sum printed. On my laptop, this took 2.9s with the dSFMT implementation, 2.0s with SSE2 enabled, while XorShiftRng took 5.2s, and Isaac64Rng 7.9s. Even in C, [xorshift1024*](http://xorshift.di.unimi.it/xorshift1024star.c), is much slower than dSFMT, but slightly faster than SFMT. This makes me think the u64 -&gt; f64 conversion is the main contributing factor in the difference in performance.
I think the language version should be optional metadata in the source. imagine being able to recompile old code in 20 years.
It *does* occasionally show the NSFW ads, so I think it is wise to avoid the hotlinking at all.
Reddit does display the host next to each link, and most web browsers I know have built-in functionality for seeing the target location of a link before clicking on it. (If you're hanging out on social media while at work, you can't really blame anyone but yourself for getting pinged for visiting 'bad' websites.)
And before you know it, you have a purple (or worse, yellow) flair on /r/rust. They're the type of people you don't want to meet in the street.
All Rust strings are always encoded as UTF-8. It is just meaningful to work with code points or graphemes, not the individual bytes.
Why do so many people seem convinced that Rust can't interface with C? It can, if it couldn't Servo (and rustc itself?) would have been much harder to develop. Also most of the graphics libraries.
My insti's firewall gave a big red "**Warning**, this site is for **academic purposes only**" banner -_- They don't really catch you for trying to open blocked sites, and I have a handful of tunnels I can switch to, so no harm done.
Normally, using generics is indeed the ‘Rusty’ thing to do (compared with Go, at least), but in this particular case, using traits or enums without generics is probably best, as it seems that there’s no case in your language where you need the type of a value to be enforced at compile time of the language itself. I might be mistaken about that point so I’m going to just provide a direct solution to your problem if you’re fine with your current approach: replace `LangVal&lt;LangObj&gt;` with `LangVal&lt;Box&lt;LangObj + 'static&gt;&gt;`. Using traits isn’t quite the same as in Go: you *could* use generics, but in this particular case you want the type of a value in the map to depend on the key, rather than being the same type all the time. Because of this, you need to use what are called ‘trait objects’, which are basically like using interfaces as types in Go, except they always need to be behind a pointer (because their type can’t be statically determined). That would make your example look like this: type LangVal = Box&lt;LangObj + 'static&gt;; trait LangObj {} struct NormalObj { properties: RefCell&lt;HashMap&lt;String, LangVal&gt;&gt; } impl LangObj for NormalObj {} struct Nil; impl LangObj for Nil {} I’ve separated `Nil` into its own type because I see no reason it should be considered separate, but you can keep it as an enum if you wish. I’ve also added `+ 'static` to the trait object, which specifies that the object within the trait object can’t have any non-`&amp;'static` references in it. This is the easiest way of fixing the error you described. The other option is to simply use the existing `LangVal` enum like so: enum LangVal { Nil, Obj(RefCell&lt;HashMap&lt;String, LangVal&gt;&gt;), } Here we’ve removed the trait completely, which simplifies things a lot. The downside with this approach is that it can be less extensible: instead of easily being able to add a new implementor of your trait, you have to add a variant to your enum instead, which can’t be done from other crates, can affect code in a backwards-incompatible way, and affects the entire enum’s size. If you weren’t planning on implementing `LangObj` for much else, then I’d recommend this approach.
I think Rust is fairly competitive to Haskell when it comes to elegance: fn quicksort(input: &amp;[int]) -&gt; Vec&lt;int&gt; { let next = match input.head() { Some(next) =&gt; *next, None =&gt; return vec!() }; let (lesser, greater) = input.tail().partitioned(|&amp;x| x &lt; next); quicksort(lesser.as_slice()) + [next].as_slice() + quicksort(greater.as_slice()) }
That example isn't legal Haskell--f doesn't have the type you've written. It has a dependent type, something like (n: Int -&gt; {F : Int -&gt; Type} -&gt; {a : F n} -&gt; a -&gt; {Eq a} -&gt; Bool). And even if you have full dependent types, you can choose to keep the restriction that all implicit args must be resolved at compile time. I think Agda and Idris work that way. [Edit: oops, never mind. As explained by dpx-infinity below, this comment is all wrong.]
You should list #rust-webdev too! We exist :)
Who cares about 4chan thinks about anything? The website is a cesspool of misogyny and god only knows what else.
I tried to mutably iterate all (unordered) pairs of a Vec&lt;T&gt; and failed.
I've added it, I'm finding out about all kinds of irc channels I never knew about...
The most common complaint I get is that the language is "ugly," which is very subjective. I think they actually mean that they are confused by certain constructs (such as lifetimes). The second most common complaint is that they don't understand why Rust exists because they are either happy with C++ (memory safety issues and all), or they don't really understand how Rust helps to prevent memory safety bugs. I know this isn't really related to the language design, but I mention it because it has been a VERY common complaint.
That example typechecks for me in the FP Complete online IDE (I don't have GHC installed on this machine). What type error do you get when you try it?
&gt; the language is "ugly," which is very subjective I don't think it's confusing, or even very ugly, but I do think it's noisy. Lots of sigils and symbols, angle brackets (especially bad when nested) and curly braces.
I think it's because of the `&lt;&gt;` for generics, `*` for dereferencing, `'` for lifetimes, `()` for function calls, `;` for line separator, `{}` for blocks did I miss any?
Well, you have Makefile instead of makefile...
You missed ```||``` for closures (thank god people aren't going to see the old proposed syntax for unboxed closures, ```&lt;'a&gt;|&amp;: x: T|``` anyone?), ```&amp;``` and ```&amp;mut``` for references, ```let mut``` (and the existence of ```let``` in general), explicit ```self```, too-short names for things (e.g. ```fn``` vs. ```func``` or ```function```), and much, much more. People are very good at complaining about syntax :P
The main selling point of Rust versus C++ is the lifetimes, but that's a fairly advanced feature. You need to have quite a bit of experience of programming before you even appreciate lifetimes. (Disclaimer: I'm *very* new to Rust - so maybe I've missed the point of Rust! Got a lot of C++ experience though.) To a beginner, lifetimes just look like a problem better solved by garbage collection! So, for now at least, I think Rust needs to be sold to the more advanced C++ programmers. I don't see how the less advanced programmers will go for Rust, when (as far as they can see) C++ has everything already (larger community, more software, more tools). .. just a thought
No, it is legal Haskell. This is vanilla System F, there is no dependency of types on values.
I think Rust is much easier to use than C++ for a less experienced programmer. The things that you only learn to do in C++ through a great deal of painful trial and error are enforced by rustc. It is very, very hard to release a broken (in a memory-safety sense) Rust library, or even a non-threadsafe Rust library, without opting into doing so. That's far from the case in C++.
I wish it was more high level by default. By that, I mean nearly Python/Ruby-like **but** with optional, yet first-class, lower-level constructs. That way, you get a nice language to get in and to get things done and all the power of a complete system language with all the bells and whistles.
To say that otherwise, I find Rust as it is a little verbose but fine. I just wished I could use it in some places without explicitely bothering about precise types, memory layout and lifetimes. Because, except if you ar writing a driver or a kernel or of you are writing for a constrained targets, chances are high that you don't need all of that everywhere.
What color should the bikeshed be?
Rust tried to have its cake and eat it too by having garbage collected pointers (they weren't actually garbage collected, but that was the plan). People even thought they would be the default type. But nobody used them and they caused serious problems with practically every other aspect of the language (seriously, go back and look at old Rust design proposals--huge portions of them were devoted to trying to figure out how to make seemingly simple ideas work with a garbage collector). So they were removed. Maybe someday it will have garbage collected pointers again, but I doubt they will ever exist outside of a library or be a first-class supported option. And without garbage collection, you either give up memory safety, or you do have to deal with precise types, memory layout and lifetimes.
I think the biggest problem I have had is with lifetimes. When you're putting an immutable reference into a struct, the amount of explicit `&lt;'a&gt;`s you have to put in is incredible. I think the compiler should know that the immutable reference should stay valid for at least as long as the struct is valid, unless you explicitly pass it a different lifetime. Why doesn't it do this? Edit: Note this is from someone who absolutely adores Rust, and while I may be a newb, I think Rust is my new favorite language (I'll be sad to see you go, C).
That's not what the lifetimes are for. The reference will always be valid for at least as long as the struct is valid. The problem is that you can (and often have to) have separate lifetimes. Not just 'static and 'a on the struct; I have structs with three or more different lifetimes, I have bounds on functions that are different from the struct bounds, and (in the future) I will have lifetimes on closures. And in all probability, so will you, if you don't already :). All of those are different cases, even though they're all variants of "the immutable reference is valid at least as long as the struct is valid." You *need* to be able to talk about those bounds separately, and if lifetimes are optional on the structs then there's no way to actually refer to them. I actually would like to see Rust provide a lint to make lifetimes *more* explicit. The existing elision rules are already rather confusing and have a tendency to take a lifetime for too long. When a lifetime is taken for too long, the effect on your code is that you suddenly can't do things that are obviously safe, get stuck in "lifetime hell", and end up RefCelling your way out of it (and giving up Sync, and the guarantee that your program won't crash, in the process). Very often, the solution is in fact another lifetime on the struct, sometimes along with explicit "outlives" bounds, but people don't realize it because there isn't really good documentation on advanced lifetime usage yet. All of this might not matter that much in the common case, as an application end user, but as an end user you're probably not putting too many references into structs anyway.
Huh. I really need to figure out a good way to figure out lifetimes. I don't understand them at all. Edit: Well, don'tcha know there's a [Rust Reference and Lifetime Guide](http://doc.rust-lang.org/guide-lifetimes.html)
&gt; The most common complaint I get is that the language is "ugly," which is very subjective. subjective indeed; Coming from C++, to me the overall syntax is the thing I like most about it. It's familiar enough that a C++ conditioned brain can read it, but its' much cleaner. let, fn -&gt; , lambdas, proper tuples, expression syntax, immutable default clearing out 'const' noise .. all great stuff. people are just fickle.. so many different preferences in this world. My own personal complaint is that it demands more naming (functions in traits, no overloading/default args etc), that might go away a bit when there's tooling with full context sensitive autocomplete doing on the fly lookup
Hm, did that get updated recently? Last time I looked I didn't remember there being any examples with two or more lifetimes. Either way, cool!
Every time a trace task is completed, a [counter is incremented](https://github.com/ruud-v-a/robigo-luculenta/blob/master/src/task_scheduler.rs). Every time a tonemap task is completed, the counter is divided by the number of seconds elapsed since the previous tonemap, and these results are averaged over 512 measurements.
I feel with the safety and speed, Readability has suffered. Looking at other people code now and again i do find it much harder to read but you could argue its because the language is young.
Yeah, this was mentioned in the video as well (originally proposed a few years ago if I remember correctly, but was previously rejected, though Stroustrup seem to be hopeful that it will be accepted this time around). I like the idea about "making generic programming like *normal* programming" (though I personally feel that what's "normal" tends to be too subjective, and changes over time :p), but I've yet to see any new syntax that wouldn't make things even more confusing for many programmers who simply do not really understanding generic programming (and how generics works in a language like C++ or Rust).
Is reference counting this problematic?
To be devil's advocate ... It may be easier to write *correct* code in Rust than in C++. But if we drop the requirements for correctness, C++ is easier to write and compile! A beginner programmer will use `let` at first, and then change some lines to `let mut` when needed. So far, so good. But then they'll run into problems when they want multiple mutable references to the same thing. They'll just see the borrow checker as something annoying that is difficult to work around. This isn't a criticism of Rust, I love the strictness around mutability and borrowing. But I can imagine a less experienced programmer being frustrated at how difficult it can be to port code from C++ to Rust, if the C++ code uses (abuses?) mutability and borrowing.
String handling is often mentioned as a pain point. `String`, `&amp;str` and `&amp;[u8]` are all around and it's sometimes unclear how to work with them from the start.
That's the infuriating thing, there are two common conventions: lowercase.ext or Captialised. e.g. Makefile, Rakefile, setup.py. pom.xml. Cargo.toml manages to follow neither. Still, it would be pretty amazing if that ends up being the biggest wart in the Rust ecosystem.
&gt;But then they'll run into problems when they want multiple mutable references to the same thing. Isn't this the point when a beginner programmer should discover that naive multiple mutable references are unsafe?
There are a bunch of smaller, more specialized ones like #iron or #teepee too, but I don't know if you want to list those.
As somebody already pointed out: for example, conversions to iterators and back would be a hidden detail with HKTs, simplifying the code. Also, the operations could be structure-preserving where applicable (I don't have an useful example for this from the top of my head). That being said, I must agree with your point that too much abstraction hurts. But for the functional crowd, HKTs are hardly too much abstraction :-)
HKTs are IMO such a broadly applicable concept, that the amount of sugar/usecase-specific-constructs one would have to add to get the benefits is just too much. Better to have a general concept than many ad-hoc additions.
&gt; As somebody already pointed out: for example, conversions to iterators and back would be a hidden detail with HKTs, simplifying the code. But that also hides the associated cost. Going through an iterator at least makes it somewhat obvious what the complexity involved means. &gt; But for the functional crowd, HKTs are hardly too much abstraction :-) But I'm not from the functional crowd, my hammers of choice for which I use Rust now, normally are C and C++. For me HKT sounds nice in theory but it does not *actually* help me solve problems I have. In fact they sound like they could make my life harder :)
Very noisy 'fn'-lines when generic. :-(
Yeah, inline ML type syntax has a tendency to do that. Cant change it now though...
Yet, that thing is consumed by exactly one class of programs: those handling cargo related tasks. It doesn't impact any generators that can only handle one naming scheme, nor anything else. There is no practical argument for or against and a lot of breath is wasted. I'm surprised by the amount of discussion around that. How is that infuriating?
Ideally the advent of Rust 1.0 would enable this without such metadata (assuming that there's a good long while before Rust 2.0), but it is a possibility. I think that Racket does something like this.
As a side note, your LangVal type looks like the Option&lt;T&gt; type, which you might want to reuse to get the set of already implemented functions in the stdlib.
&gt; Isn't this the point when a beginner programmer should discover that naive multiple mutable references are unsafe? It's not necessarily unsafe, it can be in certain cases. And if you use references in a safe way it's pretty annoying if the compiler complains about it.
FWIW, Rust does not aim to be *totally* explicit, e.g. type inference and lifetime elision.
As an application developer (not systems).. this is the 1st thing that bothers me.
I should have said "totally explicit with regards to performance-related issues", like stack allocation versus heap allocation etc., which I guess would be a more accurate statement.
slow compilation time
Yes, that is more accurate, although there also are the features of autoderef and overloaded operators, which mean that `.`, `+` and `[]` etc. can all execute arbitrarily expensive code: a slight* concession of explicitness, in the name of nicer syntax. *Certainly the worst case of this is `.` with the deref traits, but convention is strongly that implementations of those two traits are as cheap as possible, e.g. all the implementations in the standard library are just pointer offsets/reborrowing.
Keeping in mind it is possible to string a bunch of random tools/editors together to get a mildly IDE-like experience on Linux/Mac. It's just a lack-luster experience, and is difficult to maintain and debug. You can also deploy the same toolchain on Windows, with even less attractive results. It feel wholly foreign and out of place on that platform. At this point, I would be hard pressed to convince my coworkers not to use modern C++ (or even D or Go) over Rust, but the language is young, and is interesting enough to keep watching.
It's there! But ofcourse this is only the syntax highlighting. http://plugins.jetbrains.com/plugin/7438
it would be amazing if a fully developed Rust IDE could graphically present to you lifetime issues the same way other compilers highlight code errors. Something like, if something is borrowed twice then it highlights both areas in the code in the same color
&gt;And if you use references in a safe way it's pretty annoying if the compiler complains about it. Can't you use something like mutex to ensure it's always safe?
Wow, you really hear those sorts of complaints from real people? I hear: 'I tried to have a look at rust over the weekend but none of the dependencies for [graphics binding here] would build.' and 'How do you do inheritance? Is that struct Foo : Bar {} or is that something else?' and 'I was looking at servo to see how to [thing] but I couldn't compile it. How do you tell what version of rust to use?' 
Wait, isn't `.` simply the method call syntax?
I'm thinking more in a single threaded context. The Rust borrow/reference checker can be annoyingly strict.
I actually love this feature and use it a lot. Save on naming when I'm working on the same concept through some transformation.
On the topic of garbage collection ... (but changing the topic slightly). Consider the types which have trivial destructors - basically, nobody would care if their destructors weren't called. Further, consider those trivial-destructor types that are permanentely immutable (will never be mutated by anybody, ever). There is an interesting optimization when copying from immutable types. An immutable object (or subobject) can be copied simply by taking a reference. Copy-by-value and copy-by-reference are the same thing in that case. This is *really* useful and fast in languages like Haskell where everything is immutable. In Haskell, you can write copy-by-value, but be confident the system will copy-by-reference if it thinks it will be faster. But one might object that copy-by-reference could wreak havoc with lifetimes, we would need to keep the original object around longer (not possible, in general, with objects on the stack). But if we don't care about when destruction occurs (trivially-destructable) then we simply want the object to be kept around as long as needed. (And no, I'm not about to argue for unrestricted GC for all types and to be generally (ab)used. I despise it! But there might be a special case for immutable types with very boring destructors ....) Specifically, in rust, imagine there is an `'infinite` lifetime for these types. On the face of it, these objects will never be destructed, and therefore we can copy-by-reference with impunity. Because the destructor has no interesting side effects, we can allow the runtime to destruct these ahead of schedule if it so chooses (before the end of the universe, but sometime after the last reference to it exists) if the system is running low on memory. Perhaps this needs a separate heap also, with perhaps a different operator to create things on this heap. I don't really know how to fit this into Rust (I'm *very* new to Rust). Basically, even if *I* am finished with all references to object X, maybe the runtime would like to keep it around longer in order to support the copy-by-value-but-really-by-reference optimization that it did on my behalf.
Correct, but it autoderefs until it finds a match for the requested method or field.
Even without running code `.` with dereference can easily jump somewhere outside of cache, causing (often unavoidable) performance issues, which is much less probable with "normal member access" `.` In a perfect world I'd expect a distinct symbol for "dereference and access" (like `-&gt;` in C), that would also fix the inconsistent behaviour of `.` in generics and non-generic code, but, unfortunately there is no other symbol as lightweight and non-surprising as dot.
`String` is like `Vec&lt;T&gt;`, where it manages its own memory. `&amp;str` is like `&amp;[T]`, where it's a view on memory that somebody else is managing.
I don't think this is what /u/adkahsd wants. He didn't say anything about pairs of only neighboring elements.
Yeah... I really don't like the naming convention they have for that but everything else CMake provides makes up for it.
We still have reference counting, but now it's `Rc&lt;T&gt;` instead of `@T`. But you don't need it that often. Look at it this way: in many cases there is a clear ownership relationship, and you don't need reference counting but can do it with borrowed pointers and some clever lifetimes. And when you have cycles you need to start thinking about weak references or you're going to leak... so reference counting is really nice for those scenarios where borrowed pointers don't do the trick but you're not going to construct cycles (easily avoided unless the types in your references have internal mutability). But for graphs you really want proper garbage collection.
I think one thing causing this is that explicit ownership is still a rather unique concept. And people tend to go for things they know when they first look at a new language. So someone looking for a quick comparison will indeed think `String` and `&amp;str` are overkill and complicated. Once you know how they relate to ownership I believe the design is rather clear. And I do think this can and will be fixed in the docs, just by being more explicit about ownership in introductionary resources that people tend to jump on .`Vec&lt;T&gt;/&amp;[T]` and closures are probably other candidates as well. In fact, looking at [rust-lang.org](http://www.rust-lang.org) which includes a feature overview and an example, there is no mention of "ownership", and the example does use strings, but only slices, and also doesn't demonstrate ownership. So I do certainly think this is fixable :)
 vim Car&lt;TAB&gt; "Cargo.lock" 18L, 575C Every time.
It is very easy to find examples of code that isn't unsafe but is disallowed, e.g.: let mut x = 0u; let y = &amp;x; println!("{}", *y); let z = &amp;mut x; *z = 10; println!("{}", *y); But this isn't safe in general: let convenient_constant = 0u; let mut x = Some(10u); let y = match x { Some(ref num) =&gt; num, _ =&gt; &amp;convenient_constant, }; println!("{}", *y); // prints 10 let z = &amp;mut x; *z = None; println!("{}", *y); // Maybe segfault? Or garbage? The borrow checker also prevents things such as iterator invalidation. It's really useful and does change the way one thinks about code (for the better).
It's not a fundamental issue of language design. It's bugs, and Rust has plenty :P There are some very specific examples in the bug about making the reverse complement benchmark game faster.
The biggest problem here is that you need to keep track of all references to the object if it can *ever* be safely destroyed. How do you do that, if you don't have strictly enforced lifetimes? Well, you can use explicit reference counting, which Rust has, or you can use garbage collection (automated reference counting, mark-and-sweep, and so on). But it's *never* memory-safe to destroy something as long as there's even one reference to it, unless you're confident that that reference will never be used (which implies lifetime analysis). And the garbage collector has to know not just about your type, but any type that references your type. That makes things... complicated. People tried (and are still trying!) to make this work, and the Boehm GC exists in C++, so never say never... but it does seem like garbage collection is a fairly fundamental language-level decision, even more pervasive than something like choice of default allocator or the use of interrupts for control flow. BTW, you actually described garbage collection really well there, as way to present an illusion to the programmer that a computer has infinite memory :) Also, Rust does have a kind for types with "trivial" destructors: they are ```Copy```, and they are pretty painless to work with (you can put them inside a ```Cell``` and mutate the aliased values to your heart's content).
My issue is I want to treat it like Java or C++ strings which just isn't the case. 
It's not something I hear *often*, but I haven't seen it mentioned here: Writing lowlevel/C-like/C-interfacing code in Rust is unnecessarily ugly/verbose/unintuitive/requires too many std:: helper functions. Personally I think C interfacing works pretty well and our abstractions don't work out too badly here, but at the very edge of abstraction, I sympathize with some specific complaints: * pointer arithmetic inside unsafe blocks ought to look a lot more like C code * not having the `-&gt;` operator (since `.` doesn't autoderef raw pointers) or being able to use `[]` indexing on raw pointers is also a bit annoying * raw pointer casts are a bit noisy, especially if you need to cast twice, like, once from a `&amp;` to a raw pointer, and then again to change the type of the raw pointer. Maybe an implicit conversion of `*T` to `*c_void` would help here * another related pain point is that C strings consist of signed chars and Rust strings consist of unsigned chars, so there's also a bunch of casting like `.as_ptr() as *const i8` * thinking about unwinding when being called from C code sucks too. If you aren't absolutely certain you're not going to invoke failure, you probably want to have an RAII type that calls `abort()` if its destructor is run during unwinding in every function that is called from C. * it would be cool to have some abstraction that's more general than making single-use wrapper types and more lightweight than `.finally(|| { ... })` for ensuring disposal of C library resources/handles/tokens/w/e. I'm also confused that `liblibc` omits bindings for some C functions like `memcpy`. I suppose this is because there are equivalent Rust function, but that could be an argument against a lot of `liblibc`...
God I hate angle brackets. They truly are ugly and hard to read.
If you really want to be sure in generic code that you do not dereference a pointer by accident perhaps you could do something along the lines of (now that [this](https://github.com/rust-lang/rfcs/pull/127) rfc has been accepted): unsafe trait NoDerefShenanigans {} unsafe impl NoDerefShenanigans for .. {} impl&lt;T&gt; !NoDerefShenanigans for &amp;T {} impl&lt;T&gt; !NoDerefShenanigans for &amp;mut T {} impl&lt;U, T: Deref&lt;U&gt;&gt; !NoDerefShenanigans for T {} And include the NoDerefShenanigans bound in your super generic performance-critical code. I think it might work?
 You can put export FIGNORE=.lock in your .bashrc to fix that. 
&gt; but traits/concepts are considered "implemented" if the required functions are found for a type. I don't know how you'd deal with the namespacing issues in that case. For example, if you have a type that wants to implement both `Cowboy` and `Shape` (where both have a `.draw()` method), what do you do? Furthermore, this doesn't let you implement traits for types that you didn't define in your crate without the risk of collisions: if crate A defines type T, crate B implements `.foo()` on A, and crate C implements `.foo()` on A, then B and C can't be linked together. Traits solve both of these problems.
FWIW, pretty much all the "obviously safe" cases like the above (single-threaded, plain old data) can be replaced with ```Cell``` with absolutely no overhead: let x = ::std::cell::Cell::new(0u); let y = &amp;x; println!("{}", y.get()); let z = &amp;x; z.set(10); println!("{}", y.get()); And pretty much everything that doesn't fall into that category is not "obviously safe" (sometimes it still is, but you have to think about it, especially if you allow interior pointers).
You don't need an IDE for Java or C#, either. It's just nice to have.
Yeah, I know many people like being able to do this, I'm still not completely convinced it's so difficult to come up with a new unique symbol for a variable in the same scope (I've never really had any issue with that in C as far as I can remember) but I accept it. I guess the main reason that feels weird to me is because it's one of the only scenarios I've had so far where rust is actually more permissive than C.
Oh god, how did I forget about this. Yes. This is a real issue (not syntax bikeshedding) that could make or break Rust at a corporate level.
Another real issue I forgot about. I think this is keeping a ton of people from even considering Rust.
I think it's not so much that the language is young as that you are unfamiliar with the language. Now that I have written a lot of Rust, it is very readable to me, with one slight caveat--I find Github's default syntax highlighting for Rust to be *very* hard to read, to the point where it's distracting. Since many people see Rust for the first time on Github, I think that probably isn't helping the perception.
Well, you can make it always *safe* to access, but Mutexes (or even RwLocks) have their own problems. Deadlocks, livelocks, issues while forking, getting poisoned on task failure... there's a reason why I don't know of even a single language that explicitly puts mutexes around every variable access (the closest is languages like Ruby and Python that have a GIL, aka a mutex around the *entire program*).
Perhaps it is because the Rust version of memcpy uses an LLVM intrinsic. I imagine using the libc memcpy is really shooting yourself in the foot w.r.t. performance because it can't be inlined and LLVM won't be able to optimize it as well?
Yeah your probably right but at the same time i think it takes while for conventions in how people approach development to appear make things easier to understand also.
I don't agree with any of the complaints in either my reply or iopq's post, except that ```&lt;&gt;``` are slightly annoying. I've still heard every single one of those, and a ton more besides. As for the specific arguments: some people think we should have ```let``` / ```var``` like Swift, and some don't see why we can't just have ```=``` like Python or Ruby (yes, pattern matching and scope confusion--but I think those are addressable by ```:=``` like Pascal or Go, though I haven't actually seen anyone suggest that that I can recall). I've also heard at least one person say that functions should be written ```fn foo(&amp;mut)``` rather than ```fn foo(&amp;mut self)```. Do not underestimate people's willingness to defend their syntactic pet peeves, especially in a new language where there's at least the perceived possibility to "fix" things before it is too late. Also note that some people want less explicit syntax, while some want more explicit syntax--the only way to win the syntax game is not to play, and Rust doesn't have that option.
PLEASE someone write this advanced lifetimes guide! The official References and Lifetimes guide has one tiny section on named lifetimes, and half of that is just about using them to break out of loops. I don't feel like I understand them as well as I should and yet I haven't been able to find anything to teach me.
Except you can't really do that. You'll frequently want &amp;str function arguments so the caller can pass a slice of a String or an existing &amp;str without copying.
I've been on it for a while, I just haven't been happy with my drafts. Soon! I think this guide is the most important one, which is why I'm trying so hard to get it right.
And I love them. I think they're really easy to read, and not ugly at all. Yay subjective preference!
Nope. Try a cargo project with enough dependencies and do `cargo build -j 10000`. Still slow. clang++ is faster when you compare compilation time per line of code. You can try it and blog about it so that i can read about your measurement.
Yup. I spent over week trying to get *any* simple graphics library, 2D or 3D, building on my mac. I have not yet succeeded. (Actually not quite true--I had KISS3D building at one point, and then I very foolishly tried to get nphysics 2D working at the same time, everything broke again and I've never got back to a working state.) 
it would certainly risk collisions ... so authors of traits that are used on the same types would just have to come to an agreement - just as they do in C++ when naming methods used in templated code - but then names would be more consistent across the source base. Arguments help could differentiate: a 'draw' method might take some context object (canvas, rendering system pointer whatever). In general you'd just have to name methods slightly more explicitly, and you wouldn't have to use functions' trait name to disambiguate. (cowboy.draw_weapon(), cowboy.render(pd3dDevice)) mutability might further clarify ('draw(&amp;mut self)' something that changes state.. draw(&amp;self) side effects in the rendering system. It would simplify the language in other ways &amp; ease interoperability with C++ (translation of APIs back &amp; forth) ... this would help adoption IMO, you could gradually introduce Rust into existing C++ projects just as apple allow mixing swift &amp; objC for a smooth transition. the situation I keep running into myself is with vector/matrix maths, with various types of vector/normal/point/matrix/quaternion/other representations of a transformation specialised for certain tasks; I know clearly what I want to happen on a function by function basis, but dividing this into a set of overlapping traits is more complicated .. requires more naming &amp; boilerplate, and if you get it wrong its more to go back &amp; change to improve it (... dealing with the unknown is a process of trial and error)
Have you thought about having the description of `String` as the memory owner before talking about `&amp;str` as a view including lifetimes? It seems it would be easier for people not yet fully familiar with lifetimes and ownership, and might serve to subtly reinforce those concepts in the readers mind.
I am open to all suggestions. The reason that I started with `&amp;str` is that it's simpler in a certain sense, and also the type of string literals.
Are your drafts viewable somewhere? Or maybe a list of pain-points you found while documenting? Less for reading and more for helping out and such :)
Yeah, it says "// Omitted: memcpy, memmove, memset (provided by LLVM)". imo they should at least be re-exported in liblibc with a compatible signature :( Also explicitly omitted are `printf` and `scanf` variants (maybe that decision predates support for calling variadic C functions?), `atexit`, `bsearch` and `qsort` (not sure why these would not be provided--maybe we didn't have C function pointers back in the day?), and some other stuff. Also, the `stdin`/`stdout`/`stderr` `FILE*`s aren't provided (`fprintf(stderr, ...)` is sometimes easier/more appropriate than managing a `std::io::stderr()` `BufferedWriter`, I think?), and neither is a facility to either read or set `errno` (`std::os::errno()` lets you read `errno` on unix, but not write it, and you have to cast it to compare it to any of the `EWHATEVER` constants). I expect a lot of this is going to be revisited as more platform functionality is made available.
`cargo build` also downloads the dependencies, so its not really fair to include download-from-internet + build vs. build. near the bottom of [this blog post](http://ruudvanasseldonk.com/2014/10/20/writing-a-path-tracer-in-rust-part-7-conclusion) the author has a table of compile times. if you want a more true test, compare `rustc`to `clang++`, not a build/dependency-management tool vs a compiler.
The `Iterator` trait itself suffers from it. Well, not HKT but HKL (higher-kinded lifetimes), but the concept is similar. It is hard or impossible to define a generic streaming iterator. Here's [an example](https://github.com/emk/rust-streaming/blob/master/src/csv.rs). Note the return type of `next_item`: it is fixed to `&amp;'a A`. Ideally, that type would be *generic* over `'a`. To do that, you would need to be capable of expressing `T&lt;&amp;'a&gt;` where `T` is a type parameterized over a lifetime. Note that eddyb claims that HKL is not needed for this. (I don't understand his alternate solution though.) &gt; But that also hides the associated cost. Going through an iterator at least makes it somewhat obvious what the complexity involved means. I don't think HKT's necessarily imply a hidden cost. But this probably relies on the optimizer. &gt; But I'm not from the functional crowd, my hammers of choice for which I use Rust now, normally are C and C++. For me HKT sounds nice in theory but it does not actually help me solve problems I have. In fact they sound like they could make my life harder :) How is this any different from "I haven't had enough practice with HKT, so I don't think they should be in Rust."? FWIW, I don't necessarily disagree with you. But we should try to be more principled then "they give us too much abstraction."
How do you find `&amp;str` simpler? I see that they look simpler when dealing with literals and such, but as soon as you escape the function scope you have to know about ownership and lifetimes. My personal discovery path went from `String` to `&amp;String/&amp;mut String` and then quickly discarding the `&amp;String` for `&amp;str`, seeing how the latter is more powerful. By the way, the guide itself is great and I'm surprised you managed to keep it so compact. And I love that it has Unicode examples :) Also, is the "Other Documentation" meant as references to API documentation relating to the guide, or as a jump off point to generally related documentation? If the latter, I'd propose adding `MaybeOwned`, `Str` and `FromStr`. I've seen people being pointed to at least the first and third regularly.
Erlang is interpreted, Rust is not. It's very hard to do hot reload efficiently in a compiled language; you'd either need a JIT, or some kind of late binding.
True, however seeing that it is achievable with C++, there should be a way for Rust.
There's a closed PR somewhere with one of them.
Basically, I see a pointer to an array of u8 as simpler than all of the stuff that goes along with `String`. &gt; My personal discovery path went from String to &amp;String/&amp;mut String and then quickly discarding the &amp;String for &amp;str, seeing how the latter is more powerful. You had a `String` before you ever typed a string literal? "Hello world" has `&amp;str` in it. Maybe it's that they are simpler, but not easier. ;) &gt; And I love that it has Unicode examples :) Heh, thanks. :) I try to do that on purpose. The new crates guide uses Japanese characters for this reason. &gt; Also, is the "Other Documentation" meant as references to API documentation relating to the guide, or as a jump off point to generally related documentation? Just generally related. I'd support adding links to those, feel free to send a pull request. :)
&gt; Basically, I see a pointer to an array of u8 as simpler than all of the stuff that goes along with String. Ok, I can see that, I think. I guess I went for it from a more high-level view. &gt; You had a String before you ever typed a string literal? "Hello world" has &amp;str in it. It was still during the `&amp;"foo"` times so that whole `&amp;` stuff was "magic that I would just cargo-cult for now" like lifetimes :) &gt; Just generally related. I'd support adding links to those, feel free to send a pull request. :) Excellent, I'll put it on my list.
could you do some limited code reloading with trait object vtables perhaps
Fair enough! Yeah, I don't think there's one right answer here, that was just my own logic. And I can totally see just cargo culting literals.
I'm just happy that my file system is case insensitive and I can name it cargo.toml :)
I would assume at compile time based on type information :)
It is odd how that little change makes it much more readable, but then you get the issue of replacing the comparison operators. Personally I think the way D handles it is quite nice, reusing the notation for grouping in a new context.
I personally prefer multiple repositories, and don't find this kind of setup 'more modular', but Cargo supports splitting them up. Put a `Cargo.toml` in each one of them, and it should work just fine. See Servo as an example.
Cargo used to have `[[lib]]` supported in Cargo.toml, but that was removed. There is a little bit of discussion here: https://github.com/rust-lang/cargo/issues/327. From the looks of it, you are going to need to split them up. That is the way Rust in general has been going, many crates have been moved out of the regular distribution and now live at github.com/rust-lang/&lt;some crate&gt;. Maybe a Cargo dev can be more specific, but that is the info I have been able to find. Of course, there are always git submodules, too, if you wanted to go that route. You could have a top-level makefile, which just uses `cargo` to compile each submodule
I can see a future key in `Cargo.toml` with this information. `Gemfile`s already have it. It's just not very useful pre-1.0.
Very true, but it [still satisfied him](http://www.reddit.com/r/rust/comments/2jv6e5/what_is_the_first_complaint_you_hear_about_rust/clfqnr2) :) 
Ah, true, my mistake. However, it's hard for the same reason (mutably borrowing multiple elements of a slice), and presumably the way it was solved by chunks could be applied generally to getting all unordered pairs. I haven't looked at the source, though, so maybe not. Edit: chunks_mut just uses `split_at_mut`. This would work for all unordered pairs, as well, though not quite as straight forward.
It's not the file system that is case insensitive (both FAT32 and NTFS preserve case, and as such are case sensitive), but rather the VFS abstraction (whatever it's actually called in Windows). Oh and I bet that if you tried hard enough, you could do case sensitive file operations under Windows.
I guess the convention will be that concepts begin with a capital letter. Already, in C++, most things in the standard library start with lower case letters and you can see in the documentation that capital letters (e.g. `InputIterator`) are used for the ["concepts"](http://en.cppreference.com/w/cpp/concept) that have been in the language for decades already. But really, when does the reader care if it's a type or a trait? If it's critical the reader can see, then where do we draw the line? What other type distinctions should be immediately obvious from reading? I think I like the ambiguity - it might even allow us to replace a type with a trait of the same name if we realise we should have created more types in the first place. e.g. `Circle` could become a trait with the following types: `Circle2D`, `Circle3D`, ... (well `HyperSphere` might be better, but you know what I mean).
Thanks for the response, it sounds like I'll probably be splitting things up into separate repositories to achieve the modularity that I want with the project. Probably worth what I perceive as the extra work. I was really just trying to go for what I saw as the path of least resistance.
Yeah, I thought about splitting multiple repositories and then using sub modules to make for a unified build that I can use with CI and such.
At least with the tooling, and `hub`, it's pretty quick. `cargo new foo` a few times and `hub create` and you're good to go. It's also possible that I'm just numb to this kind of extra work, I have 313 github repositories already...
Yeah. It's a typography issue. The less-than and greater-than sign are designed to point at the objects next to them, and to separate the objects on either side of them, while the left and right pointing angle brackets are designed to *bracket* the text they surround (and have less relationship with the text outside them. The downside is that they are visually much closer to parentheses. That, of course, depends on the font. On my screen, the angle brackets in the quoted monospace text look very much like parentheses, while the angle brackets in the normal text are quite distinct. YMMV.
SRL?
Why is it not explicit, then? I thought it explicitly calls whatever method name is after the `.`
My complaint is the confusing `&amp;mut`, string types, and lifetime annotations. Not that `&amp;mut` itself is confusing but more that if you use mutability you need to use the correct annotations which is confusing for me.
Excuse my ignorance, but what's `hub`? Also the more I think about it, the more having separate repos will probably work out in my favor.
Doesn't it improve with `where` clauses?
That's generally achievable by compiling in the background (or at least, running the front-end part of the compiler) and reporting the errors raised to the user, using the location information to place them correctly. Of course, at the moment that would mean embedding the Rust libraries or calling into `rustc`...
Much faster than C++ maybe, but try selling that to someone writing Java where the code is compiled in the background of her IDE? To anyone that has not suffered C++'s long compile-time, Rust is the worst they have known.
Probably this https://hub.github.com/
I knew about the tool, but not the website! Cool place to point people to :)
Cargo dev is more specific, you can have multiple crates in a single repo. (See how [servo](http://github.com/servo/servo/) does it, `src/` is the "main" crate and `components/` has other in-tree crates)
Cargo supports having multiple packages in the same repository. That's how a lot of compiler plugins work, [`docopt.rs`](https://github.com/docopt/docopt.rs), for example.
ManFlardin? 
Highlighting itself is done by Pygments.
It's worth noting that this doesn't mean you *can't*, it just means you give up some type safety.
Yeah, I've seen that syntax proposed, and I like it. I've seen some complaints along the lines that it's the opposite of what some other language (haskell maybe) does for function calls or something like that, where x y z means x(y, z) (or x(y)(z), which IINM means the same thing in haskell), but I don't see a need for parallelism with a completely unrelated construct. I also don't see any benefit of `RefCell DefIdMap Rc` meaning `RefCell&lt;DefIdMap, Rc&gt;` So I'm +1, but I don't know how much my vote means, plus it's probably a post-1.0 issue. 
OS X
It will *never* be considered safe to run code from arbitrary paths on the filesystem. The API can be made *safer* by doing verification of types using the metadata, but it still requires trusting the file.
I got [this](https://www.reddit.com/r/programming/comments/2jsrif/optifine_dev_minecraft_18_has_so_many_performance/clfl2ir)
I haven't exactly heard *complaints* about how modules work, but correctly splitting a project into multiple files is definitely one of the first stumbling blocks for new users, judging by IRC questions. I'd bet this is true for a lot of languages, though.
I know that it's a tricky problem, but this would be absolutely awesome to have.
But it can also implicitly call one or more `Deref::deref` methods.
Ever tried programming Java without an IDE? I wouldn't want to do it anymore. There's just so many useful features in e.g. Eclipse that boost productivity incredibly.
Wow. You hammered that nail pretty damn good; I wasn't aware of this until now. 
The difference presumably becomes even larger if you use the dSFMT function that generates the numbers in one call (`fill_array`).
&gt; How is this any different from "I haven't had enough practice with HKT, so I don't think they should be in Rust."? I don't know. It's not that I don't "get" HKTs, but I do not feel very comfortable with the concept. One of the reasons I'm drawn to Rust is because it solves the problem I have and it's a very practical language even though it has strong design ideas. I do not have any problems currently that require HKTs but I have so many other problems that need fixing. I do however have the fear that HKTs would move the language into an area where I would enjoy it a lot less so I'm not very happy with it. With regards to the lifetime issue: the lifetimes in itself are a concept that is unproven in many ways and there are definitely areas where it would be nice if we had better answers. For iterators there are things that could be nicer, I agree, but I don't believe that abstracting over lifetimes is going to be the answer to the general limitations of them.
If you look at the editor used by [Rust by Example](http://rustbyexample.com/), you can see some error highlighting.
If C++ compiles quickly enough for them, it should not be an issue.
Luckily my modules guide shipped recently! Hopefully that will help.
I think it is very useful to make variables mutable/immutable without renaming them.
What if the linker was also a type checker?
[cnn.com](http://i.imgur.com/bw3Wgou.png) looks good until FB jumps in. :/
&gt; With regards to the lifetime issue: the lifetimes in itself are a concept that is unproven in many ways and there are definitely areas where it would be nice if we had better answers. For iterators there are things that could be nicer, I agree, but I don't believe that abstracting over lifetimes is going to be the answer to the general limitations of them. You asked for a specific example and I gave you one, so please don't dismiss it. It's a specific limitation that prevents one from defining generic *safe* streaming iterators. (Which, if you ask me, are pretty freaking cool. What other language can do that!?) So maybe you write a custom `for_stream!` macro to do traversals. But you still can't write functions that are generic over those types of iterators. And yes, that's a major pain in the butt. I've already run up against it and have resorted to unsafe code because it was so inconvenient otherwise. As I said, eddyb has mentioned alternate solutions that don't require HKL, but I don't understand them. I don't know what lifetimes being unproven has to do with this. Lifetimes are the *essence* of Rust. Its success (as a whole) is irrevocably tied to whether there are enough programmers willing to use them. &gt; I do however have the fear that HKTs would move the language into an area where I would enjoy it a lot less so I'm not very happy with it. I certainly have that fear too. I've written enough Haskell to know what it's like to get buried in monad transformers and type constructors with kind `* -&gt; * -&gt; * -&gt; * -&gt; *` that don't document their type variables. On the other hand, idiomatic Rust code uses mutation, which is precisely the thing that monads tend to abstract over. So that has me thinking that HKT---if it existed in Rust---wouldn't as fundamental as it is in a language like Haskell.
Well, the dereferencing itself happens at runtime, but the appropriate dereferences that will be executed are all statically inserted at compile time (which I think is your point).
&gt; PS. Actually, I thought that . method runs arbitrary methods anyway, so you should expect it to have arbitrary expense... It can also be a field access, e.g. `some_smart_pointer.foo = 10` can execute arbitrary code (via inserted autoderefs) before reaching the `foo` field itself. In practice, for semisane implementations of overloadable autoderef (like those in the stdlib), this is not a problem as all overloads are/should be very cheap.
Jumping out of cache is essentially O(1) and fairly small (doesn't mean it is negligible, of course), while a (braindead) user-defined autoderef could decide to download a multigigabyte file or solve an instance of 3SAT before returning.
&gt; Cargo used to have [[lib]] supported in Cargo.toml, but that was removed The only thing that was removed was the ability to write `[[lib]]` instead of `[lib]`, cargo never (by design) supported multiple libraries existing in a single Cargo.toml. Instead, the recommendation is multiple `Cargo.toml`s either in a single repo with `path` dependencies between them, or as separate repos with normal `git` dependencies.
I don't think trusting the file is really an issue. You are implicitly trusting the library with arbitrary code execution when you call into it. Whether or not that code actually respects whatever type information / metadata the compiler (hypothetically) injects is not worth worrying about. Assuming the file hasn't been tampered with, it should be possible, in theory, to call into a dynamically loaded rust library with the same guarantees you get normally, right? I'd imagine that things that were 'static in the loaded library would need to be used as if they had the lifetime of the dynamic library, and there are probably other issues, but it doesn't seem like it should be impossible.
It's harder to implement as an iterator, though, because it wants to hold onto each element mutably to return multiple times (also mutably).
RFC for the proposed linking change: https://github.com/rust-lang/rfcs/pull/404
Do people expect a good, featured IDE at this stage though? Rust hasn't reached 1.0, there's still a lot of feature/syntax change going on, it seems to me like it's too early to expect one.
&gt; Gfx is a safe, low level 3D API on top of OpenGL Then &gt; It is designed to be fast at modern 3D APIs like Metal, Mantle... How can something built *on top* of OpenGL be faster than APIs that operate *below* OpenGL? Am I not understanding how this works correctly? Is it not just a Rust integration of OpenGL calls with optimization built into the abstraction?
See, according to the docs, *this shouldn't work*. I tried to set this up for rust-scan a while ago, but couldn't get Cargo to accept it locally, which would have made development a huge pain. The joys of in-progress software, I suppose. :D
I think reasonable people who are involved with Rust probably don't. I think people looking at Rust from the outside as a potential language to learn and apply to their daily work probably do. 
I just had an idea for a daemon that monitors a child process' executable with the Linux `inotify` syscall and restarts it when it gets updated, communicating with it over piped I/O. It'd be useful for hot-swappable components for, e.g., a webserver. Dunno how viable that is, but it'd be a neat experiment. I don't know if you can even overwrite a running executable. 
I missed this meeting and the last one due to travel, but should be back for the rest of them.
[**@IamTheEddy**](https://twitter.com/IamTheEddy): &gt;[2014-10-22 02:18:58 UTC](https://twitter.com/IamTheEddy/status/524746706932760578) &gt;They made me a [@rustlang](https://twitter.com/rustlang) cake for my birthday. wat [*pic.twitter.com*](http://pbs.twimg.com/media/B0hGWHLCcAAClqj.jpg) [^[Imgur]](http://i.imgur.com/U5xfRG0.jpg) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/2jyd17%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
Cake for the rust-lang gods~!
&gt; unique concept I see what you did there..!
&gt; I don't think trusting the file is really an issue. You are implicitly trusting the library with arbitrary code execution when you call into it. This isn't how Rust works. Indexing into an array could be considered as implicit trust that the data conforms to the assumptions of the index, but Rust draws an explicit memory safety boundary. If you have external knowledge allowing you to trust that path, then you can use an `unsafe` block to load it as a library. However, the API itself is *always* going to be `unsafe`. Rust has a clearly defined definition of safety which would have little to no purpose if all kinds of holes were poked in it. &gt; Assuming the file hasn't been tampered with, it should be possible, in theory, to call into a dynamically loaded rust library with the same guarantees you get normally, right? There are no guarantees about it because it's a file at an arbitrary path containing arbitrary data. It's always going to be considered unsafe. Libraries are free to run arbitrary initialization code, and the file can contain incorrect metadata or memory unsafe / malicious code.
This (and more) can be replaced by adding a few helper functions (e.g. going to `&amp;mut [T]`) to the `&amp;mut [T]` `iter_mut` iterator, that is, iterators are more general.
&gt; another related pain point is that C strings consist of signed chars and Rust strings consist of unsigned chars, so there's also a bunch of casting like .as_ptr() as *const i8 This isn't true: C's `char` can be either `u8` or `i8`, it is not defined as signed, instead, the C standard leaves it to be defined by the implementation/platform. `libc::c_char` aims to match the signedness of the current platform. &gt; I'm also confused that liblibc omits bindings for some C functions like memcpy. I suppose this is because there are equivalent Rust function, but that could be an argument against a lot of liblibc... Not just equivalent, [100% identical](http://doc.rust-lang.org/nightly/core/ptr/index.html#reexports). If there's any problem it's just a lack of documentation about where to find the replacements, IMO.
Indeed. Let me see what I can do about that...
Make that `Rand` only, `SeedableRng` requires `Rng` for some reason.
I guess what I mean is that, if I have external knowledge that a binary can be trusted (say a verifiable build from a known source, or a binary signed by a trusted party), then "the code might be malicious" isn't something anyone needs to write code to address. In the case I described, I shouldn't have to use an unsafe block, or I should at least have a better interface than symbol -&gt; raw pointer.
&gt; In the case I described, I shouldn't have to use an unsafe block Yes, you should have to use an `unsafe` block. &gt; or I should at least have a better interface than symbol -&gt; raw pointer. Sure, but it's still not a suitable for a safe API. &gt; if I have external knowledge that a binary can be trusted (say a verifiable build from a known source, or a binary signed by a trusted party), then "the code might be malicious" isn't something anyone needs to write code to address. This is *exactly* what an `unsafe` block is for. It is a promise to the compiler that despite the operation being unsafe in general it is safe in this specific instance.
I found this interesting: &gt; Most false positives are caused by either (a) pointer aliasing, (b) conditionally acquired mutexes, or (c) initialization code that does not need to acquire a mutex. Rust solves (a): I think (a) is the primary reason why it is an error in Rust but just a warning in C++. As far as I know, Rust does not solve (b) and (c).
... Borrows for the borrow throne?
Why not? Borrows for the borrow throne!
I guess you're right. I wasn't really thinking about the same definition of "safe", I suppose.
Hah, I didn't! :D
Can I borrow a mutable slice?
I think the Rust libraries do solve (b), since (by default) the only way to access mutex protected data is via the return value of the `lock` method, so the normal analyses apply, conditional locking or not (I suppose it does mean that one might need an `Option` in Rust where one is not required in C++).
Does it make a difference for performance?
Perhaps a lint for integers which infer type, defaulting to i64 for safety? And +1 to renaming int/uint; it's far too tempting to use these now. Personally, I think (u)size and (u)word are the best two options. I'd like to avoid underscores in pointer-sized types; again, the goal isn't to punish low-level programmers.
This is a great write-up. It summarises the current state of affairs really well. I also agree with the points raised regarding choice of integer types. It still irks me that some people advocate `int`/`uint` as good defaults. For the default type of integer literals I actually think the status quo of not having one is correct. If there is no right choice don't make it for the user. I never quite understood from the meeting minutes why having a default was deemed necessary. As a suggestion: it might worth noting `.checked_add()` as another method for handling potential overflow.
Should be more clear, my girlfriend and a friend
[My friends set fire to it shortly after](http://i.imgur.com/g02cQ3K.jpg)
&gt; another related pain point is that C strings consist of signed chars Actually, it's worse than that. C strings consist of `char`s. (Ie, `char*`, not `signed char*` nor `unsiged char*`.) Whether `char` is signed or not in C is implementation dependent.
That's what Scala does, but at least in Scala it means they can't use [] for indexing. I think Rust is trying to stick closer to C-like syntax. (Or, really, some mix of C-derivative syntaxes and ML.)
Happy birthday! They did a great job on it.
Again, keep in mind that Rust must be *better* than C++ for people to transition to it. If they do, many of the items in my list will go away, but many of the items on the list will never happen if people don't start moving. For example, I don't think Mozilla has any plans to work on an official Rust IDE. Unsafe code is currently hard to write not only because there isn't nice sugar for common unsafe operations, but because most Rust code is safe. To elaborate: in C++, unsafe is the default, so the STL tries hard to avoid exposing interfaces that can be used unsafely (it does not always succeed, of course: see iterator invalidation). In Rust, that's not the case. Many libraries expose interfaces that depend on lifetimes for safety. Transmute in Rust is thus *incredibly* dangerous. Rust also has extra aliasing guarantees (around &amp; and &amp;mut) that do not exist in C/C++. If you have to use unsafe everywhere, then you lose many of the advantages of Rust's borrow checker which means that if you have to do it a lot Rust must be nicer to use than C++ for unsafe. IMO, that is not currently the case, for the above reasons, plus others that are hopefully fixable. Rust won't work on everything LLVM can target, but even if it did LLVM doesn't target nearly as many architectures as GCC. I doubt LLVM will ever work on many older architectures, even if it's possible to do so. If people want to use Rust on those architectures, they will with high probability have to not only convert their projects to Rust, but also write the necessary hooks for LLVM. The C syntax compatibility matters because you can't take advantage of macros in header files--and I don't see a way to avoid that in the general case. For many larger projects, this is going to severely limit the utility of an automated solution like rust-bindgen. Labeled control structures are fine, but they cannot capture all of the control flow that goto can. In state machines this can make a substantial difference. I believe (though I'm not certain) that "labeled match arms" would allow replicating the full functionality of goto. As it is, if you want to capture this control flow in the general case you must write assembly. Templates are far more powerful than Rust's generics, even with associated constants and HKTs. I agree that traits are awesome and there are good reasons why Rust does not implement them as C++ does, but the fact remains that you can't do nearly all the things you can do with C++'s. As an example, in C++ it is possible to write a template that only accepts prime numbers. As a slightly more sane example, in C++ you can write a typesafe "dimensional units" type, where you can multiply and divide different types of units without worrying about the precise order and it all typechecks. That is not possible in Rust. Rust is thus leaving behind quite a bit of template metaprogramming. My point about monomorphization and LTO is related to Rust's standard library. The C++ STL is not nearly as over-the-top with generics as Rust's std, so the effects are not nearly as pronounced. Boost is, but it doesn't come with the language by default. Yes, ultimately companies will only ditch C++ for Rust if the cost-benefit analysis proves worth it. But there are going to be costs, not just benefits, to the transition.
Rust does handle (c) because initialization happens before the move into the Mutex. Unless I'm misunderstanding, anyway.
I disagree. It is important to set a good example from the beginning.
&gt; They are not such nice names though, but perhaps that is OK, since we should (mostly) discourage their use. I agree. Ergonomic discouragement is important, and I have long advocated for this in relation to the current `int`/`uint` types. `size` might not be the best name, because it is valuable real estate for user applications. I still say that `intptr` and `uintptr` seem to be the nicest. - https://github.com/rust-lang/rust/issues/9940 - https://github.com/rust-lang/rfcs/issues/161
&gt; `size` might not be the best name `isize` and `usize`
It's a shame that I can't make it there :(
If we are to make people to be more explicit about integer width, then I feel that we should make "at least n-bit" integers ergonomic to use. Then "int" could be defined to be "at least 32b" (edit: as clarification, the equivalent of `int_fast32_t` in C), of course assuming that we have machine sized ints (intptr or whatnot) available too
&gt; This is actually quite a rare situation, the usual case is when indexing into an array, which is itself quite rare in Rust (since we prefer using an iterator). Even array indexing may not require a pointer sized integer most of the time since you often know the maximum size. Only arbitrary sized array (only limited to available memory) need pointer sized index.
She made the cake to shut him up.
The analysis can be somewhat path sensitive, by reifying the state of being locked into an object (which is what I was alluding to the stdlib doing), e.g. let mut maybe_locked = if b { Some(mutex.lock()) } else { None }; if b { maybe_locked.take().unwrap(); // drop &amp; unlock } 
&gt;I never quite understood from the meeting minutes why having a default was deemed necessary. The main argument was that unrestricted number literals are common in tests (most important), examples and tutorials, and it was annoying to annotate them with suffixes. I never found this argument convincing since the annotation is usually minimal (one letter) and not every literal has to be annotated, and there's no consensus on the fallback type.
Yes, absolutely (I only know of RuntimeCompiledC++). There are also a number of systems for doing it with C (look at http://en.wikipedia.org/wiki/Dynamic_Software_Updating). For the last couple of months I've been thinking and planning a lot for writing my own "live programming" language/IDE. For live programming the need for hot code reloading is obvious. For the implementation language I'm still not sure, candidates are either C++, Rust or Elixir(runs on the Erlang VM). They all have their different advantages. I have to say that Rust is very tempting to use (and I think it should be possible to do hot code reloading without being too ugly.. with compiler-generated Rust code for state transfer), but I would miss these things: * A mature GUI toolkit and lots of libraries, like you would have in C++. * Powerful macro system, like you would have in Elixir. Really hope Rust developers can find some inspiration there. Anyway, back to the topic of dynamic loading... What I imagine could work (correct me if I'm wrong or missing something!) : The executable is a small runtime system that loads your application code from a dynamic library (using std::dynamic_lib). It then calls your library's main() (which has #no_mangle attribute set). From you main (event)loop : * include a watch for the reload command to come over a socket or such * do you normal application stuff, but if the command to reload comes, copy stack variables to heap, exit your main function with a return code to signal that it should not exit but instead be reloaded and also pass a pointer to a struct with all stack data(which in turn points to the rest of your state). * control is now back in the executables code. It can unload old library, load new library, call the main function there and pass on any old state. * the new library main can detect that it is not starting from scratch but should copy existing state in to itself (instead of initializing from scratch) before returning into the event loop. This is basically how Kitsune does it for C. For rust memory safety features, you don't get any safety across from the main executables code into the dynamically loaded stuff. But I assume it should be fine within your dynamically loaded rust-code? And since all the interesting stuff is in the library (control never leaves it until it's time to exit or reload) it should be fine. Open questions (I have not had time to research yet): * I assume that compiling a library for each task is out of the question? Could they communicate if they were dynamically loaded from separate libraries? * Any funny things going on in the Rust runtime that would make this not work? 
Still to close to the idea of 'sized' types for my liking.
A nitpick, that should be Rust-cake for rust-lang gods! 
In terms of portability, the fact that LLVM doesn't target some NetBSD/OpenBSD ports means that you can't really get Rust code in the mainline of these projects. So, for instance, although you could write an X.509 parser in Rust and plug it into OpenSSL, it would probably not be accepted into LibreSSL.
Maybe not exactly what you want, but you can mark the unimplemented trait methods with `#[deprecated="This function needs to be implemented on a subtrait!"]`, and then mark the implementations on the subtraits with `#[stable]`; I believe that will cause compile-time *warnings* at the call sites, but I don't know if the attribute on the overridden method would silence the warning. I started a thread on discuss.rust-lang.org to suggest a `#[stub]` flag, that would behave like `#[deprecated]`, but for methods that are coming instead of going. But I didn't consider how it would interact with downstream implementations on traits. I never got around to writing up an RFC for it. I don't think the behavior of `unimplemented!()` got much consideration, since it doesn't actually appear in the Rust source anywhere besides its definition (and a few tests). It seems like it was only meant to be used during development, as a generic way to get the compiler to shut up about empty methods (which is incredibly useful, as you probably realize). It would definitely be useful to have a lint to make sure it doesn't find its way into production. 
Isn't the second `if b` redundant? Why not just `maybe_locked.take()`? Wouldn't that move the guard outside the option and drop it?
interesting, sounds related .. i see the 'unimplemented!()' macro isn't quite right, its just like a wrapper for fail!(), maybe it could be beefed up to do this, 
Yes, we are talking about the same thing. The situation has not immediately improved now, but iterators allow, e.g. let mut remainder = some_vector.iter_mut(); loop { match iter.next() { None =&gt; break, Some(x) =&gt; { for y in iter.reborrow() { // ... } } } } where `fn reborrow&lt;'a&gt;(&amp;'a mut self) -&gt; ItemsMut&lt;'a, T&gt;` is essentially the most general `clone` possible with a `mut` iterator (it is the same thing as reborrowing a `&amp;mut` to a `&amp;mut` with a shorter lifetime). This could also be done via `iter.as_mut_slice().iter_mut()` where `fn as_mut_slice&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut [T]` is going from the iterator to the (remaining) underlying slice. This approach is more flexible than the old one, e.g. `next` can be replaced by `next_back` or any other iterator functionality, rather than requiring the hardcoding of each pattern in the slice module. (To be clear, neither function I mention above exist in the stdlib yet.) &gt; Edit: I'm not sure, this kind of iterator is even implementable due to possible mutable aliasing issues. No, I don't think it'll work. The iterator interface does not allow it! You are correct, the "focus" iterator you propose is not possible with a `std::iter::Iterator`, but it is not illegal with a hypothetical more general trait used by `for` loops that allows one to express that the returned items may expire by the next time `next` is called.
I was just translating /u/sanxiyn's example precisely, but yes, you can definitely just use the `Option` to control dropping.
It's not a matter of minimalism, it's a matter of first impressions. And then you immediately get into a position where you have to explain this whole blog post to someone, and then they say 'so if it's not a problem in real programs then why do I have to understand this now?' It's happened to me dozens of times already.
One may wish to construct a large object in-place inside the mutex to avoid a large memcpy, but we can also avoid locking in this situation, by allowing unsynchronised access via `&amp;mut`: the type system already guarantees uniqueness and so the dynamic assurance of this fact (locking) can be elided. (That said, we don't currently provide functionality for this.)
I was thinking it might be better to rename `unimplemented!()` to `undefined!()`, as in Haskell. Much easier to type. Lining up a set of types, then filling in the 'holes' is a really powerful way of working. I don't think you would see it much in committed source code though - it's more of a development aid.
&gt; And if so: how? What do you mean by this? The actual process is the same in the standard library as out of it: write `impl ... for ItemsMut { fn borrow() { ... } }`, but I guess this isn't what you're asking. &gt; Or would you just add it to the impl of ItemsMut? Yes, this is the likely approach.
&gt;I'm sure others might object to the practice. Yes, they would- the .NET Stream classes are an example of this and I find it quite grating.
its either that or a mess of single function traits :) if there was a way of getting a nice error message out, it might be a nice compromise
I mean many methods are part of the iterator interface and I was wondering whether something like reborrow could be offered generically or whether it only makes sense/is implementable for specific things like ItemsMut.
`iptr`, `uptr` as alternatives to `int` and `uint` or is it too unintuitive? Or `i64` as default to see if overuse of `int` and `uint` is solved?
While I understand where you're coming from, your entire comment could just as easily be talking about lifetimes, right? They're annoying, difficult to explain to people who are used to just waving them away, yet at the same time they are there for very important safety reasons, right?
This thread has been linked to from elsewhere on reddit. - [/r/rrrrrrruuuuuuuuuuuust] [IamTheEddy&amp;#x27;s GF and friend made him a Rust cake](http://np.reddit.com/r/rrrrrrruuuuuuuuuuuust/comments/2jzone/iamtheeddys_gf_and_friend_made_him_a_rust_cake/) *^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)* 
Lifetimes very rarely come up early on, especially with elision rules. As it stands, when I do an 'intro to Rust' talk, I can skip lifetime syntax alltogether, or save it for the end, but my slides are littered with `5i`.
I'd like to voice my opinion for word and uword, again.
The thing is that the current default is not very sane, because it encourages using `int` instead of `i32` or `i64`, which causes portability and performance issue. For example, the function `pow` in the standard library uses `uint` for the exponent. It does not make any sense that the exponent is pointer-sized, the only reason `uint` is used because it is the default.
I think `i32` would be a much better default, because it does not have the portability (and performance) issues that `int` has.
Fear not: This is only turned on if optimization levels are below maximum. So runtime performance of generated code is not impacted.
&gt; error: unable to infer enough type information to locate the impl of the trait core::kinds::Sized for the type &lt;generic #42&gt;; type annotations required I'm not getting that when trying it out in the playpen. Also don't know what it's about. &gt; Some(ref node) =&gt; (**node).length() You can write that expression as `node.length()` since `.` will automatically deref. &gt; error: mismatched types: expected core::option::Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt;, found core::option::Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt; (expected type parameter, found type parameter) Yea, this one looks confusing. The issue is the following stuff: impl&lt;T&gt; List&lt;T&gt; { ... pub fn append&lt;T&gt;(&amp;mut self, element: T) { You're declaring `T` twice here, once on the type and once on the method. Drop the ones you're using on the methods and your example runs for me. 
Looks like you've hit this bug: https://github.com/rust-lang/rust/issues/18040 I've kind of given up on writing Rust code until this issue is fixed :p (the error messages are just wrong IMO). As for the real problem, I think it's due to the usage of `#[deriving(Clone)]`, you need to make sure `T` implements the `Clone` trait. Adding a `where T:Clone` in a few places should make it compile. Meaning: - `pub struct List&lt;T&gt; {` should be `pub struct List&lt;T&gt; where T:Clone {` - `struct Node&lt;T&gt; {` should be `struct Node&lt;T&gt; where T:Clone {` - `impl&lt;T&gt; List&lt;T&gt; {` should be `impl&lt;T&gt; List&lt;T&gt; where T:Clone {` - `impl&lt;T&gt; Node&lt;T&gt; {` should be `impl&lt;T&gt; Node&lt;T&gt; where T:Clone {`
I think in the end it will come down to a hard choice between i32 and i64 as a default. The former is a correctness hazard, the latter is a performance hazard.
Good point!
Yeap ... You were right about those "T" in function declaration :) It worked. Now by implementing another function i have a problem: &gt; error: binary operation `==` cannot be applied to type `T` &gt; linked_list.rs:69 if self.value == after {
That's actually pretty nice!
&gt; The former is a correctness hazard, the latter is a performance hazard. No, both are correctness hazards. If you're not thinking about bounds, then you shouldn't be using fixed-size integers. The issue that matters is the number of overflow bugs slipping by programmers and tests. I'm not convinced that a 64-bit integer will reduce that number. It will certainly reduce the number of test failures due to overflow bugs but it may not reduce the number of latent bugs not caught by tests.
It is a problem in real programs. It's not appropriate to use fixed-size integers without thinking about bounds. If you don't want to think about it you should be using big integers, and if there is a pain point with big integers in Rust then the real problem should be fixed.
Borrow checker errors come up almost immediately in any real code. You can skip over it if your goal is promotion rather than teaching but people are going to be left helpless when they actually try out the language.
Defaulting to `i64` isn't any safer than defaulting to `i32`. Latent overflow bugs not caught by tests are the problem. It's not convincing to make the claim that 64-bit integers will result in less of those without backing up why you think fewer overflow bugs will slip by tests. An overflow occurring *less often* in practice is more harmful than one that's easy to find and fix.
It has nothing to do with "word size". It's defined as pointer size, which is 32-bit on x86, 64-bit on x86_64 and 32-bit on x32 despite it having 64-bit general purpose registers like x86_64. Anyway, word size across the x86 ABIs is defined as 16-bit. A 64-bit integer is a quad word on x86_64, not a word.
&gt; But usually you don't want overflow. Then why is overflow defined? ~~Undefined~~ Unspecified overflow would at least allow static analysis tools to point out errors.
 let i: u8 = slice.head().map_or(0, |x| *x) would be my favourite.
The `intmax_t` / `uintmax_t` types were a bad idea because it's not possible for the implementation to extend them without breaking the ABI. The definitions on major platforms are already incorrect because 128-bit integers are available but these types are 64-bit.
Rust doesn't run on a platform where 64-bit integers are faster and it is unlikely that such a platform would be created due to the widespread usage of 32-bit integers. There is a use case for the largest integer type supported by hardware for niches like big integers where a wider type means the ability to do more work with each operation.
`i64` as a default would be an enormous performance issue on platforms with 32-bit registers and a significant one on platforms with 64-bit ones because many operations are usually slower.
But that would taste terrible. D:
It would be nice if it had a good heuristic for the tipping point where turning it on is beneficial to compile-time.
Thanks for doing that! I'd help out if I had the expertise :P
Happy 8th birthday !
I thought that 32bit integers are slower than 64bit ones in some cases on x86_64. A recent example pointing towards that is [here](https://news.ycombinator.com/item?id=8346336) (HN comment for [this](http://zinascii.com/2014/the-8-byte-two-step.html) article): &gt; The first 'movl' wipes out any bits greater than 32 if they are there. This is because x was defined as an int, and not a long. In general, you might be better off always using long unless there is a specific reason not to. Note that I'm not claiming that this is true for general case, but exactly because these sort of micro-optimizations are so tricky I feel like it would make sense to give compiler headroom to use either 32b or 64b as it seems fit.
Ah, I might take that back then.
If you are using `==` on T, you need to specify T to be PartialEq
You can also have cake for the cake god `^_^`
Just the [usual note](https://github.com/glaebhoerl/rfcs/blob/scoped-attributes-for-checked-arithmetic/active/0000-scoped-attributes-for-checked-arithmetic.md) that it shouldn't be *undefined behavior*, which is horrible (compiler gets to assume overflow is impossible and optimize based on it), only an *unspecified value*.
That sounds wonderful to me. :D
Too bad it won't work. Rust is too awesome to keep to yourself. 
I kind of like the way Go handles constants. Basically, constants are arbitrary precision but are converted automatically when needed and safe (at compile time so there is no runtime overhead). So, for example, 9000 would not be a valid i8 or u8 (since it is out of the range for those types), but it would be a valid i16, u16, i32, u32, i64, u64, etc. Is it feasible to do something similar in Rust?
It is nice being able to develop against function signatures, so you can run the compiler incrementally and not have a huge pile of compiler errors when you finish a module. It'd be nice, however, if `unimplemented!()`, or `undefined!()` if you prefer, had a lint that produced compiler warnings either at the invocation site or at the call site of the invoking method, which would turn into an error with `#[deny(unimplemented)] `, making it perfect for OP's use case. The lint itself would be incredibly useful, as the compiler would essentially enumerate your TODOs every time you run it. 
Lots of people weighed in on this the other day in #rust-internals: https://botbot.me/mozilla/rust-internals/2014-10-16/?msg=23623618&amp;page=9 &lt;bstrie&gt; strcat: I know you've had opinions on this in the past, what do you think &lt;strcat&gt; bstrie: I'm against having a platform dependent integer as the fallback &lt;strcat&gt; I don't care about any other part of it
The point is that in a real program, you have a function definition, which forces inference, and so you don't end up annotating the literal. Many examples just have a few lines of code in a main function and that's it, and so never have anything that can force the inference.
Yes, when I'm talking to someone for ten minutes or half an hour, I have no expectations that they'll suddenly be proficient in Rust. That's unrealistic.
A lint for when we do fall back to the default, whatever the default is, is an excellent idea. I wonder if we could get away with it being on by default, or if that would be too annoying.
Because sometimes (if rarely) you do want it, and for better or worse, there is an assumption in the systems programming world that you can get this behaviour if you want it. I think a static analysis tool should be able to point out errors in any case - you will get some false positives, but that is often OK for static analysis.
This is actually pretty similar to how Rust handles numeric constants - I believe we warn if the constant is too big for its inferred type. An un-suffixed numeric constant can be used with any numeric type.
I think the truth is somewhere between these two viewpoints - i64 is safer than i32 - there are obviously examples that overflow in i32 but not i64. But the safety benefit is not that clear cut because many computations will overflow whatever bound is set, if you leave them long enough, so increasing the precision just puts off overflow, which makes such bugs less likely to be caught by tests. I.e., both are correctness hazards, but i64 is marginally less of one (and possibly not enough of one to make the trade-off against performance worthwhile).
It's fine if there's no default and it's fine if the default is `i32`. If it's a platform dependent size, then there's a portability problem. If it's a 64-bit integer, then it's a significant performance issue on platforms like x86_64 and 64-bit ARMv8 and an *enormous* performance issue on 32-bit architectures.
&gt; I thought that 32bit integers are slower than 64bit ones in some cases on x86_64. A recent example pointing towards that is here (HN comment for this article): In reality, many 64-bit operations are slower than 32-bit ones and they are never more efficient. Even basic arithmetic ops like multiplication are slower with 64-bit integers on AMD CPUs and anything but the most recent Intel CPUs. A 64-bit integer is twice as large, so they use 2x as much space in cache and 2x as much memory bandwidth. It also has *half* of the throughput for SIMD operations. &gt; Note that I'm not claiming that this is true for general case, but exactly because these sort of micro-optimizations are so tricky I feel like it would make sense to give compiler headroom to use either 32b or 64b as it seems fit. It isn't true at all. Preferring 64-bit integers is the *opposite* of what performance-aware code will do. It even applies to looping over an array with an index.
If you care about perception, then you should be worried about all of the bad press Rust gets due to the endless benchmarks comparing code using Rust's `int` against C or C++ code using `int`. The obsession with pointer-size integers is also a major performance and correctness problem throughout the standard libraries.
Defaulting to `i64` just means that very few people will be thinking about overflow despite it still being an issue for pathological inputs. In practice, it won't occur in the tests or normal usage conditions which makes the bugs that much more scary because they won't be discovered. &gt; and possibly not enough of one to make the trade-off against performance worthwhile The perception issue due to the poor performance of `int` on x86_64 *today* is a far bigger problem than perception issues due to integer suffixes. It has been a regular occurrence that popular articles comparing language performance use `int` in Rust and C and determine that Rust is up to 2x slower. It often gets corrected but only after the damage is done because the traffic to the article has dwindled already. Defaulting to 64-bit integers on architectures with 32-bit registers would be a *disaster* because it's not even a hardware integer type. We don't even have 128-bit integers on 64-bit, despite that being comparable to 64-bit integers on 32-bit.
how do I do that ? where should I write #[deriving(PartialEq)] and should I implement eq method ? If yes, then how if I don't know it's type ?
Sounds similar to what `#[static_assert]` does. I don't think there's a way to (ab)use it to test for this though
You need to add a bound to all `impl` and `struct` definitions: http://is.gd/c1Ch0F the [traits section of the guide](http://doc.rust-lang.org/guide.html#traits) covers trait bounds.
Yes, I am pretty sure I side on 'fallback to i32' for this reason.
You can use `impl&lt;T: PartialEq&gt; List&lt;T&gt; {` to indicate that `T` must implement `PartialEq`
Because `x.method()` can in fact mean something like `(****x).method()`, with each of `*` calling different `Deref` implementation, executing arbitrary code. Dot operator in Rust is intentionally magical.
&gt; I worry a little bit about future proofing, but I guess we've got a few years before 64 bit is really dominant. A 64-bit integer is *always* going to be twice as big as a 32-bit integer. It is going to use twice as much memory bandwidth, twice as much cache and have half of the throughput in vector operations. It isn't suddenly going to become only 15% larger than a 32-bit integer in a few years. It will eventually perform as well for all of the basic operations on 64-bit platforms, but it's not there yet and that doesn't mean it will have comparable performance in the real world where cache and memory bandwidth are very important.
Which operations specifically, are worse on x86_64 with i64? I've seen these claims, but no actual data to back it up, and in fact, with my own test, addition was slower with `i32` than `i64` was slower than `int`.
I wonder why LLVM itself doesn't run in parallel.
Shouldn't the compiler emit a more helpful message? It knows the trait restrictions on the `==` function.
So the article mentions varying fixed-size integers, and arbitrary-size integer. Are there fixed-size integers, but with overflow checks? Or is this a an option that can be turned on and off for all or a subset of fixed-size integers? Even if the overhead of that checking isn't acceptable in an application, it can be used in development and debugging to weed out overflows and be partly used as a guide for which variables that need to be *wider*. Overflow bugs seem both very subtle, and like something that is hard to reason about a priori unless you actually have some upper bounds on some other variables that you know of ahead of time. It might also be hard to uncover bugs by things like unit testing, because they might only happen at a certain scale which the unit tests might not cover. Maybe testing and running the application in this kind-of debug-mode (with overflow checking and whatever else enabled) is the only viable way of uncovering them, in some cases. 
I'm still uncomfortable with the fact that non-release builds will have widely-varying, nondeterministic performance.
I think it would have been grand pre-1.0, one of the reasons I took so long to dive into Rust was that I would checkout a project on github that was less than a year old and it wouldn't build with the current compiler. I had no clue which version they were using. And I had no context in terms of what syntax changed. One can view the pre-1.0 world as a compressed microcosm of what future dev will be like over the span of 5-10 years. Much of the cruft that is in current languages is from making backwards compatible fixes. Java had a huge problem in 1.4/1.5, entire codebases had to get upgraded. Python has the 2/3 split. While a version in the project file (Cargo.toml) would be nice, it would be _nicer_ to have it or an override in the file itself. Think of this scenario, you get dropped in to a legacy Rust app in 10 years. It was written against Rust 1.1 but the current compiler is Rust 3.2. You'd like to go in and make some changes to a 500KLOC codebase AND you would like to use the new parametric module frobdication feature in Rust 2.5 but it isn't backwards compatible. Luckily the Rust compiler knows about [1.x,3.2] semantics and syntax and mix new and old code together (where valid). I can keep 498.5KLOC of my code on Rust 1.1 and a couple files can use 3.2 features and libraries. No other language I know of can do this. Go has some nice tooling around upgrades, but not in terms of making changes to language features. While we are at it, I don't know if Rust supports it, but importing two different versions of the same library would be amazing for ongoing support. The only way to currently support a 5-10 year dev cycle is either continue to port the application forward to new compilers, or vendor your libraries and be able to do a `build world` on your entire toolchain plus libs. Save that whole thing to a VM and be frozen at some point in the past. 
You could feed the `-Z time-passes` info back to Cargo, which will store it under `target/`. Then it can decide how to run each build based on the previous one.
Other than `insert_after`, no methods require `T: PartialEq`, so they could be in a separate impl so that they're usable even when `T` doesn't implement `PartialEq`.
It is completely unreasonable to have overflow undefined: every arithmetic operation with signed numbers would need to be `unsafe` or else one can break memory safety without unsafe. Being unspecified would be fine in this respect.
I would think that the suggestions with `ptr` in the name would mislead beginners into thinking that the type was actually a pointer. I much prefer `isize` and `usize`.
I think this relies on the internal implementation details too much, and it really only applies to `&amp;mut T` iterators, but there is a possibly a way to be slightly more generic than just a plain inherent impl on `ItemsMut`.
There are not (other than the BigInt types), afaik (I don't know the ins and outs of libnum). There are intrinsics for doing checked arithmetic. There has been a lot of talk about doing something in this area, but unfortunately this has focussed on what should be the default case (and we've settled on unchecked arithmetic there, and its too late for big changes). It would be great to have such types in a library and a good test of Rust's language features to ensure they are as easy to use as the built-in numeric types.
[Intel had/has patches](http://lists.cs.uiuc.edu/pipermail/llvmdev/2013-July/063796.html) for running a subset of LLVM's optimisations passes in parallel, I don't know if it ever landed (or why not) upstream.
So what do you suggest? Keeping things the way they are now and renaming `int` and `uint` to something that signifies they are of pointer size?
I like that Scala doesn't use [] for indexing. They use () instead: val thirdString = stringArray(3) Which ends up looking like a normal function call from Int to String. Also, I think any method that needs a function from Int to String can be able to be passed that array, further unifying the syntax.
&gt; Which operations specifically, are worse on x86_64 with i64? Multiplication is one common example. See the latency / throughput columns in these instruction tables for Core 2 or the AMD CPUs: http://www.agner.org/optimize/instruction_tables.pdf On Haswell, 64-bit integers are closer to performance parity for *scalar operations*. It obviously has half the throughput for vector operations along with 2x memory bandwidth usage and 2x cache usage. &gt; with my own test, addition was slower with `i32` than `i64` was slower than `int`. You're not doing accurate measurements. The implementation of `int` is identical to the corresponding fixed-sized integer...
As long as literals aren't defaulting to a pointer-size integer, then I don't think there's a problem. I think the current situation is fine. It would nice if those types had better names but I've given up on getting that changed.
For some reason, iterators are faster with `int` and `i64` than with `i32`. (I didn't do enough tests; `i64` and `int` normalized out). `for _ in range(0,1000000i32)` is .13 seconds slower. Edit: Okay, using `int` as an iterator, and just adding `1` a million times, the differences are negligible.
What ended up working for me: Rust is a big cry baby if you already have MinGW in your PATH. What I did was remove MinGW from my PATH and then reinstall Rust. Works like a charm. That being said, Cargo is a big fuss and practically impossible to use on Windows. (Pretty sure this is libcurl's fault, although it could be a variety of things.) I became tired of all this Windows trouble so I just tried running in a VM, had Rust running within 30 seconds, had cargo working within another 15. 
Again, you're not doing accurate measurements. None of those loops takes *any* time because they optimize out as a no-op. A benchmark with optimizations disabled is meaningless because you're not even using the good instruction selection. It's *especially* meaningless when you're benchmarking the time taken by a high-level abstraction like `range`.
Yeah, I can see how those are problematic. The `least_t` and `fast_t` types would solve this problem well enough, though.
It's only a default, right? If people care they can just turn it off? Or is it just a public-perception thing?
&gt; Unfortunately to unlock all the optimizations within a crate, you need to process it all in a single LLVM context Does this mean then that the particular way a project is split into crates can impact the applicable optimizations? Say, if Servo was built in a single crate instead of muliple components, could it end up being better optimized?
Fine. Using while max != 0 { num *= 2; num /= 2; max -= 1; } not using optimizations, `i32`s are fastest, `i16`s are second, `ints` and `i64`s are the same. Max is initialized to 100000. Using optimizations, with a println!{} at the end to make sure that it isn't optimized away, and `num *= 3; num /= 2;`, it comes out to: * i64 - 52986538ns * int - 52339096ns * i32 - 33707761ns * i16 - 34893162ns And these numbers are generally the same +/- 0.005 seconds. Edit: And my question was whether you have any data for this. I've obviously given my own, and proven your point, but I would like to see the data you used to come to this conclusion as well.
Counter example from the standard library: pub fn pow&lt;T: One + Mul&lt;T, T&gt;&gt;(base: T, exp: uint) -&gt; T
I've noticed it on many occasions and I trust the Agner instruction tables as a confirmation of those observations. Even though 16-bit integers are slower than 32-bit ones, they can often be a better choice simply due to the size. Cutting the size of data structures like vectors in half makes a big difference. Even ignoring cache, the memory allocations will be much faster and it will need to copy 1/2 as much data during a reallocation.
The public perception of debug builds is going to be awful anyway... Rust doesn't work well without optimizations :).
A basic version of this is really not that hard to write. There are multiple implementations of a `Checked&lt;T&gt;` type out there, including [bjz's](https://gist.github.com/bjz/9202512) and [mine](https://gist.github.com/Florob/0ec238fa00a0c9b40bf7). Still works with current Rust, the language must be stabilizing ;) Obviously it can't support literals, but I think it's sufficiently nice to use.
why? Does it matter so much about non-release builds. And I think performance is still deterministic, just lower.
You should still use both, since parallel codegen doesn't parallelise the rest of compilation, but dependencies mean `-j n` builds are very sequential
You can build with link time optimisation to get the benefits of a single crate for release builds, while still retaining the benefits of having multiple crates for development.
I could be wrong but I think that work is dynamically assigned to the codegen threads, and this will affect (e.g.) which functions are available for inlining where. I don't expect good *absolute* performance from non-release builds, but it sounds pretty disorienting to have performance change a lot on every build for no reason. We'll see how much of a problem it is in practice.
http://is.gd/eDMPyd let i: u8 = if let [head, ..] = slice { head } else { 0 };
&gt; if let I don't think rust has a concept of primitives. This behavior is the same for any `Copy` type.
That is true, but it is still deterministic, just not easily predictable (that is, for the same code, you will always end up with the same assignment to threads, but changing the code could give unpredictable effects). I am hopeful that on a large code base, the performance will not vary too much, there will just be more noise. But we'll have to wait and see how that plays out in practice.
You probably need to experiment what value of n gives you the best performance for a given program. I expect for building rustc, it won't matter too much, since the largest amount of time is spent on code which is sequential from make's point of view. I have found that scheduler overhead is much smaller than the benefit from using more threads. For example, if the optimum number of threads is 6, then there is a massive benefit going from 3 to 6, but not much of a loss going from 6 to 12. Therefore I tend to err on the side of more threads.
&gt; Ever tried programming Java without an IDE? In my years as a consultant I've come across more than one occassion where all I had was an ancient version of vi. That and javac. Of course I work faster (and probably better) using an IDE, but it's certainly possible (if not always enjoyable) to program without one. *But* I have more than a decade of experience with Java. A newbie will appreciate (and benefit from) the hand-holding an IDE affords much more. So given that I'm a newb in Rust, I think a good IDE would move Rust from "assess" status to "seriously condider", by reducing the time between making an error and being berated for it by the compiler/IDE.
Most of 32bit operation are faster, even on 64bit processors, division especially. For instance this code is two time slower, on my 64 bit Linux computer, if i use u64 instead of u32 : http://is.gd/abgXvb
In many cases, you can arrange your architecture so that restarting single parts of it causes practically no downtime, i.e. only a short delay in handling input. 
Are Rust bigints as convenient as any other number type? Could bigints be almost as fast as other numeric types in the small int case?
If you use Sublime Text with Rust, the linter now works with Cargo and multi-file crates. Simply enable the `use-cargo` or `use-crate-root` options in the linter settings.
Yeah, I probably mixed up the terms.
&gt; What you are suggesting here could put Rust on the road of a dependently typed system. Oh, no!
It doesn't work... Error: &gt; error: attempt to bound type parameter with a nonexistent trait `Show` &gt; impl&lt;T: PartialEq+Show&gt; List&lt;T&gt; {
I am not the original author but I helped fix it up and learnt a bit of python in the process :P I mainly started working on it because I also wanted the feature haha
double edge sword... its noisier because of traits (IMO), but you could say its' more natural being there than having a preceding template&lt;typename....&gt; .. its' less like the generic typeparams are a bolt-on, which makes sense if they become the rule rather than the exception (writing heavily reusable code..)
+1 ... navigating big UI apis,graphics APIs .. whatever, a good IDE with autocompletion is indeed worth its' weight in gold; but thats' not the fault of the language
i'll give +1 to modules being confusing - the mix of relative &amp; absolute, circular glob import problems seem to complicate having a load of common uses project wide; I've seen some interesting ideas like making it try paths relative then absolute, and "inherit-use" which would be nice Looks like the Qt chap encountered the same issues anyone else would trying to wrap a full on C++ library ... rust is just a different design, so its fitting a square peg in a round hole. I agree with the author that overloads &amp; default parameters are very useful for complex api's like guis, but if you'd started out without them and written new software it wouldn't be a showstopper.
Well, I send my thanks to everyone involved :)
I guess you need a `use std::fmt::Show;` before using it (I forgot that `Show` isn't in the prelude).
"It was a significant improvement that now many a silly mistake did result in an error message instead of in an erroneous answer. (And even this improvement wasn't universally appreciated: some people found error messages they couldn't ignore more annoying than wrong results, and, when judging the relative merits of programming languages, some still seem to equate "the ease of programming" with the ease of making undetected mistakes.)" -- Edsger Dijsktra ([via](https://plus.google.com/+LiamOConnorDavis/posts/Y4as3WeDuxq))
Haskell has similar issues w/ the IO type. Once you understand, it makes sense, but writing "hello world" needs it!
Bigint can be improved (gmp do better than the standard library) but they will always be slower than any fixed size integer even for small numbers.
When learning a new programming language, I usually create a single file in that language that demonstrates everything I learned. Looks like someone had the same idea.
&gt; All other things being equal, you want to use the smallest integer you can for performance reasons. Programs run as fast or faster on smaller integers Is this true? I thought processors handle native sized integers the fastest. Or is this just referring to the benefits of being able to fit more data in cache since it'll be more compact?
Fit more data in cache is a good reason, but not the only one. For instance 32bit operation are significantly faster even on 64bit processor. As you can see in [GBGamer117's benchmark](http://www.reddit.com/r/rust/comments/2jykbw/thoughts_on_numeric_types/clh0cez) or [mine](http://www.reddit.com/r/rust/comments/2jykbw/thoughts_on_numeric_types/clhdcr9) 
Good point.
I'll also add [mine](https://gist.github.com/phaylon/3167023fc59621cdda24) for reference, mostly because being able to do math directly on results makes me giddy. Incidentally, this one also still compiles and I wrote it quite some time ago. I might be able to make it look a bit nicer with `where` clauses though.
I don't really think there's a reliable way to solve this without interfacing with Cargo or rustc more directly. Generally speaking, the SublimeLinter package seems to work by extracting errors from command output. The command used is `rustc` or `cargo build`. Without further information about how files relate to each other, I don't think it's possible to do anything about this issue. A similar problem will happen if you include a `mod foo` in your source file, and there is an error in the module. The Rust compiler will first hit that error and then abort, which means that the rest of the source code in your file will not be linted until the error is fixed. Linting a file in isolation has the problem that it will fail on any dependencies (extern crate, mod in another file, etc). I think short of integrating with Cargo or rustc via a native API, these issues are pretty much unsolvable :(
Awesome! I was wondering when cargo support would land, and now its even more productive that before! 
In theory the compiler might be able to prove that 32-bit integers are large enough and optimize the bigints away. (Not that this is what currently happens.)
What do you mean by "native sized"? The size of `int` or `uint` is not related to performance, it is related to the size of the memory you want to be able to address. If you mean by "native sized" that it is the size with the best implementation in hardware, then this is not what `int` is.
In some very simple cases with only affectations and bit operations, maybe. But I think it wouldn't work anymore as soon as you do arithmetics since every operation might increase the size over 32 bits. 
There is a new directory in src/test called run-pass-valgrind. These tests are just like run-pass tests, except that on linux/bsd/OS X 64 we run them under Valgrind. If you have tests where you want to ensure we aren't causing memory errors, add them here.
There is an open bug on this idea: https://github.com/servo/servo/issues/3669 I have thought about writing an external xpath library, which is possibly less familiar than CSS selectors to webdevs, but isn't too tricky otherwise, for doing this sort of thing. 
I thought we were talking about the small int case. If you actually get large integers that overflow, then you can't use the native integers anymore and it does not make sense to compare performance in the first place.
Lifetimes are the thing that make it possible to be memory-safe without garbage collection.
 // hypothetical structure in lifetime-less Rust struct Foo { map: HashMap&lt;&amp;str, &amp;str&gt;, } fn make_foo() -&gt; Foo { // these strings are owned by the scope of the call to make_foo() let key = String.from_str("hello"); let value = String.from_str("value"); let mut map = HashMap::&lt;&amp;str, &amp;str&gt;::new(); // insert slices of the strings into the map map.insert(key.as_slice(), value.as_slice()); // then return our new foo Foo { map: map } // problem: once make_foo() returns, the 'key' and 'value' strings it owns // will be destroyed, thereby invalidating the string slices in 'map'. // very bad! } Lifetimes are rust's mechanism to prevent this kind of code from passing the type checker. // so let's give Foo a lifetime. here 'a refers to the lifetime of a Foo object. struct Foo&lt;'a&gt; { // 'a states that the lifetimes of the references within the map // must point to objects whose lifetimes are at least as long as // the lifetime of the Foo object itself map: HashMap&lt;&amp;'a str, &amp;'a str&gt;, } // here's a structure to keep the Strings for the keys/values struct Bar { keys: Vec&lt;String&gt;, values: Vec&lt;String&gt;, } impl Bar { // 'a here means that the lifetime of the returned Foo object // is constrained by that of the Bar object on which this method // is called. Since the Strings in Bar are the source of the string // slices stored in Foo's map, this constraint is satisfied. fn make_foo&lt;'a&gt;(&amp;'a self) -&gt; Foo&lt;'a&gt; { let mut map = HashMap::new(); for (key, value) in self.keys.iter().zip(self.values.iter()) { map.insert(key.as_slice(), value.as_slice()); } Foo { map: map } } }
&gt; When trying to use handler functions stored in structs, the compiler starts to throw up all kinds of lifetime related errors and I end up implementing my handler function as a trait. For the moment, our closures are quite limited: you can't really store a closure in a struct, or return a closure from a function. But it's possible with unboxed closures (that are not quite ready). Unboxed closures are just a trait with sugar around, so it must be basically what you have done by hand with your trait.
The lifetime guide is definitely a good start, but doesn't go into great detail. I'll try and find more resources on the web for now, I guess I'll get a grip on it eventually :)
I still don't really understand that. Naturally, when the HashMap is a member of a struct, and those &amp;str's are member of the HashMap, they should all have the same lifetime. The compiler could then throw errors whenever those strings are mutated outside of the lifetime of the struct? Am I not getting this?
As everyone else has said, Rust is not Rust without the lifetime system. Eliding some lifetimes will only make it much harder when you get to the more complex stuff. Your self confessed lack of knowledge of the language and how the language works does not lend to your argument that lifetimes are bad and/or wrong. The nugget of advice here is we need better documentation. There are requests for this everyday, so lets keep annoying /u/steveklabnik1 about it (not sarcasm :D ).
Think about it like this, you can't think of lifetimes COMPLETELY like this, but for this post, imagine a lifetime is just a scope. Have a look at this code: fn create_hashmap() HashMap&lt;&amp;str, &amp;str&gt; { let data = map_file("something.txt"); let hash = HashMap::new(); hash.insert("piece1", data.at(0)); hash.insert("piece2", data.at(64)); hash } Lets assume for a moment that the braces here mean lifetime `'a`, anything created within these braces is automatically going to live as long as `'a` does. When `let data` is assigned the mapping, that mapping will only live for `'a` (at the end of the scope, aka at the end of `'a`, it will be destroyed). Consider now what `&amp;str` means in the return type for this function, it is a reference to some data that lives for some amount of time, but how long? Looking at the insert, we see the first string, "piece1", is a string literal, and will live for the whole program lifetime. This is the lifetime `'static`, which means "always in scope". But what about the second string that comes from data? What lifetime do we assign that? It can't be `'a`, because that dies at the end of the function and so returning data that lives only inside `'a` would be invalid. It can't be `'static` because the data we're pointing to is definitely not static. The answer is there is no lifetime you can put here, the code is invalid. This applies everywhere, function arguments, return values, and structs as you mentioned. Any time the compiler doesn't know for sure that data is invalid, it will ask you to add lifetimes to prove it.
Use String not &amp;str then?
A `&amp;str` has an inherent lifetime that is deeply connected to the actual value, since a `&amp;str` is a pointer to some "random" piece of memory, so it is limited to be valid as long as that piece of memory is (and the compiler has no control over this lifetime in general, as that would require GC). One cannot change the lifetime of a `&amp;str` (or any borrowed pointer) by how it is used, so placing things into a `HashMap&lt;&amp;str, &amp;str&gt;` requires that the lifetime of the `&amp;str` works with the `HashMap` value, but it will not (and cannot) extend the lifetime of the new `&amp;str` to force it to work. The explicit named lifetimes are just how the compiler reasons about these lifetimes, and how the programmer can precisely communicate the desired relationships between references to the compiler.
To be fair, the 'a syntax is simple in simple cases, and complicated as hell in others. struct Foo&lt;'a, T:'a&gt; { data: &amp;'a T } impl&lt;'a, T&gt; Foo&lt;'a, T&gt; { fn returns_to_scope(&amp;'a self) -&gt; &amp;'a T { self.data } } Mmm... what does that actually do again? The returned &amp;T now has a lifetime which is ah... at least as long as the structure it belongs to? Wait, but it's a &amp;T on the structure! So you can only put a reference into it if the reference is at least the lifetime of the structure. Make sense? fn foo(_:int) { trace!("func pointer"); } type HasInt = |int|: 'static; let x:HasInt = foo; Right, so HasInt is a function pointer (or closure) that has a lifetime of at least 'static. Nice, what does that mean again? Oh right, it means that you can only put a fp that is a static function (ie. top level) in it right? ... nope. let y:HasInt = |_:int| { trace!("closure"); }; toda! In fact, you know what, I don't actually even know what that 'static *actually does*. In fact, lets get into it: struct HasThing { foo:Bar&lt;Thing + Send&gt; } struct HasFoo { foo:Bar&lt;Foo + Send + 'static&gt; } Hm... there's a difference here I'm sure. So, Bar is a struct generic over T, and T must be Foo and Send and 'static. What? Why 'static? Oh, its because when you're generic over a *trait*, and Foo is a trait~ So you need to explicitly specify the lifetime bound on the trait. What does that mean again? 'static. Ah, on a trait that means um... the pointer that implements the trait must have a lifetime of at least 'static, the entire scope of the program. No wait, that would mean that you could only put static mut values in it. um... once again, you know, I actually don't know what 'static implies in this context. I mean, don't get me wrong, lifetimes make rust rust, not D. They're absolutely invaluable. In simple cases they're also relatively easy to grasp. ...but lets not pretend they aren't some pretty difficult and obscure uses for them in rust. These concepts are generally very poorly explained anywhere: - What is a lifetime on a structure, and why is it ever useful? - What is a lifetime bound on a trait, and what does it mean? - What is a lifetime bound on a closure and what does it mean? - What is 'static, and what does it mean? (because it *certainly* does *not* mean the associated value must live for at least the lifetime of the program) - If you have 'a on a struct and 'a on a function, are they the same 'a? Or does it depend? (ie. you override lifetime names by going fn foo&lt;'a&gt; when 'a already exists in the context without errors) - Do blocks (ie. { ... }) have a lifetime, and how do you access it? (eg. return value is valid for the block function was called in)
The `&amp;str` inside the HashMap is a pointer to something outside the HashMap. The `&amp;str` has to live at least as long as the HashMap lives, but it could live longer. Think about it in Java, C#, Haskell, or Python. If you have a string in a hash map, and you take the string from the hash map and assign it to a variable, that string will live longer than the hash map will live. Rust makes this explicit, which allows you to get rid of the garbage collector. C and C++ trust you to do the right thing, which means you don't have a garbage collector but you might crash. If you wanted a HashMap with elements that had the same lifetime as the HashMap, you would use `HashMap&lt;String, X&gt;` instead of `HashMap&lt;&amp;str, X&gt;`.
Yes. Rust makes the ad-hoc cultural practices from C++ land explicit and statically verified.
No, you're quite right. There are not any good advanced lifetime guides on the internet. Many of their features are not even documented outside of the compiler's source code, RFCs, and Niko's blog. Hopefully this will change soon.
In almost every case where you're using lifetimes in a struct, you're probably doing it wrong. For example, `HashMap&lt;&amp;str, &amp;str&gt;`. Usually you'll be wanting a `HashMap&lt;String, String&gt;`; `&amp;str` is a slice of a string — a reference into a string. In general you want structs and other things to own their data. You might sometimes want `&amp;` pointers if you're sure that your struct will only need to exist within the lifetimes of its components. For example, a custom iterator should contain borrowed references, since the data it refers to need not be owned by it. A HashMap — probably not, unless you're sure you want to use it that way. Elision works pretty well for functions, and functions are precisely where borrowed references are used the most. For structs/etc, there are usually many ways of specifying lifetimes, which makes it hard (impossible?) to elide the lifetime. Not to say it can't be done, but in most cases the compiler wants you to specify a lifetime because there's more than one way to do it. ===== &gt; The usefulness of lifetimes hasn't really hit me yet. The usefulness is as follows: the entire borrow checking mechanism is dependent on it, and it's an integral part of the type system. _Explicit_ lifetimes are not so useful. As mentioned before, in most cases if the compiler is asking you for an explicit lifetime, make sure you really want to use a borrow instead of owned data or a box. If so, then think about how long the reference should live for your code to make sense. ===== There's a lot of room for improvement, though. Usually my way of dealing with lifetime errors is to keep changing things till stuff works, though I've gotten better at it these days ;)
&gt; Then you can change it to HashMap&lt;&amp;'static str, &amp;'static str&gt; and get rid of the lifetime on MyStruct. Then you will only be able to store string literals or constants in it, as they're guaranteed to be around longer than anything else (i.e. the 'static lifetime). (Note that one can retain the original flexibility of having non-`'static` lifetimes by writing `MyStruct&lt;'static&gt;` in these circumstances, e.g. `fn create_mystruct() -&gt; MyStruct&lt;'static&gt;` if the internals were all string literals.)
What I don't get if they're compile-time guarantees and the compiler won't let you compile without adding them in some cases... why are they explicit? Can't you just have the compiler add a lifetime where it's required in every case that you have a compiler error if one is not present?
I grasp the importance of lifetimes but I still agree with OP. There has to be something to make them more digestible. Case in point: Chris Morgan's article about the various String in FizzBuzz - so much work for a silly little problem.
The exact desired lifetime configuration can be ambiguous (especially with the declarations of types), and inferring the lifetimes based on the internals of functions would break from Rust's current rule that the type signatures of any called functions (not their contents) are all that is needed to type check a chunk of a code. This also allows the external API of a library to subtly change just by adjusting the code of a function in a hard to detect way, and anyway, this intersects the standard discussion about inferring the types of functions Haskell.
Of course, I was just saying that as soon as arithmetic operations on variables are involved, I doubt the compiler can be wise enough soon to distinct integer that will always handle small values from the ones who might grow. Even if he was smart enough right now to optimize, most of the time, bigints to fixed width integers , I think it would be a bad default type since performance predictability is an important goal for Rust. You would never know for sure if your int is a fast one or not.
I totally disagree with you. Completely and totally. One of Rust's strengths is that it supports many ways of using memory. There are many occasions where references are a better approach than direct ownership. This can result in huge speedups to parsers, for example. It is the basis for Rust's iterators, mutex guards, and many other helpful patterns. They can be used with arenas to allow precise control of allocation lifetimes. In the case of HashMaps, you can use them as "indexes" into preexisting data (often a much more flexible pattern than direct ownership), which generally requires borrowed references. Often explicit lifetimes are also useful even in cases where they might not be necessary to get a function to initially compile, so that you don't end up taking ownership for too long (leading to restrictions in APIs that are actually safe). Equally often, they are needed for functions with subtle memory relationships between different structures. Lifetimes will form the basis of data parallel APIs as well. They are also useful for exposing safe APIs to unsafe code. Really, there are just way too many cases where they're useful or necessary for blanket advice like "you're probably doing it wrong" to be correct. Just because they are complex does not mean they are not useful. Instead, we should focus on documenting them better and making it more obvious how to use them effectively.
I think you got his point completely and totally wrong. Neither did he claim that lifetimes are not useful nor that `HashMap&lt;&amp;str, &amp;str&gt;` is wrong in general. I think Manis just wanted to point out that you shouldn't put a reference in a struct just for the sake of having a reference. I got the impression that this was the main misconception the OP had. Or to quote Manis: "In general you want structs and other things to own their data.". Which is true. Look for example at the mutex guard you mentioned. The underlying `Mutex` actually owns it's data. You should only use references when you need them and when they are usefull. Not because you can.
I don't think it's true that "in general you want structs and other things to own their data." That's exactly the point I was disagreeing with (well, one of them--there were several explicit allusions to explicit lifetimes not being very useful, which I also disagree with). I think it's too broad and I don't think it's obviously better in Rust. I think this is a carryover attitude from C++, because it's generally unsafe to store non-smart pointers in structures in C++. In Rust it is perfectly safe and they have lots of advantages (like no allocation / tiny copy overhead, and giving the caller the opportunity to decide where the data are stored, including on the stack). They can also completely eliminate the use of Rc in many cases. What's the pedagogical reason that structs should own their data in Rust? With upcoming data parallelism APIs, the biggest current objection (that you can't share structures with references between threads) will disappear. I believe that any time you have immutable data, and in some cases when it's mutable, using references instead of direct ownership is worth considering. (I appear to have deleted part of my post, yay! But I had a description of here of why I don't think Mutexes are a good example of this, since they actually need to own their data to preserve memory safety; if that's a requirement Rust will already prevent you from using references there, or you're using unsafe code and most idioms related to safe code don't apply).
Lifetimes mean Rust does not have to follow all the C++ idioms to retain memory safety. You are free to do more efficient things (like the equivalent of map&lt;char *, char *&gt;) without worrying about them causing crashes.
I barely got started on Rust. I see lifetimes as the designated driver keeping track of all his drunk friends and gathering them together when its time to leave. This allows you to party without worrying any of them will turn up in the morning on a sidewalk with wet pants and in the middle of a pool of puke.
Thank you for saying this. I think the complexity of lifetimes is kind of like the various restrictions on quantum computing. Like quantum computing offering fantastical algorithms (a sublinear linear search?), Rust offers seemingly fantastical capabilities (C++ speed, no data races, *and* memory safety?). People are bound to be suspicious: what's the catch? Well, like the bizarre restrictions on quantum computing (only certain classes of algorithm get a speedup, crazy hard to build), explicit lifetime annotations are the catch :) They're what make Rust's claims feasible ("oh, well, I guess you can do it if you have to add a ton of custom annotations to everything"). I think people still have this idea that lifetimes are a wart, not a fundamental part of Rust. That they can somehow be elided out of existence. But if they could, we wouldn't need Rust at all! The only reason elision works as well as it does is because *people* are predictable. I've used the example before of how high-performance implementations of Prolog will just always index the first argument of predicates--there's no reason theoretical reason it should work, but a lot of the time it does! Same with elision. In the general case, you can't predict how people want their API to be used, so you need to allow for lots of different possibilities. A very common example of this is that Iterator can't support mutable windows, because it would need an explicit lifetime, and there's no way for the API to support both that and collect / peekable.
(Disclaimer: I have very little Rush experience. (But lots of C++)) &gt; You are free to do more efficient things (like the equivalent of map&lt;char *, char *&gt;) "free" ... until the borrow checker says "No, I won't let you do that." I would say that, in some contexts, Rust gives you less freedom. Premature optimization should be avoided. Why spend time battling with lifetimes, to get a particular piece of code to compile, when perhaps the by-value semantics would be just as fast? And anyway, if Rust doesn't like our references, it might mean our program is incorrect after all.
My point was that &gt; 50% of newcomer time 'lifetime mismatch' errors are probably from people using `&amp;'static str` as through it was a String. It's a pretty common thing to see people asking about. 
&gt; I don't think it's true that "in general you want structs and other things to own their data." That's exactly the point I was disagreeing with (well, one of them--there were several explicit allusions to explicit lifetimes not being very useful, which I also disagree with). Meta point: markdown allows for quoting text by prefixing the text of a quoted paragraph/sentence/fragment with a `&gt;`, which means you can address a point specifically, to avoid confusion.
I realise these seem more scary with trait objects. (... but thats' not the use case I had in mind here.) I guess you'd have to prevent instantiating a trait object for a type with such unimplemented!() defaults
Checking in from bangalore. Did something come out of this?
I'd argue that having a structure with arbitrary pointers which are *not owned* is a carry over from C++. How is: struct Foo&lt;'a&gt; { b: &amp;'a Bar } categorically better than: struct Foo { b: Wrapper&lt;Bar&gt; } I can name some immediate downsides: - Only one mutable instance of Foo can exist at once for a given &amp;'a Bar. - Foo is lifetimed so any FooBar that contains a Foo must also now be 'a (lifetimes infect parent structs) - Some 'parent' must own the original Bar, and decide when to drop it &lt;-- This is actually a memory leak situation vs. - Wrapper can check and generate a temporary mutable &amp;Bar reference from any mutable Foo safely - Wrapper can exist inside a parent with no explicit lifetime - Wrapper 'owns' the actual Bar instance, so it automatically cleans up when no Foo's are left Where Wrapper is some safe abstraction that stores a *mut Bar in a way that keeps track of it and allows you to control what happens to the Bar instance when all copies of the Wrapper&lt;Bar&gt; are discarded? That's what Arc, Mutex etc are doing. If those are too 'heavy' then you can write your own abstraction easily enough. Certainly there are severe performance penalties to copying values instead of using references; but most of the safe abstractions don't do that. I'd say Rust definitely favors ownership over references. 
Well my usecases for `&amp;` in structs are usually struct Foo&lt;'a&gt; { bar: &amp;'a baz } **Seems** like a nobrainer to allow ellision in such cases
`|_:int| { trace!("closure") }` *is* a static function, as it does not close over any variables. If you tried something like type HasInt = |int|: 'static; fn myfn() { let y = 0u; let x:HasInt = |_:int| { println!("closure {}", y); }; } then it wouldn't compile, because `x` contains a reference to a local variable, so it can't be, say, returned from `myfn`.
This is not really true – `&amp;'a Bar` can be copied, so you can have as many `Foo`-s as you want. You *do* need a parent to root `Bar`, but Rust won't let you create a memory leak with it. Certainly, `Rc&lt;T&gt;` (or `Arc&lt;T&gt;` if you're multithreading) does behave a lot like `&amp;T`, except that it does not have lifetime bounds, but an individual `Rc&lt;T&gt;` pointer does not really own its pointee.
It's not categorically better. It's also not categorically worse. From your downsides: &gt; Only one mutable instance of Foo can exist at once for a given &amp;'a Bar. I may be confused, but I at least as I parse your statement that's incorrect. You can certainly have multiple mutable instances of Foo for a given &amp;'a Bar. Do you mean you can't have Bar be mutable? Because that's only true if you are talking inherited mutability. Internal mutability is very useful, and in fact required if you want to share the data structure at all and be able to mutate it. &gt; Foo is lifetimed so any FooBar that contains a Foo must also now be 'a (lifetimes infect parent structs) I don't view this as an automatic downside, because it presupposes that named lifetimes are a bad thing in the first place, which is what I'm disagreeing with. It's also not always true, because you can sometimes make lifetimes 'static at some point in the parent hierarchy (I have recommended this to people before in some situations where it made sense). It's very situation-dependent. &gt; Some 'parent' must own the original Bar, and decide when to drop it &lt;-- This is actually a memory leak situation It's not a memory leak. If you allocate Bar somewhere, you have direct control over when it's dropped, which is often desirable. Again, it depends entirely on your use case, but quite often it's useful to be able to allocate groups of related objects in TypedArenas and destroy them all at once. &gt; Wrapper can check and generate a temporary mutable &amp;Bar reference from any mutable Foo safely &gt; Wrapper can exist inside a parent with no explicit lifetime &gt; Wrapper 'owns' the actual Bar instance, so it automatically cleans up when no Foo's are left &gt; Where Wrapper is some safe abstraction that stores a *mut Bar in a way that keeps track of it and allows you to control what happens to the Bar instance when all copies of the Wrapper&lt;Bar&gt; are discarded? That's what Arc, Mutex etc are doing. I originally thought you were talking about Wrappers in general, but I am pretty sure that you are just talking about Rc and Arc at this point. Lifetimes let you get rid of Rc and Arc safely in many cases. That's one of their major advantages over just using shared_ptr for everything. In the general case (not just refcounting), many structures with *mut Ts do actually end up requiring explicit lifetimes--they use variance markers like ContravariantLifetime&lt;'a&gt;. And often you don't want to deallocate the moment the reference count hits zero, so again that's not always a win. &gt; If those are too 'heavy' then you can write your own abstraction easily enough. I use Rust because I don't want to have to reason about raw pointers all the time. It's quite hard to implement Rc / Arc safely. And they're already about as cheap as they can be in the general case, if you want cheaper you have to use lifetimes. If you are proposing that I give up compile time predictability, guaranteed safety, and speed in order to (maybe?) avoid writing a lifetime sometimes, then I don't think we are going to agree. &gt; Certainly there are severe performance penalties to copying values instead of using references; but most of the safe abstractions don't do that. Rc and Arc are more expensive than using references, as well as being less compact. For the latter, copying the data is probably faster in many cases. They are also less predictable. And ironically, they can actually leak memory quite easily, if you create a reference cycle and don't explicitly break it with a weak pointer. I'm not saying they're not useful, they totally are, but I do not see how they're an argument against lifetimes. &gt; I'd say Rust definitely favors ownership over references I don't think that has been adequately demonstrated. Rc and Arc are references in all but name: the biggest difference is that they don't have explicit lifetime handling, so they must do dynamic checks of varying expense to be safely dropped, while lifetimes don't require that.
This comment and mentality are not helpful. What would it accomplish if we told everyone who didn't immediately understand something to simply stop trying and go away? 
As does D. C++ also has kinds of kinds (sorts), which even Haskell doesn't have. One of the benefits of having a Turing complete template system.
It is not a no-brainer to me. Allowing elision in that case would then require knowing the full contents of a struct (even the private fields of a struct defined in some upstream crate) to be able to deduce the full type. This is not required now: just looking at the type 'signature' of a type (not its contents) tells you all the lifetimes and generics used, just like looking at the type signature of a function tells you all the lifetimes and generics used (the current lifetime elision rules are purely based on the signature and its types, no need to look at function contents). It is especially important to know the full type of a type, since these are what drive type checking etc. With type elision like that, I find it likely that one could have a reasonably large program mentioning no lifetimes until suddenly adding a struct field causes very surprising lifetime errors in random places due to elision. E.g. going from `struct Foo { x: uint }` to `struct Foo { x: uint, y: Bar }` where `struct Bar { x: &amp;str }`, would break a function signature like `fn do_stuff(x: Foo, y: &amp;str) -&gt; &amp;str`. With the original `Foo`, lifetime elision works fine, with the second `Foo`, the borrowed poiner in `Bar` would force `Foo` to have one, resulting in lifetime elision failing due to the existence of two input lifetimes.
What if your code looks like this: 1. Get strings 2. Make hashmap 3. Insert them into hashmap 4. Do stuff with hashmap 5. Read out the strings 6. Do stuff with strings The lifetime of the hashmap is 2-5. The lifetime of the strings is 1-6. That's why the lifetime of the hashmap and the strings has to be different. Otherwise you couldn't deallocate the hashmap until you're done with the strings.
Really, is that how it works? (genuinely curious) So a closure defined inside a fixed scope has a 'static lifetime if it doesn't capture any variables? ie. The closure itself is never dropped, when at the end of the block when nothing references it? What about the stack frame attached to the closure?
Mm... good point. It would have to be an &amp;mut Bar for that behaviour (ie. You can only have a reference to it in one place). My bad.
&gt; It's not categorically better. It's also not categorically worse. I'm completely happy to agree with that. Some of your other points are dubious, but I don't want to fight about it. I'm happy to disagree with you on a few of the points you've raised. *I think* that the bulk of serious rust code that's out there at the moment, demonstrates that practically speaking references are best when used as such; temporary borrows for fixed scopes. ...but sure, I'll accept that Rust doesn't particularly favour one over the other, for some of the relevant points you've raised (there definitely is a cost in using abstractions).
a closure without variables doesn't have any stack frame attached to it – it is just a function pointer (+ a null pointer for the non-existent stack frame, because closures are 2-pointers long), and functions are not freed (you can see this [here](http://is.gd/SgAqlm)) If the closure *does* have variables, then of course it contains a reference to a stack frame, and it can't live longer than that frame (otherwise, it would be accessing freed memory).
I'm also happy to disagree, and can probably even guess what points you disagree on, since one or two were a bit specious :) I don't disagree about the bulk of serious Rust code out there. However, I think that's probably not representative of the language's capabilities, for a variety of reasons: * Much of the more complex code was written when there was still @mut T, and was thus hastily converted to Rc&lt;RefCell&lt;T&gt;&gt; even where that was not necessary. * Lifetimes have gotten progressively more powerful in Rust, and mutability rules stricter and more sound. Many of the usecases for which I'm currently using &amp;references would not have been possible in Rust 0.11, but were in Rust 0.12--so this is relatively recent stuff. * Partly for the above two reasons, there's a significant lack of documentation on advanced lifetime use, so it's very hard to figure out what's actually possible at the moment. Now that I rarely find myself fighting the borrow checker much, and have internalized ways to quickly resolve common errors (two minutes instead of two days), I've been using references with named lifetimes pervasively in my own code and found to work quite well in practice. Sometime soon, I plan to write down what I've learned in the hopes that others will find it useful.
A common pattern in C++ land is to put both the map and the data into the same struct, so they get destroyed together. Correlated lifetimes are managed via objects instead of independently on the stack. Is there a terse idiom in Rust that "just works" for this case? struct Foo { data: MMappedFile; hash: HashMap(&amp;str, &amp;str); fn new(filename: &amp;str) -&gt; Foo { let data = map_file(filename); let hash = HashMap::new(); hash.insert("piece1", data.at(0)); hash.insert("piece2", data.at(64)); return Foo {data: data, hash: hash}; } } Alternatively, the map's scope is fully contained within the data scope. Is there something terse for this pattern? // Can only be used within the scope/lifetime of new's argument. struct Foo { hash: HashMap(&amp;str, &amp;str); fn new(self &lt;= data: &amp;MMappedFile) { let hash = HashMap::new(); hash.insert("piece1", data.at(0)); hash.insert("piece2", data.at(64)); return Foo {hash: hash}; } } fn FooConsumer() { let data = map_file("lucky.txt"); { let foo = Foo::new(data); // do something with foo } // do something else with data } The idea being that one rarely wants / needs complex lifetime management, either in C++ or Rust. Equiscoping or subscoping are sufficient for the vast majority of the cases. It would be nice to know there are simple idioms that don't scare newbies (and veterans!). 
&gt; I think the complexity of lifetimes is kind of like the various restrictions on quantum computing This is being rather unfair and unproductive: I don't think lifetimes are particularly complicated, nor should they be regarded as something scary. They're simply connecting a pointer to the scope which owns the data it points to. At the moment, it just takes a bit of practice to get the hang of them. Hopefully we (anyone teaching Rust/lifetimes) will also practice and improve how they are taught, but describing them as something mysterious or scary is definitely not the right way (maybe it's slightly unfair to describe quantum computing as mysterious and scary, but that's the connotation society has... no need to attach it to lifetimes too).
To be pedagogical, you can never safely have an aliased &amp;mut reference, but I know what you mean. However, Rc and Arc don't offer that functionality either; they act just like &amp; references in that respect. The closest they come is make_unique, but that has such specialized behavior that I honestly can't quite figure out when it's a good idea to use (the only time I thought it was doing what I wanted, it turned out to be a bug in some of my unsafe code :(). Internal mutability is more a job for Cell, RefCell, Mutex, RWLock, the atomic types, etc, which you can use with &amp;references just as easily as you can with Rc or Arc.
There is a caveat that I do not know how to get around that I will explain, but here is the code first: struct Foo&lt;'r&gt; { data: MMappedFile&lt;'r&gt;, hash: HashMap&lt;&amp;'r str, &amp;'r str&gt; } fn new(filename: &amp;str) -&gt; Foo&lt;'r&gt; { let mut foo = Foo { data: map_file(filename), hash: HashMap::new() }; foo.hash.insert("piece1", data.at(0)); foo.hash.insert("piece2", data.at(64)); foo } The caveat: You'll notice first of all that I have moved the data into the structure right at the start of the function, as far as I can tell you have to do this to achieve this. To explain why, I will first explain the lifetimes in the structure definition. By parameterizing a structure with a lifetime, the compiler insists that lifetime lives as long as the structure itself. This is kind of implicit, but will be intuitive after you write a bit more rust. Now that the lifetime is in scope, you can use it to parameterize the MMappedFile, telling the compiler that that file will live as long as the structure does. We can now use this same lifetime parameter for the `&amp;'r str` references in the HashMap. You can see now how the compiler can guarantee that the data in the hashmap is valid, as you told it that the data within the strings must cannot outlive the data in the mmaped file. The reason for the caveat: In order to get those `'r` lifetimes to line up, the structure has to already exist, because It's the structures lifetime parameter that binds the two members lifetimes together. So it has to come first, not at the end as in your paste. I don't know if there's a way around this, but It's not that much of a change really. **Keep in mind the above is a fake API, for a quick example of the same thing you can try quickly on play.rust try this one:** struct SelfVec&lt;'r&gt; { data: &amp;'r str, list: Vec&lt;&amp;'r str&gt; } fn main() { let mut x = SelfVec { data: "Foo", list: Vec::new() }; x.list.push(x.data); x.list.push(x.data); }
Thankfully these types of issues should be going away in the near future. https://github.com/aturon/rfcs/blob/collections-conventions/active/0000-collections-conventions.md#the-equiv-problem
'static in Rust is kind of weird. It is just the longest lifetime bound, and : means "outlives." So T: 'static doesn't tell you anything about individual instances of T, just that the type T is defined for any lifetime bound 'a, since 'static: 'a for all lifetimes 'a. As a case in point, Send: 'static and Mutex&lt;T&gt; only works for T: Send, but you can easily define a Mutex&lt;uint&gt; because uint is defined in every lifetime. Lifetime bounds on closures can be thought of as bounds on the equivalent unboxed closure structure. So the stack doesn't factor into it (nor do the function parameters) unless it closes over something. When it doesn't, it's basically just a zero-size struct. Zero-sized structs are defined everywhere unless otherwise specified, so it's easy to see that it should be 'static. IMO it's a rather confusing name.
Thanks for the answer. But it is somewhat unsatisfactory that we have to explicitly deal with 'r annotations and angle brackets for common design patterns that are completely obvious to humans. The terse notation in the examples from my post above should be completely obvious and unambiguous to the compiler as well.
Heavily agree. Especially when you have to introduce a lifetime after the fact, and then refactor it **everywhere** else. It's really bad. I have no idea how it can be made more friendly. Having said that, in the Rust code I have written so far, 99% of the time lifetimes are abstracted away behind the libraries Rust provides. I'd like the above case to somehow be solved, but it does seem that if you write correct rust lifetimes stay out of your hair.
The module system is perfectly fine as it is. Worked for python the last 20 years, I never heard a considerable complaint about the module-per-file aspect. Edit: I actually thing that the current state is a prove of how well it works. Considering how much Rust changes at the moment it is astonishing how good projects are able to follow the progress of Rust.
The problem which causes a lot of pain to me is that it's very hard to split `impl` across a lot of files. In my Markdown parser I use a lot of nested modules which add new methods to the top-level parser struct, but this requires adding a new trait in each such module; this becomes even more painful if I need some internal methods on the parser in each module which are not supposed to be public; this means even *another* trait, local to the module. And adding traits for extension methods means writing method signatures twice, so if they change, I need to update them in two places too. It would be really great if Rust supported adding `impl`s for structures coming from parent modules. It worked some time ago, but it was broken with static methods and it was removed recently.
but does python have the same mod/use split micromanagement as rust? Doesn't it just have a graph of `imports` between files? I think python is more like my suggestion 7 above... if one module imports others as * , don't they just come through fine from others ? Module per file wouldn't be bad if they 'managed each other' in a graph. Its the circular glob bug that's killing me , really, and the separation of use/mod. The python idea makes more sense to me, because you directly express file dependancies &amp; namespacing imports at the same time.
&gt; I have thought about writing an external xpath library, which is possibly less familiar than CSS selectors to webdevs, but isn't too tricky otherwise, for doing this sort of thing. And "static" CSS selectors (those you'd use in scraping) can fairly easily be compiled to XPath (which is useful when doing class-based selections, because that's horrendous to do in XPath unless you've added a custom XPath function for that)
A lot has been said here already, but let me try to answer as well :) &gt; One aspect of Rust though seems extremely unsatisfying to me: lifetimes. First, let me say that to not get confusing answers and long discussion about how "Rust without lifetimes is not Rust", its important to clearly separate two things: There is the _concept_ of lifetimes that is ingrained into the typesystem and how Rust works, and there is the _syntax_ for _named lifetime_ paramters, which exist because the compiler can not reasonably infer the actual lifetime configurations without the user of the language having no idea whats going on most of the time. Most of your gripes seem to be with user interface for lifetimes, that is the syntax, and thats valid critique and possible to still apply tweaks too. But the core concept and semantic of lifetimes will not change. --- &gt; Their syntax is ugly. Unmatched quotes makes it look really weird and it somehow takes me much longer to read source code, probably because of the 'holes' it punches in lines that contain lifetime specifiers. There where months of discussions and proposals and alternatives before this syntax got picked. In the end, while no one was entirely happy with it for the reasons you stated, it was the best fit for the very constrained syntactic space Rust has. And you will find that after a little while, your brain will have no problem differentiating a leading `'` in the type grammar from a matched set of `'` in the value grammar, just as it has no problem with differentiating matching `&lt;&gt;` in the type grammar and unmatched `&lt;&gt;` in the value grammar. --- &gt; The usefulness of lifetimes hasn't really hit me yet. While reading discussions about lifetimes, experienced Rust programmers say that lifetimes force them to look at their code in a whole new dimension and they like having all this control over their variables lifetimes. Meanwhile, I'm wondering why I can't store a simple HashMap&lt;&amp;str, &amp;str&gt; in a struct without throwing in all kinds of lifetimes. Again, once you've properly separated semantic from syntax this confusing lessens a bit. Fundamentally, the semantic of all lifetimes is that they start and end on the call stack, so concrete lifetimes are determined by the content of function bodies, and not inherent to an object itself. { let a = ...; // The *variable* a has a lifetime, not its value or type // The lifetime ends if a goes out of scope } Lifetimes in types mostly appear in form of references like `&amp;'a T`, where they express "The value of type `T` lives in a variable that is only valid for a specific lifetime `'a`". And because it does not make much sense to define your custom type to be only valid for the third `if` block in the function foo of module bar, most type definitions that contain references end up being generic over them, which leads to all these `&lt;'a&gt;` --- &gt; When trying to use handler functions stored in structs, the compiler starts to throw up all kinds of lifetime related errors and I end up implementing my handler function as a trait. I should note BTW that most of this is probably caused by me being a beginner, but still. Not sure what exactly you're trying to do here, storing function pointers? --- &gt; Lifetimes are very daunting. I have been reading every lifetime related article on the web and still don't seem to understand lifetimes. Most articles don't go into great depth when explaining them. Anyone got some tips maybe? Well, the name "lifetime" might not give you good results for general web searches yet, as its a kinda vague name and thus people use it to mean different things in different languages. And Rust itself also develops faster than old docs on some other sites can die, so you'll often find confusing old articles. For the time being, Staying close to the official Rust project is your bets bet for good docs: The official guide, recent blog post by Rusts core developers, the Rust IRC channel, etc. --- &gt; I would very much love to see that lifetime elision is further expanded. This way, anyone that explicitly wants control over their lifetimes can still have it, but in all other cases the compiler infers them. But something is telling me that that's not possible... At least I hope to start a discussion. I point I always make is that ellision != inference. Ellision in Rust currently only referes to a purely mechanical syntatic sugar you are allowed for function definitions (and soon impls), as those are cases where in most cases its always exactly the same thing you want: Taking a reference and returning a reference derived from a taken reference. Notably, the sugar is for the lifetime parameter itself, not for the type that has one and that that paramter gets applied to. Inference on the other hand would be for the compiler to actually deeply look into the type and all its component and figuring out everything itself, not requiring the type to have an explicit generic lifetime paramter in the first place. Which while doable, has one big problem: The three ellision rules for applying a lifetime paramter are easy to learn once, and then reverse in your head if you stumble over code that makes use of them. While inference means you don't have lifetime paramters to begin with, and requires you to look deep into the type definitions itself, and possibly many other places where the type is used, so you don't get local reasoning about a line of code anymore. And, again, it is not possible to use Rust without using the concpet of lifetimes, a reference _always_ uses them. 
If you value predictable results more than compiler optimizations, then it makes sense to default to the wrapping behavior. If you care that much about avoiding overflow, you can use `checked_add`. (You could even use a custom add that only checks for overflows in debug builds, then you would get something better than a linter IMO.)
I didn't mean "mysterious and scary" for either. That is entirely your interpretation of my words and I never meant to connote it. I was actually referring to something I read on Scott Aaronson's blog, in response to someone who said quantum computing would never work in practice--that in fact the "messiness" of quantum computing was exactly the sort of limitation you'd expect from something that worked in the real world. What I meant was that like quantum computing, Rust doesn't rely on magic, and the fact that lifetime management isn't totally trivial (hence not automatically inferrable by a compiler) is an indication that lifetimes are solving a real problem. Perhaps in retrospect I could have come up with a better example of a messy real world solution than quantum computing :)
&gt; If I'm feeling generous, at the very least, they are orthogonal: a type can be private and 'static, or public and not 'static, the two properties are totally independent and it makes a lot of sense to avoid muddying the waters by considering them independently. That's pretty much what I was trying to say--well, more specifically, I was saying that lexical scopes are not the same as lifetimes. &gt; Why is recursion and a base case overkill? It seems like the perfect way to define it, since types inherently have this recursive structure. It's not awful for a formal definition, I just wish there were a cleaner way to intuitively get the point across.
&gt; Unmatched quotes makes it look really weird Just think of it as apostrophes instead of quotes. :)
Yeah, this is what I meant, pretty much. (also, the name is Manish, but everyone gets that wrong anyway :P )
The problem is that you want `checked_add`(`_in_debug_builds`) 99% of the time, and `wrapping_add` 1% of the time (more or less just hash functions). 
&gt; That's pretty much what I was trying to say--well, more specifically, I was saying that lexical scopes are not the same as lifetimes. Eh, even (non)lexical scoping is orthogonal to the privacy of types. &gt; It's not awful for a formal definition, I just wish there were a cleaner way to intuitively get the point across. Any `'`s in the definition?
Your reaction is interesting because I come from a mostly C background and I absolutely *love* lifetimes. Many people in this thread are trying to explain the lifetimes and how they work, I think the best way to explain why lifetimes are good would be to just start every Rust tutorial with a C tutorial. Get the coders coming from more managed languages understand what it means to handle raw pointers and track down weird undefined behaviors because you were handling some freed memory. After that the lifetime/borrow checker and you will be best buds. The thing is, when you're writing that kind of close to the metal/not garbage collected code you *have* to do lifetime management. The only difference is that Rust actually bothers checking that what you're doing is valid which in turns means that you have to be more explicit about what you're doing. If you were writing C you'd have to ask yourself the exact same questions. If you didn't the only difference is that you'd get a segfault (if you were lucky) at runtime instead of a nice compiler message letting you know that what you're doing is unsafe (and sometimes even how to fix it). It means the compiler actually helps you instead of basically going "sure, whatever you say mate". To you lifetimes might look like an added weight, something slowing you down when writing code in rust but to me it's the exact opposite. In C every time I iterate an array, dereference a pointer or something like that there's always a small pause in my coding flow where the "NULL/out of bound pointer check" part of my brain triggers and forces me to double check that I'm actually doing what I think I'm doing. "Can ptr be NULL here?" "Is this function allocating the return buffer or should I provide it? And should I free it?" "What does this function return in case of an error?" It's especially bad because things like accessing freed memory, dangling pointers and out of bound access can just appear to behave correctly or at least not crash right away and make it very difficult to debug. In rust however I know that as long as I'm not writing unsafe code the compiler will tell me when I do something illegal or at worse I'll get an explicit runtime error for things that cannot be checked statically.
Sometimes it feels like we're standing on an island, the Island of Rust, with some other islands visible in the distance, like the Island of Scala, the Island of OCaml, the Island of Haskell, and the Island of C++. But in reality, we're all floating in a big sea of research and experimentation. This presentation summarizes *a lot* of the type system research that eventually became the foundation of Rust. Many of the ideas should be recognizable. (Sometimes even the vocabulary!) It's worthwhile to familiarize ourselves with the giants whose shoulders we're standing on. When I first found this and started reading, I thought, "hmm, this might be interesting material for /r/rust". As I read further, that became, "this should be required reading for anybody working on Rust". (Note: I'm not sure how to read many of the typing judgments either (particularly the ones with backwards turnstiles), but the text stands on its own quite well most of the time.)
FWIW [this Sublime Text plugin](https://github.com/glennw/RustAutoComplete) does the path-searching for you (tab-complete use statements, among other things), even for your own modules.
ok it seems some of my problems today come from trying to conflate hierarchy with dependancy a slightly too much, and of course being biased by what does &amp; doesn't work in C++. mod.rs -&gt; (a load of sources with horizontal interdependencies) -&gt; common.rs for external dependancies used throughout this directory I'd been trying to stuff 'common' bits in 'mod.rs' , and frustrated that I can't just use super::* deeper in, wanting one file that 'does the organizing/creates the context' Seems now you can actually make a submodule within mod.rs itself (for common) , avoiding needing another file - i'd tried that some weeks ago and the compiler didn't handle it. Although another might be a lot clearer.. I guess i'm being fussy wanting to avoid '2' common context files.. but maybe my thinking is biased because there's some situations where C guarded includes do actually sort themselves out if you have a circular include, (even if its not such a good idea and might cause other ordering problems..), and I'm used to being able to stack up the dependancies linearly in one 'master library include' (like #include vector.h .. #include matrix.h ..depends on vector, already in.. etc) its possibly because its' context rather than dependancy that i'm expecting circular arrangements should work. Maybe if a circular glob-import could be detected and spit out a better error message this would feel more natural, and this common submodule does basically do the job of "inherit-use" that I had wanted. 
I admit that I struggle with lifetimes. I find the syntax a bit confusing and adding them means inserting the lifetime marker at a lot of places. Also the fact that they are declared similar to generics is sometimes a bit odd. Then again, I am not the person that puts too much weight on syntax oddities, I am interested in what the system does. BUT. I love lifetimes. As a person that spent most of their time in languages with GC, my whole thinking about this was "lives until the garbage collector cometh". When doing C, thinking about them was a pain, because they are something to think about, but a concept stemming from how the whole rest of the language worked. Rust makes them explicit and something that I have to deal with if I write code where I have to think about them. While they are still hard for me (because of my bad training), they also help me a lot.
Btw, just today I tried to use it and Racer ate all my RAM on method completion x_x It was ok before, should probably fill an issue..
i think I know what he's talking about - you've got to make a trait to extend a class, and once you've made a trait, you can't see those methods unless you also bring that trait in; I've just not been bothered with this and just glob import everything. Its more names, more micromanagement Splitting functions on a type between multiple locations is basically one of the biggest reasons I want to escape C++, its needed for dependancy reduction. You can do it with free functions in C++ but then you're bouncing between method &amp; function syntax which looks messy and doesn't exploit autocomplete
It would be nice to make `MMappedFile` an owned type rather than a borrow, so it shares `Foo`'s lifetime automatically. This would require specifying "the lifetime of the containing struct" or "the lifetime of a sibling field" or something, which I don't think is possible at the moment.
First thing noticed: exposing raw pointers through public API isn't a good idea, they'd better to be wrapped and used through wrapper method. Also there are a couple of places where borrowed pointer is sent directly to C API, I'm not sure it is a good approach.
Thanks a lot for your valuable input! We know that our current interface is not safe, and we are really looking to improve our approach in the future. For instance, we'd rather want to have a native struct for the socket, and have the various functions as impl, but we are trying to figure out how to access the structure in the callback: the callback gives a pointer to the C structure, and we are trying to design a fast mechanism to provide the object as the argument of the native wakeup callback. 
I was referring to my project [here](http://github.com/netvl/md.rs), in particular, things like [these](https://github.com/netvl/md.rs/blob/0afc91433180f4f88d2eba86e5741dd59b0781ea/src/parser/block/lists.rs#L45-L53). I tried writing a macro which would remove such boilerplate, but I weren't able to do so because Rust does not like `$($args:tt)*`-like patterns, there is even a bug on that. Or do you mean my last sentence about broken static methods? I don't have an example at hand, and anyway, there was an already implemented RFC about restricting `impl`s for in-module types only, so it wouldn't work now at all, I believe.
No, I didn't mean `use`ing traits, this is not really a problem. I meant *writing* such traits in the first place. While it is understandable that you can't add `impl`s to completely unrelated types (from another module, for example), it's much less understandable why you can't add module-local `impl`s for types from *parent* modules. After all, child module is a part of parent module - just look at `--pretty=expand` output.
Well, I suspect that for number crunching `checked_add` is too slow even for debug builds. If you consider performance, I'm not sure that you want `wrapping_add` only 1% of the time.
You only want wrapping *semantics* 1% of the time.^1 You want it to be *unchecked* (as opposed to necessarily *wrapping*) for performance more than 1% of the time, but in that case you're not depending on overflow wrapping, rather it is your belief that it's not *going* to overflow. ^1 this statistic pulled out of my butt, but feels accurate
Thanks so much, your post is what made lifetimes "click" for me!
ok i'd agree submodules should have parent visibility.. they should be a refinement of whats above. (and if you want to hide something, you can shove it in its own subtree)
Aha, thank you. (Yes, this is what I meant.) Am I interpreting it correctly that you need to jump through these hoops to get *method syntax*, and that freestanding functions would also be a solution, just less 'nice'?
Wow, this is great, thanks for the share! How would you go about mapping the terminology from these slides to Rust? `'a` is a region, and `&amp;mut T` is a unique type?
Something like that. I think Rust's lifetimes are simpler in a way than regions as here. (Might be interesting to make a cheatsheet of which things from which slides correspond to which Rust things, at some point, but I don't want to take away the fun of people figuring it out for themselves.)
Officially, linear types must be used *exactly* once and affine types may be used *at most* once. (Note that "use" doesn't *necessarily* correspond to "mention".) Practically, what this means is that linear types must always be explicitly consumed, whether by passing ownership to another function or actually "destroyed" with something like `free()` or `drop()` or `close()`, while affine types are allowed to fall out of scope silently. (In both cases, you're not allowed to use it after it's been consumed, i.e. the "at most once" restriction.) Rust's case is kind of interesting, because types with destructors *are* allowed to silently go out of scope, *but* the compiler automatically inserts `drop()` calls in those places. So it could be interpreted as either affine or linear types, depending on your perspective. (I'm not actually sure which interpretation is more theoretically correct.)
I thought the point was that I want unchecked additions sufficiently often, and to make it predictable I most conveniently make it wrapping. The only reason that it is wrapping is that this is the most convenient predictable behavior. (Addition is still fulfills the group axioms. If we make it not wrapping, we lose associativity, commutativity etc.) (I'm not sure whether we actually need predictable overflows, but that was the premise of my comments.)
Thank you! Linear types make sense to me, but I'm still trying to understand what the use case would be for affine types. What would be a good example of an affine(ly?) typed variable that could go out of scope silently? Could it be something as simple as: fn do_things() { let x : i32 = 3; } Is any stack-allocated variable without a destructor considered affine? &gt; Note that "use" doesn't necessarily correspond to "mention". ...you just blew my mind. How would one use an affine typed variable without mentioning it?
I see. I think doing it through playing with `priv` should work pretty fine and with a bit of wrapping could be O(1) (just one extra function call), will try to write a sample code later. But the general idea is the following - set pointer to object into `priv`, provide an internal callback which takes `priv` and threats it as object and later on call user provided callback with object as parameter.
One quick question: is it right that socket after `accept` works with the same callback?
yes, the callback is shared between the listening socket and all the accepted ones. For more info about our C API, see the [User manual](http://162.13.84.104/user_doc.pdf)
I care about *everyone* avoiding overflows (and bugs, in general), and let's face it people will routinely use `+` rather than `checked_add`; it even composes better: 3 + 2 + 2 3.checked_add(&amp;2).map(||(a: int) -&gt; Option&lt;int&gt; { a.checked_add(&amp;2) }) 3.checked_add(&amp;2).unwrap().checked_add(&amp;2).unwrap() Let's be honest, `checked_add` is so overbearing it's a non-starter to use it extensively; and certainly nobody will extensively. However, I am not proposing here that `+` should behave like `checked_add`; what I am proposing is that we make the result of `+` unspecified so that static and runtime analysis can immediately point their fingers at any overflow (or underflow). You could activate special build options to *trap* on overflow for example (like Clang does with `--sanitize=ubsan` for undefined behavior). It is not, therefore, a matter of performance, nor is it really a matter of predictability. It is first and foremost a matter of *correctness* of the code, where instituting wrapping disable all automated assistance.
Depends on your perspective/interpretation. Conceptually `panic!()` inhabits every type (as does `loop { }`). So if you're willing to think of these as "a value" of the given type, then yes. Otherwise no. But the RFC I linked makes it explicit with "unspecified result or no result" (for the exact reason you mention).
A style nit: Rust isn't an acronym, so you don't have to shout it! :) (Although I guess it could be an acronym... **R**eally **U**nsafe **S**tuff **T**aboo?)
I just want to thank you for mentioning this. I've been working on a safe API around an unsafe FFI and I've been wondering what sort of best practices there are for this sort of thing. You answered one of my big questions (I was going to do Vec&lt;T&gt;, also).
What alternatives exists to sending borrowed pointers directly into a C API? I only ask, because my current project has been using this for data coming from rust into the API I'm wrapping.
Note: - `Vec&lt;T&gt;` means that the function *consumes* the buffer (though it may after return it or another or nothing); no C correspondence (no linear types in C) - `&amp;mut [T]` means that the function *refers* to the buffer, and may modify its content but not its length; ie `T* + size_t` - `&amp;[T]` means that the function *refers* to the buffer, and may not modify it; ie `T const* + size_t`
&gt; Just so I'm sure I follow: &amp;mut T is affine because it is: &gt; &gt; * not `Copy` &gt; &gt; * not `free()`ed, `drop()`ed or otherwise destroyed as it leaves scope Essentially correct. &gt; Why might one have a `&amp;mut T` variable but not use it? Simply to freeze what it references? I'm still trying to wrap my head around the zero-use possibilities of 'at most once'. This is where the word "use" introduces confusion. :) Reading or writing through an `&amp;mut T` doesn't count as "using" it *in this sense*. Let's use the word "consume" instead. An `&amp;mut` reference doesn't need to be consumed, i.e. freed or dropped in any way. You can read/write through it and then it goes out of scope. &gt; Would `Box&lt;&amp;mut T&gt;` not be considered affine because it would need to be `free()`ed? If Rust required explicitly calling `free()` or `drop()` on it, then definitely (it would be linear). As it is, you *can* call `drop()` explicitly, but the compiler will do it for you automatically if you don't. So whether it's linear or affine depends on whether you define linear as the programmer explicitly consuming it (in which case it's affine), or just as it being consumed whichever way (in which case it's linear). (As in my original comment. I'm not sure which definition is more correct.)
Interesting, I would actually have supported the *reverse* idea, ie that a parent could have an `impl` for one of its child's `struct`. Why? Because coherence: A child cannot ensure that another child is not going to implement the same trait for the same type; a parent could. A solution might be to simply drop the *one (struct, trait) =&gt; at most one impl* rule and adopt something else: - maybe it could be one *visible* `impl`; seems brittle though - maybe it could be *naming* `impl`s - ...
If people really almost never rely on wrapping addition, then your automated assistance can still work and you'll have very few false positives. Maybe the best option is to add an explicitly wrapped addition? It could just to the same as normal addition, but it would make the intentions clear. Why do you want unspecified behavior? For optimizations?
It's fine when the object will only be used for the duration of that function call. If the C library will store that pointer and use it later, then you probably should allocate it in the heap. The easiest way is to make a `Box&lt;T&gt;` and transmute it to `*const T` or `*mut T`. When it's time to free the object, transmute the pointer back to `Box&lt;T&gt;` and let it fall out of scope. Safely using a C library from C requires the same distinction, so it will be clear in the docs (or else it's a major bug in the library).
I also was surprise that no two word owned strings, maybe three words of current String is really not big problem for modern devices. Also deriving implementations use String.
Awesome! 
The thing is, the lifetime of an object is a concept applicable to any language; in short, it represents the time when you can use that object. In many languages, such as Java or Perl, objects also have a lifetime: - Java: an object lives until terminated by the Garbage Collector (some time after it becomes unreachable) - Perl: an object lives until its reference count drops to 0 or until the end of the program, whichever comes first however they are not directly exposed to the user. Many users think this is great (worry-free!), until they have to track a space leak and try to figure out how long each object lives (and why some live much longer than predicted). Users of non-managed (barebone?) languages such as C or C++ are more aware of lifetimes: a dangling pointer is a pointer to an object whose lifetime expired, when trying to get to the object through it you get incomprehensible crashes, corrupted memory, etc... a whole lot of fun. What Rust does, actually, is simply to track lifetimes *explicitly* in the language in order to allow both the user and the compiler to reason about them. A pre-requisite to understand lifetimes is of course to understand *reference semantics* as in the case of C pointers: that is references which, unlike in managed languages, do not extend the lifetime of what they refer to.
It's important to point out that Box&lt;T&gt; does NOT use C's malloc, and C libraries that try to free a Box&lt;T&gt; will do Bad Things.
This is EXACTY how I feel.
I'm not so sure about `Box&lt;str&gt;` but I think there are use cases that will benefit from having better dynamically sized types support in Rust. Being able to do something similar to C++'s `new[]` operator with `box` might be nice. I'm wondering if it might be possible to have a way to create instances of dynamically sized structs/arrays on the stack (using some compiler generated `alloca` calls) and on the heap (using `box` with some extension) without needing `unsafe`.
Nice! That seems vastly more useful than [the Reference's list of influences](http://doc.rust-lang.org/reference.html#appendix:-influences-and-further-references). Perhaps we should consolidate the two?
* bindgen: I tried it, but gave me so much extra types I didn't need (pulled in from header the initial header included), and probably I did not try hard enough. * public C signatures: that is indeed a good point! will make 'em public. EDIT: about bindgen: just realized that -match can help me, will consider using this as a starting point indeed!
We'll need `new[]` for `&amp;[str]` too. As far as I can tell, `alloca()` would make it impossible to access existing data in the stack frame since the offsets would be dynamic. I guess one could store the offset too, but then the code will have to subtract pointers for every access of a local variable.
But what if you value *correct* results above everything else? Specifying int behavior as wrapping will prevent the compiler from trapping overflows even on architectures where this is implemented in hardware (MIPS, for example, and, I am sure, there will be more in the future), because the compiler won't be able to tell whether the wrapping is was intended or not. I think Rust should leave it as unspecified to leave this door open. At the same time there should a documented way to get wrapping behavior for the 1% cases when it is required. Whether this should be done via defining separate "wrapping int" types, via attributes, or as functions is up for debate.
I'm going to be the third person to say, "I don't think it's very fair to say..." I didn't say "something more digestible"; I said "something *to make them more* digestible". I know that their existence is a great value to the language, but I have zero-experience designing a language. Speaking only as a user of languages, I merely infer that I want to use lifetimes without writing an additional 15+ LoC to satisfy the type safety (as shown in Chris Morgan's article). I'm not proposing an alternative or a solution, but I can provide feedback to those who do focus on the language design. Maybe I'll be satisfied with all the changes at v1.0...
/r/playrust This is for the programming language.
We're definitely having another one, as far as I see.
I didn't realize that Godoc had that feature, but that's awesome, and now I find myself wishing that Rustdoc had a text option as well.
Python does not have `mod` as it doesn't need it – Python-3's `import` behaves like `use`. Also, glob imports are discouraged in python. Rust's `mod` isn't particularly interesting – you have one use per file to create the module tree and that's it. It's more of a substitute to big makefiles than anything.
oh lol sorry :(
I would expect either `Option&lt;&amp;T&gt;` or `Option&lt;Box&lt;T&gt;&gt;` if the values are always originating in Rust code, or just `*[const/mut] T` if you're being handed them from C code.
I wonder if this can be solved by having a JSON output option for `cargo doc`. Then any number of documentation tools could be built on that. One problem is that currently links in documentation are just normal HTML links assuming a certain structure, and not semantically pointing somewhere else so that a different renderer can use a different way to refer to them. There is some discussion about it [in here](http://discuss.rust-lang.org/t/rustdoc-restructuredtext-vs-markdown/356).
&gt; we don't have any owned strings which are immutable/ungrowable (memory efficient) What would be the reason for the memory efficiency? Not having a capacity field?
Yeah, I was leaning towards `Option&lt;&amp;CustomPtr&gt;` for passing the pointers in, since that does null-pointer optimize.
Nice! That will be very useful with my [Heroku buildpack](https://github.com/emk/heroku-buildpack-rust) for Rust.
Is this how `alloca` works in C?
Yes, something like that. This means that functions containing `alloca` are generally slower. For this reason, LLVM will not inline functions using `alloca` unless the caller also uses it.
I suppose it should be possible to minimize this by using `alloca` last (as in after all the local variables are already on the stack), but there's no way to avoid this problem entirely.
Does it have to do with the printlns?
No, i forgot to remove them, before posting, cause I was testing the output with a small array... I removed the printlns and the performance is : &gt; The amount of time needed to sort is 15568 to sort 20k elements ...
What might be hurting you here is the bounds checks done by the indexing operator, `[]`. If you're absolutely sure you won't go beyond the bounds of the slice, then you can do `.unsafe_get(uint)`, which eschews these checks. Note that this has to be done in an `unsafe{}` block.
Maybe s/he's thinking about persistent strings, strings that are immutable and share structure? Though I guess that kind of string wouldn't be straightforward to use without some kind of garbage collection(?).
You almost never want this, since it's incompatible with mutable borrows. If a value that can contain a reference with the same lifetime as itself is mutably borrowed, then the borrower could mutate it to contain a mutable reference to itself. This means that any future access to the value could result in aliasing mutable memory, which would make Rust's memory safety model unsound.
&gt; So it could be interpreted as either affine or linear types, depending on your perspective. (I'm not actually sure which interpretation is more theoretically correct.) Doesn't it just become a matter of answering: "Does the compiler allow me to try to `drop()` this more than once?" If not, I don't see any reason not to call it a linear type.
Funnily enough there has been working on using a variation of linear logic to model a type system for quantum programming languages. You can find a related thesis here: http://personal.strath.ac.uk/ross.duncan/papers/rduncan-thesis.pdf
Something like https://github.com/servo/string-cache ?
I wish i had the time to go through your code and find whats causing it to be slow since a difference that large between your speeds should not happen. Algorithmically, Heapsort and Mergesort both have a worst case of around O(nlogn) which is probably what will happen on a unsorted list of random ints. http://en.wikipedia.org/wiki/Heapsort#Comparison_with_other_sorts This article sheds some light into the ideas behind cache misses and other things which may be a factor when comparing heap sort performance with merge sort. That said, I'm more apt to believe that there is something underperforming here in the Rust code you have. Are you compiling with -O2 or -O3 like you should be? If not, there could be some key optimizations you are missing out on. Some one else already mentioned removing bounds checking, but in my experience, bounds checking only cut .01% of my application when I tested it. (It can't really hurt to add it though if you want!) In your mergesort, are you using array.swap there as well? I looked at the source of the swap function but it didn't look like there is any reason why it'd be slow. Other than that, I'm not sure (I hope someone can clarify this) but you are calling `&amp;mut [int]` which causes a move correct? If its a move, does rust have to actually move/copy the memory around or is that optimized out to only fiddle with pointers? I'm not sure of the answer, but if it is moving the actual memory then that sounds pretty expensive. (I kinda think its not though) The last resort I would try is to rewrite it and just use the version of the algorithm that is on the wikipedia page :) Usually they are very good at explaining things such as why this version is good and bad and what things can be done to make it better.
I'm probably not as familiar with the algorithms as you are (it's been a while since university), but it seems like you're probably running into cache-locality issues. Algorithmic complexity is nice, but in the real world your code runs on a CPU with a limited amount of fast cache. What is happening in the heap sort (judging from what I can see of the algorithm) it will be doing more random jumps, which the prefetcher finds much harder to follow (so your CPU sits idle waiting for memory). Merge sort has much more linear search behaviour and adjacent accesses, which are much (much) easier for the prefetcher to pre-empt. This is supported by [Wikipedia's article on Heapsort](http://en.wikipedia.org/wiki/Heapsort): &gt;Merge sort on arrays has considerably better data cache performance, often outperforming heapsort on modern desktop computers because merge sort frequently accesses contiguous memory locations (good locality of reference); heapsort references are spread throughout the heap. For more information you might also check out [this video](https://www.youtube.com/watch?v=YQs6IC-vgmo) which gives a good example of how hardware realities can trump algorithmic ideals.
hahaha, please read the sidebar
I haven't studied your code too closely, but at a first glance the recursion inside the loop in `max_heapify` looks suspicious. If you want a reference you can look at `rust/src/libcollections/priority_queue.rs`.
`Box&lt;str&gt;` is supposed to just work, but discouraged because using `String` directly is usually faster to work with.
Ah, like this? struct Foo { x: int, y: &amp;'&lt;something magical&gt; int, } fn foo(f: &amp;mut Foo) { f.y = &amp;f.x; // f and f.y mutably alias } I suppose there's no immutable way to construct an object like that anyway, and even if there were the compiler wouldn't be able to distinguish aliasing mutable references from non-aliasing ones.
Is Rusts 'mod' a throwback to the time that the module tree wasn't strictly enforced to be the directory structure. My suggestion #7 is to fuse use/mod, and assume the namespace structure is the directory structure (plus local in-file mods); follow the graph to compute the set of used sources, and you get to easily pick a subset for , say, per-platform builds, or testing (I believe it would be handy for *any* source file to be a potential build root, for testing or example purposes ) - and it would save users "bouncing" between module/crate granularity choices.
yeah I've already made an RFC almost exactly like that and it was rejected. `use mod foo` and `use mod foo as f`it would be temporarily more complex, but intended to replace the existing behaviour. I believe the existing behaviour is a throwback to when Rusts didn't enforce directory structure mod-graph; and now it would be more convenient to express different information.
Ah, thank you! If I were implementing the Iterator method myself, I'd change the signature. However, it's a part of the servo url crate, so I would assume those authors are more correct than I. Does that mean I should use `Vec::into_iter` instead? The reason I put the explicit lifetime paramater on the `mut_fn` is for a bit more complicated reason. When I tried without, I got error: cannot infer an appropriate lifetime for autoref due to conflicting requirements If you don't mind, I'll ask you the bigger question: is it possible to have a parent struct share an owned variable, in particular a `String`, with child structs, in this case as `&amp;str`s? That's why I needed the lifetime; I imagine the borrow checker isn't happy that I am lending out a variable somewhere for an indefinite amount of time. Is there a way using lifetimes to say I want this? Here's a short, trivial example: struct Parent&lt;'a&gt; { children: Vec&lt;Child&lt;'a&gt;&gt;, content: String } impl&lt;'a&gt; Parent&lt;'a&gt; { fn add_child(&amp;mut self) { self.children.push(Child{ content: self.content.as_slice() }); } } struct Child&lt;'a&gt; { content: &amp;'a str } fn main() { let mut p = Parent{ children: Vec::new(), content: "asdf".to_string() }; p.add_child(); } Is this just not possible without delving into `Rc`s and `RefCell`s?
Would `box(malloc)` make sense?
Don't forget about swap, which will also do bounds checks! Just write it yourself.
I'm trying to mimic the API exposed by `std::os::env` so that they can be substituted on the fly. Maybe I should expose iterators as well?
Maybe I'm just dense, but I don't see how a `&amp;[str]` could work, or what the semantics would even be. Could you explain?
Yeah, this is what we do in glfw-rs: https://github.com/bjz/glfw-rs/blob/master/src/lib.rs#L371-L433
i was putting an item dependancy graph in here, that could easily be modified to just spit out a call graph. It can spit out dot files. The tool does also give you a summary of callers in the HTML view (listed at the bottom of the page, I haven't researched how to do fancy popup menus etc) Unfortunately its not compiling at the minute. Needs to be updated for the latest version.. I wasn't sure how much the compiler AST api was going to change in future, and I had some months away from Rust. https://github.com/dobkeratops/rustfind 
Profilers like perf, callgrind and Instruments.app work with Rust.
Not having a capacity field means that you allocate exactly as much as you need without the buffer.
Yeah, especially the type notation :[
I just looked at the source; seems like we're already memory efficient here. It uses `alloc_or_realloc` instead of preallocating a buffer (though that can be done too)
It's a DST thing. I think we can access its contents with some combination of the borrow and index operators. Not sure if there is sufficient support for that though.
String cache is awesome, but not exactly what I meant :)
Really? So if I say something like let foo = 42 then the constant `foo` can be used as any of those types? Did I miss that in the documentation somewhere?
This link should be in the sidebar.
If that really is a DST thing I'd certainly be interested in details on that. `str` is unsized and in that type there is no fat-pointer to store its dynamic size. Which is why I was saying I don't see how that could work. Is there an implicit size somewhere with that construct? Are all elements of the slice supposed to have the same length? FWIW, right now rustc ICEs when trying to declare a variable of that type.
That's not true. Rust forbids all memory unsafe code outside of `unsafe` but it also forbids a large subset of useful, correct code. For example, nearly every collection in the standard library is ending up being based on `unsafe` code. It's only *necessary* to do that for `Vec&lt;T&gt;` but it's not possible to express many patterns optimally in safe code.
There are methods to convert `Box&lt;[T]&gt;` to and from `Vec&lt;T&gt;` and it's possible to coerce `Box&lt;[T, ..n]&gt;` (thin pointer) to `Box&lt;[T]&gt;` (fat pointer with a runtime length). There isn't a way to create `Box&lt;str&gt;` right now AFAIK because `"foo"` is `&amp;str`, not a fixed length variant of `str`.
I reported that ICE yesterday, [apparently it's not present in the nightly](https://github.com/rust-lang/rust/issues/18280) I'm not too sure how it gets stored. `&amp;str` is a fat pointer, but `str` itself doesn't store the size. I'm not very clear on how DST works internally so I can't tell how `Box&lt;str&gt;` would work.
Oh, didn't know that. However if you're dealing with an immutable string, you'll never call `push` so it's automatically non-buffered.
(Moved to here, it is really an answer to the original poster.) /u/Veddan's answer is correct. Judging from the comments I'm sure you (the original poster) have literally translated the pseudocode from [Wikipedia](https://en.wikipedia.org/wiki/Heapsort), but you are missing a `heapify` function (the function named `max_heapify` is actually a `siftDown` function). Two functions are different in that: * `heapify` constructs a heap out of the unsorted array, by adding each element to the in-array heap. It calls `siftDown` *n* times. * `siftDown` recovers the heap property when only a subtree starting at `a[start]` is known as bad. It is both used by the `heapify` (for the bottom-up construction of a heap) and by the main sort routine (for popping the maximum element out of the heap while maintaining the heap property). With a proper fix I've got the algorithm correct, and the resulting sort takes 2.0ms for 20,000 elements while the stdlib sort takes 1.4ms. Not quite bad!
It is `!`; [example][pp]. [pp]: http://play.rust-lang.org/?run=1&amp;code=fn%20main%28%29%20{%0A%20%20%20%20let%20x%3A%20u8%20%3D%200b10010111%3B%0A%20%20%20%20println!%28%22{%3A08t}%22%2C%20!x%29%3B%0A}
For what it's worth, this is also documented in the [reference](http://doc.rust-lang.org/reference.html#unary-operator-expressions) but not in the [guide](http://doc.rust-lang.org/guide.html) (actually, no bitwise operators there). We will definitely have hard time to explain things only in the reference. :S
It's rather unlikely that bounds checks will be the difference between 3.7s for 10 million elements and 15s for 20 thousand. This is a 3.5 *order of magnitude* difference*, while bounds checks are almost certainly much less than this (it would be surprising to me if it was a single order of magnitude). We should be encouraging profiling and algorithmic improvements before recommending `unsafe`. *The ratio between the quantities `time/(n log n)` for the two algorithms is about 3300.
`Box&lt;str&gt;` is a fat pointer, similar to `Box&lt;Trait&gt;`, `Box&lt;[T]&gt;` and `Box&lt;DynamicallySizedStruct&gt;`. Last I checked (many months ago?), only `Box&lt;Trait&gt;` is really supported. I'm not sure if that has changed.
That is a different ICE. I'm somewhat certain that `&amp;[str]` (unlike `Box&lt;str&gt;`) is an illegal type, even if DST was fully implemented. Happy to be proven wrong though.
Yes, boxed traits work. I'm not clear on the actual implementation of DST for boxes (on mobile, can't check), but boxed strs should work, agreed. 
Thanks for the suggestion. I did it almost as you said: `pico_stack_init()` is now the constructor for the `stack` empty struct, and `stack_loop()` is now a method. Not sure about driver initializations yet, I guess we need a more elegant interface for that as well, but at this moment every driver has its own creation function, so it makes less sense to have them as stack methods, for the sake of modularity.
&gt; but rustc emits an error when you err while gcc creates buggy machine code. That's a bit exaggerated. Much of the time, the algorithm is perfectly correct and gcc will produce working code, while rust gives errors. So gcc wins there. A rust compilation error doesn't mean there *is* a problem, just that there might be. It's better to say that rust requires proof that the program won't segfault, and it is very fussy about a very high standard of proof. There will always be a class of programs that are provably free of segfaults, but where rust can't find the proof. Rust should keep working on making this class of programs smaller, e.g. lifetime elision. But, as programmers, as with any language we should be more patient. We shouldn't prematurely optimize. Rust is giving you problems with the lifetime of your references? Fine, just don't use references and pass by value where possible. When the borrow checker challenges you to battle, you are allowed to run away :-)
Would it be useful to compile a list of safe idioms currently disallowed by rust? Perhaps there could be a meta issue with the aim of making rustc smarter about these cases in the long term.
You can rewrite this: fn main() { let mut c = Test{ field: "asdf" }; c.mut_fn(); println!("{}", c.get_torrent()); } into this: fn main() { let mut c = Test{ field: "asdf" }; { c.mut_fn(); } println!("{}", c.get_torrent()); } Borrows are lexical in Rust (at least at the moment), so sometimes you need to narrow down the scope for which a borrow is taken, even if it is not explicit.
&gt; Why can't rustdoc output plain text like godoc? Because nobody's written that output, or a converter from the json output to plain text.
There's also playform game, it recently became less minecraft and more voxel (http://www.reddit.com/r/rust_gamedev/comments/2k66gy/playform_a_progress_report/).
NIce :) Not a big deal, but traditionnaly tests are put at the end of the files and strcts at the beginning (it's not really an official coding practice but it is the most commonly used :) )
&gt; A child cannot ensure that another child is not going to implement the same trait for the same type This is not really important if `impl`s in the child mode are visible in this module only, and this limitation is fine - something which *should* be visible from outside of the child module is its interface and it should be defined formally, with traits, for example.
I am happy with the system as such, my only complaint is that the tree based nature makes compilation quite slow. I wish our compilation model was optionally mods instead of crates. To be absolutely honest I wish globs would not be a thing. They make it even more confusing asncurrently already to find out where stuff is coming from. I would ban them on any rust project I work on.
Why would you need Garbage Collection? Since the strings themselves cannot refer to anything, reference counting is sufficient.
&gt; Much of the time, the algorithm is perfectly correct and gcc will produce working code, while rust gives errors. So gcc wins there. A rust compilation error doesn't mean there is a problem, just that there might be. I think you have your "much" backwards: for most rustc errors, there is actually a problem, that is, a certain configuration of inputs/calls will cause code to be memory unsafe. I'm thinking particularly about 'obvious' errors (but insidious errors) like a temporary not living long enough, or invalidating an iterator. Sure, there are some instances where it is safe but the compiler is just not intelligent enough, but there's often (except for one case in particular) a simple local perturbation that fixes things. &gt; Rust should keep working on making this class of programs smaller, e.g. lifetime elision. Lifetime elision did not change the range of programs that rustc accepts, it is purely syntactic sugar to make some valid code slightly less verbose, there is a trivial rule to map between them: fn foo(x: &amp;T) -&gt; &amp;U fn foo&lt;'a&gt;(x: &amp;'a T) -&gt; &amp;'a U (I suppose you could argue "rustc accepts more text as valid rust", but the new possible inputs are not different to the old ones in any interesting way.)
This makes so much sense! The only reason this isn't in c is that c uses ints as booleans.
Unless it's an unsafe pointer inside CustomPtr.
Did you change some of the settings when rendering the image? It tried to run it but the resulting image looks very noisy like it's using way to few photons…
Haskell supports [a form of generics](http://www.haskell.org/haskellwiki/GHC.Generics) that allows (with some effort on the part of the trait author, like this RFC): class Serialize a where put :: a -&gt; [Bit] default put :: (Generic a, GSerialize (Rep a)) =&gt; a -&gt; [Bit] put a = gput (from a) get :: [Bit] -&gt; (a, [Bit]) default get :: (Generic a, GSerialize (Rep a)) =&gt; [Bit] -&gt; (a, [Bit]) get xs = (to x, xs') where (x, xs') = gget xs -- ...stuff... data UserTree a = Node a (UserTree a) (UserTree a) | Leaf deriving (Generic, Show) instance (Serialize a) =&gt; Serialize (UserTree a) which 'derives' an impl of the custom `Serialize` trait for the custom `UserTree` type. (In Rust terminology.)
Having a big block of code for the `derive_scheme` stuff is kinda hideous. I don't suppose you could just have it delegate to a macro? pub trait Show { #[derive_scheme(show_folding_scheme)] fn fmt(&amp;self, fmt: &amp;mut Formatter) -&gt; Result; } macro_rules! show_folding_scheme { // Something will have to be done about the hygiene of "fmt"... // This might be invoked by having auto_derive unpack the tuple elements to // locals with appropriate names, then expanding the macro. ($name:ident ($first:ident $(,$rest:ident)*)) =&gt; { try!(fmt.write_str(concat!($name, "("))); try!($first.show(fmt)); $( try!(fmt.write_str(",")); try!($rest.show(fmt)); )* fmt.write(b")") }; ($name:ident {$first:ident $(,$rest:ident)*)) =&gt; { ... }; ($name:ident) =&gt; { ... }; ([$first:ident $(,$rest:ident)*]) =&gt; { ... }; ({$first:ident $(,$rest:ident)*}) =&gt; { ... }; } On the upside, this would encourage even more dogfooding of `macro_rules` :D 
Nice! But callgrind is real-time tool, but I'd like to see and static one.
&gt; FWIW, the compiler actually internally knows a call/use graph for its linting, so theoretically it could be adjusted to spit this out... there's even a library for generating graphviz graphs that rustc already uses for some output like this. I actually found a related issue: https://github.com/rust-lang/rust/issues/558. Pretty old issue.
This is brilliant! Initially my plan was to "register" a `#[phase(plugin)]` method that explains how to combine the substructure and use it in the `derive_scheme, however that's very similar to what we already have which is rather annoying to use. The macro seems like a really great way to do this! Do you think you could write something for `PartialOrd` with the macro syntax? At the moment I haven't figured out how to do it with the current proposed folding syntax (PartialOrd is a bit tricky).
I don't think this can work well with just the structural information - you can do that today anyways. I believe, in the common case anyways, that you want the method calls to be pre-generated for you. Maybe the most general solution could be replacing the field types in the definition with the appropriate method calls, at least for visitor-like traits, such as `Show` and `Encodable`
Pretty compelling reason to go ahead with the xpath idea. Xpaths are also mightier than CSS, so it's worth learning them.
I suggest looking at the result of compiling some code with `--pretty=expanded`. `expand_deriving_*` and `cs_*` are very confusing. I don't think `PartialOrd` needs a deriving scheme that matches by layout (unlike `Show` and `Encodable`), but a simple folding macro won't work AFAICT. Most of the destructuring should be provided by the compiler, we need to minimize what the user has to write -- here there's a lot of destructuring that may not be necessary.
Thanks a lot, that was the missing piece for me! Of course a move could invalidate all those pointers and that's also the reason the code will work as long as everything happens within main. I wonder if it would be useful to have a trait that marks a type as not being movable. I don't know.
Would that be any different from `static`?
Hmm, good point. Maybe not. I have to think more about this and also learn more about `static`. Thanks!
&gt; You can build with link time optimisation That's currently not possible while using cargo though, right? And I've seen cases where LTO actually makes the code slower, so it's not exactly the same thing
If there was a way to flatten a subtree (treat an entire subdirectory as a single module) - I could probably live without `pub use ..::*`. But I'd still want to glob use such common types throughout a source-base. (i.e. vector maths in a 3d app). There are reasons to divide files other than namespacing, whilst default namespace-per-file is useful I think there should be a workaround.
Are you making some sort of guide, or just writing for yourself?
&gt; If there was a way to flatten a subtree (treat an entire subdirectory as a single module) You can do that by pub using them into a outer module. My redis-rs library exposes a single module.
yes you can do it, with lots of glob uses.. both exposing the submodules to clients who use the 'directory root', but also exposing the submodules to each other 
You noticed!
you've got private default... having to write "pub" AND having to manually import it is too much, surely. and I'm talking about within a subtree .. one logical library like "vector maths" , where I know whats going on. within the library, everything can use everything else just fine. If there's any point where things need to be hidden , its only going to be once, at the "master module" the outside world sees, not between every single component. Maybe we need 2 new non-garbage-collected languages, one for all the explicit masochists (the types of people who delayed "auto" in C++ by many years whilst enthusing about how precisely typing out all those lengthy iterator signatures is 'helpful'), and one for the rest of us
yup, and *I'm talking about sharing lots of symbols between modules in the same crate* I guess you must really like header files, they're nice and explicit. Nothing quite like typing things out twice to make sure you know what you meant. 
That was kinda what I was thinking too. I understand that lifetimes can be useful when dealing with complex memory management, but sometimes Rust NEEDS you to notate lifetimes even when your trying to do fairly trivial things. This could be fixed by expanding lifetime elision.
That actually seems to be the best solution for my specific problem. :)
I am a newbie, that's for sure. But I do (and did) understand the difference between String and &amp;str. The initial strings were parsed into another struct, where they were contained in String's (a Vec). The HashMap was simply a presentation of the data in the original struct and I used &amp;str's to enhance performance (and because it makes more sense). Eventually, I changed the code to read the data directly into the struct that had the HashMap and changed it to HashMap&lt;String, String&gt;. I think the mistake I made was thinking that this memory safety would come pre-packaged and happened completely automagically in Rust, but it doesn't, lifetimes are required to do this. And that does make sense actually. Gotta just dive into them ;) Although I do think they could be enhanced in some ways!
I come from a C++ background too and agree with SkepticalEmpirircist. In the end it's what you think takes more time: (1) Understanding Rust lifetimes and adding them to your code until the compiler accepts them. (2) Learning C++ memory management and fix the occasional segfault when you fuck up. And that's exactly why it might be a good idea to infer lifetimes as much as possible. Every minute adding lifetimes to Rust code that is actually completely safe is a wasted one. On top of that I like to argue that (C++, Rust, any language) code that is so complicated that explicitly annotating lifetimes is easier than just looking at it, is a sign of bad architecture. So when you get better and better at C++, the chances of running into complicated ownership issues become lower.
&gt; That's currently not possible while using cargo though, right? I guess so, but it doesn't seem like something that is hard to fix: `cargo build --lto` or some such. (I opened [#759](https://github.com/rust-lang/cargo/issues/759).) &gt; And I've seen cases where LTO actually makes the code slower, so it's not exactly the same thing Did you compare against manually putting all the code into one crate? (If not, it's not a comparison that's particularly relevant to this discussion.)
&gt; (2) Learning C++ memory management and fix the occasional segfault when you fuck up. &gt; On top of that I like to argue that (C++, Rust, any language) code that is so complicated that explicitly annotating lifetimes is easier than just looking at it, is a sign of bad architecture. So when you get better and better at C++, the chances of running into complicated ownership issues become lower. Any problems in C++ only manifest *if you're lucky*. The fundamental brokenness of the "just use C++ properly" approach (which is essentially what the above statements are) is displayed by the consistent way in which applications like web-browsers are pwned. I would guess that the vast majority of nontrivial C++ programs have memory safety holes and violations that could be used as security exploits and attack vectors; it's just that most applications are not interesting targets for black-hats so no-one has bothered to discover them. This is especially important for things like crypto libraries, which need low-level control to avoid timing side-channel attacks, but *definitely* should not be vulnerable to memory safety exploits (since that leads to, e.g., an attacker reading private keys directly out of memory). FWIW, understanding Rust lifetimes is actually not much different from understanding the lifetimes that are implicit in C++ code. Maybe the explicit annotations can get a little confusing, but practice seems to make perfect (that is, quite a lot of people have learned Rust effectively, a lot of whom were confused by lifetimes at some point (e.g. me)). &gt; And that's exactly why it might be a good idea to infer lifetimes as much as possible. Every minute adding lifetimes to Rust code that is actually completely safe is a wasted one. No, I entirely disagree. Every minute adding lifetimes to Rust code is ten minutes (or ten hours) in future when the compiler points out you've done something bad, because it can deduce this via the lifetimes that were added. This avoids the crazy debugging one has to do to work out why the heap is being corrupted. It's not the *now* that is important, it's the future, when code hasn't been touched for 6 months and no-one remembers the precise details of how everything needs to fit together to be safe. Adding more inference is one possibility to reduce the usually-small amount of effort it takes to add explicit lifetimes in cases where the current elision doesn't work, but this replaces that with the possibility of very confusing error messages (e.g. a tiny adjustment to the body of a function or struct can cause some other function in a completely different module to fail to compile) and the non-trivial risk of making breaking API changes without realising it. The compiler actually has useful error messages about many lifetime situations, even suggesting a configuration of lifetimes that is more likely to work (but it may not be the one the programmer wants); these diagnostics will only improve as time goes on.
Well, it should at very least be compiled with -O2 but probably -O3
It'd be nice if it also had a clang comparison, and a gcc released in the past decade. ED: Someone want to try reproducing?
Isn't it a bit skewed? The GCC version used is super old, same for the JDK, JS was tested on IE6 (seriously, wat) and Opera 8, and Ruby is from 2003... and it's just the compilers I happened to know about. I don't see how comparing a compiler that came out this month with stuff from 3+ years ago makes any sense, really.
It seems to be compiled via cargo with the `--release` flag. (At least, I assume that's what the `Rust 0.13.0, --release, Linux` line means in the table.)
I have a similar benchmark which does a lot of FP math, results are actually quite similar. https://github.com/nsf/pnoise
&gt; I opened #759 That's great! I'll be following it :) &gt; Did you compare against manually putting all the code into one crate? (If not, it's not a comparison that's particularly relevant to this discussion.) Fair point. No, I did not. I'll be sure to do so when I encounter any case like this again
&gt; RefCell&lt;DefIdMap&lt;Rc&lt;Vec&lt;Rc&lt;TraitRef&gt;&gt;&gt;&gt;&gt; can it go as far as guessing when they're brackets and only substitute then? That would be awesome. 
It seems to have manual taylor-series based implementations of the trigonometric functions. I would guess these are accurate-enough for the task at hand but lose precision in edge cases (e.g. large values, or values close to &amp;pi;/2 for tan) unlike the libm ones that most other implementations are using. These presumably work really well with the .NET JIT. (Other than the statement about the implementation of the trig functions, I'm completely guessing here.)
been doing similar in emacs `(global-set-key (kbd "M-&lt;f12&gt;") (lambda()(interactive)(browse-url (concatenate 'string "https://static.rust-lang.org/doc/master/std/index.html?search=" (thing-at-point 'symbol)))))` I like the fact the rustdoc html page works ok with keyboard input , not so much of a jolt launching from a text editor 
The image will be noisy initially, but it will improve if you leave it running for a while. The image in my post is the result of several hours of rendering. It is that slow for two reasons: algorithms for scene intersection and path tracing are not optimised, and this is a spectral path tracer, so it has to trace rays for every visible wavelength. A traditional path tracer can extract all colour information from a single ray, but it has to fake things like thin-film materials or chromatic aberration.
Using 10e6 iterations and using `time` to bench fbench.c program I get (totally unscientific w/o erros) c: **1** rust: **1.21** $ cc -v Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn) Target: x86_64-apple-darwin13.4.0 Thread model: posix $ rustc -v rustc 0.13.0-nightly (f168c12c5 2014-10-25 20:57:10 +0000) Unfortunately Apple removed their ancient gcc in 10.9. Note: I hade to modify the c program slightly to get it to compile and had to remove the user input to get reliable timings…
It is possible to write a type that allows references into self. The way it works is basically that during typechecking the type ends up borrowing itself, which means it becomes unmovable as there is no way to let the borrow end before the type itself goes out of scope. However, as this requires writing a reference to self into self, its only possible to implement with internal mutability by having for exampel a `Cell&lt;&amp;'a Self&gt;` field.
I'd be impressed if they actually found a way to benchmark QBasic legitimately on the same hardware as the rest.
Well, I was writing to the people I hoped would answer my question, and I wanted to make it easy for them.
Haha, the hidden depths of Rust :) Thanks, I have to try that some day!
Hmm, have you done any work to boil the problem down to the minimal code required to cause the error?
... Null pointers? I see no pointers here.
There's an `unwrap` call in `read_to_queue`, which might be what he's talking about. Either way, after fixing the `pop_char`, all the warnings and removing the unused code, I get no error (although the program never terminates since `consume_print` loops forever, it also never stops polling the queue as `Consumer.pop` is non-blocking)
Thanks for the hint. I managed to get it to work by removing the mutex as well. See comment [above](https://www.reddit.com/r/rust/comments/2kdndj/producerconsumer_null_pointer_problem/clkcyew). I'm trying to isolate the error with the mutex atm. ----- edit: the mutex was causing the problem, see OP edit. I'll try the AtomicBool next. ----- edit2: fixed it with AtomicBool, thanks :)
Minor criticism: there's no need to have `new_endpoints` not named `new`, since it's namespaced anyways. The general convention is to have anything with a single constructor call it `new`.
Three things: 1. this is a library, so your Cargo.lock should be in your gitignore 2. You might like to compare your code to https://github.com/rust-lang/rust/blob/fb169d5543c84e11038ba2d07b538ec88fb49ca6/src/libsync/comm/duplex.rs , which was recently removed. It was removed because it didn't compose well with other stuff, not because it was inherently bad! 3. I ended up implementing a version of this in my solution to the dining philosopher's problem: https://github.com/steveklabnik/dining_philosophers/blob/master/src/main.rs
Thanks for your help, but the problem turned out to be with the mutex. Switching to AtomicBool fixed the problem, and I reported the mutex issue.
Well that explains why I got no error, the mutex's part of the code I nuked in your original snippet (since it's not really used)
This isn't done yet! And I don't yet claim that this is good code, in any way. Thoughts and pull requests accepted!
Thanks steveklabnik. I removed Cargo.lock. I was wondering why the standard library didn't implement this when I read [Rust for rubyists](http://www.rustforrubyists.com/book/chapter-06.html). Now I see they did but had to remove it. I hadn't heard of the dining philosopher's problem so tonight I'll be reading into it and digging through your source code. Another big take away was that I'm being too strict with my type parameters. `DuplexStream&lt;S, R&gt;` seemed a little weird at first but, now I think I understand(correct me if I'm wrong). One part of the stream can send data of type `S` and it's other half can receive data of that type, while sending data of type `R`, and the other half receiving of type `R`. So just having type `DuplexStream&lt;T&gt;` makes constraints that are unnecessary. 
A naive question because I've only been dipping into Rust but could you be using the try! macro more instead of calling unwrap and does it really make a difference?
In this case, it doesn't make much. Try is nice when you want to propagate errors up the stack, but in this case, I'd prefer they failed loudly.
https://github.com/alexcrichton/green-rs
Awesome! That works. What would the type be then? Say I had a function: fn use_it() -&gt; Box&lt;Fn&lt;???, int&gt; + 'static&gt; { box MyStruct } What should be the type in the ???s?
What you really want is [higher-ranked trait bounds](https://github.com/rust-lang/rust/issues/17661), which unfortunately are not implemented yet. They should be done by 1.0. When they are implemented, you’ll be able to do something like this: fn use_it() -&gt; Box&lt;for&lt;'a&gt; Fn&lt;(&amp;'a int,), int&gt; + 'static&gt; { box MyStruct }
Dependent + linear typing has always made me drool. However, here it looks like the two paradigms aren't very well integrated together.
I wish you could use the uniqueness types as plain old types, and vice versa.
I haven't seen this before: `let todos = self.iter().map({|todo| json::encode(todo) }).collect::&lt;Vec&lt;_&gt;&gt;();` How does the `_` in `collect::&lt;Vec&lt;_&gt;&gt;()` work? Does it just mean "infer this later"? Is it possible to do something like `let todos: Vec&lt;_&gt; = xxx.collect()`?
So, yeah, for doing some game development in Piston I whipped up a (not very good) UDP networking library. Currently, it handles virtual connections, timeouts, and packet sequencing, but doesn't do anything with acks. I'm hoping to continue iterating on it until it becomes useful, and switch to it being trait-based so you can swap it out for TCP. I've got an example, super-simple game here: https://github.com/AngryLawyer/SuperMegaTag - which is like playground tag. It's not got any clientside prediction, but it kinda works. And here's a picture of it in action. Not very pretty. http://i.imgur.com/w2PuI0u.png In the other thread, someone pointed out Enet - http://enet.bespin.org/ - which I didn't know about, so I'll probably be looking at that for inspiration.
FWIW style-wise * you could remove the local type annotations in `main` * I'd suggest changing `read_to_queue` to `read_to_queue&lt;T: Buffer&gt;(reader : &amp;mut Buffer, mut px : Producer&lt;String&gt;)` that way you can give it an stdin (`BufferedReader&lt;StdReader&gt;`) or a `MemReader` or `BufReader` (very useful for testing as you create them from byte vecs and byte slices) 
You give a hint to the compiler that you want the items to be collected in a `Vec`. The compiler then figures out what the type of `_` is. Your second example could also be used.
is this going to be maintained?
we probably did but conditions change.. reassess. Now that we have multiparam type classes, I'm looking much more enthusiastically at Rust again and wanting to write lots of overloads 
Servo needs it if I'm not mistaken, so I'd assume it will still be maintained until they have a replacement for it.
Thanks for your suggestions :). I'll keep them in mind for the future. One question, though. You meant `read_to_queue&lt;T: Buffer&gt;(reader : &amp;mut T, mut px : Producer&lt;String&gt;)`, right?
Yes I do, sorry 'bout that.
so it looks like enet is in C, is using the ffi a bad idea here? I notice many libraries are being built, which is great, but sometimes I wonder if using the ffi is smarter. I'm new to rust so I wouldn't know exactly
Magic :|
This has been mostly a learning exercise for me, being the first proper library not based around FFI I've written for Rust. I'd recommend people use enet if they want something stable and well-tested, but if you fancy something purely written in Rust perhaps this is for you.
Are you on a Mac and installing Rust via Homebrew? You may be running into [this issue](https://github.com/rust-lang/rust/issues/16029). [This comment](https://github.com/rust-lang/rust/issues/16029#issuecomment-57258593) explains what's going on and has some workarounds.
Also see the [RFC](https://github.com/rust-lang/rfcs/pull/387) and niko's [WIP branch](https://github.com/nikomatsakis/rust/commits/hrtb-2)
Type inference/reconstruction
I don't know the first thing about Idris, but if uniqueness types are different from normal types, then that implies there are uses for uniqueness types that do not overlap with the uses of normal types (and vice versa). Trying to be generic over both would restrict you to the intersection of possible operations on these two types, which for all I know could be an empty set.
&gt;I can see from residual docs on the web it used to be impl Type: trait name... What was the reason for changing it.. It was changed because the syntax was misleading. The new syntax make the meaning of impl much more clear : the trait is actually the implemented thing, not the type. This blog post explain the problem when the old syntax was around : http://pcwalton.github.io/blog/2012/12/30/the-two-meanings-of-impl/ 
Using unique types as plain old wouldn't make sense as then they wouldn't be unique, any more. The other way round makes more sense, restricting a normal type to be used under a uniqueness discipline. That should be possible to do transparently, as uniques are a subkind of plain old kinds. At least conceptually, that is, if not in implementation. But I'm somewhat glad Edwin got around doing this in the first place. His usual answer to "we need XYZ in the type system" was "well you've got dependent types, implement it". Of course you *can* implement everything you ever wanted in the type system, it being as turing complete as any total language ever will be, but having to wrap everything up in a functor or worse just to use uniqueness types would be nasty. 
One obnoxious limitation is that a UniqueType isn't a Type. It's its own universe. 
NMIGP's engine (http://github.com/xmppwocky/name-my-indie-game-please) has working local prediction (and update delta compression) and uses a similar networking protocol.
As another commenter explained succintly, non-unique types can be used anywhere unique types are. A non-unique type is essentially a unique type with an infinite number of copies.
Thanks for the link. And looking at that, it seems back then trait-methods didn't use `a.foo(b)` syntax, rather `TraitName::methodname(&amp;a,b)`, hence it looked more important for them to make that clear. but now its' `a.foo(b)` ... I would argue that's not as important as the coherence between the trait description &amp; the method signatures. Any other discoverability issues could be cleared up with error messages . ("foo not found, use scope Foo to access it") I know they're recovering the ability to use TraitName for 'UFCS' - but thats got yet more specialised syntax, I think? 
Also using ArchLinux, but with XFCE, Vim and Geany. I just recently started using Geany when gEdit broke due to GNOME 3 updates and it's quite nice. It has surprisingly comprehensive Rust support (highlighting, tag matching, semi-functional completion).
http://steve.klabnik.usesthis.com/ Only difference is I'm currently using Ubuntu rather than #!. I had an... accident a few months back, and a Ubuntu install was handy.
I'm using OSX + vim. Totally happy with my setup.
No, they aren't. You misread what I said above.
Any reference or records for the details of this decision?
Linux Mint 17 + GVim. Was using Windows 7 Home + GVim but I decided to migrate for various reasons. I still keep Windows around for games that don't work in Wine.
&gt; I need to get better at tmux. This near the top of my todo list, I've been fiddling with [byobu](http://byobu.co) which seems like it could awesome if I could just find the time to actually use it.
Both `Type` and `UniqueType` are also `Type*`, not in the `:` but some kind of subtyping way. There's also `Type* : Type`, which looks like `Type : Type` which is of course nasty, but I guess that's just some syntactic messup over Idris universe polymorphism. Should probably be `Type* : Type 1`. Long story short: Yes you *can* write code that's polymorphic over both: (.) : {a, b, c : Type*} -&gt; (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c (.) f g x = f (g x)
I'm running Arch and using vim to edit rust with [this](https://github.com/wting/rust.vim) plugin for rust syntax highlighting. This after I've been trying to set up a tiling window manager called bspwm. It's pretty cool actually, although I'm no expert and haven't got quite up to speed yet. [Screenshot](http://i.imgur.com/loYkyS5.jpg). I still need to mess with the config a bit
Is using `Type*` in Idris something you have to remember to opt-in to? If so, is it common practice to prefer `Type*` over `Type` when possible?
Archlinux, i3, GVim+urxvt for compilation. Sometimes also Mac with MacVim when I'm on my work laptop. However, Linux is much more convenient, IMO.
One thing I don't understand about Idris is why it needs type classes (traits). If it has powerful type system couldn't such things be directly implemented in this type system?
Ubuntu VM with a a Rust LXC for building and testing. Editor is Vim (with [rust.vim](https://github.com/wting/rust.vim)) when editing on the VM or Sublime (with [Rust](https://sublime.wbond.net/packages/Rust) and [Sublime-linter-rust](https://sublime.wbond.net/packages/SublimeLinter-contrib-rustc)) if on host. I've been meaning to setup some sort of auto complete with Racer but laziness always wins :(
I'm but a poor simpleton who uses Fedora 20 with kate and konsole and didn't bother to customize much.
emacs, osx laptop, linux desktop. 
The TL;DR of that link is that Rust has two forms of `impl` syntax, and the blog post posits that they have different-enough behavior that it's important to create a visible distinction between the two. With the syntax proposed here, I presume the two forms would look like: for SomeType impl SomeTrait { // to implement a trait for SomeType impl { // to add new methods ...which would require reversing the decision that these different concepts should look different. It's true that multidispatch traits weren't on the radar back then, but I don't know if that's enough. Would need to see actual examples of generic-heavy or multidispatch-using code being made nicer as a result of this proposal.
Aren't green threads supposed to be lightweight?
AFAIK it's new in 0.9.15.1, which came out literally hours ago, so I doubt there's any common practice for that. It probably should. When you don't give the implicit argument, there, idris fills it in thusly (use `:set showimplicits` to see such stuff): {b : Type} -&gt; {c : Type} -&gt; {a : Type} -&gt; (__pi_arg : (__pi_arg4 : b) -&gt; c) -&gt; (__pi_arg5 : (__pi_arg6 : a) -&gt; b) -&gt; (__pi_arg7 : a) -&gt; c ...which is probably not too good an idea. But then, this kind of thing is very common when evolving languages like idris: The type checker leads, type inference follows. Why work on inference if it's not strictly needed and the type system is a moving target? This doesn't, at all, look finished. But for what it is, it's already impressive.
It does! https://github.com/steveklabnik/rustmvc/commit/9a85a958deb0e3521075d219c119b8dcf7ee824d
- usually: manjaro/arch with xfce, emacs or sublime - sometimes: win 7, emacs or sublime
&gt; If it has powerful type system couldn't such things be directly implemented in this type system? You can't actually look at the context, even in custom tactics, which is a thing you'd need to do as you'd need to locate your dictionary that is somewhere "out there". That would be a nice feature to have, but doing everything that way would probably bog down performance quite a bit: Right now there's the `instance` tactic primitive to look up instances, which looks through a specially kept list of instances, not the whole context. Might be a good idea to pimp that a bit, make it more flexible etc. without necessarily going all the way and make the whole context accessible. Oh, and idris doesn't need typeclasses for overloading, btw. That is, if you can dispatch statically, you probably don't need a class, the compiler will just find the right function among many of the same name by type.
playform isn't really voxel anything.
I use Arch with Cinnamon, zsh, GNOME Terminal, and a bunch of custom fonts like Ubuntu and Inconsolata. I usually switch between Vim and GEdit depending on whether or not I am making quick changes.
I'm using [Geany](http://www.geany.org/) on Linux with its built in Rust syntax highlighting, autocompletion, and configurable build commands to compile with Cargo all straight from the editor.
hi, [pull request sent](https://github.com/gmjosack/pgs-files.rs/pull/2) for some little things I noticed. Also, I wonder about two things, that maybe more experienced Rust devs could comment on. First, the API includes filtering functions like get_entry_by_name(). I'm not sure if this is a worthwhile thing to have or not, considering the client can already achieve the same results using iterator's methods. Second, the code has two versions of all methods, one that assumes the usual location for each file and one that allows the user to specify. Is this flexibility needed, or not? And, if so, is separate functions the best way to express it, or an Option&lt;Path&gt; as an argument? In any case, nice to see this library. Was it also going to support atomic changes to these files, by any chance...?
Ubuntu and OSX. Using Vim on OSX and the netbeans plugin on Ubuntu since it seems to be doing a good job on the syntax highlighting.
Windows 7, Notepad++, PowerShell, and Msys. *weeps softly*
Awesome! Thanks :). I'll work around that for now, but it's great to know that that is coming.
Arch linux, Kate and sometimes KDevelop, in Vi-compat mode. Kate is really nice, but I've been trying to get into using Vim instead. It's ecosystem is just so fractured and no good tutorials on the common plugins that make it more bareable :/
As I pasted in IRC agner.org has a [manual](http://agner.org/optimize/optimizing_cpp.pdf) that goes over the common ways to implement CPU dispatching (§13, especially §13.5). I'm pretty sure ISPC also does CPU dispatching so you could check to see how they do it.
I wouldn't advocate changing the 'inherent impl' syntax; also I only suggested `for .. impl ..` because I thought there might have been another reason against `impl type : Trait` .. I have certainly seen how you can grep for 'all impl's of a trait' easily at the minute. I don't have a preference on the specific syntax, its the order that matters .. self first r.e examples: impl&lt;T:Float&gt; Mul&lt;Vec3&lt;T&gt;,Vec3&lt;T&gt;&gt; for Matrix43&lt;T&gt; { fn mul(&amp;self,b:&amp;Vec3&lt;T&gt;)-&gt;Vec3&lt;T&gt; { ..} } // swapped: still complex, but at least the trait line mirrors the function signature, impl&lt;T:Float&gt; Matrix43&lt;T&gt; :Mul&lt;Vec3&lt;T&gt;,Vec3&lt;T&gt;&gt; for fn mul(&amp;self, b:&amp;Vec3&lt;T&gt;)-&gt;Vec3&lt;T&gt; { ..} } can you imagine trying to do Dimension Checking aswell :) Anything where you're relying on expressing more through types... the more you can tell the type system, the more the typechecker can help people use your functions correctly. So you start having special case Matrices for 'orthonormalized', or vectors that are normalised,or vec3 point vs offset (implicit w=1,w=0), etc... // matrix * normalised vector yields a non-normalised vector impl&lt;T:Float&gt; Matrix43&lt;T&gt; :Mul&lt;Normal3&lt;T&gt;,Vec3&lt;T&gt;&gt; for fn mul(&amp;self, b:&amp;Normal3&lt;T&gt;)-&gt;Vec3&lt;T&gt; { ..} } // normalized matrix * normalised vector yields a normalised vector impl&lt;T:Float&gt; OrthonormalMatrix43&lt;T&gt; :Mul&lt;Normal3&lt;T&gt;,Normal3&lt;T&gt;&gt; for fn mul(&amp;self, b:&amp;Normal3&lt;T&gt;)-&gt;Normal3&lt;T&gt; { ..} } // point - point = vector , implicit 'w=0' impl&lt;T:Float&gt; Point3&lt;T&gt; :Sub&lt;Point3&lt;T&gt;,Offset3&lt;T&gt;&gt; for fn sub(&amp;self, b:&amp;Point3&lt;T&gt;)-&gt;Offset3&lt;T&gt; { ..} } ... which means myriads of these single function trait impls. I suppose I could look into macros (for rolling single-function traits?), but I worry about error messages and of course making my code look different to everyone else .. 
Out of interest, what sort of projects need to interface directly with these files? Unless it's super secret until release, you don't have to tell ;)
We do it in Ruby too, but you need a `&amp;:name` rather than `name`.
Debian + tmux + a very modified vim.
Nope, Linux via AUR. Looks like this may be a problems for me in the future though, thanks :).
&gt; the code has two versions of all methods, one that assumes the usual location for each file and one that allows the user to specify. Is this flexibility needed, or not? And, if so, is separate functions the best way to express it, or an Option&lt;Path&gt; as an argument? I'd love to get some advice here. In python I'd just use a default argument and in something like c++ function overloading. I'd be pretty unhappy with the api requiring the user to pass None in the majority of the time but it's nice to be able to control which file you're parsing for testing and also for non-standard files such as the cache files dropped by nsscache. Ultimately a better library would access nss directly, which is something I want to work on, but I wanted to start smallish for my first project. &gt; In any case, nice to see this library. Was it also going to support atomic changes to these files, by any chance...? I'm currently planning to implement many various command-line utilities I'd expect that to be necessary for some of those tools so likely so.
Awesome. Let me know if you run into any difficulty using the library. 
Kubuntu + Sublime Text 3 + RustAutoComplete/Racer Although I hope to migrate on Windows one day
I'm not a fan of the syntax, where it magically detaches the left side of the call in the last line.
Hi OP, where is the relevant pull request/issue for multi dispatch? I would like to know how it's implemented.
Not that I know of. That's one complaint I have with Cargo is it's got limited capabilities for passing arguments to the compiler. And that makes it a pain when you have a lot of dependencies and you're forced to invoke `rustc` manually. One area where this becomes a real problem is debugging macros, because you can't pass `--no-analysis --pretty=expanded` to see the code that's being generated.
Arch + sublime text 3, which works but at times I find it lacking for lack if a better word. After reading all this I'm gonna try Geany too...oddly enough I'd never even heard if it before. 
Arch + XFCE or I3 + Emacs I'm just starting to switch to I3, I've been using it for a few days now and I'm still not sure if I will go back or not.
OS X as host, with rust+vim+tmux in a VM. This is my work notebook after all, so I like to keep everything (including client projects) well separated. I use Vagrant to manage all those environments.
cargo build &amp;&amp; rustc src/file.rs --emit ... -L target/deps Or that's what I've been doing when I need to access the rustc option list at least.
Is there some reason that Cargo doesn't support arbitrary args to rustc? I don't think anybody would mind a --rustc-args "--foo bar" option.
What font do you use in that screenshot?
&gt;I'd love to get some advice here. In python I'd just use a default argument and in something like c++ function overloading. I'd be pretty unhappy with the api requiring the user to pass None in the majority of the time but it's nice to be able to control which file you're parsing for testing and also for non-standard files such as the cache files dropped by nsscache. What you're doing currently is idiomatic; this is, if a little ugly, a way to avoid runtime costs (specifically checking the optional args).
In the readme file, the comma in the last line is grammatically incorrect: "This library is unstable because, channels themselves are unstable in Rust."
You overestimate my budget. $70 for an editor, no matter how awesome, is simply something I can't afford right now.
Arch, awesome wm and Vim with rust.vim
I'd love to have it. I'm guessing it's just not high in the Cargo team's priorities. But I don't really know what's going on there. Edit: there are plans to add more compiler flags: https://github.com/rust-lang/cargo/issues/544 But not for arbitrary flags passed to `cargo build`, and instead using subcommands to customize the build.
[PragmataPro](http://www.fsd.it/fonts/pragmatapro.htm).
They're equivalent, `where` was added recently because it is cleaner, and because it is required for associated items. [The RFC has a good motivation section.](https://github.com/rust-lang/rfcs/blob/master/text/0135-where.md#motivation)
&gt; I don't think anybody would mind a --rustc-args "--foo bar" option. The thing is, flags are not repeatable. Putting a key in your `Cargo.toml` would be a bit closer, at least. But generally speaking, the idea is to collect up common use cases, and make Cargo do the right thing, rather than force everyone to re-build common flags for all situations. For example, this is a conversation that's happened in the past: &gt; A: How do I pass compiler flags through Cargo? &gt; &gt; B: You can't. Why? &gt; &gt; A: How do I pass compiler flags through Cargo? &gt; &gt; B: You can't. What are you trying to do? &gt; &gt; A: I need `-g` so that I get debugging symbols. &gt; &gt; B: Ah! We already pass `-g` when you do a `cargo build`, and we don't (and add optimization flags) when you `cargo build --release`. Of course, this means that until said flags and use-cases are identified, there will be a bit of friction, like in this case.
Ubuntu+xmonad+tmux+vim. Fairly simple setup that works wonders 
Yes, that is what will happen, as long as `foo` is used somewhere that type inference can infer a precise numeric type. If type inference can't infer a precise type, you need to use a suffix. I'm not sure how well documented this is...
&gt;But generally speaking, the idea is to collect up common use cases, and make Cargo do the right thing, rather than force everyone to re-build common flags for all situations. I very, very much agree that this is a great end-goal to have. The more cargo can handle for us, the better. However, I think having the ability to pass arbitrary, if weird, flags to rustc through cargo is important. Developers need to do crazy stuff all the time, and waiting for upstream to get it can be painful. Arbitrary flags like that should be trivial^**TM** to implement, and work well as a catch-all.
Sublime Text is only nagware, i.e. The $70 only gets it to stop asking you to buy every 20 saves. So you could use it, fully featured, forever for free so long as you don't mind clicking "cancel" every 20 saves. Having said that, if you like it I think it goes without saying buying it helps support the project ;)
I believe that `cargo build -v` will.
It's not about difficulty to implement, it's a philosophical choice. And, not that Cargo is bound to Rust's release schedule, but things will make their way upstream _much_ faster in Rust than in other languages.
The RFC is here: https://github.com/aturon/rfcs/blob/associated-items/active/0000-associated-items.md
[Xubuntu](http://xubuntu.org), [Zed](http://zedapp.org), and Xubuntu's default terminal emulator running bash for cargo/rustc/etc.
&gt;The other way round makes more sense, restricting a normal type to be used under a uniqueness discipline. That should be possible to do transparently, as uniques are a subkind of plain old kinds. You can do this, via the type `Type*`. It will accept both Types and UniqueTypes, but if it's passed a Type it can only be used in ways in which a UniqueType could be used.
To take up a contrary position for the purposes of Having an Argument: The problem with that attitude is the implicit assumption that the Cargo devs know better than I do. There have been quite a few cases where I've compiled with debugging info, even in release builds, because it lets me generate better diagnostic logs. &gt; Aside: This is a somewhat moot point at the moment, given that Rust can't *use* that debugging information on Windows. Anyway... This position also means that Cargo is a bit of a PITA when developing syntax extensions. Instead of being able to just build, I have to drop down, trigger the build, copy the command, edit it to add the switches I need, then run it. It's a source of friction that simply doesn't need to be there if it'd just pass switches on to `rustc`! I think it would be better if we could pass arbitrary switches, with a lint on the arguments to say "note: you don't need to pass `-g` on debug builds" and a corresponding `--no-shut-up-i-really-want-that-flag` switch. :P
OSX + Sublime3 + iterm 2
ubuntu, emacs &amp; evil
You always need an escape hatch!
First, to clean up some syntax errors: [playpen](http://play.rust-lang.org/?code=%23[deriving%28Show%29]%0Astruct%20SomeStruct%3CT%3E%20{%0A%20%20%20%20value%3A%20T%0A}%0A%0Astruct%20Setter%3C%27a%2C%20T%3A%27a%3E%20{%0A%20%20%20%20vref%3A%20%26%27a%20mut%20T%2C%0A%20%20%20%20new_value%3A%20T%0A}%0A%0A%2F%2F%20no%20luck%20with%20this...%0Aimpl%3C%27a%2C%20T%3E%20Setter%3C%27a%2C%20T%3E%20{%0A%20%20%20%20fn%20set_it%28self%29%20{%0A%20%20%20%20%20%20%20%20*self.vref%20%3D%20self.new_value%3B%0A%20%20%20%20}%0A}%0A%0A%2F%2F%20this%20doesn%27t%20work%20either%0Afn%20also_set_it%3CT%3E%28setter%3A%20%26Setter%3CT%3E%29%20{%0A%20%20%20%20*setter.vref%20%3D%20setter.new_value%3B%0A}%0A%0A%2F%2F%20nor%20this%0Afn%20set_me_too%3CT%3E%28setter%3A%20Setter%3CT%3E%29%20{%0A%20%20%20%20*setter.vref%20%3D%20setter.new_value%3B%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20s1%20%3D%20SomeStruct{%20value%3A%2025u32%20}%3B%0A%20%20%20%20let%20s2%20%3D%20SomeStruct{%20value%3A%20123.45f64%20}%3B%0A%20%20%20%20println!%28%22PRE%3A%20s1%3D[{}]%20%20s2%3D[{}]%22%2C%20s1%2C%20s2%29%3B%0A%20%20%20%20{%0A%20%20%20%20%20%20%20%20let%20m1%20%3D%20Setter{%20vref%3A%20%26mut%20s1.value%2C%20new_value%3A%20250%20}%3B%0A%20%20%20%20%20%20%20%20let%20m2%20%3D%20Setter{%20vref%3A%20%26mut%20s2.value%2C%20new_value%3A%201234.567%20}%3B%0A%20%20%20%20%20%20%20%20m1.set_it%28%29%3B%0A%20%20%20%20%20%20%20%20m2.set_it%28%29%3B%0A%20%20%20%20}%0A%20%20%20%20println!%28%22POST%20s1%3D[{}]%20%20s2%3D[{}]%22%2C%20s1%2C%20s2%29%3B%0A}) Now, the compiler complains about moving, so let's restrict `T` to be only types that can be copied implicitly. [playpen](http://play.rust-lang.org/?code=%23[deriving%28Show%29]%0Astruct%20SomeStruct%3CT%3E%20{%0A%20%20%20%20value%3A%20T%0A}%0A%0Astruct%20Setter%3C%27a%2C%20T%3A%27a%20%2B%20Copy%3E%20{%0A%20%20%20%20vref%3A%20%26%27a%20mut%20T%2C%0A%20%20%20%20new_value%3A%20T%0A}%0A%0A%2F%2F%20no%20luck%20with%20this...%0Aimpl%3C%27a%2C%20T%3A%20Copy%3E%20Setter%3C%27a%2C%20T%3E%20{%0A%20%20%20%20fn%20set_it%28self%29%20{%0A%20%20%20%20%20%20%20%20*self.vref%20%3D%20self.new_value%3B%0A%20%20%20%20}%0A}%0A%0A%2F%2F%20this%20doesn%27t%20work%20either%0Afn%20also_set_it%3CT%3A%20Copy%3E%28setter%3A%20%26Setter%3CT%3E%29%20{%0A%20%20%20%20*setter.vref%20%3D%20setter.new_value%3B%0A}%0A%0A%2F%2F%20nor%20this%0Afn%20set_me_too%3CT%3A%20Copy%3E%28setter%3A%20Setter%3CT%3E%29%20{%0A%20%20%20%20*setter.vref%20%3D%20setter.new_value%3B%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20s1%20%3D%20SomeStruct{%20value%3A%2025u32%20}%3B%0A%20%20%20%20let%20s2%20%3D%20SomeStruct{%20value%3A%20123.45f64%20}%3B%0A%20%20%20%20println!%28%22PRE%3A%20s1%3D[{}]%20%20s2%3D[{}]%22%2C%20s1%2C%20s2%29%3B%0A%20%20%20%20{%0A%20%20%20%20%20%20%20%20let%20m1%20%3D%20Setter{%20vref%3A%20%26mut%20s1.value%2C%20new_value%3A%20250%20}%3B%0A%20%20%20%20%20%20%20%20let%20m2%20%3D%20Setter{%20vref%3A%20%26mut%20s2.value%2C%20new_value%3A%201234.567%20}%3B%0A%20%20%20%20%20%20%20%20m1.set_it%28%29%3B%0A%20%20%20%20%20%20%20%20m2.set_it%28%29%3B%0A%20%20%20%20}%0A%20%20%20%20println!%28%22POST%20s1%3D[{}]%20%20s2%3D[{}]%22%2C%20s1%2C%20s2%29%3B%0A}) Now it's complaining about the fact that, in `also_set_it`, you're trying to modify something through an immutable pointer. Let's add some mutability in there. [playpen](http://play.rust-lang.org/?code=%23[deriving%28Show%29]%0Astruct%20SomeStruct%3CT%3E%20{%0A%20%20%20%20value%3A%20T%0A}%0A%0Astruct%20Setter%3C%27a%2C%20T%3A%27a%20%2B%20Copy%3E%20{%0A%20%20%20%20vref%3A%20%26%27a%20mut%20T%2C%0A%20%20%20%20new_value%3A%20T%0A}%0A%0A%2F%2F%20no%20luck%20with%20this...%0Aimpl%3C%27a%2C%20T%3A%20Copy%3E%20Setter%3C%27a%2C%20T%3E%20{%0A%20%20%20%20fn%20set_it%28self%29%20{%0A%20%20%20%20%20%20%20%20*self.vref%20%3D%20self.new_value%3B%0A%20%20%20%20}%0A}%0A%0A%2F%2F%20this%20doesn%27t%20work%20either%0Afn%20also_set_it%3CT%3A%20Copy%3E%28setter%3A%20%26mut%20Setter%3CT%3E%29%20{%0A%20%20%20%20*setter.vref%20%3D%20setter.new_value%3B%0A}%0A%0A%2F%2F%20nor%20this%0Afn%20set_me_too%3CT%3A%20Copy%3E%28setter%3A%20Setter%3CT%3E%29%20{%0A%20%20%20%20*setter.vref%20%3D%20setter.new_value%3B%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20mut%20s1%20%3D%20SomeStruct{%20value%3A%2025u32%20}%3B%0A%20%20%20%20let%20mut%20s2%20%3D%20SomeStruct{%20value%3A%20123.45f64%20}%3B%0A%20%20%20%20println!%28%22PRE%3A%20s1%3D[{}]%20%20s2%3D[{}]%22%2C%20s1%2C%20s2%29%3B%0A%20%20%20%20{%0A%20%20%20%20%20%20%20%20let%20m1%20%3D%20Setter{%20vref%3A%20%26mut%20s1.value%2C%20new_value%3A%20250%20}%3B%0A%20%20%20%20%20%20%20%20let%20m2%20%3D%20Setter{%20vref%3A%20%26mut%20s2.value%2C%20new_value%3A%201234.567%20}%3B%0A%20%20%20%20%20%20%20%20m1.set_it%28%29%3B%0A%20%20%20%20%20%20%20%20m2.set_it%28%29%3B%0A%20%20%20%20}%0A%20%20%20%20println!%28%22POST%20s1%3D[{}]%20%20s2%3D[{}]%22%2C%20s1%2C%20s2%29%3B%0A}) ...and now it works! Looks like you had the right idea with the lifetimes.
&gt; We are working on switching to glutin Oh man. My heart just stopped. Glutin is my #1 favorite rust project. Seeing it get such an important project as a dependency is absolutely amazing.
If you get rid of `also_set_it`, you don’t need any `Copy` bounds at all, which significantly increases its usefulness: [playpen]. [playpen]: http://play.rust-lang.org/?code=%23%5Bderiving%28Show%29%5D%0Astruct%20SomeStruct%3CT%3E%20{%0A%20%20%20%20value%3A%20T%0A}%0A%0Astruct%20Setter%3C%27a%2C%20T%3A%27a%3E%20{%0A%20%20%20%20vref%3A%20%26%27a%20mut%20T%2C%0A%20%20%20%20new_value%3A%20T%0A}%0A%0A%2F%2F%20no%20luck%20with%20this...%0Aimpl%3C%27a%2C%20T%3E%20Setter%3C%27a%2C%20T%3E%20{%0A%20%20%20%20fn%20set_it%28self%29%20{%0A%20%20%20%20%20%20%20%20*self.vref%20%3D%20self.new_value%3B%0A%20%20%20%20}%0A}%0A%0A%2F%2F%20nor%20this%0Afn%20set_me_too%3CT%3E%28setter%3A%20Setter%3CT%3E%29%20{%0A%20%20%20%20*setter.vref%20%3D%20setter.new_value%3B%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20mut%20s1%20%3D%20SomeStruct{%20value%3A%2025u32%20}%3B%0A%20%20%20%20let%20mut%20s2%20%3D%20SomeStruct{%20value%3A%20123.45f64%20}%3B%0A%20%20%20%20println!%28%22PRE%3A%20s1%3D%5B{}%5D%20%20s2%3D%5B{}%5D%22%2C%20s1%2C%20s2%29%3B%0A%20%20%20%20{%0A%20%20%20%20%20%20%20%20let%20m1%20%3D%20Setter{%20vref%3A%20%26mut%20s1.value%2C%20new_value%3A%20250%20}%3B%0A%20%20%20%20%20%20%20%20let%20m2%20%3D%20Setter{%20vref%3A%20%26mut%20s2.value%2C%20new_value%3A%201234.567%20}%3B%0A%20%20%20%20%20%20%20%20m1.set_it%28%29%3B%0A%20%20%20%20%20%20%20%20m2.set_it%28%29%3B%0A%20%20%20%20}%0A%20%20%20%20println!%28%22POST%20s1%3D%5B{}%5D%20%20s2%3D%5B{}%5D%22%2C%20s1%2C%20s2%29%3B%0A}
It seems theoretically possible. BTW, you can match on a tuple to match on two values at once. match (foo, bar) { (Variant1, Variant1) =&gt; ..., (Variant1, Variant2) =&gt; ..., ... 
Thank you for working through this! My program works now, but I fear I'm still confused about the solution. Hopefully my ignorance can be of some help to others who are working to understand Rust :). Let's focus on the struct: struct Setter&lt;'a, T:'a + Copy&gt; { vref: &amp;'a mut T, new_value: T } The type-expression/-constraint/-restriction "T:'a + Copy" seems to attribute a lifetime to the *type* T. This is weird to me. Shouldn't a lifetime only be an attribute of a particular value/reference/field, and not to the type of a particular value? For instance, in the declaration "new_value: T", I see nothing relevant to a lifetime of either new_value or to the type, since in "new_value: T", new_value obviously lives exactly as long as the instance of the struct lives, being a value and not a reference. So it seems really weird that the *type T* is carrying around lifetime data rather than just the field "vref". In other words, in what sense can the *type* T be constrained to have a particular lifetime, as opposed to just a value of that type? With regards to the "Copy" trait -- that's just entirely new to me. None of the "official rust guides" mention it. You explain that the Copy trait applies to "types that can be copied *implicitly*." I don't really follow this. The doc for this trait says it describes "Types that can be copied by simply copying bits (i.e. memcpy)." But in the case of vref, I'm not copying it, I'm taking a reference to it. And in "new_value: T", I *do* need to copy a value -- but normally, I never need to tell the compiler that a i32 or f64 or whatever is copy-able -- I just assign the value to a variable and the compiler makes the copy. Why do I need to explicitly mention the Copy trait here? Presumably, that gives a different interpretation to the statement "*setter.vref = setter.new_value;" than would be the case otherwise. But for the life of me, I can't see what else that assignment statement could possibly mean, since apparently that operation must have some defined meaning when T does not implement the trait "Copy" (since I've never used that trait before). 
I was worried that you were implying that simply by adding `also_set_it` I would also need to add `Copy` bounds to the struct and also other functions. But apparently not. Phew! This code works fine: struct Setter&lt;'a, T:'a&gt; { vref: &amp;'a mut T, new_value: T } impl&lt;'a, T&gt; Setter&lt;'a, T&gt; { fn set_it(self) { *self.vref = self.new_value; } } fn also_set_it&lt;T: Copy&gt;(setter: &amp;mut Setter&lt;T&gt;) { *setter.vref = setter.new_value; } fn set_me_too&lt;T&gt;(setter: Setter&lt;T&gt;) { *setter.vref = setter.new_value; } But I'm still confused by why `also_set_it` needs a `Copy` bound and not `set_me_too(..)`, not to mention why `T` in "`struct Setter&lt;'a, T:'a&gt;`" needs a lifetime.
Oh, I didn't think of that. That works pretty well for my purposes. Thanks!
Aww the permalink. The correct permanent URL is [this one](http://www.minecraftforum.net/forums/mapping-and-modding/minecraft-mods/1272953-optifine-hd-a4-fps-boost-hd-textures-aa-af-and?comment=43757#c43757). It describes sp614x's experience with the bad interaction between the vanilla Minecraft and Java's GC.
If you spend much time in a REPL, vim-slime is a godsend, and it had the side benefit of finally forcing me to learn TMUX. I would recommend it for that alone. 
So many archlinux users here. Probably a selection bias for the bleeding edge lovers. Ive been mulling over a switch from Ubuntu for a while now...maybe it is time to bite the bullet. Anybody have suggestions for an arch-friendly tiling manager that I won't spend years trying to learn how to configure?
Image css gone bad :( Change in /css/pixyll.css Line 24. img{ max-width:100%; } instead of img { width: 100%; } 
Fixed, thanks
huge fan of evil. Do you have any Rust specific extensions?
This is funny cause I wrote almost the same code yesterday (for passwd only) for a project (I'm working on an nss plugin).
NixOS and kwrite/kate. Which reminds me I've been sitting on a cleaned up syntax file for kate, and haven't updated the one in-tree... Someone bug me on IRC to do that.
I think the best hope here is the [Mesa software mode](https://github.com/servo/servo/issues/3789), which we can sandbox completely. I want to build a high-security mode where all of Servo is heavily sandboxed, except a minimal UI task that communicates with the engine over a well-defined IPC protocol and receives bitmaps to display. At this point it really is a VM and an OS that you can VNC into :)
That will break page sharing between processes. The kernel can get away with it because there is only one copy of the kernel loaded, but that's not the case for libraries and binaries. It's probably much better to use a macro to generate two modules, one for SSE4 and one for non-SSE4. Or even better, compile multiple versions of the library, so that you can also turn on autovectorization and other automatic optimizations with SSE4 (although this might be tricky to get to work with Cargo properly, not sure). 
Just searched for Glutin, however their example already putted me away. unsafe { window.make_current() }; Why should this be unsafe code?!
[We already had one or two debates about safetiness and OpenGL](https://github.com/bjz/gl-rs/issues/185). `make_current` is not unsafe per se, but you can easily screw things up with OpenGL contexts (they are bound to the current thread, the loaded functions belong only to the current context, etc.). The `unsafe` here is a kind of warning that extra care should be taken.
Isn't there a way to generate different binaries of a module, and define a function that when the module is loaded chooses which binary to load?
Seems pretty similar to (or the same as?) [Haskell's pattern guards](http://www.haskell.org/haskellwiki/Pattern_guard).
I've recently considered buying it just because I've used it for a while. But I had no idea it's $70 - YIKES. In any case, I've been using it with the nag no problems -- it really doesn't bother me at all.
From my experience in Ada/Oberon/Modula-3 family of languages, the correct way is to have make_current() implementation unsafe, not the public interface.
I sense some code smell of using unsafe for things it shouldn't be used to. You description sounds like unsafe is a monad for code correctness, instead of a wrapper for C style low level tricks.
FWIW, this is now rather old, and I personally would be unlikely to do anything with any answer in the near future (which I'm also relatively sure does not exist without severe changes to the BDW library, due to pervasive globals). (Also, for reference, the pure Rust GC I mentioned I was working on became (unmerged) [PR 11399]( https://github.com/rust-lang/rust/pull/11399), which includes a fairly significant discussion on GC + Rust, although the thread is now long dead and probably outdated.)
`Copy` just expresses whether a type moves or copies if assigned/passed by value. Both are the exact same basic operation, the only difference is that a move is a copy where the original location gets statically marked as unusable afterwards. That is, you get a compiler error if you try to access the original after the move. If nothing is known about a generic type `T` it can only be moved, thats kinda the base case as far as semantics for types go. You mostly just have to deal with the `Copy` bound if you want to write generic code that has to copy the generic type somehow, and you don't want custom code to do that copy. (You'd bound on `Clone` otherwise) By bounding on `Copy`, you accept less types (all those that are `Copy`), but are allowed more operations in the implementation (copies instead of moves) 
A `-n` would be nice, to dump what cargo would do but not actually do it.
This would be a good use case for `#[unsafe(...)]` which I'd proposed elsewhere. Eg define a new safety context and the attribute `#[unsafe(opengl_contexts)]` which can be used on a block to silence unsafe-for-opengl stuff, or on a method/function to mark it as unsafe-for-opengl. (Instead of abusing `unsafe`, which is for memory safety) This can be done with a syntax extension, but compiler support for it would be awesome.
OSX + Sublime Text 3 + Terminal, [Screenshot!](http://i.imgur.com/U8VHjYn.png) Not entirely happy with it to be honest. I'd love to run some form of Linux (not sure which one - does anyone have a suggestion?) using Vim alongside OSX - I'd use OSX for everything but programming. Perhaps running in a virtual machine in fullscreen so I can easily swap between the two systems...
Yeah, I'm not sure if that's in or not. I feel like I saw an issue...
Can someone please explain briefly what WebIDL means?
With browsers, most of the stuff exposed to javascript is written in the low level code of the browser (C++, Rust, whatever). For example, when you call `submit()` on a form element, or `getElementById()` on `document`, the JS engine is passing that call on to Rust (or C++) code. Each object in the DOM (`document`, the form element, etc) will have a corresponding Rust object which handles the method call. So `&lt;form&gt;.submit()` would actually call [`Submit()`](http://doc.servo.org/script/dom/bindings/js/struct.JSRef.html#method.Submit) on a `JSRef&lt;HTMLFormElement&gt;` in Servo's specific case. Anyway, the webidl helps define the interface exposed to JS. [Here is the webidl for the `&lt;form&gt;` element](https://html.spec.whatwg.org/multipage/forms.html#htmlformelement). It also helps generate bindings between the JS engine and the Rust code, for example [this auto generated trait](http://doc.servo.org/script/dom/bindings/codegen/Bindings/HTMLFormElementBinding/trait.HTMLFormElementMethods.html) maps to the webidl, and if you look at the source of the file there's a bunch of unsafe (autogenerated) code interacting with the spidermonkey API and hooking everything up. IDL (Interface Description Language) in general is useful for defining language-agnostic interfaces; WebIDL is the same thing just geared towards binding JS with some other language.
Glutin is awesome. My dream is to change just a flag in Cargo and make the app run in the browser...
OSX and Emacs (+ evil + flycheck and many others). Flycheck provides on-the-fly syntax checking, which I find quite useful. [Screenshot](http://i.imgur.com/rbTkoYZ.png) [Nedrew's homebrew tap](https://github.com/nerdrew/homebrew-tap) is a god-sent for Homebrew users.
Isn't `rustc` the escape hatch?
&gt; Probably a selection bias for the bleeding edge lovers. I'd say so, considering we're all in a subreddit for a programming language that's pre-1.0 :) About the WMs, I don't use one but have been meaning to learn them. The magazine LinuxVoice did a really great comparison of the major ones (all of which run on Arch), but I can't remember which issue (and there's only 9 issues...so it sholdn't be hard to find). Also the Arch Wiki has some great in depth articles and comparisons. EDIT: I just searched through LinuxVoice and it's Issue # 6 for those interested. Look at the "Group Test" section.
So the story is that they replaced a careful Java programmer minimising allocations, with a standard Java programmers not used to realtime.
My, how Rust has changed. We should almost ask SO to just kill all the old questions ;)
No, make_current is very obviously unsafe. It can cause otherwise-safe code to segfault, by replacing the thread-local GL context.
Somehow I feel that unsafe is getting misused. It started to be like unsafe in Modula-3, C#, to mean array access without bounds checking, pointer arithmetic and similar use cases. Now I see it being used as a mechanism to express code correctness. Under this assumption all Rust code that can trigger program termination, like out of bounds array access, is unsafe.
The public interface doesn't expose any naked pointer operations. Why should it be unsafe to start with? Unsafe seems to be being misused to express correctness.
It's how I debug in languages with strict compilers. Compile, fix obvious errors, repeat until it works.
I wouldn't be surprised at all if you can write `mem::transmute()` (i.e. arbitrary casts) with nothing but `window.make_current()`, by messing around with cross-context GL calls. It's definitely a memory safety issue in this case.
GL certainly can expose naked pointer operations if you misuse it: for example the size of memory that `glTexImage2D()` reads or that `glCopyTexImage2D()` writes to depends on numerous parameters and global per-thread GL state. Thus if you mess up multithreaded context creation you can perform arbitrary reads/writes out of bounds. Source: I've had to debug crashes caused by GL drivers doing the wrong thing here. :(
Sure, but in the tradition of safe systems programming languages, you would have a public glTexImage2D() function using proper Rust types, which would then wrap the unsafe version of glTexImage2D(). Not expose it directly to client code.
It honestly blew my mind when I first discovered that the `match` block supported nested patterns.
I wonder if the issue causing it to unwind is a missing heap allocator.
Yes, seeing that visual basic has better performance than C immediately made me very suspicious of the benchmark value..
Archlinux + KDE + vim + bash: http://imgur.com/GRPbcQH Sorry I don't have anything better than one of the examples from the rust guide right now.
Interesting. I always assumed that the JIT would perfectly handle such cases by avoiding the allocations.
Yes. Especially since using the bools often allows for 'strange' states. Take for example `!is_connected &amp;&amp; is_leaving`. It probably doesn't make sense to have this possibility. By using an enum you cannot represent such nonsense states, which will probably help avoid bugs.
Well, that solves one kind of problem, but what about is_blue, is_large, is_fast, where any combination of the fields could be valid? Is there a better pattern for this? I think this lint would cause more annoyance than it alleviates. 
Currently the JVM's escape analysis capabilities are annoyingly lacking. 
This is the reason why we haven't ever discouraged people from asking questions here rather than on SO. :) At some point prior to 1.0 we will absolutely need to go through and update every single question, either to correct it or to indicate its irrelevance. Google searches will be a nightmare otherwise.
I think for these you would have an enum for each: color, size, and speed. Then, if you need to find out specifically if it's blue, you could match against the enum. I've only found, in my personal code, need for one bool or less per struct. Do you have a non-trivial example? Because I don't think I've ever seen more than one bool in a struct in good code.
I explicitly tried to avoid 'ZOMG Rust is the best and other languages suck" here. Don't want that kind of attitude to become a thing.
A config could be represented as a struct. Configurations often use boolean values. 
You struck a really great tone in the article; thanks for that! Love the way you champion rust.
Rust's syntax extension system, while clunky, is pretty awesome :D I'm hoping to see some better docs for it post-1.0, though.
Great, thank you :)
Are you going to pick up teepee? 
I second it. Very nice and not bashing at all. I liked how you explained why Rust was good or not as good in the sections.
Servo (to my knowledge) hasn't made a decision yet. https://github.com/servo/servo/wiki/HTTP-library-requirements
A property such as `is_blue` seems like exactly the kind of thing one should avoid. Imagine enum Color { Blue, Other, } When is something blue? When `it.color == Blue`. When in the future we want to add other colors, such as `Red`, we can simply add them to the `Color` enum. Now we do not need another method `is_red`, but we can simply do `it.color == Red`.
I remember reading somewhere that Servo was considering [Hyper](https://github.com/hyperium/hyper), though I don't think they have reached any conclusion yet.
In any language which supports tagged unions, bools are definitely an anti-pattern. Types are great documentation; using bools for cases is like using integers for errors. Source: functional programmer in the industry for a few years.
Does Rust have first-class support for state machines? Do explicit state machines have an important part in idiomatic Rust? I don't understand the error message.
The problem is that Rust's type system is not expressive enough to expose a safe type with this purpose. In general, a requirement to use unsafe exposes weakness in Rust's type system. The other thing you should keep in mind is that Rust is very ambitious re: what it considers safe. Specifically, it guarantees data race freedom even with shared, mutable state. Very few languages make that kind of guarantee. Thus, code that might be considered safe in other systems languages is not considered safe in Rust. (It is possible that in this specific case a safe interface could be created, but I know of similar circumstances where it's not possible. For example, there's no way in Rust right now to say "this type is only safe to create in the main thread" so a function with this requirement must be declared unsafe since it is up to the user to enforce the invariant).
More of this, please.
No, unsafe has a very specific meaning in Rust. The list of behavior considered unsafe is documented (perhaps not *well* documented) and one of the requirements of any safe interface is that it must not be possible to trigger *any* of those behaviors using entirely safe code. So if you expose an interface that allows data races under some circumstances, even though it won't under the specific ones that you call it under, that's not a "code smell"--that's a sign that unsafe is being used correctly. Unsafe functions should never actually be *called* in circumstances where this unsafe behavior will occur, they're an indication that there are invariants required for memory safety that the compiler cannot fully enforce. If it is possible to write a wrapper around this unsafe method that allows the compiler to enforce this, you are not precluded from doing so.
Indeed. I have been doing some de-booling in libsyntax: - https://github.com/bjz/rust/commit/94d6eee3357e24913d1331b1fe0bd4e4524bdab6 - https://github.com/bjz/rust/commit/cd049591a25973cd41ca5b69e7a151ae5fa0b71f Bools are definitely really hard to understand as a reader, even with docs.
There is [a guide](http://static.rust-lang.org/doc/master/guide-plugin.html) - always room to improve it though.
Playing Minecraft was what convinced me that GC should only be used when latency doesn't matter -- and that latency almost always matters. For those that don't play Minecraft, let me fill in a missing piece of information from the post -- the garbage collections take over 30 seconds each. In other words, you play for 4 seconds, then it collects for 30, then you play for 4, then it collects for 30, etc...
Wow, it has been a while, but if I remember right, the `extra` crate was renamed to `std`, and the `std` crate was renamed to `core`. EDIT: for finding a specific module in the `std` crate, the new docs page is great: http://doc.rust-lang.org/nightly/std/index.html
Any idea how I can debug this further? My gdb skills are not great. 
A perhaps related article: http://existentialtype.wordpress.com/2011/03/15/boolean-blindness/ &gt; There are few things more stupid in the world than code that compares a pointer for equality with null, then branches on the outcome, and then finds itself needing a sat solver or model checker to propagate the provenance of a boolean that should never have been computed in the first place! \^ this can be related to part of the motivation for implementing refutable pattern matching in rust.
I recall a brief discussion on Meta SO about this, since obviously SO encourages the use of SO, the outcome was: - explicitly tag the language version on the question (if specific) OR - explicitly tag the language version on the answer (which will then provide a history of evolution within a given question) In the first case, one question per version, you can boost the new questions by adding a disclaimer at the top of the old ones: &gt; This question is about a very old version of Rust, for an equivalent question pertaining to Rust 1.0 see "..." which should also boost the page rank. In the second case, there is no need to game the search engines, however you still the "recent" answer to bubble up which can be accomplished by either having the question author switch the accepted answer (ideal case) or having SO users massively upvote the new answer. I don't think that either alternative is strictly better, some questions naturally pertain to APIs/features that have changed so fundamentally that they are now irrelevant in which case it seems natural to tag the question (and dismiss it) while others are still relevant and would be more likely to be amenable to answers for multiple versions.
You'd have to query all the global GL state (e.g. `glPixelStorei`) on every call to ensure safety and that would be too expensive.
I think the error message could use some love, specifically I have often encountered lots of booleans simply to represent orthogonal options: is_ascending: bool, is_restricted: bool, is_alternate: bool, ... In this case, the attributes are naturally disjoint... however this is still slightly horrendous. Specifically there is a high risk of accidentally reading/setting the wrong `bool` and the type system will not help. Therefore, a "better" interface is to create one `enum` per attribute (and not one `enum` for all attributes as for state machines).
This reminded me of [Robert Harper's opinion on booleans](http://existentialtype.wordpress.com/2011/03/15/boolean-blindness/). He discusses some problems with booleans (i.e. you have to know where the boolean value comes from to understand what it actually means). The alternative is using enums and pattern matching.
I think an honest answer to the first question should have been a "No", though. Beginners would be much better served by learning programming concepts in a more established language (like Python, Ruby, Java, maybe even C++). In fact, at the moment, I would answer the question "Should I learn Rust?" with "If you have to ask this, you probably shouldn't".
I don't think there has been any SHA1 implementation in the standard library for a long while now (ever since they decided to remove most of the crypto stuff from the standard lib). You'll probably need to roll your own or look for a third party lib (though I hope someone who actually knows crypto stuff would come up with a semi-official lib for this stuff). This might have have what you need https://github.com/DaGenix/rust-crypto
I generally shy away from telling anyone who wants to learn something straight-up 'no.' You'd be surprised how amazing some people are. I'd rather advise them that they might have problems. I can appreciate a stronger negative response, it's just not how I roll.
I think you were a great ambassador for the language, given the small space you were given, and the questions you were asked. The "100% control" comment struck me as a little fanboyish - I think that could have been toned down. Perhaps you could have added that it does take a bit more effort to get programs past the compiler than in other languages though, to counter those that think we are selling snake oil. That is one of the prices we pay for safety+control. Also I am worried about us focusing too heavily on the safety aspects - mentioning Rust's abstractive power, expressive type system, and refactorability might also be useful. Safety is nice, but it is not reason enough on its own to give Rust a look.
As an irrelevant aside, I first parsed the url as co-dementor, which I suppose says quite a bit about my recent reading habits.
Yea, it looks like 0.10, according to RELEASES.md
&gt; if I remember right, the `extra` crate was renamed to `std`, Nah, `extra` used to contain all the other libs that are now bundled separately - things like `getopts` and `term`. `core` was originally the name for `std`, but it now refers to a subset of its functionality (the bare minimum of functionality needed for a Rust program).
Surprised (in a positive way) by the mentions of productivity politics at the end. This probably isn't the venue to discuss stuff like that, but in case people are interested: Some see reduced workdays (to six or even four hours, as [Bertrand Russell argued in 1935](http://en.wikipedia.org/wiki/In_Praise_of_Idleness_and_Other_Essays)) as a solution; some see [basic income](/r/basicincome) as a solution (this is not necessarily an xor). Looks like some would like [wednesdays off](http://www.reddit.com/r/worldnews/comments/22imk9/swedes_to_give_sixhour_workday_a_go_municipal/cgnd6zr). Then there are the Star Trek / The Culture scenarios, but that's more full-blown post-scarcity. And there's the [scenario where we just accumulate more and more stuff](http://youtu.be/MvgN5gCuLac) and [continue to work bullshit jobs](http://www.smh.com.au/national/public-service/the-modern-phenomenon-of-nonsense-jobs-20130831-2sy3j.html). I apologize in beforehand if this turns into a thread that should've been in /r/politics. :\^)
Heh. I suppose reimplementing INTERCAL in Rust might be interesting :p.
&gt; Surprised (in a positive way) by the mentions of productivity politics at the end. This all is quite offtopic, so I'll just say I'm glad that you're glad. We already had one contentious thread about my personal politics on this sub, and it's not super relevant, so let's just leave all this at that.
I think the problem is mine, as I look at Rust's *unsafe* and think of Oberon's SYSTEM, Modula-3 UNSAFE MODULE, Sing# unsafe and so on. From this whole discussion what I got is that Rust's *unsafe* is trying to be more than what the type system allows to express. Is there a document that defines what Rust's concept of safe is? Or how can I as Rust developer express the semantics under which a given function call is safe for my application domain? For example, how to express that a given sequence of database calls are unsafe.
(To be precise, `core` now contains a lot of things that work without any external dependency, e.g. no memory allocation or other operating system services required, that is, it contains more than the absolute minimum, which would just be a few language items.)
http://doc.rust-lang.org/reference.html#unsafety Specifically, http://doc.rust-lang.org/reference.html#behavior-considered-unsafe While it might be possible to use unsafe to preserve other invariants, safe Rust code is not required to enforce any other invariants, so the utility of this is limited. There are, however, some properties important for memory safety, and about which one may reason in the type system, but whose safety is reliant on guarantees that the compiler cannot enforce. In Rust, these are currently modeled as special traits called *kinds*. For example, the assertion that all shared references to a type ```T``` are threadsafe is modeled in Rust's type system by saying that ```T``` implements the ```Sync``` kind. In general, whether these kinds hold can be determined at compile time by recursive examination of the fields in the structure, but occasionally types need to explicitly opt-out (e.g. ```Cell```) or opt-in (e.g. ```Mutex```). The full discussion of how this works is rather involved, but it's the motivation behind two new features Rust is getting called "unsafe traits" and "default traits" which will in combination essentially allow users to create their own kinds and have the compiler enforce them in a similar way. One may always opt out of an unsafe trait in safe code, but explicitly opting in is considered an unsafe operation. This is a continuation of Rust's general progression towards implementing as much as possible in libraries, rather than special-cased compiler features. Such types might allow you to express the invariants you're thinking about in a generally safe way. To be absolutely clear about what unsafe traits are for, though: they are still ultimately about memory safety. Marking a trait unsafe means that whatever invariants it is "marking" as being preserved can be relied on in situations where if they were violated it could cause one of the unsafe behaviors listed above to occur in safe code. It doesn't mean that any traits that specify behavior on the part of their implementors have to be marked unsafe; you only need to do so when you want to rely on that behavior for memory safety.
That's a good idea. I will do that when I get home later this afternoon and report back my results.
Undefined behaviour in C/C++ is a fairly close approximation to what is considered invalid, and so any function/operation that can be used to hit such things should be marked `unsafe`. The reference has [a section]( http://doc.rust-lang.org/nightly/reference.html#behavior-considered-unsafe) with a list.
&gt; Also I am worried about us focusing too heavily on the safety aspects - mentioning Rust's abstractive power, expressive type system, and refactorability might also be useful. Safety is nice, but it is not reason enough on its own to give Rust a look. I agree. I think the following section from the article is kind of symptomatic of that: &gt;That said, Rust makes you care about a lot of details which you don’t have to worry about if you’re coding in Go, and so if you’re happy with Go’s downsides, programming in Go make more sense than in Rust. This kind of sounds like you would only want to use Rust for safe low-level stuff, neglecting other advantages Rust may have over Go. (An arguably more expressive type system, the combination of generics and traits etc.)
&gt;Also I am worried about us focusing too heavily on the safety aspects - mentioning Rust's abstractive power, expressive type system, and refactorability might also be useful. Safety is nice, but it is not reason enough on its own to give Rust a look. These are actually some of the biggest reasons why I use Rust, so I'm glad you mention them.
Could be simpler in that `HashMap::new()` simply isn't available to the plugin when it tries to call it. In that case, we should see the same problem with TreeMap. I'm far from an expert on dynamic linking but you might need to link the stdlib somehow.
I love the more expressive type system, but mentioning anything about it devolves into flames _very_ quickly, and I didn't want to distract in that way. That said, I do believe Rust is useful for much more than a tiny niche, so I'll think about how I present this here, thanks.
Yeah, but there are no doc comments on most of the enum variants &amp;c and it sometimes gets confusing, I'll work on this when I get time.
That's along the lines of what I was thinking. Though I would think the plugin lib would be linked to libstd at compile time. I think maybe it is linked but the libstd methods are in an inaccessible part of memory, causing an error when trying to run any method. Anyway, I will investigate more when I get home.
I don't know if there's anything specifically for C programmers, but the blog post series [Rust for C++ Programmers](http://featherweightmusings.blogspot.co.nz/2014/04/rust-for-c-programmers-part-1-hello.html) by /u/nick29581 might be close to that.
&gt; A Rust program needs to be sufficiently polite! I had the opposite in mind. I was thinking it would be fun, for a pet project, to fork `rustc` and change all the error messages to be hilariously rude and abusive. If you're gonna argue with the compiler, you might as well have one that can spit the insults right back at you! I'd offer some examples but I wouldn't want to give someone the wrong idea. Edit: Just read the Wikipedia article for INTERCAL. Best geeky laughs I've had in a while.
Yeah, it's hard. To articulate my comment a bit better, I think the issue is that even though you can do a ton more than other languages within the bounds of safe code, for 100% control you have to drop down to `unsafe`. For that reason the juxtaposition of '100% control' and 'absolutely safe' stuck me as a little dishonest. It's not technically wrong, but it gives the wrong impression, and could leave folks thinking the claims are too good to be true.
Ah, I think I'm getting it now. Thanks for the pointers on `Copy` and "lifetime bounds."
I understand that you had to be brief, and I agree that just saying "more expressive" without a good (and presumably long) explanation invites strong disagreement. If you don't want to go into detail, you could say "if you prefer Go's approach" instead of "if you’re happy with Go’s downsides". That would have sounded less like "Rust is for low-level stuff" to me in the given context. 
I think so long as you don't mention it in the same space as Go (which unfortunately could be interpreted as a jibe their way), it is definitely worth mentioning. It's one of our great strengths over C and C++. We might not be as powerful as Haskell, but it is still well above what 'most' programmers have used before.
I would rather say that in Rust, it is possible to expose completely safe APIs with the full power of C. I don't think that gives the wrong impression, and it clearly articulates that Rust doesn't magically make unsafe code safe, but allows you to create clean boundaries between safe and unsafe code. Perhaps this claim seems too weak ("does that just mean Rust has a really good C FFI?") but what's interesting about Rust IMO is that it can expose safe APIs that *wouldn't be safe* in other languages.
The theoretical basis of dark magic in category theory?
As you realize once you start writing a lot of Rust, the problem is that even with excellent escape analysis you need to be incredibly careful about how you hand out references or you'll end up accidentally escaping. And finding all the places where it *is* possible basically requires whole-program analysis, which is something a JIT is definitely not going to be able to get away with. Add that to the fact that the JVM exposes lots of reflection APIs (so it's not always safe to inline) and it becomes a quite difficult problem.
No, it's the same concept. Anything that can cause memory safety violations is `unsafe`. Incorrect multithreaded use of OpenGL can cause arbitrary reads and writes to arbitrary locations of the address space of your program, which can be used to write `transmute` or perform literally any other arbitrary action. Therefore it is unsafe.
That's not part of the C or C++ standard though. If I were counting compiler extensions that would be a much longer list :P
It might be good to let the developer select between compile-time and runtime i18n, depending on the size and the needs of the project. All things considered, I don't see much benefit in compile-time i18n besides being a good learning opportunity for the compiler plugin API. Runtime i18n could easily be extended for other resources, like images.
I think "*completely* safe with the *full* power of C" is far overreaching and is not doing any favours to Rust. There is clearly safe code that cannot be expressed without `unsafe`, it seems entirely unreasonable to expect that every instance of this can be wrapped into a safe highly-efficient interface (performance is part of having the full power of C). It is definitely true that a lot can be expressed in safe code, and even more true that we should be striving to be able to express more and more in safe efficient code, but saying things that are almost certainly false does no favours to the reader who comes away with a false impression, or to the community as a whole. (Things like unchecked indexing are examples of this.)