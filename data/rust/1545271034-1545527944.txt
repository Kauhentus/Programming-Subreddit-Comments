Good font but I might have suggested something else, if only for the bragging rights.
https://oxidized.systems was a Mastodon instance started by the rust team, I believe. But they didn't really use it after spinning it up. 
I know what you mean. However, when starting off examining the internals of a crate, having the top-level cleanly and thoroughly documented is really handy.
I didnt even see the words. I just looked at it thinking wtf.
It depends on the specific use-case, but I've noticed that anonymous imports into the global namespace(prelude) is much more widespread and accepted in Rust than Python. It's common in example code, tuotorials etc. This is a community-standards and best-practices issue vice anything to do with Rust itself. &amp;#x200B; Personally, it causes consistent trouble when trying to understand other people's crates. (Where did this import come from? Is this part of Rust's stdlib? Which prelude did it come from?) Traits complicate it, since they often have to be imported, but aren't called; I'm suspicious this is one of the reasons why it's more popular in Rust than other langs.
This is widespread through the community (Extensive API docs, but little-no tutorials or instructions for popular crates), but hadn't considered this as a cause.
I like the way the warp is coming along. * The middleware is local to the routing (Filter) and not the entire request chain. * The Rejection system is better, but it is too far away from the request handler for my liking. I still use a Response builder to create my error response. The best part is that I can use `map` instead of `and_then` for a return of `impl warp::Reply`. * The `{get,post,put,delete}2` methods are more consistent, though the extra `2` is slightly annoying.
In my experience, the majority of times I create a `prelude` module I end up deciding it is a mistake. While it can feel tedious to specify individual imports, this extra step of effort makes the code file as a whole easier to understand when I read it in the future. If you need to import a lot of things, consider breaking the `prelude` module into more specific modules with descriptive names, so that it is more clear where various types and functionality are coming from.
Why do closures have more memory overhead than functions? Please eli5 if possible :) In the TRPL, it says "When a closure captures a value from its environment, it uses memory to store the values for use in the closure body.", but isn't this similar to just passing a value as an argument to a function? Where does the overhead come from?
May I ask why you opted for federated instead of fully distributed?
Why do closures have more memory overhead than functions? Please eli5 if possible :) In the TRPL, it says "When a closure captures a value from its environment, it uses memory to store the values for use in the closure body.", but isn't this similar to just passing a value as an argument to a function? Where does the overhead come from? P.S. I accidentally posted this just now in an old thread, sorry for any confusion
Failing to load some JavaScript seems to trigger it (same error with Firefox + uMatrix until allowing JS from cloudfront.net)
You should see /u/ssokolow's comment in another thread for a better explanation: [https://www.reddit.com/r/rust/comments/a638lo/bootstrapping\_rust/ebtnte5](https://www.reddit.com/r/rust/comments/a638lo/bootstrapping_rust/ebtnte5) In short, being able to bootstrap a compiler from source (in this case, compiling the latest rustc starting from a C++ compiler) is an important *security* measure against a self-replicating compiler backdoor (see Ken Thompson's lecture, ["Trusting Trust"](https://www.archive.ece.cmu.edu/~ganger/712.fall02/papers/p761-thompson.pdf)). If we have multiple compilers for Rust, then a backdoor that isn't in the source code can still be detected by compiling rustc with different compilers, and comparing the binaries. Of course, this does assume that our C++ compiler has no such backdoor either, but C++ has a larger set of compilers and auditors looking at it. This is all theoretical; such an attack (to my knowledge) has never been spotted in the wild in a major language. That said, when the stakes are the security of an entire language ecosystem, we can't risk this vulnerability either.
If yoy see this in any of my crates, please file a bug. I try to be conscious of always showing imports for exactly the reasons you state.
Most of the documentation and discussion websites for Rust use animation and scroll-linked effects that are gratuitously inaccessible. This means I *can’t read any of the documentation*, unless I want to spend hours reverse-engineering someone else’s CSS. This drove me away pretty quickly. I would love to see the existing documentation made more accessible.
Did you read through this subreddit, before posting here? If you didn't: Do it now and let me know if you still think this is the correct subreddit for you. ;-)
Short version: The most popular alternatives to the "Big Five" are all using federated protocols. Short(ish) version: Fully distributed pushes the network load onto the user devices. With a federated system we can keep the end-user requirements lower. Server-side we are actually aiming to be able to run full-instances on low-power inexpensive hardware. &amp;#x200B; Longer version: [https://github.com/Aardwolf-Social/aardwolf/wiki/Why-not-Distributed](https://github.com/Aardwolf-Social/aardwolf/wiki/Why-not-Distributed)
&gt; But it doesn't work for silencing people. Hit them hard enough and it does. &gt; If enough projects start verification from deep enough, Doesn't matter how much verification they do, you only need to hit the project managers with the wrench until they agree to distribute your infected binary. Even if they tell people, will everyone hear in time, or will they already have been infected? This sort of verification only works if the *users* do it, every single one of them. And it's unreasonable to expect everyone to invent the computer and independently verify every piece of software on it before they use it. If everyone doesn't verify themselves, they're getting a binary somewhere, and all someone has to do is hit the distributor with a wrench. &gt; if there is any hidden backdoor, it'll be possible to find it. Finding it is great and all, but between "Being infected" and "finding out", a lot of damage can be done.
I see what you mean. Maybe something like appending `?` to the comment declaration could identify internal documentation. So you could have the front facing documentation for people consuming the crate and "internal" documentation for the real guts of it. That would be really helpful for some things like webrender where much of the documentation doesn't tell you how to use it, just how it works. 
*sigh* sorry 
omggg futures and tokio lead only to despair.
Thanks for sharing that! I will have to take. Closer look at Pom, seems like a cool approach. 
I have ran into two things: lack of good XML libraries and lack of good interoperability with C. I do not think bindgen works good enough for many things, for example if I want to write a PostgreSQL extension a huge issue is that PostgreSQL's source code is heavy reliant on macros and bindgen cannot convert macros. I am not even sure bindgen is the right solution, maybe it would be better to be able to write parts of the crate in C.
&gt;am i trying to force the wrong tool for the job? Yes you are, but that's definitively doable! Try out wasm bindgen, it'll help you communicate between js and rust. Just keep in mind that rust will make this a lot harder. I'd say wasm is only worth it when you absolutely NEED high performance or you want to reuse code between the web and other platforms.
I think that goes against the philosophy of rust. That could too easily be a silent bug. 
Error handling, back traces and conversion.
Arrays with more than 32 elements.
I think it's just slice patterns being ignored. IIRC you can't do anything with a slice pattern in an `if let` or `while let` either even though that construct exists specifically to allow destructuring of non-exhaustive patterns.
You're looking for /r/playrust.
Ah yeah... the doc push effort is big. I should have called that out more. I’d love to get you involved in that. Ping me on Gitter whenever. 
No beautiful green threads a la Go’s `go` command (or something similar). 
We agree :-) The work on Tokio Trace is pretty far along. You can follow along here and get the idea https://github.com/hawkw/tokio-trace-prototype It should be merged into the main Tokio repo soon. 
I've written a small but real side project with warp (https://gitlab.com/maxpolun/teaz) and it's possible to work around the private apis, though it's annoying and had unnecessary boilerplate. 
But I would rename `take` to `consume` because it doesn't return anything (unlike `Option::take` and the derived versions for Vec, slice, String etc).
Use the `unreachable!` macro
Thank you!
I guess r/playrust is more toxic than r/rust. 
As far as C is concerned I don't know what you're comparing it to (C++ maybe?). Rust is one of the friendliest languages to calling C and being called from C. 
- Incompetent people recklessly using unsafe with abandon. - The rust team actively encouraging a python2/3 style quagmire where code will never be updated for new standards but still widely used, increasing the bugs in the compiler and preventing possible optimizations. - 50% year over year increase in nightly compiler use because the ecosystem refuses to use the stable compiler. - The insistence on using rustup instead of the OS package manager, which is unsafe and leads to more people using nightly. - The tiny std lib, seriously you can’t trust the community with this stuff, there isn’t any way to ensure any quality or responsibility at all. 
Hey!
If you break from the loop instead of returning, and the loop is the last expression, then you don't need the `unreachable!` macro. `break depth` vs `return depth`.
Unfortunately it still seems to be giving me the error of mismatched type: ``` warning: unreachable expression --&gt; src/main.rs:67:21 | 67 | return depth | ^^^^^^^^^^^^ | = note: #[warn(unreachable_code)] on by default error[E0308]: mismatched types --&gt; src/main.rs:59:9 | 52 | fn min_depth(&amp;self) -&gt; i32 { | --- expected `i32` because of return type ... 59 | / while !to_visit_queue.is_empty() { 60 | | let current_rank_deque = to_visit_queue.pop_front().unwrap(); 61 | | let mut next_children = Vec::new(); 62 | | ... | 77 | | depth += 1; 78 | | } | |_________^ expected i32, found () | = note: expected type `i32` found type `()` ``` This problem doesn't exist when I add the implicit return back in - even though the function will never actually hit the implicit return in operation.
Macros are an important part of many C APIs, both for simple defines (e.g. various constants including which version of the library you have included) and for functional macros.
Excellent. I’m keeping my eye on wlroots progress and by extension way-cooler. Hope to switch to it in the future.
I just tried break depth and the compiler definitely doesn't allow that. Maybe I'm not understanding what you're saying
bindgen catches constants defined as macros doesn't it? For function like macros, that's the bad design I'm talking about. I've personally written some really hacky C macros in my life (it's turing complete after all) and like I said, that's an issue with C more than Rust. It's probably possible in theory to reimplement the C preprocessor in a crate, but since C++ uses the same preprocessor as C it's probably never going to be equivalent. For `make install` what about `cargo install`?
Use a `loop` instead of a `while some_condition`, if you know the condition will always be true.
I've edited my response. I hope that helps.
Bindgen has some support for constant macros but in my experience it is buggy, and the need to implement the C preprocessor is exactly why I doubt that bindgen is the right solution. Maybe something like [dpp](https://github.com/atilaneves/dpp) or [rust-c](https://github.com/lemonrock/rust-c) is the right solution. And as far as I can tell `cargo install` does not support shared libraries, man files, or installing anything other than just executables.
You can still replace that `return` with `break` and if that condition matters, don't forget to `assert!` it.
CPUs predict indirect branches too: https://en.wikipedia.org/wiki/Branch_predictor#Indirect_branch_predictor
I feel like these implicit compiler transform are integral to the rust language. I would consider a Rust without deref coercion to be too impractical to use, for instance. The desire to be explicit should be focused on runtime aspects (how do errors get handled, when does the file get closed, etc.) I can imagine that it could be educational to see what is going on behind the curtain but I don't think turning "implicit features off" is the right approach. Part of being proficient in Rust is being able to take advantage of the assistance that the compiler provides so that your code is concise and easy to read (and doesn't drown in pedantic compiler appeasement noise).
As a newer user: just figuring out what traits are implemented for a type. Just trying to figure out the true list of methods I can call on String or Vector, for example.
Hm... I'm curious why this post received down votes. Depending on where they live, $1000 might be the equivalent of 2 to 6 days of full time work. That doesn't sound unreasonable. (I don't know how much work such a linter would be) 
You're looking for /r/broodwar.
&gt; I think this conveyed more information than the original comment while being much more respectful. My message was not about "what unsafe is", my message was "RTFM and don't jump into FFI until you understand basics"
I always wondered how to associate a request ID with related futures in a not too obnoxious way. Tokio Trace will probably answer that question and I am really excited about that! BTW first time I heard about the project.
Look at this page: https://doc.rust-lang.org/std/string/struct.String.html Look at the sidebar, under the heading where it says "Methods from Deref&lt;Target=str&gt;". All of these methods aren't really on the `String` type. Without deref coercion, if you have a `String` variable and you want to invoke any of these methods, you have to do it as `s.as_str().len()`, for example. Are you sure you want that?
you can place the unimplemented!() macro at the end of the fn. it returns the type !, which is the same as "it returns never". This lets you compile functions without returning a proper type.
I think I've made ws-rs choke on a small-ish number of messages. If I request 500 connections (with out.send(string).unwrap()), the echo server/probe pair [here] works fine. However, if I increase it to 0..501, nothing ever prints. Am I just overwhelming the FIFO? Here is the server: https://gist.github.com/rust-play/aabe1c07dee4a9b33db1ebd8a3b5f0cc Here is the probe: https://gist.github.com/rust-play/93fd0651708d4368b10d2ccafc4ec2d8
So much this. What I really want is for enum variants to be their own types, but if I can just define a bunch of structs and not have to write a macro that wraps `Bar` into `Foo::Bar(Bar)` ever again, that is a win in my book!
As dbaupp said, if you use `loop`, the compiler will understand that the only way you can exit the loop (absent a `break` statement) is by returning. With the while loop, it's possible in theory, in some cases, that the compiler could prove to itself that you'll never exit the loop. But in general that's equivalent to the Halting Problem.
On that note should regex be in the standard library? Just look at how Java's time library aged. Being a crate has allowed Rust to have the fastest regex engine out there.
You may have been right... tiberius' consumes (transforms) statements and connections (to future results) and looks like it won't be compatible with r2d2, which expects to share the connection object and then put it back in the pool. Actually this really irks me. How are you supposed to reuse the connection? I will open an issue with tiberius.
Nice write-up of your experience. I like that you also described the "frustrating phase" of the learning process :)
Is there a plan to implement merges (simple ones like left and inner) or is that out of scope? 
I hope some of you will find it handy.
Congratulations!
Yes I think the awesomeness of Cargo has made that an easy choice. But of course, Cargo is awesome because it builds on a couple decades of open source library packaging experiences. It's possible that Java made the right choice for its time too, since we're talking about 1995.
You probably need to create your own Pointer and Reference types to be able to encode different address spaces. The NVPTX backend has this same problem (e.g. dereferencing a pointer to CPU memory from the GPU might not make much sense).
That would only be possible with a Rust runtime system. But a runtime system makes it a lot harder to use Rust from other languages, because now you would have to periodically trigger the runtime system to make its work. So no more easy creation of a Rust shared library with a C ABI, which can be used by pretty much every language out there.
`to_visit_queue` is mutable. You cannot ensure the whole loop will run. You're assuming, due to context, that it will always run. Never assume things as it causes bugs now or later. Listen to the compiler :)
https://imgur.com/gallery/N6aImsQ
I was giving just one example from your post.
*James Maynard Keenan intensifies*
&gt; It's possible that Java made the right choice for its time too I really enjoy your choice of words. 
Great! Two things I noticed after a very quick review: You use no `unsafe`, so why not add `#![forbid(unsafe_code)]` to `lib.rs`. This will make it clear that the code is completely safe and enforce it automatically for future changes. `XmlNode` contains an owned string, but the constructor accepts an [`&amp;str`](https://github.com/dam4rus/msoffice-pptx-rs/blob/master/src/xml.rs#L22). If you accept a `String` instead, this saves an additional allocation. If you use `Into&lt;String&gt;`, you'll get the best of both worlds.
Good idea :) thanks!
Is this on *nix? What does ulimit say your open file limit is? It could be that the process has too many files/sockets open. If that's the case, it would be a system tuning issue.
Yes?
I am not sure what you are referring to; if you are looping over `Base*`, then you have no guarantee that all elements have the same dynamic type, so surely the `match` cannot be outside the loop.
Thanks!
Since this is a library, you should consider replacing the `println` statements with calls to a logging facade such as [log](https://github.com/rust-lang-nursery/log). This gives consumers of your library more control over logging, but requires them to add a log implementation to their project. Another option would be simply removing the `println` statements all together. 
Oh, great point. I'm on Linux 4.15. ulimit says my open file limit is "unlimited." 
&gt;You use no `unsafe`, so why not add `#![forbid(unsafe_code)]` to `lib.rs`? This will make it clear that the code is completely safe and enforce it automatically for future changes I totally didn't knew I can do that. I wonder, is there any other benefit for adding `#![forbid(unsafe_code)]` , like optimization or something? &gt;`XmlNode` contains an owned string, but the constructor accepts an `&amp;str`. If you accept a `String` instead, this saves an additional allocation. If you use `Into&lt;String&gt;`, you'll get the best of both worlds. I'm still trying to wrap around my head of how to pass a string to a constructor, since there are so many ways to do that. After reading more about `Into&lt;String&gt;` it really looks like the best option here. Thanks for the tip!
The pertinent part here, I think, is that I'm sending all 500/501 requests before I do the |move| (which I don't fully understand yet). &gt; for i in 0..501 { &gt; let strs = vec!["Hello Websocket ".to_string(), i.to_string()]; &gt; let outstr = strs.join(""); &gt; out.send(outstr).unwrap() &gt; } &gt; move |msg| { &gt; println!("Got message: {}", msg); &gt; out.close(CloseCode::Normal) &gt; } 
Yeah OOP was new, it's just a shame that they have two time libraries now. The newer one being basically a port of JODA time a third party library.
https://doc.rust-lang.org/std/string/struct.String.html Scroll down to trait implementation section.
Ok, yeah looks like it's an app default, you need to do some app config yourself (not sure how though). When you run the connect helper, the `out` variable you're sending to is the sending portion of a synchronous channel. The settings for your websocket connection are [here](https://github.com/housleyjk/ws-rs/blob/master/src/lib.rs#L238), the important parts being `max_connections: 100,` and `queue_size: 5,`. When the sync channel is created in [io.rs](https://github.com/housleyjk/ws-rs/blob/master/src/io.rs#L101) they create do `mio::channel::sync_channel(settings.max_connections * settings.queue_size)` where the argument is the buffer length of the queue. So in short, there are opaque defaults in the function you're using, and one of those is a sending queue length of 500.
Cool! I never understood what Tokio was about but this is a good example of what it actually does, in code.
Thank you for the help! I'll make sure to read the documentation a bit more extensively now...
Well, you need somewhere to store those values so that you would know what to pass as those extra parameters. Currently closures basically desugar to structs with appropriate implementations for `Fn`, `FnMut`, and `FnOnce` traits. So something like this: fn call_this&lt;F: Fn(i32) -&gt; i32&gt;(f: F) -&gt; i32 { f(2) } let x = 3; let y = call_this(|t| t + x); desugars to something like this (you currently cannot implement `Fn` traits by hand, so this is kind of imaginary syntax): fn call_this&lt;F: Fn(i32) -&gt; i32&gt;(f: F) -&gt; i32 { f(2) } let x = 3; struct Closure&lt;'a&gt; { captured_x: &amp;'a i32, } impl&lt;'a&gt; Fn(i32) -&gt; i32 for Closure&lt;'a&gt; { fn call(&amp;self, t: i32) -&gt; i32 { t + *self.captured_x } } let y = call_this(Closure { captured_x: &amp;x }); So closures need that extra memory because they are basically structs that store references to captured variables (or the variables directly if you use a `move` closure).
This is really cool! Read some of the code and it's quite readable (I think it's really nice that you use a bare function taking in configuration to run, instead of `Config::run`). But besides that, what I'd love to see is some `#[bench]`marks. Would be interesting to see using `AtomicUsize` to count file sizes using multiple threads to see if becomes insanely fast.
I tried tower-web and really liked it But actix-web with proper handlers is also good
predicting memory access might as well be instant compared to the time to load it 
mio_httpc will allow you to create concurrent http requests without a large dependency stack. It only uses mio so that means you write a mio polling loop and call mio_httpc from there. 
&gt;I wonder, is there any other benefit for adding #!\[forbid(unsafe\_code)\] , like optimization or something? I think the benefit is two-fold; it allows code reviewers to immediately see that you only use safe Rust, and it warns new contributors about using unsafe code. Compilation-wise I don't think it makes a difference.
If I could have two wishes for rust that would make me the happiest. #1. Async / await and friends to be in nightly (without feature flag) #2. Rls. So much rls. rls. All of the rls. I think rls is the one aspect of rust that makes me want to pull out my hair the most. Coding rust on a large ish project is awful. I could just as well just be using syntax highlighting alone. Important. This is not a complaint, there are smarter and more motivated people working really hard on this. And it's a really hard thing to solve. And they deserve all of the kudos. But when they get rls to rival the ide experience of other more established languages, boy, is it gonna be smooth sailing. 
[weequwest](https://github.com/saethlin/weeqwest) may be of use. 
Maybe just use hyper vanilla. AFAIK hyper now includes a tokio runtime. Not sure if that's still too many dependencies for you. Otherwise there should be something on mio
It seems nice, but also adds a lot of dependencies.
I'm trying to check it out now, but I can't compile the example given on [Crates.io](https://crates.io/crates/mio_httpc)? `--&gt; src/main.rs:35:40` `|` `35 | let v = mio_httpc::extract_body(&amp;mut resp);` `| ^^^^^^^^^^^^ not found in \`mio_httpc\`` &amp;#x200B; `error[E0599]: no method named \`call_simple\` found for type \`&amp;mut mio_httpc::CallBuilder\` in the current scope` `--&gt; src/main.rs:14:10` `|` `14 | .call_simple(&amp;mut htp, &amp;poll)` `| ^^^^^^^^^^^` &amp;#x200B; `error: aborting due to 2 previous errors`
I believe Andy no longer has time to work on this project, but, it’s really cool.
Many probably aren't too different from other languages, but rust does have some extra ones, like: - using unsafe specifically to avoid lifetimes (sometimes OK, but can be a red flag for treating Rust like C) - using `Any` when dealing with types all in the same crate, or using `Box&lt;dyn Trait&gt;` when a 2-5 variant enum would suffice (again, can be OK, but both are red flags for trying to implement OOP/Java-esq solutions in Rust without adaption) - too many different modules for things closely related. If the average file length is under ~75 lines, then they probably don't need to be so separate. Splitting out files when starting creating them will just fragment the source and make it more annoying to do proper organization later - `clone` all over the place. This one isn't horrible, but most times cloning can be avoided by better design. I'd be more wary of this in libraries than applications, but even in apps it can be a red flag for under-thought-out design. - overuse of macros. If there are tons and tons of places where the only way to get code reuse is through macros, then the overarching design could probably be a bit better. There are notable exceptions, like `nom`, but in general functions allow much cleaner and more self-documenting code. I'm sure others can add to this list. And all of these should be seen as flags for further investigation, not necessarily as outright bad.
You could try [chttp](https://docs.rs/chttp) which supports concurrent requests out of the box without too many dependencies. By default you can stream multiple response bodies concurrently. If you want to also await the response headers in parallel, you'll need to use the futures-based API by enabling the `async-api` feature. Disclaimer: I'm the author of chttp.
Sorry about that, I have to update the README.md with the API changes of later versions. Use this as a reference: https://github.com/SergejJurecko/mio_httpc/blob/master/examples/get.rs
Why would it be different for Rust? 
It sounds like the maintainer doesn't have time anymore so I'm guessing forking is an option
I'm happy that you like it!
Hi I've forked this rust crate that helps write xll functions. HTTPS://github.com/ronniec95/xladd I added some slightly more ergonomic APIs. I use Excel to prototype and experiment so being able to write functions in rust has been quite productive. Hope it helps
Heh, it didn't occur to me that this is actually a pretty neat simple, practical example of tokio. Good point!
Oh, I see. I wonder if there's a way around that. Maybe using traits and (zero-sized) types representing each peripheral?
If you're worried about dependencies, and it's just get requests, might be worth writing with just mio (rust epoll wrapper, essentially), partly as a learning experience. Once I did bare metal tcp/udp I realized it was actually pretty simple (in many cases) and often turn to it when performance is crucial. Just an idea, might not fit your goals. 
- Unnecessary or badly commented use of `unsafe` - Downcasting - Excessive use of `Mutex`, `Arc` or `RwLock` - Excessive use of static variables - "Hacking" ownership rules with `Option` - Cloning thing when passing them by reference would do - Passing things by reference when moving them would do - Structs that try to represent functionality rather than data (i.e: `MyStructFactory` or `ThingamajigMachine`) - Implementing `unsafe` traits unnecessarily - Not isolating `unsafe` code properly
&gt;The overuse of \`RefCell\` is a sign of unidiomatic Rust code. [https://raphlinus.github.io/personal/2018/05/08/ecs-ui.html](https://raphlinus.github.io/personal/2018/05/08/ecs-ui.html)
I flag all uses of `unwrap` in my code. I usually only use it when I'm writing things quickly and don't want to worry about error handling yet.
Thanks, I'm trying it now, but getting an error ('Call failed: Other("No TLS")', src/libcore/result.rs:1009). Is there something I need to set up?
Well, even that doesn't include things like `into()` from all the various from implementations of `From`. You basically have to know that having a `From&lt;&amp;'a str&gt; for T` means you can run `string.into::&lt;T&gt;()`. And even those `From` impls don't show up on that page unless one of the types is actually `String` and not something that `String` implements `Deref&lt;Target = T&gt;` for.
Cute! Congrats from one trans dev to another 💖
Hi, I've seen your crate before, but it not clear how to use it. What is missing is some kind of step by step guide people could follow and actually see the function running in Excel. I know, a lot of people know how to do this but there is a lot of people on lower levels as well. I'm trying to help people who are coming from the Excel world, people who don't yet know what a compiler is, but who know how loops, if, etc. work because they have been working with VBA inside Excel. If they look at your code in github, they just see a wall (and I'm quite sure many of them could be potential users of your crate, they just don't yet know how all this works). I know all this because I'm coming from there. I started studying python some 3-4 years ago and Rust one year ago. I've now managed to write for example a sudoku solver with GUI (GTK), API for both PostgreSQL and MSSQL, a simple GUI using WinAPI etc. But even for me all those words like Excel4, XLOPER etc. are just gibberish. I now have more confidence to start googling what that all means but for beginners, that's just overwhelming. They want first to see results that prove it's all worth it. To get Excel hackers to use that crate, we have to provide with easy steps a complete beginner can follow and when they see that "Hello,world!" message box popping up in their Excel, their brains start to get excited: "Hey, I *really* can use Rust in Excel. That's *reality*! It's possible for *me*." They have to see it with their own eyes before their brain says it's possible for them because they have never done that kind of stuff before. After that, they are unstoppable. For people with little programming background one has to show steps in the level of "Copy paste this there, don't worry if you don't yet understand why." so that they will see what is possible in general. That's what I was trying to do: 1) Write a simplest program of square, 2) compile it to a library with one command, 3) copypaste few lines of code to VBA and 4) run it. It would be super helpful if you could write a little tutorial (probably with screenshots) on Medium how to use your crate. If it looks easy enough, I'm quite sure it will get some traction. How cool is to have your own Excel add-in? :) Now it's just not clear if it's going to take 30min or days and that is turning people off (other than Rust programmers).
My favorite Swift syntax is the decision to use the exclamation point for unwrapping that may panic. It's a nice visual way to convey danger to the reader.
In several places, you convert an Option to a Result by using `ok_or_else`. You can use `ok_or` instead like so `ok_or(Err(MissingAttributeError("foo")))`. In some of those places, you actually can avoid converting from an `Option` to a `Result` entirely by creating Result variables from the beginning like so `let mut foo = Err(MissingAttributeError("foo"));`. You can then initialize the struct by writing `Self { foo: foo?, bar: bar?, baz: baz? }`.
&gt; given that they are compiled from scratch Which does not happen that often. I doubt we should be deciding on what goes into `std` based on the time to compile from scratch.
I for one rather spend my time with languages that accept binary libraries, instead of waiting a full build + dependencies, every time I need to start a new proj or update dependencies. So the question is how much that will matter for Rust to win devs from system languages that embrace binary libs.
I'll admit I've done the Option ownership hacking. I'm not sure how to avoid it in some cases.
Thank you! I think that failing is what most, if not all of us, experience learning Rust, however we can learn so much from failing, which is important. I haven't seen too many people highlight this side of Rust, so I really hope everyone enjoys it.
In no particular order * `Box&lt;SmallType&gt;` * `fn foo(String)` which doesn't store the argument anywhere; equivalently with other heap-allocated types * `fn foo(&amp;str)` which converts the argument to `String`; equivalently with other heap-allocated types * `for` instead of using iterator combinators to create a collection * `vec.contains()` (You probably want `HashSet` or `HashMap` * `non_smart_pointer.clone()` when sharing immutable data between `futures` tasks or threads * `String` or `&amp;str` where the program deals with OS paths (use `PathBuf`/`&amp;Path`) - this is most likely a bug * Instantiating a collection (using `.collect()` or manually) - many things can be accomplished without it * Not using `BufReader` when reading files. (False positive: crates with weird interfaces such as `csv`.) * Broken interface segregation principle * (`Ref`)`Cell&lt;Option&lt;Rc&lt;Self&gt;&gt;&gt;` - likely a cause of memory leaks * An attempt to implement `fn with&lt;T, F, R&gt;(&amp;mut T, f: F) -&gt; R where F: FnOnce(T) -&gt; (T, R)` * `.unwrap()` or `.expect()` without a comment outside of `fn main()` * `a_number as OtherNumericType` without a comment * Using `Arc&lt;Mutex&lt;T&gt;&gt;` or `Arc&lt;RwLock&lt;T&gt;&gt;` and never overwriting `T` after instantiation. * `if foo.is_some() { foo.unwrap() }` * Library crate missing docs on public items. * Hand-coded (de)serialization instead of using `serde` or something similar * Attempts to use singletons unless there's a physical limit (like embedded etc) * Converting all errors in a library to `std::io::Error` * Not using futures when things can block * Calling `Read::read()` or `Write::write()` * Struct with `pub` fields that maintain an invariant * Struct with non-`pub` fields and `pub` setters * In case hand-parsing is needed, not using state machines (implemented with enums) * `*` imports (unless it's versioned prelude) * Stringly typed interfaces * Functions returning an error when receiving invalid input (pass arguments by newtypes that can only contain valid values) * Indexing arrays, especially in for loops * `fn argument_order_matters(T, T)` - easy to accidentally swap; use `struct` or different types I wonder how many of these are handled by `clippy`... :D
Try the [ffmpeg-next](https://crates.io/crates/ffmpeg-next) crate. You should be able to get full access to most videos contents and be able to trim and add to videos.
You can't. I think it's fair to call this a code smell because it could be indicative of an organizational problem, but it isn't always avoidable.
Thank you for reporting the problem! I talked to Team Hashnode and it seems to be fixed, now. I am sorry for the inconvenience.
&gt; And as far as I can tell cargo install does not support shared libraries, man files, or installing anything other than just executables. This is intentional and AFAIK Cargo will not get this ability. You can use an external tool if you need it. Technically you could even use `make install` to install Rust stuff, it's not like `make` is C-specific.
I'm curious: Why did you go for length-tagged fields rather than mapping to a datagram unix socket (SOCK_DGRAM or SOCK_SEQPACKET) that can represent individual messages natively?
You might be interested in [this github issue](https://github.com/sfackler/rust-postgres/issues/233) over at the rust-postgres repo. They've spent some time thinking about this issue since there's an (in progress) postgres driver for tokio (which may also serve as a source of inspiration.) But yeah I'm not a huge fan of how async/sync IO has bifurcated the ecosystem of Rust libraries.
I tend to find that this problem can be circumvented usually by separating the concerns of two parts of the program that have become conjoined under the existing design. For example, the threads managing a `Job` struct should not have their handles owned by the `Job` itself. Whereas this sort of design might be permitted in other languages, it doesn't play nicely with Rust. Avoiding this sort of badly conjoined design tends to solve the problem and prevent the overhead of a runtime check when unwrapping the `Option`.
Hi I don't think you've seen my crate before because I only just wrote it! The original author you may have seen. And yes I had identical questions to you. There is another project aarc_xl which is on my github page that is a step by step guide. Export.rs to define the interface to excel Basic_stats.rs conversion layer between excel types and rust types Error.rs to manage rust to excel error handling using failure crate I hope this helps but I'm as in the dark as you. I just figured it out over the last few days myself! 
Thanks! I might actually try that. Are there any resources you can recommend as a starting point?
My motivating use case was to be compatible with [`dscfg`](https://crates.io/crates/dscfg). Datagram unix sockets are a good idea. PRs welcome! :)
&gt; I came up with this crate because I wanted a way to parse binary data formats as easily and efficiently as in C, but without all the safety gotchas. This solution is most useful for sparse parsing, i.e. when one isn't interested in the values of all fields, only a few (otherwise using the byteorder crate directly is probably a better choice). Shameless plug, [I also have my own crate for binary serialization](https://github.com/koute/speedy) which unlike yours is mostly useful when you want to process the whole structure instead of only a few fields. [And it's pretty fast too](https://github.com/koute/serde-bench)!
Use case: Write a tool that does both, perhaps in different modes, without duplicating code.
Yes indeed. Solving that is part of the plan. 
Yeah that's just an example of where it's likely inappropriate. There are many use cases where you need an uninitialized value, or perhaps a field you need to take ownership of in `drop` or something, where using an `Option` is unavoidable. It's a bit like `null` in other languages: the vast majority of the time nothing should be `null`, but sometimes you really do need a value that can temporarily be empty.
&gt; Any plans to extend this to support the other direction; writing a struct into a byte stream/array? For that you might want to check out [`speedy`](https://github.com/koute/speedy), which is my somewhat limited but potentially useful serialization crate which does this kind of a thing bidirectionally.
This is definitely a code smell, but there are actually also places where it is absolutely necessary and not a bad thing at all. The book goes into an example of this for when you need to take ownership of a field in order to do some operation on it. https://doc.rust-lang.org/book/ch20-03-graceful-shutdown-and-cleanup.html
You must call with one of the features and pick which TLS implementation you want to use rtls, openssl or native. Readme has examples for running get example
One solution would be to write another async connection pooler. It should not hand out a connection object directly, but act more like a worker pool.
Haven't tried it yet but just an English nitpick for the docs Your first line is &gt; The most simplest of doing error handling That should probably be &gt; The simplest method of handling errors Otherwise this all sounds very nice!
In such a case I'd tend to prefer to use a combination of `std::mem::swap` and `std::mem::uninitialized` for such situations given how easy it is to isolate the unsafe code.
Yup, that is why I called it a minor issue. My only big issue is that there is no good way to interface with the C preprocessor.
I did not expect that you actually have deeper thoughts about that! So without having read the long answer: I applaud that you seem to do research before deciding, that is not common today!
Going to try to come up with a few things that haven't been said already: * Lack of docstrings * Breaking projects into lots of crates even though those crates all really relate to the same problem, or the project is very young -- this says "overengineering" to me * Lack of examples * Any use of `unwrap()` or `expect()` in library code, at least for things other than "basically must fail" errors such as poisoned mutexes. * Using macros to define DSL's as part of the external API, at least when that isn't the goal of the crate. For example, &lt;https://docs.rs/gfx/0.17.1/gfx/macro.gfx_defines.html&gt;. This is often really tempting to do because it lets you conveniently define fairly complex systems, but I've slowly come to believe it's usually not a good idea. Every time I've seen it, it's ended up being more work and less flexible than just documenting how things are *supposed* to interact.
Now that I've read it: valid reasons. Anyways, a twin-solution like "distributed + federated servers as high-performant caches" _could_ reduce load on user devices easily. Also, multi device support is (IMHO) not a problem. Please consider this: http://beyermatthias.de/blog/2018/02/25/blueprint-of-a-distributed-social-network-on-ipfs---and-its-problems-2/ And feel free to mail me if you have questions! (Note, though, that I'm on a sabbatical in Mexico and have a slow or no internet connection from time to time)
I mean I guess that's fine. I generally avoid unsafe code that isn't needed.
I usually try to put "how" and "why" overviews in module documentation. ...or into the docs for a a key type or method, and have others refer to it when necessary.
Ah, completely forgot about that! Thanks!
What are the benefits of the model Dyon uses? For something fairly simple like this, I'm not sure what I'm supposed to be taking away from the Dyon implementation.
I feel that it's okay as long as it's *small* and ubiquitous. `ggez` has kind of drifted this way accidentally; there's no specific prelude, but doing `use ggez::*;` is common, and the only thing it imports are the `Context` type, which is used literally everywhere, and error/result types, which don't clash with existing names.
What is going on in this line? bottles(b)" of beer on the wall, "bottles(b)" of beer.\n" Oh... I see it now. I think some whitespace around the `"` might help me be able to grock it. I read `"bottles(b)"` and think that IT was in the string quotes!
Very nice! I've found a new favorite toy. :D
How does the enum thing work as an alternative for Box&lt;dyn Trait&gt;? Suppose a I have a vector of trait objects and I don’t know what exact structs are gonna get passed in. I’ve been using the Box solution for that and I didn’t know there were alrernatives
If you know what the structs can be limited to, then you use an enum instead and your type be a Vec&lt;enum&gt;, possibly eliminating the need for a trait entirely. But sometimes dynamic dispatch is necessary.
I just want construction generics. But unfortunately, just that feature is going to be a huge catalyst for change in design everywhere
Will something like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=885704515adee7b14176be3fa18c8296) work?
Unfortunately it requires a decent amount of boilerplate. You have to create an enum with a variant for each possible type, and then implement the trait for it. In some other languages that have union types (e.g. `i32 | String`), this is made simpler. I think there is an RFC for them, but I'm not sure what the status of it is.
Yeah, it's easier to see in in an editor with a colored syntax.
Agreed! I had trouble with "why is `tokio::run` not returning, what task is still running, ugh" and ended up with a bunch of `trace!("task wakeup, current state: {:?}", self.state);` calls in my code to help figure it out. A dedicated system that's part of tokio itself would have been super helpful :)
that link doesn't work
It is a fairly balanced binary tree with a depth of 300 and many duplicate nodes in all layers
I solved my problem by skipping duplicate nodes in the tree. The tree was so huge and contained so many duplicates that skipping made an enormous difference. I expect techniques like using a Vec as storage for Node etc. could improve performance a little further, but skipping was enough to bring my program down to &lt; 1s execution time, which is good enough.
Well that's handy! I'm not sure why I couldn't see all of the Iterator methods for a vector, then.
There have been many pre-RFCs and RFCs for anonymous enums over the years, but little progress made on really agreeing on how exactly they should work; type based or index based, accessors or destructuring, etc. * [RFC Issue 294](https://github.com/rust-lang/rfcs/issues/294) * [RFC 402](https://github.com/rust-lang/rfcs/pull/402) * [RFC 514](https://github.com/rust-lang/rfcs/pull/514) * [RFC 1154](https://github.com/rust-lang/rfcs/pull/1154) * [RFC 2587](https://github.com/rust-lang/rfcs/pull/2587) (currently open, but discussion seems stalled out without much consensus) * [Pre-RFC alternative to 2587](https://internals.rust-lang.org/t/pre-rfc-sum-enums/8782) * [Another active pre-RFC](https://internals.rust-lang.org/t/pre-pre-rfc-structrual-enum-types-implemented-in-enum-exchange/9051) There are lots more pre-RFCs on the topic on [internals.rust-lang.org](https://internals.rust-lang.org/).
The features shown here are link-blocks and link-loops with their custom syntax. If you play around with the example, you might notice that there not that many ways to write the same program. Dyon forces you to use new-lines and parentheses in a such way that the code is more readable. A little more technical details: You can put bools, f64s, strings and links inside link blocks which gets "flattened" to a link. The link structure is more efficient to use than arrays or string concatenation. It is also debug-able, meaning you can iterate through the elements after generating the link, but before converting the link to a string. Links are also understood by Dyon's lifetime checker, so it kind of fits together with the language overall by being simpler to use than arrays (if you return an array, you must either clone it or its items must outlive the return value). You can also declare variables or call functions that return `void` inside link blocks, making it easy to combine generation with execution. The link loop skips items if you call `continue`, and `break` has similar semantics by skipping the generated items in the last iteration. Dyon uses special-purpose loops a lot, kind the way you would use iterators in Rust. The benefit of special-purpose loops is that you get e.g. `return` work the way it works normally. This is very convenient because in the kind of programming I often do, I loop through things all the time. There `sum/prod/any/all/min/max` loops and others. Dyon also composes "secrets" which are kind of constructive proofs, such that if you ask "are all items in list larger than X?", then if not, then you can also ask "why not?" and get the answer in the form of the indices of the loop-composition for which the answer was "no". It exploits a dual-representation of mathematical functions, one that performs the search and the other that performs the theorem proving. Search is often easy to write, but theorem proving is hard to get right. This way you can write very short programs as search and get the correct theorem proving for free.
What is that? Can you show an example that shows how drastically different things could be? Thanks
Just generally the one side where you have collections instantiated in fixed sizes, and then in other cases where compile time values will be used for other optimizations, say if you had a CSV parser and line ending was a compile time parameter. Not saying that will happen, but similar will.
That does! I haven't dug that hard into HashMaps, didn't think to do it this way, but it probably makes it easier using a simple \`Struct\` instead of a \`Turple\`, since its harder to mix up the parameters. Thanks for the playground link.
Could you describe the problem created by "a ton of dependencies"? Is your binary size too large, for example?
Index-based, accessors? Hmm, the most promising idea and to be too only allow them as an implementation of impl Trait. That avoids all the pattern-matching questions, and I think it covers most of the cases where an enum is more performant but a lot of extra work
I'm currently learning Rust by doing the Advent of Code in it, and I use collect() all the time. If you don't mind me asking, what's the reason you include it in the list, and what would you recommend instead?
Error handling is a big pain point for me. I just don't really know what to do. Should I be using failure? Should I be using error-chain? Should I be creating my own error types? Do I have to implement From&lt;ParseIntError&gt; for MyError and every other standard lib and crate error type and have a massive boilerplate error file? I feel like I need a lot more guidance, but the community seems a bit in flux.
IDE support above all is what I desire. I use a combination of both intellij-rust and vscode+RLS because they both work in different cases. The IDE experience is sometimes torturous compared to java. I tried to use RegexSet from the regex package and it just plain wouldn't provide completions in either setup. 
&gt;An attempt to implement `fn with&lt;T, F, R&gt;(&amp;mut T, f: F) -&gt; R where F: FnOnce(T) -&gt; (T, R)` Why is this a red flag for you? I'm quite new to Rust and haven't been able to figure out yet what people generally think of such a function, but so far people don't really seem to miss it.
You are CRAZY (in a good way).
They were originally planned for the language, but scrapped early in because of the philosophy of "pay for what you use." The closest equivalents would be rayon and futures.
&gt;* The rust team actively encouraging a python2/3 style quagmire where code will never be updated for new standards but still widely used, increasing the bugs in the compiler and preventing possible optimizations. Python is a good example of why backwards compatibility is important. JavaScript on the other hand is a good example of why backwards compatibility can be harmful. It's a lose-lose situation.
&gt; Index-based, accessors? Index based is like the ["disjoins" proposal in RFC 1154](https://github.com/canndrew/rfcs/blob/master/text/0000-disjoins.md) or the currently open [anonymous variants proposal in RFC 2587](https://github.com/eaglgenes101/rfcs/blob/master/text/0000-anonymous-variants.md); they act like a tuple in which only one element can be filled at a time, rather than a union of types in which each type can only appear once. If you only allow each type to appear once, there can be problems of composability; if, for example, you have `i32 | T`, where `T` is generic, what happens if `T` is also `i32`? Does it fail to compile, does it work but the type can just support `i32` without being able to distinguish the two cases, etc? Accessors refer to an explicit notation for talking about one of the type variants, such as in [RFC 2587](https://github.com/eaglgenes101/rfcs/blob/master/text/0000-anonymous-variants.md) which uses `::0`, `::1`, etc, while some of the other proposals only allow accessing the different variants via a destructuring `match` statement.
Fair points, but in this case `MissingAttributeError` will contain a clone of the `XmlNode`'s name, which is a `String`, in the future to provide better error message, just like the other error types. In this case, wouldn't `ok_or` be worse than `ok_or_else` because of eager evaluation? I would assume that, cloning a `String` costs more than creating a closure. I'm not really happy about cloning a `String` into an `Error`, but i need the `Error` to live longer than the `XmlNode.`
I think u/EarthyFeet is referring to const generics, as described in [this tracking issue](https://github.com/rust-lang/rust/issues/44580). I don't believe there is such a thing as "construction generics."
I could definitely be missing something, but if I understand this signature correctly, this cannot be (safely) implemented because F wants a T parameter, but you only have have a &amp;T, and T is neither Copy nor Clone so you can't pull it out from the reference.
Woudl have been cool to have this besides the ? operator. So `foo?` means unwrap foo and propagate errors, while `foo!` means unwrap foo and crash on errors.
Which one do you work in most, intellij or vscode? and when does intellij work when vscode doesn't? (also happy cake day)
Thanks for proof reading... I guess I was exaggerating a little bit too much... Also, English is not my first language 😜
A hybrid solution sounds really cool, and groundbreaking actually ;) To be fair though, our current issue is that we could really use help with getting a minimum viable product first. Especially considering the current climate of the web. Many of the larger services are starting to lose their userbase over privacy concerns. Therefore getting a working app, even if it's not perfect, takes a bit of priority. 
Tuples are convenient, but I tend not to use them if there's ever a chance I'll have to refer to one without looking at the definition in front of me. Plus by making it your own type you have the power of adding behavior to it if you ever needed to. Also, instead of using a HashMap, you could also just use a HashSet. It allows you to get a little more creative and it may look cleaner. Look at [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d48976b487f5771582c9f379713e2527) for an example. 
BTW, you can derive these traits: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f0b316474431ca96b208e0afec2161f3
&gt; In case hand-parsing is needed, not using state machines (implemented with enums) Could you provide some links on how to do this properly?
&gt;hovering over the type with documentation above single-line attributes led to 100% CPU usage https://xkcd.com/1172/
Oh cool, thanks for explaining those concepts. I would assume they would be union types, i.e. if `T = i32`, then `i32 | T` is the same as `i32`, but I see that they are other ways to think about it. I think that enums are already a pretty good solution for the other case. (Coproducts, I think they're called?) The nice thing about only using them for `impl Trait` is it avoids all of those other issues. All you can do is call the trait methods that are exposed, or convert to a `dyn Trait`. And as an optimization, when it converts to `dyn Trait`, it could figure out what the type of the value is and use its vtable directly, instead of creating a vtable for the union type.
I think this would help? https://www.reddit.com/r/rust/comments/a7n5hb/enum_dispatch_speed_up_your_dynamic_dispatched/
vscode+RLS pretty much always gives me the same errors as cargo check and infers the same types as the rust compiler. Intellij-rust is pretty much better at everything else. It's more performant, completions tend to work almost all the time, it can auto add imports, go to source definitions, rename types, extract variable etc. But intellij-rust sometimes says valid rust is invalid or otherwise disagrees with the compiler (they built their own parser and borrow checker, which I think is the path to madness, but means they get a load of stuff for free from IntelliJ). I've also spent many years using IntelliJ for Java so that's a big plus for me. tl;dr: intellij-rust is better when it works, but often disagrees with the rust compiler so I fall back to vscode. 
https://github.com/Kixunil/dscfg/blob/master/README.md
especially when auto-complete with RLS doesnt show traits that are not imported. I understand why it works like that, but its incredible frustrating to have to know what traits to include to parse a string to an integer for example. 
You defined MissingAttributeError as a structure containing a `&amp;'static str`. It can't contain a clone of `String` by definition and lives as long as the program does.
Exactly! I'm going through the AoC right now and I'm more used to Scala's language support. Trying to figure out where I am an what I can do next.
&gt;Not making proper use of iterators (unless performance is of notable concern - iterators tend to incur a marginal overhead when compared to good old loops, despite zero-cost claims). I'd love to know more about this, do you have a link to discussions about this?
No, not really. It just takes longer to compile and it seems unnecessary.
I use collect all the time too. It's fine. I think the key here is that we're talking about "red flags," which are warning signals. They don't always indicate a problem, but are things that might draw one's attention because there might be a better way. With collect, the idea would be to attempt to solve your problem without building up a new collection in memory because building that collection might be expensive. It's mostly a performance concern I think. In AoC, this doesn't matter too much and intermediate collections can often making the code much simpler. For example, `collect` is something I would use sparingly in a core library where performance is a concern, mostly because `collect` doesn't really let you amortize allocation. Now, in cases where you know that the cost of allocation is dwarfed by something you're already doing, then it's probably fine! (I think this general line of reasoning applies to most/many things mentioned in this thread.)
I'd say that `collect` is fine and in fact super great when you actually are building a collection. But sometimes people will do it just to drive an iterator. i.e. they want to print a bunch of values but since iterators don't do anything until they're used, they add a `.collect::&lt;Vec&lt;_&gt;&gt;()`at the end instead of driving the iterator with a `for` loop or something.
Aw heck yes. I would LOVE to make React that much easier to integrate with a Rust project.
Maybe in the future it should be possible to do `for &amp;[x, y] in s.windows::&lt;2&gt;()`. Where windows becomes an iterator over &amp;[T; 2].
Just keep everything a iterator until you need to actually collect it. Collect is fine if you need to convert the iterator to a data-structure, but a lot of times you just need to iterate over it. Calling collect to later iterate is a waste of time and will cause useless allocations. Worse: let vec: Vec&lt;_&gt; = (0..100).map(|v| v*v).collect(); for v in vec { println!("{}", v); } Better: let iter = (0..100).map(|v| v*v); for v in iter { println!("{}", v); }
Oh, I didn't read the function signature carefully enough, thanks for pointing that out! I was thinking of this: fn with&lt;T, F: FnOnce(&amp;mut T)&gt;(mut t: T, f: F) -&gt; T { f(&amp;mut t); t } `with` is probably not the right name for this, Kotlin seems to call it `also`.
Just the initial compile is slow, right? IMHO, that's not too high a cost to pay for reqwest's "batteries included" nature, but you could always go down to the `hyper` level if it's too much. I'm looking at reqwest's dependencies now and don't see much cruft. At most: * `encoding_rs` is only used for `Response::text`, which you aren't using for JSON. I suppose that could be an optional (but included by default) feature. * `uuid` is used for random boundaries in `multipart/form-data` requests, which I don't quite get (thought multipart stuff was supposed to be delimited in some other way that doesn't require the boundary doesn't occur in the content). `uuid` is tiny though so I don't think it matters much for compile time. * `base64` for basic auth, which you aren't doing. Perhaps auth methods could be pluggable via separate crates (perhaps should, as some have much more exotic requirements). The other stuff I think you need. `https` should not be considered optional in a high-level API, IMHO; everyone should be encouraged to use it, so no point in compiling it out. `tokio` and `hyper` of course are what it's built on. And the automatic compression stuff achieves significant bandwidth savings. What would you take out?
Nothing of the top of my head, other than a fair bit of first-hand experience writing high-performance code. Iterators almost always come with a marginal overhead compared to iterators, except in incredibly trivial cases where it's obvious to the compiler that the iterator is just equivalent to a flat loop.
I am running my rust code with \`cargo run\`. Then, I hit \`segmentation fault (core dumped)\`. I tried adding \`RUST\_BACKTRACE=1\`; it doesn't seem to show the stacktrace. Eventually, I added \`println!\` everywhere to see which line caused the problem. The error is not surprising because I do use raw pointers. What I wonder is that if there's a better way of debugging \`segmentation fault (core dumped)\`. Adding \`println!\` isn't productive. Thank you
Anything practical to use? Last I heard was [this issue](https://github.com/rust-lang/rfcs/issues/1220).
Backtraces are what you get when you panic (your program is aborting itself) but segfaults are lower level than panic (the OS is killing your program). To get a backtraces out of a segfault, you need to run your program inside a debugger like gdb, as you would with a C program.
Didn't clippy warn about `ok_or` saying `ok_or_else` is better because it may not need the value passed, when `ok_or` always needs the default value (may allocate and things like that).
Looks limiar to my project \[websocat\]([https://github.com/vi/websocat](https://github.com/vi/websocat)). &amp;#x200B; websocat -Eb ws-l:127.0.0.1:8080 unix-connect:/path/to/socket &amp;#x200B; websocat -Eb seqpacket-listen:@abstract\_address ws://echo.websocket.org
Templates on a webserver. I've been spoiled by webpack. I'd love to have a templating library that supports two modes: \* Live reload - always render the templates from the file system \* Compiled (release) - compile the render logic into the final binary (a la askama) I really love Askamas method (object + attribute for template path). I just hate recompiling for small teaks.
this subreddit it is for the rust programming language, not the video game
My similar project [websocat](https://github.com/vi/websocat) can do SEQPACKET: https://github.com/vi/websocat/blob/master/doc.md#seqpacket.
You are looking for /r/playrust
You're looking for r/playrust. This is the subreddit for the rust programming language.
The simplest way would be to have one thread read stdin and a different thread continuously output to stdout. It's possible to use a single thread for both if you use something like Tokio or Mio (those libraries wrap platform-specific async IO facilities like epoll on Linux and IOCP on Windows), but that's substantially more complicated, and for a simple terminal program my instinct would be to stick with threads.
Even if you're writing naive code with Rust, I'd say don't worry about performance problems if you're still learning. The gamedev community is only getting better here. Oh, wait...did you mean Rust the game? ;) That's over on /r/playrust. But you're welcome to stay and to learn about code if you're interested!
I mean, in Rust code review specifically, I suspect it's gonna be mostly the ussr one
You can put things like `#[cfg(feature = "foo")] { ... }` inside your doc tests, and then hide the ugly lines from the rendered result: https://doc.rust-lang.org/rustdoc/documentation-tests.html#hiding-portions-of-the-example
Maybe Rust could do something like the Haskell platform, and include really popular generic libraries in that distribution. We have Rust 2019, why not have Rust 2019 Platform Edition?
What about `foo()‽`
I assume they want to compile to webasm, right? It seems to make sense. They want react to be light, small, and efficient. Webasm can provide those gains, but only if the language is fairly close to the metal and doesn't come with a lot of baggage and giant compiled binaries. So what are your options? C, C++, and rust? Rust seems to make a lot of sense. Am I thinking about this right?
Oh, I see. I'm afraid I've done that. Thanks for the explanation, also to the other replies :)
The section under [Linear Transformation](https://myrrlyn.net/blog/misc/rust-flow#linear-transformation) seems to imply that calling map on a Result would modify the value (and type!) of the existing binding. In the code snippet, four different Results are produced, and none of them is of type Result&lt;u64, String&gt;. As rustc will helpfully point out, the return values should be used.
Basically, you can consume an iterator in a few ways 1. You can create a list from it (don't do this for infinite iterators) (vec or other collection) 2. You can give it to another function (e.g. zip). 3. You can fold it down to a single value 4. You can do something for each element The only time you want to use `collect` is (1), the rest of the time you don't ever need to collect all elements in memory, and doing so wastes performance.
Yeah, I would say you're right. Looking into the crystal ball for a second, if they went forward with this they would most likely start with the reconciliation algorithm. That is a relatively expensive computation which determines which elements have changed and are flushed to the DOM. This could see a significant performance boost if done correctly. Also, not all browsers support wasm so they will most likely have to feature detect wasm, shipping a JavaScript equivalent algorithm for users on unsupported browsers. Rust may provide a solution for building the JS version as well, but I don't know enough about that. Hopefully the rust team and the react team can work together on this, React is pretty big :)
I think you may be right. I'll consider it - I'm very new to Rust, so I don't know enough about the specific dependencies and just thought that it seemed unnecessary to have about 140 dependencies while something like mio\_httpc has about 60. I didn't realise https support required so many additional crates, so thank you for mentioning it. I definitely can't do without it. In any case, the web requests are only part of a minor feature of my application, so I probably shouldn't spend too much time on it.
Thanks
Thanks
Thanks
Your code has race condition bugs. In extend between your ref count check (which might be 0 at moment A) and your extending it at A+1 another thread might have gotten a slice of your vec.
`foo()❤️`
Ah, ok. I mixed things up. Thanks for clarifying those. That would be awesome! I can give you comments on your post then if you want.
I can’t find the thing I was looking for, but the comments in https://github.com/rust-lang/rust/issues/49410 seem to imply a few command line switches.
Oh, I hadn't seen/considered `mio_httpc`. It doesn't use tokio, which is probably most of the difference. If you have no other need for tokio, that might be significant. But at least for now, it's what the cool kids use for async, so you might end up pulling it in anyway.
That feed says they're all trolling so...
That's the most beautiful thing anyone has said to me :)
We proposed this in 2016 (IIRC) and the community hated the idea.
I'm attempting to convert from csv to json (csv and serde crates) without having to deserialise into a struct first. How do I do this with the crates above? I can't figure it out at all
`foo()🔥`
There are associated type constructors, a.k.a. generic associated types - that would allow for example an iterator item to have a lifetime associated with it. Chalk is a prerequisite for this I think.
I don't know if you are open to using nightly (for now -- hopefully will be stable this year!), but Rocket 0.4 has [live template reloading](https://rocket.rs/v0.4/news/2018-12-08-version-0.4/#live-template-reloading) now, and compiles templates into the binary on release.
It's a breaking change then.
&gt; An attempt to implement &gt; fn with&lt;T, F, R&gt;(&amp;mut T, f: F) -&gt; R where F: FnOnce(T) -&gt; (T, R) So no [replace_with](https://crates.io/crates/replace_with) / [take_mut](https://crates.io/crates/take_mut)? But how do I implement a state machine without a temporary invalid state then (in panic=abort case)?
&gt;Response Ok. I think I'll probably just use reqwest then. Thanks for your trouble!
Nice! This is a good improvement over my usual go-to, of `du -h -d2 | sort -h`. I really like the aggregate feature, that's something that I have wished some other similar tools would have. Couple of things I notice off the bat that could use improvement. * The used space bars take up too much space, and waste a lot of space, especially on narrower terminals (I generally use 80 column or half-screen terminals). The wasted space seems to be due to the fact that they are scaled against 100% of the top level directory, but if you have a lot of directories which each take up a small fraction of the total space, then you have a lot of wasted whitespace. ├─ servo │ ███│ 9% 5.23 GiB │ ├─ target │ ░░░│ 44% 2.33 GiB │ │ ├─ debug │ ░░░│ 99% 2.33 GiB │ │ └─ &lt;aggreg │ ░░░│ 0% 7.72 KiB │ ├─ .cargo │ ░░░│ 26% 1.40 GiB │ │ ├─ git │ ░░░│ 78% 1.10 GiB │ │ ├─ registr │ ░░░│ 21% 300.89 MiB │ │ └─ &lt;aggreg │ ░░░│ 0% 742 B │ ├─ .servo │ ░░░│ 11% 611.48 MiB │ │ ├─ rust │ ░░░│ 96% 591.04 MiB │ │ └─ &lt;aggreg │ ░░░│ 3% 20.44 MiB │ ├─ .git │ ░░░│ 11% 605.78 MiB │ │ ├─ objects │ ░░░│ 85% 516.43 MiB │ │ └─ &lt;aggreg │ ░░░│ 14% 89.35 MiB │ ├─ tests │ ░░░│ 4% 233.20 MiB │ │ ├─ wpt │ ░░░│ 95% 223.53 MiB │ │ └─ &lt;aggreg │ ░░░│ 4% 9.67 MiB │ └─ &lt;aggregate │ ░░░│ 1% 91.63 MiB * It is slow. I compared to my usual go-to solution on my `src` directory, and it is much slower. This is on macOS, and this is after having run these a few times to warm up caches. I suspect the reason for this is that you're doing too many syscalls; the code [seems to use `Path` to refer to files in many places](https://github.com/nachoparker/dutree/blob/master/src/lib.rs#L475), and then does various tests on those paths, which mean extra system calls to query the same information multiple times. I would recommend instead passing around `DirEntry`, from which many attributes can be queried without extra system calls, or even better using the [`walkdir` crate](https://docs.rs/walkdir/2.2.7/walkdir/) which is a pretty well optimized directory walker that provides its own slightly richer `DirEntry` that caches additional metadata. $ time sh -c 'du -h -d3 | gsort -h' ... real 0m36.430s user 0m1.801s sys 0m14.163s $ time dutree --aggr=100m -d3 163.37 real 11.67 user 104.74 sys /u/nachoparker, would you prefer these filed as tickets on GitHub?
Shoot, I see how it can be read that way. I'll take the post down in case you're right. 
&gt; Excessive use of `Mutex`, `Arc` or `RwLock` I suspect this might maybe apply to me. Could you elaborate on what is considered "excessive"?
&gt; A default behavior that "intelligently" expanded the "most relevant" directories would be really nice. I'm not sure what rule to use, but something like "anything that makes up more than X% of the total space I'm analyzing right now gets expanded" might be a start. Yes, this would be great, also combined with the aggregate feature; what I usually want to see is "what files are directories would make a significant difference in my free space if I deleted them"? A default of auto-expanding at 10%, and auto-aggregating at 5%, or something of the sort (I just picked those somewhat arbitrarily and would need to experiment to see what would be the most useful) would help me quickly be able to see just the things that would make a substantial difference in used space, while not being bogged down with thousands of small files. Another possible way for auto-aggregate to work would be to have it not based on the size of the individual file, but aggregate when the total size of that file and everything smaller was below a certain percentage threshold. I have some directories, like my `src` directory where I check out a lot of open source projects that interest me, where there's a long tail of smaller stuff that's probably not worth sorting through because even if I deleted all of it it would be less than, say, 5% of the total space. If you just had a fixed auto-aggregation threshold of files below 5% of the total space, then you could have a lot that add up to more than 20%, say. But if you have a threshold based on everything for which the sum is less than 5% of the total space, then you could get a better sense for how much is worth sorting through and deleting. Anyhow, just some thoughts, from someone who is a bit of a data hoarder but on a laptop SSD, so has to frequently sort through a lot to figure out what to delete.
Thanks, didn't notice
&gt; Is there a way besides setting author to blank in structopt [...]? I had the same problem, but also only found this way. I find this especially annoying since you have to repeat this for every subcommand. (For versions, on the other hand, you can disable showing them for all subcommands with `#[structopt(raw(setting = "AppSettings::VersionlessSubcommands"))]`).
I just spoke with the maintainer of bigdecimal-rs. He asked me to post the following, since he's not a redditor: The maintainer of BigDecimal wants to include an optimized static-size variant of BigDecimal, but unfortunately can't devote enough time to it right. He certainly would appreciate discussion at [https://gitter.im/bigdecimal-rs](https://gitter.im/bigdecimal-rs)
&gt; This patch release fixes a build failure on powerpc-unknown-netbsd by way of an update to the libc crate used by the compiler. Great to see a fix being included in a patch release for a non-tier1 platform.
&gt; Breaking projects into lots of crates even though those crates all really relate to the same problem I've done this in the past to improve compile times. Unfortunately incremental compilation isn't yet good enough to be as good as breaking a project into subcrates.
&gt; How badly have I implemented this No worries, we're all learning :) &gt; have I missed any serious safety problems Yes, at least two serious problems. The first (this one's fixable): unsafe { &amp;mut *(&amp;self.inner as *const Vec&lt;_&gt; as *mut Vec&lt;_&gt;) }; Converting a shared reference into a mutable reference causes undefined behavior. Some say that even _thinking about_ converting a shared reference into a mutable reference causes undefined behavior. See for example the scary warnings about transmute [in the 'Nomicon](https://doc.rust-lang.org/nomicon/transmutes.html). You should use an `UnsafeCell` internally instead, if you want to do this kind of conversion. (If you're interested in writing unsafe code for real production use, you should absolutely read the Rustonomicon cover to cover. It's also a great way to build your understanding of the language as a whole.) Another problem (this one's harder): if self.ref_counter.load(Ordering::SeqCst) != 0 As /u/abudau pointed out, this check doesn't prevent races. Two threads could attempt to `extend` at the same time, for example. One way to think about what's missing here, is that the thread calling `extend` never does anything to inform other threads that it's _finished_. The standard way to use an atomic to protect a critical section, would be something like having `0` represent "available" and `1` represent "locked", and having threads entering the critical section `compare_and_swap(current: 0, new: 1)` on the way in and `store(0)` on the way out. To deal with contention from other threads, rather than just panicking if the `compare_and_swap` fails, you could attempt the `compare_and_swap` in a loop, on the assumption that it'll only fail a few times before eventually the other threads finish. At that point, you'd have a custom "spinlock". You could compare that to [one of the spinlock implementations available on crates.io](https://crates.io/crates/spin). That said, there are a couple options you could consider from the standard library, which don't require you write any unsafe code. You probably already looked at `RwLock`; that would be pretty standard for use cases like this, but I'm guessing you started this project precisely to avoid using locks. Another option, if you're fine with panicking when there's contention, is actually just `Arc`. The `Arc::get_mut` method can get a mutable reference to the contents of the `Arc` if there are no other clones outstanding, and you could implement `extend` with that.
That's only available on quantum architectures.
You can take a look at what clippy warns about [here](https://godbolt.org/z/vkBXQn). Basically, if a function call is involved in constructing your error, and rustc fails to inline it or its contents, you'll get a performance penalty.
The good news is, someone literally *just* made a crate to do it automatically! https://crates.io/crates/enum_dispatch https://www.reddit.com/r/rust/comments/a7n5hb/enum_dispatch_speed_up_your_dynamic_dispatched/
Thanks for the help. I've fixed those problems, but I still don't exactly understand the `Ordering`s for the atomic types. Is it okay to use SeqCst for everything in this case? If it's just for optimization, than I'll probably avoid it for now.
Do you have any advice on how to fix it? Right now, I have this, if self.ref_counter.load(Ordering::SeqCst) != 0 { panic!("Modifying ExtendVec while borrowed!") } while !self.modifying.compare_and_swap(false, true, Ordering::SeqCst) { } with self.modifying being an AtomicBool storing whether it is being modified, and self.ref_counter being an AtomicUsize storing the number of references. Currently a slice could be borrowed before the modifying lock is taken, but even if I had both of the conditions in the loop (with `&amp;&amp;`), a slice could still be made in between those two. Is there any way of combining two atomic operations on two different variables together/
&gt; std::mem::uninitialized Thats even worse and dangerously unsafe, though? And swap would require making a duplicate empty item to swap in for no reason
Interesting, what did they dislike about it? Was it because it's possibly biased in the sense that someone has to choose the "blessed" packages?
I forget exactly, but it was called the “rust platform”, you should be able to find reddit threads on it.
Should I use rust? I am a developer who has written a library in C++. I have heard many good things about rust and intend to give it a try. I am also creating a "default runtime" per se, for my library. Currently the runtime has not been started, but will use Chromium V8 (which I have been able to embed on osx). I am currently rewriting a lot of my library code to be testable. Firstly, would it limit my libraries usability in C++ by creating it in rust? Based on my research I would need to provide similar data structures written in C/C++. Secondly, would it be easy to integrate a rust executable with an already written C++ dynamic library? Based on my current knowledge of rust, I would opt for keeping the library in C++ and writing the executable in rust.
Yes, [sometimes it can](https://github.com/seanmonstar/futures-fs/issues/3#issue-273782690).
can you post the source of this "naive" rust server, because I'm just going to say it shouldn't beat Actix. That doesn't make sense. nginx will use caching and some other smart things to speed up static file serving, but isn't static file serving usually bottlenecked on the bandwidths available to the server?
It’s interesting that actix isn’t faster than the nieve server; it should blow it away on this kind of test. How did you run the test? Like what commands?
If all the code in the dependencies would have been directly inside of the reqwest crate you would not have noticed it. Unless you really want to avoid increasing your binary size, I would recommend not being too cautious about indirect dependencies, as long as you vet your direct dependencies to be reputable.
It's a red flag, not necessarily wrong. If you handle panics correctly, it's OK. The thing is too few people handle it incorrectly.
I dunno. nginx is written in C, and hyper-optimised for this use case. It's gunna be pretty hard to beat... Actix surely has some overhead for it's actor model?
To work around the problem, borrow a few immutable references to your entities, then get the necessary decision data out of them. Once you have collected the information, you can do the same loop where you mutate each element. It'll look something like this: ``` let mut mgr = create_mgr(); let e1 = mgr.get(3423); // get(&amp;self, id: usize) -&gt; &amp;Entity let e2 = mgr.get(9938); let information = compute_something(e1, e2); // NLL, should probably work, otherwise scope the code before so the references are dropped let e1 = mgr.get_mut(3423); e1.set_something_based_on_information(&amp;information); let e2 = mgr.get_mut(9938); e2.set_something_based_on_information(&amp;information); ``` ps you probably want to use `usize` and not `u128`.
Because most people trying to do it forget to handle panics correctly.
nginx is a general purpose tool, it isn't "hyper-optimized" for this use case. Actix is [right up at the top of the charts](https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=plaintext) for serving plaintext content. In the last benchmark round, it was *literally* the top performer, but it has been slightly edged out this round. Still, 7 million requests per second on a single server? Yeah, that's some good stuff.
It can be safely implemented but it's tricky and you end up forcing abort if f panics.
That's exactly why I included it in the list. Thanks for giving explanation, so I don't have to!
Rust 1.31.1 or as I've recently taken to call it Rust 2.xkcd
The code for the naive rust server is here: https://gist.github.com/codesections/71893961840f5bd080225eafe063afd8 and the code for the actix-web server is her: https://gist.github.com/codesections/cccbd299f11e16e4e9c9fb53101b1ced Note that I limited the actix-web server to one worker since my intention was to test single-threaded performance. Without that limit, the actix-web server did much better (~49k req/sec). However, with an equal number of threads, nginx also did even better (~80 req/sec). All of these benchmarks were taken using the `bombardier` tool (a simple benchmark tool written in golang) 
As I mentioned in reply to coder543, I used `bombardier` for the benchmarks. https://github.com/codesenberg/bombardier. I performed its default test (`bombardier &lt;URL&gt;`), which saturates the server with 125 simultaneous connections for 10 seconds and reports how many responses it receives. 
&gt; I could just as well just be using syntax highlighting alone. In my case I often am, considering how often RLS hangs and won't restart(just hangs again) It really hates cargo workspaces and/or opening files in sub-crates and/or opening files not part of the current project all I want for Christmas is a semi-reliable RLS experience.
A quick glance at the code looks good to me. Great to see documentation and a detailed README. Also +1 for sr.ht.
Good question, when trying to find one, I found that my first Rust project ever contains a good example: https://github.com/Kixunil/image_concat/blob/master/src/main.rs - look at Parser enum and its step() method. Please ignore panics and the rest of possible problems - I was learning then, but did the parsing right, when we disregard panics. I have another project with FSM that is private for now. It may be open source one day, but I don't promise anything.
Why do you think you need multiple mutable references to the same Element at once? That is so impossible in Rust that it barely even makes sense to say the words "multiple mutable references." That is your root problem.
What would that look like? I thought one of the main ideas of rust is that you can't just take ownership of a thing you don't own... and if you can't do that, how could you give ownership of a thing you don't own to another function? I could maybe see doing it with some super-unsafe raw pointer junk (like, `self as *const _ as *const u8` and then copying u8's to an owned piece of memory the same size as T. This is pretty obviously a bad idea though)
Thanks for all the info, and thanks for sharing your work with us. I did some very basic benchmarking with rust bench on division and found your crate and bigdecimal to be quite similar, but the decimal crate was faster (by a factor of ~3). Very glad to hear that you've done all this testing for correctness.
It would be nice if there was a way to use this with externally-defined traits.
Thanks for sharing! I noticed it was meant as a module intended to be added to the "num-*" project but wasn't merged due to doubts about the best design. I don't know how if thats a long time ago or not but do you know if there is any progress on that end?
You can avoid it with \`mem::uninitialized()\`, but you really need to be sure that the value does actually exist before you try to use it or..... \*\*finger guns\*\*.
That was a long time ago, and I'm not sure if anything has changed since. IIRC, the reason it wasn't merged into num was mostly just due to feeling like it ought to be a separate crate anyway. If anyone else remembers, please jump in
It's dangerously unsafe only if the value may actually be uninitialized before you read from it. If you do something really simple like: \`\`\` fn read\_byte(r: &amp;mut Reader, c: &amp;mut u8) { \*c = r(); } fn read\_file(r: &amp;mut Reader) { c = unsafe { mem::unitilialized() } ; loop { // This value is never read before being initialized, and you can // prove this, so it's fine. read\_byte(r, &amp;mut c); } } \`\`\` If you ever have any conditional codepaths though so you can't ensure 100% the data will be initialized before using it though, yeah, use something a lot, lot safer.
&gt; I suppose it is not public though? Not yet. Soon though. :)
Congratulations, Andrew! I don't know anything about Korean but I do appreciate the effort you've put into your documentation.
[may\_http](https://github.com/rust-may/may_http) is simple to use, and if you're familiar with Go, you will find [May](https://github.com/Xudong-Huang/may) very natural. 
In general, `SeqCst` is the most conservative and the proper default, and everything else is an optimization. The property you lose when you do something other than `SeqCst` is kind of subtle. If you want to drink from the firehose on this topic, watch Herb Sutter's [Atomic Weapons](https://youtu.be/A8eCGOqgvH4) talk. (Almost three hours put together. This stuff is _nuts_.)
If it's used in a very specific isolated environment from which it cannot escape (i.e: a `drop` method) I see no problem with using it.
&gt; Is there any way of combining two atomic operations on two different variables together. There is not! But that's a really good question, and it turns into an entire sub-field of computer science: https://en.wikipedia.org/wiki/Software_transactional_memory Luckily, in this case, we don't need anything quite so powerful. You can solve this problem by protecting all accesses to `ref_counter` with `modifying`. That is, callers doing an `extend` will take the modifying flag (_before_ checking the reference count), and callers making a slice will also take it, just briefly while they modify the count. That gives you the guarantee that as long as you're holding the flag, you know no one's going to swoop in and change the reference count out from under you.
I've seen people decide to wrap almost everything in these types to avoid the wrath of the borrow checker. This isn't good: all of them incur a runtime cost when using them. Instead, it's better to plan out in advance exactly where you need to guarantees they provide and whether you can instead use a subset of their capabilities or even not use them at all. For example, a very idiomatic thing to do is to avoid sharing memory between threads at all by making use of a set of `mpsc`s to communicate information.
I think it's still a rad idea; There are clearly some areas where there's one or two canonical blessed crates by the community... e.g `rand` and `rayon`. I even think you don't need to pick between two popular crates in the same domain... if there are two, include both.
If that object implements Drop, then you will have to prevent it's destructor from running, which is pretty easy to miss. Using Options to "hack" around the lifetime system may indicate poor design. And if the design can be refactored, great. But, if not, falling back to unsafe code without a good performance reason instead of just living with the Option feels unnecessarily risky.
AFAIK, there are no guides avail.
I think this all depends on the context. Personally, I've used both methods in the past although I try to avoid using either whenever possible.
Would it be possible that you could add some 1:1 comparison with `failure`? Btw. `failure` is still 0.x. Have you thought about improving it instead of creating new crate?
Thank you!
My guess is it couples the standard library to a non essential library that could become an anti pattern or outdated like half of the Android library or the old Java Time library. Better to let natural selection occur in cargo allowing more people to iterate on libraries that aren't essential or that the community isn't certain about being the best implementation imo.
I would suggest reqwest for http request and serde for deserialising json to struct.
Thanks for the help! I guess it will probably end up being useful to know how to properly use atomic operations, at least in the simpler examples. 
You should add a code example to the `README`. It doesn't have one, and the docs' main page doesn't have one, and the first time we see some code that use chainerror is the **second** chapter of the tutorial. I suggest you put that example from the third chapter of the tutorial in the `README`. And of course, add a dump of that code's execution.
Someone just asked the same thing: https://www.reddit.com/r/rust/comments/a7xl3i/beginner_best_lightweight_crate_for_simple_https/
Won't nginx also use `sendfile()`, whereas it looks like the naïve Rust implementation is reading the file contents into the process?
https://github.com/rust-lang-nursery/failure is currently the best approach.
It's certainly doable, but it might be a bit of an intense process as a first approach to Rust. Rust is able to FFI with C, but interacting with C++ directly is more complicated and not always doable. So you would probably need to make a C interface for the C++ library. Once you've done that though you can use [rust-bindgen](https://github.com/rust-lang/rust-bindgen) to automatically generate callable functions structs on the Rust side. From there you can either directly use the raw bindings in your executable (which will involve `unsafe` blocks since the Rust compiler can't verify that foreign code is upholding the right invariants), or you can create another library on top of the bindings that abstracts away the unsafe stuff and then depend on that in your executable.
Static files support in actix is very simple, it uses simple cpupool for reading files.
Correct me if I'm wrong, but procedural macros cannot modify existing code, only append to it, usually implementing traits or methods. Next, you have to [enable the `extra-traits` feature](https://github.com/dtolnay/syn#optional-features) on `syn` to get `Debug`. Furthermore, [Repetition in `quote!`](https://github.com/dtolnay/quote#repetition) uses `into_iter()`, so it takes ownership of the iterable item. For your vec, you should take a reference to it and give that to `quote!` instead, as `&amp;Vec` implements `into_iter` as well, but doesn't move the vector.
Do you know how you would do it with mio/tokio? Or resources for this? Thanks for answering
Adding more features doesn't necessarily mean software will have worse performance. nginx can serve as a reverse proxy, a load balancer, a cache, etc. *if you configure it to do that*, but if you configure it to just serve static files as fast as possible it can do that to.
I don’t really care too much about how it works on the baremetal or even if they are actually greenthreads. I care how syntactically easy it was to use them in Go. I haven’t learned how use futures yet in rust nor rayon but when I glanced at them they didn’t have the same convenience. 
&gt;There are notable exceptions, like `nom` [combine](https://github.com/Marwes/combine) is a library that accomplishes many of the same goals as nom, with much fewer macros. Not saying that nom is bad for using macros but even in a case like that there are other design decisions that are possible.
Can I convert `Vec&lt;Box&lt;dyn Trait + Send&gt;&gt;` to `Vec&lt;Box&lt;dyn Trait&gt;&gt;` (or other marker trait) without cloning or unsafe?
You have multiple warnings for unneeded mut and unused imports. Aim to have no warnings if possible, they make it harder to spot read the compiler output when working on the code and might hide more important warnings. Your tests are in lib.rs, the convention is to keep unit tests in their respective files. For example the text\_xml\_parser test can be moved to xml.rs without change. As its a new project I recommend moving to Rust edition 2018 right away It should be quite easy to do with the instructions provided on [https://rust-lang-nursery.github.io/edition-guide/editions/transitioning-an-existing-project-to-a-new-edition.html](https://rust-lang-nursery.github.io/edition-guide/editions/transitioning-an-existing-project-to-a-new-edition.html). drawingml.rs is getting quite big with more than 5000 lines, I highly suggest splitting it under a drawingml module. Use the `dyn` keyword when returning a trait to communicate that the call is using dynamic dispatch. `pub fn from_file(pptx_path: &amp;Path) -&gt; Result&lt;Self, Box&lt;dyn std::error::Error&gt;&gt;` &amp;#x200B;
I'm curious too, but I'm also curious why anyone would implement that because it would work out to not do anything particularly useful vs `{let o = &amp;mut the_thing_with_too_long_name; ...}` `with` in languages that have it is usually syntax sugar for issuing multiple method calls without having to repeat the lvalue (what's the new term, "locator expression"?) but Rust doesn't have the syntactic support. It's clear enough to create a short alias within a curly-brace scope, like I suggest. And that would be simpler than `with`. If `with` has the above signature then it also forces an unnecessary clone - unnecessary because the uniqueness of the `mut` reference will remain protected while `with` is on the stack. Therefore the `f` closure could just take the `&amp;mut` and `with` doesn't need to do *anything.* I could see *related* techniques being useful. Iirc the Redox kernel has a Mutex variant that requires you to pass the lock-holding code section as a closure. 
This. 
Sure! I'm just suggesting and maybe I get a spark of an idea lit up with you ... ;-) Either way, this sounds like an awesome project and I will definitively check it out as soon as I'm back home (mid February) and see what I can do to contribute! Non-centralized social networks are on my back burner for quiet some time (fully distributed, though I wouldn't mind joining your effort and maybe we can work in the "fully distributed"-part later on... But joining efforts instead of doubling it is always better, hence I'd love to contribute)!
You could still use Serde for that. Imo, it offers more flexibility, as you can transcode from json -&gt; protobuf for example. It also allows decoding as strongly typed or weakly typed.
 let new_vec = vec.into_iter().map(|x| x as Box&lt;dyn Trait&gt;).collect(); This works, however, I'm not sure whether or not it allocates. It shouldn't need to.
You're `collect`ing into a new collection, so it would have to.
&gt; Using macros to define DSL's as part of the external API Why not just create a function that will take a struct with the "parameters" of your DSL and convert it to the actual thing? I.e. converts a `std::fs::ReadDir` into a `my_crate::Directory`
How is support on tier 2/3 platforms in practical terms? Does it break every other release or is it mostly pretty stable? 
True! I think `nom` is an exception more because of its' explicit design choice to use macros than because what it does inherently requires macros. I haven't used any of combine, but I wouldn't be surprised if the different choice lets it be easier to learn and understand than nom.
As an update to this, I revised the naive implementation with the thread pool as laid out in The Rust Programming Language. This increased its performance to ~40k reqs/sec. Interesting to see that it flipped position with actix. I guess actix is very good at handling multiple threads, which makes sense given its architecture 
gottem bois
Thanks, super helpful! I'm actually using the macro to make an additional Struct with options to be used in a trait I wrote. It's kind of a pain in the ass to do all the impl to get it working but it's all super boiler plate so thinking proc-macro is the way to go. I'll work on it a bit and post my findings. Appreciate the comment!
You described almost to the letter what I was trying to do last week. Unfortunately I never got a good answer. I'll be watching this thread in hopes you are able to get to the bottom of it and and I'm able to use your solution. 
Ugh! Thanks for confirming. I'll write something up if I manage to get this working and understand what I'm doing.
I do a bunch of my Rust dev on `x86_64-unknown-freebsd` and for the most part it’s stable and working. 
The `allow`, `warn`, `deny`, and `forbid` attributes only affect compiler diagnostics, by making that specific one become silent/a warning/an error. More information [here](https://doc.rust-lang.org/reference/attributes.html#lint-check-attributes).
I thought it wasn't safe to take references to uninitialized stuff, and swap takes references?
Do thermal printers have a common way of interacting with them?
But it is read, though, as a reference. References must be valid, and that isnt valid?
Just ported over https://github.com/etrombly/sandbox only took about an hour. RTFM is really nice to work with now. Great job.
"for the most part", so, what parts aren't quite as stable?
It's only unsafe if you try to interpret it as something meaningful. Which `mem::swap` does not (it just considers it a block of bytes, because thankfully Rust doesn't permit types to care about their location in memory)
`mem::uninitialized` and null pointers are completely different things. Think of `mem::uninitialized` as just equivalent to constructing a type but without placing anything meaningful in the bytes it would otherwise occupy. Provided you perform a valid write to it first, there's no problem with it.
This is a bit of a weird one, and I believe your problem depends on some really subtle details of lifetime inference. In particular, when you call `graph.add_vertices` in your second example, I believe you're forcing the borrows `&amp;a` and `&amp;b` to use the *same lifetime* as your mutable borrow of `graph`, because in your signature for `add_vertices` you use the lifetime parameter `'a` for both. Based on some quick experimentation in the playground, replacing `&amp;'a mut self` with `&amp;mut self` in the signature for your `add_vertices` method seems to fix the problem.
Your add_vertices doesn't need a lifetime parameter on self...in fact, I'm not even sure why you're replacing the hashmap with a new hashmap that includes everything in the previous one? Why not just append to the existing hashmap? [This](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3e7b10a613aac84c63a7717c6b383bac) compiles fine and the tests pass. 
Try to think about that will happen if you'll have two mutable references to the same data in different threads. Rust rightfully refuses to accept such code. You have the following options: 1) make references read-only 2) wrap mutable reference into some kind of synchronization (e.g. `Mutex`) 3) use owned data, i.e. change signature to ` fn get(&amp;mut self, id: u128) -&gt; Element`, and use `fn store(&amp;mut self, id: u128, element: &amp;Element)` to store your changes.
Weird. I tried that exactly but it doesn't compile on my system. The borrow checker tells me that I'm not allowed to mutate self in a loop like that - which is why I was creating a new hashmap - so the method only mutates self once
that'd be nice, thanks! This is my first Rust program, I just wanted to learn the language, so I wasn't planning on extending the tool too much, don't have much time in my hands left after my work on NCP. Also, PRs are welcome ;) Thanks for the ideas!
Yeah that did it. I had applied 'a to &amp;mut self because the borrow checker told me that vertex_array needed an explicit lifetime. I thought that the borrow checker automatically gives all inputs a unique lifetime - so for instance in this case &amp;mut self would become &amp;'a mut self and presumably the borrow checker would have given 'b to vertex_array. I gave them both the same lifetime because it seemed appropriate that I would be allowing vertex_array to live as long as self would live for. Clearly I don't have a strong grasp on lifetimes - it seemed so simple in the book but in practice I find it to be very challenging.
I have built one benchmark comparison of all these crates here, with the dot product: https://github.com/veniamin-ilmer/crate-race/tree/master/benches/mathvector_dotproduct I hope over time to add in other functionality.
Often the static file server just tells the kernel to serve the file to the socket which means they can all be similar in speed
TBH I don't know but there are quite a few different types that use this same protocol (adafruit sells like 3 or different ones)
Looks like maybe you are using an older version of Yew. Seems like \`Html\` used to take two type arguments: [https://github.com/DenisKolodin/yew/commit/12bf3fd98c1aa50c04fe8e852dd4b8f17ef0fe9e](https://github.com/DenisKolodin/yew/commit/12bf3fd98c1aa50c04fe8e852dd4b8f17ef0fe9e)
Why an integer? I don't know Korean but in Chinese there is romanization that is used for input.
Sure, but then you change the formula by introducing free variables, which is okay for SAT solving, but not in the general case.
If I'm reading you right, your pattern is "I have a bunch of (presumably I/O) tasks sending data items to a central processor task"; In that case, you don't need a separate channel for each task; just make one channel and clone the Sender for each task: let (send, recv) = unbounded(); for_each_sender_task(|| { spawn_a_task_that_has(send.clone()); }) spawn_somewhere(recv.for_each(|item| { process_item(item); }))
Though this does help, in one sense. However, that pattern is emerging because I have several channels for each io task that send different data back to the central task at different times. One option I thought of is to have a single channel that sends a structure with a lot of Option fields, but that doesn't seem very good either. To generalize further, I have several independent tokio streams that don't need any input from any others to work. I don't want to wait on one. I want to check if it has a value and move on to check the next stream. That is where that pattern came in.
Uninitialized data is valid memory, it just hasn't been initialized with a value. Reading it is UB, because it's filled with whatever junk was leftover. Freeing it is begging for catastrophe. Dereferencing the value, however, for a write, does not mean it is actually read if you are assigning to the value. For example, a simple assembly instruction that shows this is: https://godbolt.org/z/6ZZYcl It's valid to write to uninitialized data, in fact, in some date in the future, it might be valid Rust to write to a struct field even if the struct has not been initialized: https://github.com/rust-lang/rust/issues/54987 The reason why null dereferences are UB is because the null pointer does not point to any valid memory, **generally** speaking, and it's defined to designate that it does not point to a valid object. If you're running an old 8086 (I hope not. It's been 35 years), well then, actually dereferencing `0000:0000` is valid, but probably not what you want to do.
Instead of a structure with a lot of Option fields, the idiomatic way would be to define an enum where each value is a different type of message that the I/O tasks can send back to the processor task: pub enum Message { PeerHas(PeerHas), DataChunk(Buf), HangUp(PeerHandle) // or whatever makes sense... } ------ Alternatively, Stream::select(https://docs.rs/futures/0.1.25/futures/stream/trait.Stream.html#method.select) might help.
Integers are constant, while romanization changes. Also if you look at the tests, numbers get kinda ridiculous when you write them out: (1234_5678_9123_4567 , "천이백삼십사조 오천육백칠십팔억 구천백이십삼만 사천오백육십칠"), (9999_9999_9999_9999 , "구천구백구십구조 구천구백구십구억 구천구백구십구만 구천구백구십구"), Anyway, my original goal was to use this library as a Korean learning tool. I wanted a way to practice random numbers throughout the day, with a correct answer so I can double check. Now with Neon + web_view.rs, I can make a website + cross-platform tool to use this anywhere. Expect a writeup on that in a few weeks or so =)
Why can't they use variables instead? Wouldn't it then have the same overhead as functions?
That said, you might not need I/O -&gt; Processing channels at all; a pattern I used in Webmetro (see [channel.rs](https://github.com/Tangent128/webmetro/blob/fe50663938d95c26a4299333617bee7d4bc79888/src/channel.rs)) was for the I/O tasks to each hold an Arc&lt;Mutex&gt; to the shared state, updating it themselves.
Filed one issue as a ticket, another as a PR. After taking a closer look at the source, I might be tempted to do a fairly significant cleanup, or it might be simpler to do as a rewrite. Wondering how amenable you would be to PRs that do things like apply `rustfmt` to make the style match the standard Rust style, and rewrite a lot of the core logic for walking the directory tree, collecting statistics, and formatting it.
Proc macros can modify code. Custom derive proc macros cannot.
you should turn off the colors to make the comparison more fair. I imagine that extra string processing also would have some effect ;)
unreachable!() is a more appropriate macro for this.
Well, this was originally just a little learning exercise for me but it sounds fun to improve it with other people. This is so different from my main open source project where it attracts many users and so few devs... it's almost impossible to get dev help. That's awesome.
Great work! It's interesting that nalgebra is about 2x slower than the other libraries -- although I suppose it makes sense given that ndarray can integrate with openblas. Did you enable the openblas feature? I'd certainly be interested in seeing other functions benchmarked.
I have to admit to passing by reference far to often when I should move ownership. It's such a different paradigm that I still find myself at odds with it, but it's so elegant when put to use right.
Yeah, I was thinking the same thing. Especially in larger application code bases, I think breaking the code into crates within the same workspace is actually pretty important to keeping compile times reasonable. For libraries, I think it depends. If you can reasonably break a library into legitimately useful smaller crates, I think that makes sense. Otherwise, I think keeping it together makes the most sense, even if it makes compile times a bit longer when working on it: it keeps the ecosystem easier to navigate, and it doesn't meaningfully impact compile times for client code because they would have to build all the smaller crates you might split it into regardless. (I also suspect that in most--though I hesitate to say all--cases, if your library is so large that compile times are a pain when working on it, that may in itself be a bit of a code smell. There are probably smaller useful libraries hiding inside of it, then.)
It's been a while since I looked doing this stuff with Mio. Meanwhile Tokio has changed so much I need to learn it all over again :p (I've been putting that off until async/await stabilizes.) But I think the core of it is the Mio [`EventedFd`]( https://carllerche.github.io/mio/mio/unix/struct.EventedFd.html) trait. You can get your hands on the stdin and stdout file descriptors, and then wrap those as EventedFd's and hand them to Mio. I'd start by reading the Mio docs and some of the getting started examples on that crate if you want to try it. Note that pipes in Windows have important differences from Unix when it comes to async IO (some Windows pipes don't support IOCP), and getting this to work cross platform might be difficult. In general, unless you want to really dive into the details on this async IO stuff, using threads is going to save you a lot of trouble.
&gt;Passing things by reference when moving them would be better Clippy seems to advocate for passing by reference [almost all the time](https://github.com/rust-lang/rust-clippy/issues/21). However, I agree that there's a lot of cases where this isn't a particularly good idea -- e.g passing an `Arc` into a function.
Just from the code you posted, it looks like you might actually want to return a tuple instead of a slice? You could return (key, value) instead of &amp;[key, value]. You could also return an array of two elements (`[YourTypeHere; 2]`), although if what you want is to return a pair of things a tuple is the idiomatic way. I'd really need more context (more code) to figure this out further. 
This has been happening to me for a while on Firefox as well.
Looks again super interesting! Some questions regarding the content: &gt; If in our running example we expect that some command will take long enough to execute that another command may arrive in the meanwhile then we can increase the capacity of the message queue If the processing always takes that long the queue will be full after some point of time. What happens in this case? Will messages get dropped? Will the current task be blocked until capacity is free (which would lead to priority issues)? Or is it configurable? Also what happens in case of `spawn.some_command()` when `some_command` already runs? Will this also queue exactly one other run of the task?
Ah the wonders of CSS.
This create seems to do something equivalent to converting "123" to 「一百二十三」. How is romanization related?
One thing I noticed is using booleans for flags: hangeul_from_int(30, false); it would be much better to use an enum: enum NumberStyle { PureHangeul, SinoHangeul, } // and then user code would be hangeul_from_int(30, NumberStyle::PureHangeul); which is far more readable and doesn't require reading the docs :)
iirc my starting point was the [docs](https://docs.rs/mio/0.6.16/mio/), which have some good examples. udp was a bit easier to start with (vs tcp) because it's more atomic, one-packet-at-a-time, vs, like tcp/http, which requires additional steps like parsing the content-length header to know whether you've received the whole payload ([thhp](https://crates.io/crates/thhp) is a fast, minimal http parser I've used for that). [this](https://docs.rs/mio/0.6.16/mio/net/struct.UdpSocket.html) example is probably the clearest about the pattern: set-up what you're waiting for (initially) at the top, then loop on `poll.poll(..)`, which blocks the thread (with optional timeout) until the os notifies you that your io is ready to go. looping on epoll (linux), kqueue (mac) or IOCP (windows) is the "bare metal" version of all "async" and it's power is how efficiently you can handle io (potentially a lot of it, with barely a dent in the cpu). a few things I remember being confusing: - it was the first time I had encountered heavy use of functions that read into buffers and return the bytes written, so in rust, like ``` fn recv(buf: &amp;mut [u8]) -&gt; usize { .. } ``` Then it's on you to pair the return value with the buffer to read it to the right length: ``` let n = recv(&amp;mut buf[..]); println!("{}", std::str::from_utf8(&amp;buf[..n]).unwrap()); ``` The pitfall is if you're expecting functions to extend the length of a `Vec` or other growable buffer, in many cases that won't work. For instance, for the `recv` function above, you could use a `Vec`, but it's length would need to be the maximum space you expect to be written to it: ``` let mut buf = vec![0u8; 1024]; // filled with zeros to start, then just keep writing over previous data // `Vec::with_capacity(1024)` wouldn't work let n = recv(&amp;mut buf[..]); ``` - I found the docs on `Poll` [config](https://docs.rs/mio/0.6.16/mio/struct.Poll.html) somewhat confusing. Generally for performance, "edge" mode is preferred. But to use it correctly, you need to loop (nested loop inside the main loop) on the resource (i.e. calling `.recv` or whatever) until you get a `WouldBlock` error. So you end up writing a lot of code like, ``` match poll.poll(..) { RECV =&gt; { loop { match socket.recv(&amp;mut buf[..]) { Ok(n) =&gt; { .. } Err(WouldBlock) =&gt; break, // note, matching the error more complex in real life // ... } } } // ... } ``` hope that helps! good luck! many people like that "async" libraries allow you to code as if it's synchronous, but it's really asynchronous. personally I find that way more confusing! so this is where I'm comfortable, and performance is great too.
You didn't test the 16 case, that's an edge case. As far as code style, I really hate passing obscure Boolean flags. Just make several functions with different names.
You could take a look at the GStreamer bindings: https://gitlab.freedesktop.org/gstreamer/gstreamer-rs There's a higher-level library for video editing called GES, gstreamer-editing-services, which provides exactly these features based on a "timeline of videos" kind of API. There's a minimal example in the repository that cuts a single video and adds an effect: https://gitlab.freedesktop.org/gstreamer/gstreamer-rs/blob/master/examples/src/bin/ges.rs If you have any questions how to use it for your use-case or run into any problems, feel free to ask here or contact me by e.g. mail. I'm the maintainer of the GStreamer bindings and one of the GStreamer developers.
I am talking about the case where the enum is the same and is immutable. for el in elements { my_op.process(el); } 
Yes, you are right. I misread. I was thinking you could get Korean words by entering numbers not turning Arabic numbers to written numbers. My mistake.
Oh wild, I just thought the source code view didn’t work at all
I think you wanted r/playrust
&gt; I think it should be safe as long as T doesn't contain a pointer. You need to `mem::forget` the T returned from the closure or else it will be dropped twice.
&gt; I gave them both the same lifetime because it seemed appropriate that I would be allowing vertex_array to live as long as self would live for. You can use the "outlives" relation with the two different lifetimes to express this without being as restrictive as requiring them to have the same lifetime. [This](https://doc.rust-lang.org/book/ch19-02-advanced-lifetimes.html#ensuring-one-lifetime-outlives-another-with-lifetime-subtyping) book chapter goes over that, and the example they use is pretty similar to what you're encountering.
The "canonical" article on this topic is 36 years old: &lt;https://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD831.html&gt;
Is there a viable alternative to using `collect` when you need random access?
Ah, then this crate is not inventing its own ad-hoc protocol but implementing an existing one - that should IMO be noted in its description.
Yes, Nginx uses `sendfile(2)`, available in Rust through `libc` https://docs.rs/libc/0.2.45/libc/fn.sendfile.html or with a more convenient interface in the `nix` crate. 
Can you also post your nginx setup?
Have you tried [actix](https://github.com/actix/actix) (not to be confused with actix-web)? Seems like the actor model should do just fine in this case.
Oh nice! It looks like it requires an attribute on the trait itself, so I guess it only works if the trait is local to the trait. Still pretty cool though.
There's a similar issue here with some links to other ones: https://github.com/rust-lang/docs.rs/issues/264.
Note that the Haskell community themselves abandoned the idea of a Haskell Platform a long time ago.
Software moves quickly. What seems to be the perfect solution now might turn out to be outdated a decade later. Because of this, any set of "blessed" packages becomes outdated quickly.
Say you have a texteditor app which can edit multiple documents in a MDI. You create a DocMgr class, write a **Doc\* DocMgr::open(path);** method and assign each editor window a Doc\* (from c++ aspects). Then you hit the first problem I've described: one manager class and multiple mutable elements. Then you write a thread which invokes every 5 seconds. The thread iterates over all Docs in DocMgr and saves a backup of the current state and set the property **Doc::backupSaved** to true. Then you hit the second problem I've described. The TextEditor component has a pointer to Doc, and the backup-thread has a pointer to Doc. To be clear, the second problem requires some kind of locking to prevent the TextEditor component writes to Doc\* while the backup thread is running. But in the data structures, there are still two mutable Doc pointers. Thats how someone would implement it in C++ (e.g. a lot fo GUI library working this way, each Component maintains a list of owned sub-components like a tree while the component itself acts as a manager of it sub components). I'm asking for the Rust solution, maybe a completly different pattern has to be used. &amp;#x200B;
Couldn't you just make a macro, so the pattern is easily repeated? ``` /// checks a stream to see if it's ready and performs common operations when NotReady or otherwise. /// Requires being in a breakable loop. macro_rules! check_stream { ($stream:expr, $val_name:$ident =&gt; $on_ready_with_value:expr) =&gt; { match $stream { Ok(Async::NotReady) =&gt; break, Ok(Async::Ready(Some($val_name))) =&gt; $on_ready_with_value, _ =&gt; finish_stream() } }; } ``` So whenever you are in a loop, and you need to work with the value but do a break on NotReady or finish_stream() when you don't get what you want, but still need to something on a Ready with a value. ``` check_stream!(stream, item =&gt; process_item(item)); ``` Of course, that depends on just how much of this pattern is the same each time you encounter it.
See my answer to ReversedGif. The main problem I have is not multiple mut refs to the same object, but multiple refs to different objects but maintained by a manager class. Immutable refs makes no sense for my in a construct like the one described above (multiple TextEditor components connected to a Doc\*). While it is save to write something like: let mut e1 = Element::new(); let mut e2 = Element::new(); it is not allowed to get the Element via a manager class because then it implies that there are multiple mutable refs to the manager, too.
If you want something with not much dependency, you can take a look to my library too if you want. https://github.com/mardiros/cabot https://docs.rs/cabot/ I've written it while learning rust too, I thing it is my first cratre actually. And could be great for learning purpose. There are an umaintained list of known alternative in the readme.
Really interesting. I discovered json_in_type vs serde_json: wow. I hope one day to see a comparison of string concatenation :-) 
I love it! But why does acid eat up the cloner :b
I'm not quite sure what you mean, there is also https://github.com/dflemstr/serde-protobuf but it seems unfinished? But yes, there are two problems, 1) serializing the data into an array of bytes and 2) exchanging the array of bytes between two components: Number 2 is hackable and for Number 1 the code already exists for the Python and Java. So another option is to use something better than protobuf AND also do a new implementation for Python/Java, but what would that be?
You shouldn't use `uninitialized` with untrusted `Reader` though, as it can actually read from reference provided for writing. `Read` trait has `initializer` to conditionally initialize bytes if trait doesn't guarantee that it won't read from them before writing there.
Deviating from my protobuf question here, when I want that two modules/components written in Rust to communicate over the network (so no legacy modules to interface to) what would be a good choice for a lightweight messaging+queuing framework that takes care of the network? So assuming the serializing is done with Serde, my search for a lightweight messaging library pointed me to https://github.com/amethyst/laminar . Any other recommendations?
Actually you'd better pass `&amp;Arc` into a function. Let the function decide if it needs to clone 
I think `unwrap` is ok in cases where failing means bug in the library. Similar to `assert`. Splitting project into separate crates creates strict API bounds. Which is a good thing. It forces author to put more thoughts into API design.
My personal worst of: - Not supporting `#[no_std]`, even using `alloc`, although a crate easily could. - Having `panic!`-ing code paths without an alternative, e.g. `a + b` without `a.checked_add(b)` or `a.wrapping_add(b)`. - Hidden allocations everywhere in library code. - Not being able to be cross-compiled as a library. Most often happens for crates with C dependencies, which assume Target=Host system and then run the wrong shell scripts, etc. Not applicable to magic crates like `x86` for hopefully obvious reasons.
Is a link like a tuple in other languages?
The important distinction is as to whether the function consumes the argument. If it does, it should be moved to avoid unnecessary cloning.
In my experience, making use of Rust's move semantics almost always results in nicer code.
I am a Korean and I can only count up to 10^12 😅 Anyway cool project. In addition to what other people mentioned, the "example usages" are most helpful when used with `assert_eq!` For example, assert_eq!(hangeul_from_int(30, false), String::from("???"))` Plus, you get a free unit test. 
&gt; Functions returning an error when receiving invalid input (pass arguments by newtypes that can only contain valid values) Depends. I don't like introducing new types for one use point. You're just moving the burden of validation to the constructor of the type and adding a lot of code bloat for little benefit.
Really cool! Was disappointed to see it does not work on stable, any chance to make it work?
Your use-case sounds very similar to mine, you might be interested in a workaround: https://crates.io/crates/parallel-iterator When I wrote the library above Rayon didn't have the par_iter feature and it's way less sophisticated compared to Rayon but that could allow easier debugging. Hope that helps.
Not who you're replying to but here are the issues tagged "O-freebsd": https://github.com/rust-lang/rust/labels/O-freebsd Some random sampling: - [backtraces have blank lines and &lt;unknown&gt; instead of function names on FreeBSD](https://github.com/rust-lang/rust/issues/54434) - [FreeBSD --disable-optimize breaks the stage 0 compile of ../src/librustc/lib.rs](https://github.com/rust-lang/rust/issues/30907) - [fs::metadata() crashes on FreeBSD 12 due to layout change in stat.h](https://github.com/rust-lang/rust/issues/42681) But I have no idea how this compares to problems specific to other platforms. Based off of the volume of FreeBSD-specific bugs I'd say it seems pretty stable for core rust-lang/rust issues.
Yes, i changed `ok_or` to `ok_or_else` because of clippy's warning. Since I will change `MissingAttributeError` to store a `String` and `&amp;'static str` as well, i used `ok_or_else` instead. I will probably benchmark which approach is better in this case. Since you mentioned inlining, will rustc inline functions even without the `#[inline]` attribute or do I need to state explicitly what functions i want to inline?
I can confirm that if you can reproduce this with the following steps: \-Create a new binary crate \-Install and update cargo-web with \`cargo install --force cargo-web\` \-Add \`yew = "0.4.0" as a dependency. \-Copy and paste the example code near the top into src/main.rs \-Run cargo web start
I don't really understand how you imagine they would work using just variables. Can you provide an example of what a closure could desugar to?
I'm not sure if there are crates that specialize specifically in string concatenation, but I'll add that to the list of functions to research. 
&gt;Box&lt;SmallType&gt; What else do you propose to get a static lifetime? &gt;.unwrap() or .expect() without a comment outside of fn main() `expect` should already have a good message. Remember Clean Code, if you write a comment then you failed to express your intention via the code, except for docstrings, which is completely different thing. &gt;a\_number as OtherNumericType without a comment you really expect `10_u32 as usize` in the code with some FFI? You have to deal with fixed-size ints gettin in/out of the library, and using it as indices in some cases. &gt;Stringly typed interfaces Not sure what it stands for &gt;Functions returning an error when receiving invalid input (pass arguments by newtypes that can only contain valid values) Too restricting. I.e. `Display`/`Debug` traits accept self and returns a result. `Result` is not designed for constructors usage only. It's good to get error earlier with "Invalid input", but you probably don't want to create `PositivePrimveNumberSmallerThan10000` instead of returning `Err`. &gt;Indexing arrays, especially in for loops Alternatives? I.e. [look at function](https://github.com/paritytech/parity-common/blob/154990798896963fedf329b026011f3f8604e5e3/uint/src/uint.rs#L1213) written by me. How do you rewrite it without indexing in for? &gt; fn argument_order_matters(T, T) - easy to accidentally swap; use struct or different types I'm pretty sure I can find lots of standard functions that take two slices, two i32, two strings etc...
&gt; fs::metadata() crashes on FreeBSD 12 due to layout change in stat.h This is the main thing Linux got right: Linux tries really hard to never ever break user programs. Most other operating systems change the types in their function signatures, the values of variants in C `enum`s, the values behind `define MACRO 2`, ... FreeBSD12 got released and the only way to support it without breaking FreeBSD11 is to create a fully different `x86_64-unknown-freebsd12` target...
Please post the code you're using. Does it also happen when you use the default thread pool config? You might want to open an issue on the rayon repo.
May I ask, why are you interested in string concatenation?
The reason you can't return a slice of refs (`&amp;[&amp;T]`) is because that slice has to exist somewhere, and it doesn't exist anywhere with BTree. In this case, you're creating an array on the stack that ceases to exist after this function ends. What you *can* do is return `Option&lt;(&amp;T,&amp;T)&gt;`, which is exactly what `data.iter().nth(pos)` returns.
&gt; Now, from what I've seen, wasm testing is only done via wasm-bindgen-test (https://rustwasm.github.io/wasm-bindgen/wasm-bindgen-test/index.html) but unfortunately it requires to write new tests. Ehm, where do you get that impression from ? 
Could you add aho-corasick to substring searching?
Not really - it's necessary then. Collect isn't evil, it can just be overused a bit.
AFAIK, in some cases rustc will inline a function even if you put an attribute to prevent inlining and in other cases rustc will not inline a function even if you explicitly tell it to always inline it. In general, I'd say it's best to let rust do its thing and only use the attribute if rust doesn't do what you want by default. Otherwise you'll just litter the code with attributes that will make it harder to read.
I am already using serde, but afaik reqwest does not support the wasm32-unknown-unknown target. 
[removed]
Not when serving over TLS! Unless you're Netflix and you patched the FreeBSD kernel to do encrypted `sendfile`. Linux has a KTLS thing too now, I've heard.
I believe rayon can judge that it's not worth parallelizing a workload and only use a single thread. I don't know if that is what is going on here, but you could take it into consideration.
Earlier, I did, and it came out of be a bit slower than all of the other four methods. After some examination, I realized comparing aho-corasick to the other crates was more like comparing apples to oranges, showing aho-corasick in a poor unfair light. And so I removed it from that table. Perhaps I'll have another benchmark that specifically goes over many-to-many string comparisons, VS the usual single-to-single string. 
&gt; the only way to support it No, not the only way. *currently* Rust supports it by [linking to the old symbols that return the old structs](https://github.com/rust-lang/libc/blob/027d4834615cf8bce3faa9c4a1c2706413a7b275/src/unix/bsd/freebsdlike/mod.rs#L1101-L1103). FreeBSD's ino64 transition did not break regular programs. It only broke compilers that have their own definitions of system structs and functions instead of reading C headers.
good work Andrew! I haven't had a change to look into the source code, only looked very briefly. I noticed that you have multiple functions to pattern match based on the conversion. I would personally flip this around: Create a `struct` example "numEncoding" that has things like "plainText, Korean, Hanja" etc Then create a HashMap of `(int, numEncoding)` That way, you would simply have a single function that takes a number and a target encoding enum. You can then simply look up from the HashMap and then pattern match on the enum to return the correct encoding. Does any of that make any sense? :) 
You mean like [this](https://github.com/hoodie/concatenation_benchmarks-rs/blob/master/README.md)?
&gt; I am not even sure bindgen is the right solution Maybe SWIG should be extended to support Rust for more advanced cases? Function macros in C might be bad, but a lot of code want to still support C89 for older C compilers.
I just added an example to the pom repo, inspired by your post. https://github.com/J-F-Liu/pom/blob/master/examples/duration.rs
Again, I'd like to repeat that we are talking about _warning signals_ here, and not necessarily things that are inherently always bad to do. In your case, I might use collect. Or if it's in a library where the allocation implied by collect could be noticeable on a performance profile, then I might instead design the API such that the allocation implied by collect could be amortized. In that case, you probably can't use collect directly, but instead manually reuse an existing allocation.
If objects are different and you ensure that manager keeps track of mutable references (i.e. code like `let a = manager.get(1); let b = manager.get(1);` will either panic or will return an error), then you can use `unsafe` code to create elements with mutable references, those elements on drop will release contained mutable reference to inner data (i.e. `let a = manager.get(1); a.drop(); let b = manager.get(1);` will work). But I would strongly advise against diving into `unsafe` waters until you'll get more proficiency with Rust. For potential solution for your problem take a look at ECS crates, e.g. [specs](https://docs.rs/specs). Overall my recommendation would be to rethink solution for your problem, as C/C++ approaches often can not be translated into idiomatic Rust straightforwardly.
Haha, sorry but thanks to everyone that told me I was in the wrong place
If you have mulyiple threads accessing the same object and some need to mutate it, then you need a Mutex, which can give you "interior mutability" - even if you just have a const reference, you can still mutate whatever is inside the mutex once you've obtained the lock.
Okay, here is an explanation as I understand it... [This](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5662ccc9f352d756c53a5626a0cfcf29) works as how you would expect. You're storing a value into bar. Nothing else needs to be done. In [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1e7ff4b1c11be40b4999fccc18205d50) case however, a reference is being stored into bar. "baz" could go out of scope before Foo. If that were to happen, f.bar would be a dangling pointer. It would access memory that no longer exists. That's called undefined behavior (UB) because something else will get allocated to that space in memory. We won't ever know what it will be and the best case scenario is a segmentation fault the next time we try to access the memory by calling f.baz. In [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6502dbfb97336a25265af57415904859) next example, f is being changed to be mutable so we can change it in the new bar_mut method. This is similar to the previous example, except now bar is being changed in a new method. This is the same as the cleaned up "add_vertices" method I gave you earlier. No additional lifetimes are necessary. If you experiment with this example and add lifetime 'a to self in the bar_mut method, you will run into the same issue you ran into on your OP. Now look at [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d99d64fdf1acf48165ba1be5feca7d10) example. You'll notice this won't compile and the error will be the same as your opening post. So we need to get back to the original question. Lifetimes are always required in Rust. The compiler just implicitly inserts them for you until you start manipulating references. When you manually insert lifetimes, you're forming a contract with the compiler that everything with the lifetime parameter you specified will live the same time. But what happens to &amp;mut self in the print_bar method? It goes out of scope at the end of it and gets destroyed. You told the compiler it will live as long as lifetime 'a though. Lifetime 'a is still living on in main! You dirty liar! Let's be honest with the compiler. We now know &amp;mut self won't live as long as lifetime 'a so let's give it a different lifetime in [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cddb3acc3cc4291d633807c2c80a14f4) example. It works! We are now doing what the compiler would have done implicitly if we didn't have a lifetime on &amp;mut self at all. Does that make sense?
Let's consider this: pub fn add_vertices(&amp;mut self, vertex_array: &amp;[&amp;Node]) { for vertex in vertex_array { // btw: vertex is already a reference here, no need to use `&amp;` in insert. self.nodes.insert(vertex); } } Here, you would try to take a `&amp;Node` with an arbitrary/unconstrained lifetime and turn it into a reference of type `&amp;'a Node` because that's what you promised to store in your `HashSet`. For this to be safe, we need to make sure that the not yet named lifetime of `&amp;Node` in `vertex_array` outlives `'a` because otherwise you would end up with dangling references in your set. This is what the Rust compiler prevents you from doing. You only need to change one bit: pub fn add_vertices(&amp;mut self, vertex_array: &amp;[&amp;'a Node]) { &gt; &amp;mut self would become &amp;'a mut self That's no good. Your `Graph&lt;'a&gt;` which stores `&amp;'a Node`s should never borrow `self` with that same lifetime. 'a is a lifetime *longer* than the lifetime of your `Graph` struct. As soon as you borrow `self` using that lifetime, you're making these lifetimes the same which basically means that the borrow of self never ends. The fact that you borrow `self` via lifetime `'a` and have the struct store other references with lifetime `'a` makes the struct borrow itself forever.
Thanks, I agree. I'll have an enum like this in the next version.
BTW: You have a couple of `&amp;`s which are unnecessary: pub fn contains(&amp;self, vertex: &amp;'a Node&lt;'a&gt;) -&gt; bool { // vertex is already a reference. self.nodes.contains(&amp;vertex) // ^ // unnecessary // &amp;vertex would be of type &amp;&amp;Node. I was actually surprized // this compiled. Apparently, Rust is able to implicitly // turn a &amp;&amp;Node into a &amp;Node. Not sure how this magic is // done. Maybe this falls under "Deref coercion". (?) } The item in an iteration you start using a *reference* to a contains is *also* a reference: for v in &amp;self.nodes { // ^^^^^^^^^^^ reference vertices.insert(&amp;v); // ^ not necessary } for vertex in vertex_array { // ^^^^^^^^^^^^ reference of type &amp;[&amp;Node;2] vertices.insert(&amp;vertex); // ^ not necessary } "References to iterables" implement the [IntoIterator](https://doc.rust-lang.org/std/iter/trait.IntoIterator.html) trait in a way that gives you an iterator yielding references. For example: impl&lt;'a, T&gt; std::iter::IntoIterator for &amp;'a [T] { type Item = &amp;'a T; type IntoIter = ::std::slice::Iter&lt;'a, T&gt;; ... 
&gt; What happens in this case? It depends on how you use the `spawn` API. Spawning a task will try to append the message to a queue. If it succeeds you get back an `Ok(())`; if it fails you get back `Err(payload)`. You could write `spawn.task(payload).ok();` to drop the message when the queue is full or you could write: loop { match spawn.task(payload) { Ok(()) =&gt; break, Err(x) =&gt; payload = x, } } To retry the operation until it succeeds. But note that this retry operation is likely to always deadlock. The task scheduler is priority based; if the queue is full it means that the task you want to spawn can't run because it's lower priority or has the same priority as the current context; this means that the queue won't shrink until the current context ends. In general you should pick the size of the queues such that `spawn` (and `schedule`) always succeed but this requires timing analysis (e.g. you need to know how often hardware tasks are triggered). &gt; Also what happens in case of spawn.some_command() when some_command already runs? This is equivalent to posting a message with payload `()` so yes, it queues another run of the task. But note that task can be stateful: they can use task local storage (`static mut` variables private to the task) or resources (state shared with other tasks) so it's not like running the task again will do the exact same thing.
hi, you can trace execution using my rayon-logs crate. it's kind of alpha now but I can give you a hand if you need it. [http://www-id.imag.fr/Laboratoire/Membres/Wagner\_Frederic/rayon-logs.html](http://www-id.imag.fr/Laboratoire/Membres/Wagner_Frederic/rayon-logs.html)
On your design/approach: This will probably end up in a lot of pain and frustration when you try to add links between nodes and use of them. I don't think it'll work. The problem here is that you want to introduce "circular borrowing". That's no good. I'd go as far as to say that borrowed references are not what you want to use to make connections between nodes. Try to check out how others have implemented graphs in Rust.
That makes sense, thanks. Yeah the number groupings above 조 were just added for completeness' sake. 
My wish is to provide a unified way to look at data alike tables. My understanding is that tuples in Rust must be declared of fixed size. I need a way to represent rows that, hopefully, make efficient to compare values, filter and iterating. Of course I could live with cloning into a vector, this is more about asking for a way. P.D: Now thinking this aloud I probably could make a row trait and implement it for tuples. Anyway, I think the only ones I need to handle are for Btree/HashMap....
I'm still trying to wrap my head around that, could you provide a small code example? I'm going to use an enum instead of the bool, but I'm also trying to figure out a structure with float support. This is what I'm currently playing around with: pub struct KoreanInteger&lt;I&gt; where I: ToString + Integer { pub value: I, pub num_style: NumberStyle, } pub struct KoreanFloat&lt;F&gt; where F: Float { pub value: F, pub num_style: NumberStyle, } pub enum NumberStyle { PureKorean, SinoKorean, } 
If you're interested what it looked like, here is a snapshot from that time: https://github.com/veniamin-ilmer/crate-race/blob/5c3aeda5e9849a5d42bf923b81b14005665c3cfc/benches/string_findsubstring/README.md
All done.
Finished the tutorial and documentation. Comments welcome.
This is very helpful, thanks! One more question. The central task needs to have sender channels to each of the IO tasks to send commands. In this case there is one sender and multiple receivers. How can I handle this case?
Look at https://crates.io/crates/custom_error for me this is best solution for error handling. Quick and with std standard. 
I've built a webserver that serves about 50MB of files. I just hold the whole thing in memory ready to send so I would expect it to beat anything else. Never tested it tho
Not gp, but the stability of that cruft-ridden ABI is what makes Linux so attractive as a platform. It's also what lets old binaries remain runnable on newer systems (with the help of the infamous glibc of course). That's not to say that FreeBSDs approach here is wrong, but given two platforms where one relies on system-wide compatibility options for compatibility and another that just works, I know which of the two is easier to target. 
I could use a macro, but the point more as I feel like this pattern is indicative of a bad design, and I am looking to confirm or deny that.
Not a fan of the clickbait-y title. You also have to be careful about not trying to apply too many of the lessons learned from Rust to other languages like C++ or TypeScript. For example, while Rust traits are quite similar to TypeScript interfaces in some ways, Rust traits are nominal while TypeScript interfaces are structural. There's not really an equivalent to a "marker Trait" in TypeScript interfaces. But it is true that in general, getting good at Rust will make you a better programmer overall, as it is (for the most part) true that getting good at any programming language will make you a better programmer. Not a "super"-developer though...
I knew there was an equally nice crate 😀
&gt; FreeBSD's ino64 transition did not break regular programs. It only broke compilers that have their own definitions of system structs and functions instead of reading C headers. Does that allows you to run a binary (e.g. from C) that was compiled for / on FreeBSD 11 on FreeBSD 12 ? Or does doing so require a recompile ? 
Old binaries remain runnable on newer kernels without recompilation. I can run binaries all the way back from FreeBSD 4.0 out of the box. (By compatibility options I mean I can easily intentionally rip out the old syscalls from a custom kernel build to disable compatibility.) It's the recompilation that's the problem, when the language is not C and doesn't use C headers.
&gt; boom No, it will work just fine, because it's linked to e.g. `readdir@FBSD_1.0`, and the symbol that uses the new struct is `readdir@FBSD_1.5`.
Thanks for the insight! Would you happen to know if a C wrapper for my library would make it usable in pure C if the C++ dynamic library binary is already compiled?
Hi, I am a Cargo team member, working on making Cargo's dependency resolution better. It is under-documented at the moment, as pointed out [hear](https://www.reddit.com/r/rust/comments/a3sav1/2019_roadmap_more_like_a_wishlist_finish_and_ship/ebe0911). Also there is at least one known [bug](https://github.com/rust-lang/cargo/issues/5529), that is going to be hard to fix. If you have some examples of odd behavior I can try to explain them. (then we can rework the explanation into better documentation. )
I just finished the chapter on generics and I understand everything, but this: struct Point&lt;T, U&gt; { x: T, y: U, } impl&lt;T, U&gt; Point&lt;T, U&gt; { fn mixup&lt;V, W&gt;(self, other: Point&lt;V, W&gt;) -&gt; Point&lt;T, W&gt; { Point { x: self.x, y: other.y, } } } fn main() { let p1 = Point { x: 5, y: 10.4 }; let p2 = Point { x: "Hello", y: 'c'}; let p3 = p1.mixup(p2); println!("p3.x = {}, p3.y = {}", p3.x, p3.y); } The problem is this line: fn mixup&lt;V, W&gt;(self, other: Point&lt;V, W&gt;) -&gt; Point&lt;T, W&gt; { Why can I use a `Point&lt;V, W&gt;` without declaring it first? I had to declare the `Point&lt;T, U&gt;` as well. Why not the `&lt;V, W&gt;` ? Seems like it's not formally correct.
Here it is: https://gist.github.com/codesections/9ac6c81f5c123fedc9a2154237800bf5 As mentioned, it is *very* simple/rudimentary 
&gt; Won't nginx also use sendfile() When I first read your comment, I was sure that this was the explanation. However, after digging in a bit, I'm not so sure. Adding `sendfile on;` or `sendfile off;` seems to have no measurable effect on the nginx performance either way (which is pretty odd in itself…). This makes me think that the performance difference I noted isn't coming from sendfile—either that, or I'm not successfully turning it off, but I _think_ I am according to the nginx docs. https://docs.nginx.com/nginx/admin-guide/web-server/serving-static-content/
You declared `struct Point&lt;T, U&gt; { ... }`, and that's all that is needed to use it - then you can use it with any two types you want, and you do not need to declare with what parameters it will be used beforehand. So using `Point&lt;V, W&gt;` is completely fine - just like `Point&lt;i32, i32&gt;`, `Point&lt;&amp;str, char&gt;`, or `Point&lt;String, f32&gt;` would be - `V` is a type, `W` is a type (because they are listed on `mixup&lt;V, W&gt;(...)`), so `Point&lt;V, W&gt;` is also a proper type that you can use for a function parameter. You did need to declare `impl&lt;T, U&gt; Point&lt;T, U&gt;` before `mixup` because this is basically to denote the type of `self` - notice that on the function itself there's no mention about its type, so the compiler wouldn't know what to do with it. However, you could have also defined mixup as a regular function: fn mixup&lt;T, U, V, W&gt;(point: Point&lt;T, U&gt;, other: Point&lt;V, W&gt;) -&gt; Point&lt;T, W&gt; { Point { x: point.x, y: other.y, } } It's just that you couldn't use this as `p1.mixup(p2)` - you must call freestanding functions as `mixup(p1, p2)`. And your change `fn mixup&lt;V, W, S&gt;(self, other: Point&lt;V, W, S&gt;) -&gt; Point&lt;T, W&gt;` didn't compile because `Point` must always have exactly two type parameters, and you provided three.
Rust solves a lot of issues with C and C++, specifically around memory unsafety and data races. For example, safe Rust, which is the version of Rust you'll be writing most of the time, will never allow you to access uninitialized or freed memory.
I wouldn't call Go "modern C".
I've created a Rust Gist for you that gives you the top level idea: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=638c24d9c2b6dd711c568f27863a5d2f use std::collections::HashMap; ``` enum LanguageType { English, French, Spanish, Italian } struct Encoding { english: String, french: String, spanish: String, italian: String } fn main() { let mut languages = HashMap::new(); languages.insert("Hello".to_string(), Encoding { english: "Hello".to_string(), french: "Bonjour".to_string(), spanish: "Hola".to_string(), italian: "Ciao".to_string(), }); // insert more mappings! // lets get some conversions: let greetings = languages.get("Hello").unwrap(); println!("{}", greetings.english); println!("{}", greetings.french); println!("{}", greetings.spanish); println!("{}", greetings.italian); } ``` 
better !
Sure, you wouldn't write OS kernel in Go.
Thanks. And how about the not-so-low-level stuff?
What sort of not-so-low-level stuff? I'd argue that this stuff is, in fact, not at all low-level for a language that compiles directly to machine code and doesn't have a runtime to manage its memory for it automatically. If you're looking for features though, here's one: Rust's design allows for things like [Rayon](http://docs.rs/rayon/) - you can basically take any collection that gives you an iterator and run functional operations like map/reduce/fold/whatever on it in parallel, with as many concurrent threads as your CPU supports, without changing a single line of your single-threaded code, _and_ the compiler will yell at you if the behavior is undefined in the presence of threads.
fasthttp(Go) is literally fast, come on guys. 
Rust is a general purpose systems programming language supporting zero cost high level abstractions. So it's easier to do some pretty gnarly stuff over lower level code. I think my problem with go is it worked so hard to provide asynchronicity that the rest of the language seems warty at times, like the error passing schema. My one problem with rust has been editor support but it looks like some big stability changes have finally landed in RLS.
Mentioned the crate in the tutorial ... They might complement beach other 
I'd absolutely recommend Rust for someone coming from your background. You will learn about a whole heap of things like static typing, and stack/heap memory management. You can learn these just as well with C++ or C, but Rust will be a much easier ride because the documentation is fantastic, and the compiler tells you when you're doing stuff wrong. Source: I learnt Rust after doing mainly PHP ans javascript stuff.
I guess because it's uncommon for open source contributors to ask for money flat-out. I think asking for money is fine, though. I won't pay for this particular thing because I don't need it but in general I don't mind if more people do this. I definitely prefer that model to people doing things for free in their spare time and burning out/starving.
Did you build in release mode? ;)
I did! But always worth checking, especially with someone as new to Rust as I am :) 
[You would in Rust](https://redox-os.org/)
I understand that the new website currently doesn't describe what Rust is or why you should use it. The [previous website](https://prev.rust-lang.org/en-US/) gives a better idea of what Rust is about: &gt; Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. - zero-cost abstractions - move semantics - guaranteed memory safety - threads without data races - trait-based generics - pattern matching - type inference - minimal runtime - efficient C bindings
Yeah, that would be a great way to leak secrets. In this case I don't pass the uninitialized memory to the reader, but you're right. 
Thanks for the tip. I started getting into that last night as I decided to start adding edges and boy was that not fun. I am used to doing a lot of OOP design and need to rethink my approach when it comes to Rust
&gt; P.S. What I wanna use it for? If you're productive with Python, I don't know that switching to Rust holds a lot of value for you. Sure, there's lots of good stuff in Rust but if you aren't hitting problems with Python I wouldn't recommend changing. The major value Rust holds for you is to reveal lower-level details that it sounds like you're not interested in. I say -- you're a perfect example of the developer for whom these higher level languages were created (I mean this in the absolutely most positive sense). You want to be productive without being bogged down by these details, so if I were you -- I'd embrace Python! (I do love Python BTW, for all the same reasons you probably do). &gt; Mostly for personal usage that for making money. Rust is not there IMO (yet), the number of jobs which mention any interest in rust is much lower than other languages. But learning lower level details of how computers operate can be highly beneficial to your career. I feel like I could do just about any job related to software -- that offers me a lot of flexibility. &gt; Mosty lightweight software or CLI "scripts" Python, Javascript, Java, C# -- these are ideal for this use case. `rust` might be an improvement over C/C++, but IMO Python really shines here. When *can't* or *shouldn't* you use Python? * When you have a use case with very low/predictable latency requirements (e.g. "must process each request in &lt; 20ms or it's a system failure [packet loss, jitter, etc]") * When you have a use case like an OS kernel, device driver, bootloader, interrupt service routine * When you have a memory-bound/CPU-bound use case that will be too slow in a managed language (e.g. iterating over enormous n-dimensional matrices, etc). In some of these cases you can still use python but move the critical portion into a C/Rust library. In fact, you already are in many cases (several standard library modules, `numpy`/`scipy`, etc).
 struct Point&lt;T, U&gt; { this `T`, `U` impl&lt;T, U&gt; Point&lt;T, U&gt; { and this `T`, `U`, are different. The names do not carry across declarations. Does that make sense?
&gt; some of the updates are quite unexpected and may happen even without cargo update command. If you can say more, that would be very helpful. The lock shouldn't be updated unless you `cargo update`, as far as I know.
Thank you! That was very helpful
This works because `u8` doesn't implement `Drop`, but for something that does, you need `ptr::write`, no?
[https://www.twitch.tv/blackaura04](https://www.twitch.tv/blackaura04) Here is what someone said in my stream. breadwitbutter: Aura is good quality streamer who plays games he likes And community likes. Good to know he communicates with his viewers well and listens as well as replies to them accordingly. BlackAura04 looks like he will evolve into a better streamer over the time he will be on twitch and will be a favorite of the twitch community one day. I did this analysis over the one year you have been streaming (estimate.) I believe that if you continue to be the person you are you will definitely grow in the twitch 
What about [with\_max\_len](https://docs.rs/rayon/1.0.2/rayon/iter/trait.IndexedParallelIterator.html#method.with_min_len), [with\_min\_len](https://docs.rs/rayon/1.0.2/rayon/iter/trait.IndexedParallelIterator.html#method.with_min_len) ?
[why not, though?](https://github.com/achilleasa/gopher-os) 
&gt; Also why was even that language created? To have the safety and comfort of a modern language like C# without needing a garbage collector. &gt; Or does it solve any major issues? There are lots of things that only assembly language, C, C++, and various lesser-known languages could do, because everything else used garbage collectors. One that might interest you is writing compiled Python extensions without giving up the safety of a more modern language. (Java, Go, and C# can't do it because they have GCs of their own. To get Python into Java and C#, they had to write whole new Python implementations, [Jython](http://www.jython.org/) and [IronPython](http://ironpython.net/), which run inside the JVM and the CLR, respectively.) Rust, on the other hand, already has things like [rust-cpython](https://github.com/dgrunwald/rust-cpython) (Python and stable-channel Rust), [PyO3](https://github.com/pyo3/pyo3) (nicer API, but still waiting on nightly for certain language features), [Neon](https://www.neon-bindings.com/) (Node.js and Rust), and [Helix](https://usehelix.com/) (Ruby and Rust) to make it easy and comfortable to safely write compiled extensions. (rust-cpython and PyO3 can even be used with [setuptools-rust](https://pypi.org/project/setuptools-rust/) to integrate the Rust complation into your `setup.py` similar to how C and C++ extensions are already supported.) &gt; P.S. What I wanna use it for? Well I won't be programming hardware for sure. Also I don't expect web stuff from that (but yes I'm aware of webassembly). Mosty lightweight software or CLI "scripts". Maybe time will tell. I like to write "high-reliability shell scripts" in Rust, because: 1. They always start instantly (unlike Python) 2. It's *really* easy to statically build pure-Rust binaries (which "shell scripts" almost always are) so you can just copy one binary to any x86 Linux box and have it work. (Don't be fooled by how large a "Hello World" is by default. There are a bunch of defaults you can turn off to make your release binaries lean... like embedding debugging symbols in release binaries to allow nice tracebacks.) 3. Rust makes it very clear where your program is using memory, so it's easy to write lightweight programs. 4. Rust's compile-time checks let you do multi-threaded stuff without stressing over whether you got it right, which is great for things like hacking together a quick-and-dirty batch image thumbnailer that still takes full advantage of your multi-core CPU. 5. Rust has a powerful type system which allows you to ensure various invariants at compile time... and that's is great for making sure my scripts don't break when some unexpected data arrives in the middle of the night. (For example, that error PHP sometimes gives about trying to set headers after the response body has already started? Impossible with the [Hyper](https://hyper.rs/) HTTP library for Rust. You'll get a compile-time error if you accidentally try to set headers after starting the body. Generally speaking, that particular trick works for anything you can express as a finite state machine.) &gt; but I'm not sure with the "Rust? What Rust?" part. As mentioned, Rust has a unique advantage. (Being safer and more comfortable than C and C++ while still doing things only C and C++ could do before.) Have you seen the [list of companies using Rust in production](https://prev.rust-lang.org/en-US/friends.html)? Also, it's [comparable to C and C++, speed-wise](https://benchmarksgame-team.pages.debian.net/benchmarksgame/which-programs-are-fast.html), which no other significant language can claim. As I remember, there are three reasons it's not even faster in that chart: 1. LLVM's optimizers aren't quite as good as GCC's yet, so some of that performance gap is just GCC vs. LLVM, regardless of language. (eg. You'd see it if they benchmarked GCC-compiled C vs. LLVM-compiled C) 2. The C++ test programs got some clever patches within the last year which make use of compile-time metaprogramming capabilities that none of the other languages on the chart yet offer. That's why GCC-compiled C++ is shown as significantly faster than GCC-compiled C. 3. The Benchmarks Game requires that tests be run on stable-channel releases of compilers and Rust only gained SIMD support in stable channel relatively recently, so there are probably still tests where C++ uses a highly-optimized SIMD implementation and Rust uses either no SIMD or SIMD with room for improvement. In theory, Rust actually has the potential to be *faster* than C and C++ because the language semantics make guarantees that would enable more powerful optimizations... the problem is that LLVM's optimizers were designed for C and C++, so they don't yet take advantage of those opportunities. &gt; P.S.S I hope you get the idea. Today's internet is like "new day new framework" and "new week new technology" and "new month ... we need to rewrite all the stuff". I don't wanna be part of that. I know exactly how you feel.
I am very happy to see this! I've been going through the somewhat unpleasant task of manually writing my own GraphQL types for Juniper. Having something like this to generate the relevant Rust types from the schema is a wonderful idea! I'll pull the crate down and give it a try.
You're confusing the video game with the programming language.
Thats awesome! Let me know if you're having any issues 😊 
&gt; Actix is right up at the top of the charts for serving plaintext content. That it is. Note, however, that the plain text benchmark you linked does _not_ include sending the contents of a file as a plain text response—it just involves sending a "hello world" text response. If, as others have mentioned, actix's (relatively) slower performance is due to reading the files into memory, then that delay wouldn't show up on the benchmark (but would be a real factor for serving static files in general)
Doesn’t collect amortise allocations whenever the iterator knows how long it is? Or am I overestimating the number of situations where your final iterator will actually end up implementing TrustedLen or ExactSizeIterator? See here: Vec’s FromIterator impl is specialised. https://github.com/rust-lang/rust/blob/b6c32da9b0481e3e9d737153286b3ff8aa39a22c/src/liballoc/vec.rs#L1827 Specialisation isn’t stable yet, but you can roughly approximate this for your own iterator adapters that produce Vecs if you have an extra trait with a different name that does the same thing but with different type constraints and more efficient allocation. I’ve done that for my own implementation of Haskell’s `intercalate` function.
Does it compile to wasm? :thinking:
You can't tell collect to "use this pre-existing allocation," and therefore, you can't reuse allocations with `collect`. The code you linked isn't necessarily what I'd call amortization, although one might consider it as such in a strict sense. It's "just" an optimization that pre-allocates to the right size, which amortizes the number of reallocs required to execute `collect`. But I'm talking about amortization at a higher level; namely, the ability to reuse allocations. Compare and contrast the APIs for `Read::read_to_string` or `Read::read_to_end` with the APIs for `fs::read_to_string` or `fs::read`. The former permits reusing allocations. The latter does not. This does not make the latter bad, it just means you can't use it if you care about reusing allocations.
I'm quite new to Rust but I would convince you these ways (please someone interrupt me if I'm wrong) : 1. Rust is potentially fast as in C++ fast. 2. Rust get rid of a whole category of bugs you had encounter (namely memory bugs, which, in my experience, is about 60%-70% of them). It just won't compile (unless you use the "unsafe" keyword, but it's meant for very specific cases). 3. Rust's borrow-checker will beat your bad habits down to the point where you will become a better developer overall. You will get a "safe" mindset. 4. Rust has a loved package manager: Cargo. 5. As a web dev, Rust's WASM story is planned to be a high priority, and a perfect fit by design. 6. Look at techempower benchmark, actix (rust web framework) is already among the fastest. 7. It's been 2 years in a row that Rust is SO's most liked language (according to their surveys). 8. Can you imagine writing something C++ fast except you have high confidence it will backfire at the worst time? It's an amazing feeling. 9. Since it's safe by design, multithread is as well (look for library rayon and Tokio). 10. Amazing community, both very welcoming (you can ask for help and code review here) and knowledgeable. 11. Definitely not "wth is Rust", the biggest are already using it. 12. You don't have to rewrite everything from scratch in Rust, FFI allows you to change parts of your current project in Rust. I think it's a good way to have a better idea. There are downsides to these, but I will conclude by saying it's the first time I experience so much passion and goodwill about a programming language. And the coming years are about to be pretty exciting on many fronts (a lot of unfinished business to complete).
Congratulations on your release!
It does! I'm on mobile now, check out the Makefile. There's a wasm version linked from the post, too.
Thank you!
There is a massive difference between Rust and Go. Go is, as you say, simple, that in fact is one of its main selling points. It has batteries included, it has just one way of doing parallel code, it's statically typed but has no generics. Rust, on the other hand, is a big and complex language, there is a lot of syntax you will not see anywhere else, like _lifetimes_. Also, the nature of constraints it forces on programmers tends to force you to _unlearn patterns_ from other languages. Are you comfortable doing class inheritance and OOP? Great! Now forget all about it, you ain't gonna need it where you are going. All that complexity and the fact that the language is genuinely hard to learn is a price we pay, but it is a price worth paying. Rust solves really hard programming problems without making any compromises on performance or correctness. You can write really fast software with it ([faster than Go](https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/rust-go.html) in most domains, on par with C or C++), your code will be easy to refactor and will likely have less bugs in it due to all the safety guarantees Rust can give you. It allows you to do things [that used to require you to be *this tall*](https://bholley.net/blog/2015/must-be-this-tall-to-write-multi-threaded-code.html), and do them safely and correctly, even if you are a novice.
In cases where nginx would use `sendfile`, I expect that could win, since none of the Rust server frameworks use that yet. Otherwise, try warp's fs filters. They make use of the OS blocksize to improve file reading speed, and then hyper uses writev so as to not need an extra copy before writing.
Regex would be fair and practical to compare. It's very adaptive.
This is so nice! I've yet to vj
Woah I love this! We use GraphQL with our rust middleware and the idea of using external schema files has been an open question. We will definitely be digging into this!
A comparison between regex and aho-corasick is not plausible on just a few inputs, so I do not thing it's practical without a very careful analysis. For example, regex will use aho-corasick itself, but not always.
Nitpicking, but `expect` is always better than `unwrap` in that case. :-P If someone just gets `unwrap failed` they're like "am I doing something wrong or is this a bug or what?" If someone sees `expect failed: "This should never happen!"` then they have a MUCH more specific place to start a bug report. Splitting a project into modules also creates strict API bounds. And splitting a project into separate crates means that anyone who tries to use the project has to understand what the differences between the crates are, and when they see `foo`, `foo-X`, `foo-Y` and `Z-foo` on crates.io, where to start looking for useful info.
Thanks, I'll try to find examples to try.
That's awesome, thanks so much for sharing! I'm on a similar journey, so it's very interesting to read stories with familiar experiences :)
I'm not sure I completely understand your question, but wouldn't tokio::spawn work for you?
Have things changed since November? &amp;#x200B; [https://andygrove.io/2018/11/datafusion-2019/](https://andygrove.io/2018/11/datafusion-2019/)
The unexpected updates usually occur when crates are added/removed to a workspace. I'll try to find some minimized examples (after holidays..).
Ah, yes. If you change the crates you depend on, then the lock file doesn’t describe things correctly anymore. So it has to update.
Thanks and good luck to you!
As far as I could tell, no. Spawn has to be called after \`tokio::run\` but it doesn't start until the run task has been completed. &amp;#x200B; I am super bad at explaining stuff, sorry. So I basically have a function that needs to get called every second. I use an interval for that one and start it using \`tokio::run\`. The other function runs every hour and does some I/O stuff. I started that one with \`tokio::run\` too. But I noticed that the 1-hour-interval function doesn't get called until the 1-second-interval function stops running. I noticed that by reducing the time for each interval from 1 hour down to 5 seconds. But it doesn't print stuff to the console. &amp;#x200B; So I basically need a way to start the \`tokio::timer::Interval\` in another thread so I can have them running simultaneously. I know that I can just use \`thread::spawn\` and it should work without problems. But I thought there might be a more \`tokio-ish\` way to do it. Maybe there is a simple function call to just start a future (which Interval basically is) on another thread.
Ah. So... C++ sucks. It is perfectly legal, in C++, to destroy and in-place new another expression in place of \`my\_op\`, under some conditions. Crazy, right? In Rust, since the pointer to the v-table is passed by copy this is not feasible and thus much more easily optimized.
&gt; it will be dynamically dispatched, and of course slower than good old static dispatch. The most important question: can you witness this performance difference in a real work load? If not, then it might not be worth going out of your way to avoid dynamic dispatch.
Ah, yes, avoiding allocations entirely is substantially better. Has anyone proposed an API for normal iterators along the lines of the IndexedParallelIterator in rayon and its [collect_into_vec](https://docs.rs/rayon/latest/rayon/iter/trait.IndexedParallelIterator.html#method.collect_into_vec)? If it were that easy, I suspect there would be a lot of low-hanging performance wins in a lot of crates.
Thanks, I will try this out. I have a list of a couple of thousand files, but only a few will need to be processed, and they tend to be clustered in the list. So it could be that what I am seeing is just the effecters of "chunking" on the iterator.
rayon is not designed for I/O bound tasks, see https://docs.rs/rayon/1.0.3/rayon/fn.join.html#warning-about-blocking-io
Yeah, I would suggest trying `with_max_len(1)`. Rayon will try to be adaptive with splitting its input (the list of files), but won't usually divide down to individual items. My guess is that this case has a thread stuck on one group of files with a lot to do, but forcing it down to 1 will make it spread out.
tokio-timer should do it even for sub-second Delays. Don't quote me on this but I dont think it has ns level accuracy but for a few hundred ms it should be fine. This would create a single timeout of `delay` milliseconds fn timeout(delay: u64) -&gt; impl Future&lt;Item = (), Error = ()&gt; { Delay::new(Instant::now() + Duration::from_millis(delay)) .map(|_| ()) .map_err(|_| ()) }
I don't know. The set of all proposals is quite large, and I don't know all of them. `collect` is nice because it is very easy to use, in part because it specifically does not support reusing allocation and in part because it uses return value polymorphism. Its entire purpose is pretty much as a convenience. Introducing new APIs that are less flexible, therefore, does not necessarily mean everyone is going to start using that and benefit from the performance gain. Moreover, not every use of `collect` impacts performance in an observably negative way. And even then, building out APIs that reuse allocations is rarely a local change. It often bubbles up and infects APIs all the way up to either the user of a library or until there exists a spot in the code in which the allocation is dwarfed by other work.
You should have a single `tokio::run`, scheduling a future that will itself schedule others via `tokio::spawn`, like: tokio::run(futures::future::lazy(move ||{ let interval = // make your interval tokio::spawn(interval) Ok(()) })) tokio::run blocks until all spawned futures are done.
When crates are added, are dependencies for unrelated crates updated in Cargo.lock?
I *believe* it attempts to make minimal changes, just like when you say `cargo update -p`. But it's been a while since I studied the resolver, I may be wrong.
I've had a problem actually executing the command correctly. Its a little different based on what shell / os you're running. Maybe that's why its not actually working, thought if it actually is back tracing it should show you lots of extra lines for the error digging what like of what file actually is causing the fault. (So it should look a little different). With Linux &amp; Fish its `env RUST_BACKTRACE=1 cargo run` [Someones else had a similar issue and here is their result](https://www.reddit.com/r/rust/comments/7h24y5/how_do_i_run_cargo_test_with_rust_backtrace1_on/)
Man, I remember playing this on the web not long after I guess the first wasm announcement for it was made and ... dying a lot. Has the gameplay changed (i.e. is it any easier)?
Can't dynamic dispatch be faster for certain workloads too?
Normally trying to read past the end of a Vec or an array causes a panic rather than a segfault. Segfaults start happening when you resort to unsafe code, so that you're skipping the checks that normally would throw the panic. In that case sometimes your OS will notice that you're dereferencing a wacky pointer and kill you with a segfault, or sometimes you'll get arbitrary undefined behavior.
Maybe? Best to come up with a real example and measure it.
Roguelikes, as a genre, are generally very punishing, so I wouldn't expect it to have!
Congrats! I *love* rougelikes.
I just switched to [docopt](https://crates.io/crates/docopt) instead. Now I am the only one who controls the help message.
https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0912ae8a7cd8b2d09125a37242e2ee6d
Please, what's the downside to using the writers approach compare to your solution. Thank you.
Rust allows you to model things far better than most languages I've used -- it really raises the bar with what you can do with types. For instance, take a look at [Rocket](https://rocket.rs) for an example of how well-written Rust can make domains like web APIs super concise, fun, AND performant.
We just put on a blog post on them, incidentally https://blog.rust-lang.org/2018/12/21/Procedural-Macros-in-Rust-2018.html
[Two links to it](https://www.reddit.com/r/rust/comments/a8cjzu/procedural_macros_in_rust_2018/) at the same time, and none of them by the author? That's a race condition *and* a violation of ownership rules.
No pun intended, you've created quite the addicting (and simple) roguelike. Congrats on your release!.
I've never written a single macro in my life, so please bear with me if these questiond sound kind of ridiculous: &gt; Each of these flavors of macros can be defined in a crate with `proc-macro = true` [specified in its manifest](https://doc.rust-lang.org/cargo/reference/manifest.html). Why is there a separate crate type just for proc macros? In what way are they different (linkage-wise) from declarative macros for example? Or, coming at this from another angle—if this is the case: &gt; Procedural macros are incorporated with the module system, meaning no more they can be imported just like any other name. Why do proc macros need the annotation in `Cargo.toml' at all?
Thanks for the post. I think one of the most difficult parts of macros in Rust is the lack of documentation, so this should help some with that. I'd like to point out two proc macro crates that might be interesting to review for someone looking for more examples: 1. [todo_macro](https://github.com/JoshMcguigan/todo_macro) - A procedural function like macro which allows creating todos in your code with a deadline, so the code fails to compile if you haven't fixed it. This was a toy example I wrote while learning about proc macros. 2. [cache-macro](https://github.com/tylerreisinger/cache-macro) - A procedural attribute macro to automatically cache the results of a function call. This crate is in the early stages, and I'm not sure anyone is using it in production, but I think it could be very useful.
I've recently bumped my nose into how `env_logger` works in tests (you have to start `let _ = env_logger::try_init()` in every test). How hard would it be to write a proc macro that inserts this statement at the start of a test?
Procedural macros are compiled pretty differently than other crates, primarily during cross compilation. A procedural macro is loaded and executed by the compiler you're running, so it needs to match the compiler's own architecture, whereas you may be compiling for something else (like wasm!). This also affects all of the dependencies for a procedural macro, they also need to be compiled for the host. Now that doesn't really *require* per se an annotation in `Cargo.toml`, but it does show off how Cargo needs to do something pretty different for procedural macros, so it at least needs to know what is a macro and what isn't. We may have taken a bit of an easy route with a declarative annotation instead of some other form of inference :)
nginx is a product of years and years of optimization by pretty much the entire rest of the world, so it would be very hard to beat. For example, it creates certain data structures optimized to the size of your CPU cache, which has to be known in advance and specified in config. It also fails if the config parameter is specified incorrectly. So I would not expect anything *naive* beat it. But equally optimized Rust could, and it's much easier to optimize safe code because you don't have to worry about breaking everything in a subtle way and only finding out in production.
On a related note: if you are using Juniper for a somewhat larger project or in production, we'd love to hear from you: https://github.com/graphql-rust/juniper/issues/304
Maybe I just misunderstood your description, but if your tool is primarily invoking gpg, why would you expect much activity in the Rayon thread pool? It sounds like most of your threads should just be waiting for a child process to exit, which won't show up as cpu usage. As for whether your gpg subprocesses can be expected to use all your cores, is there any chance one of the inputs you're processing is much larger than the others, such that most of your work can't really be parallelized?
You want the "no modules" mode of wasm-bindgen, which outputs bare js/wasm files that can be simply dumped into a &lt;script&gt; tag. https://rustwasm.github.io/wasm-bindgen/examples/no-modules.html
That is super clean! Thanks for sharing!
It's actually [not too hard at all](https://gist.github.com/alexcrichton/08ded796aa693caad8f8b0b2743579d8)! $ git clone https://gist.github.com/alexcrichton/08ded796aa693caad8f8b0b2743579d8 $ RUST_LOG=smoke cargo test --test smoke -q -- --nocapture running 1 test [2018-12-21T18:38:39Z DEBUG smoke] hello test smoke ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out 
Kudos for following through to release a game!
I'll need to subscribe to the blog. And I'll definitely give it a ready and see if there is anything I can do to improve what I've figured out.
Interesting. I hadn't heard of warp—thanks for sharing it (and for writing it, assuming you're the same seanmonster who did so!) I modified warp's `file.rs` to serve the same static file and … wow it's fast! It was at 72k req/sec—still slightly below nginx's multi-threaded results (~80 req/second) but close enough to not bother me much. I assume that warp is muti-threaded by default, right? (I'll be *really* amazed if those are single-threaded results :D )
But they're immutable references so it's ok :)
I recently got back into coding and went back to trying to make a roguelike again. I quickly got tired of C++ and have been looking into rust. It's good to know that it is viable for making games and that you enjoy it more to C++.
you're totally right, I didn't read the question carefully enough.
One potential gotcha, although you cover it briefly in the post, is that you have to use `extern crate proc_macro;` even when using Rust 2018 edition. I understand the reason why (`proc_macro` isn't explicitly listed in the dependencies), but I think cargo could handle this if it sees `proc-macro = true`. Are there any plans to change this? It seems the most consistent thing would be if you could add `proc-macro` like any other dependency, in the dependencies section of the `Cargo.toml`. 
True! My use case is SAT, so I'm in luck here ;)
Wow! You're able to get current time in a procedural macro? So it's not limited to calling `const fn`, you can literally run *arbitrary code*?!
I found the quick-xml crate, which provides a pretty direct example of infecting the API. See the `&amp;mut buf` in every usage example. Often allocations will be disproportionately costly when you have to do a lot of small ones, in which case the remedy might be something like SmallVec, and not making sweeping changes and decreasing readability to reuse tiny little containers, which by their nature are probably not being created in single-scope loops but littered throughout a codebase. I think that’s probably the right direction for https://github.com/cormacrelf/citeproc-rs, which is just a fancy vec and string joiner. (It’s malloc_tiny all the way down at the moment.) Removing collect()s in very hot code would probably make zero difference after picking an appropriately-sized SmallVec instead. If anyone’s out there trying to work out where the tipping point is for allocation cost, I would recommend https://speedscope.app along with perf record or Xcode Instruments.
Correct.
There’s been talk of fixing this for all sysroot crates, but it hasn’t happened yet. It’s not easy but it does have a ton of benefits...
Thanks a bunch! I'm not sure whether this is useful enough to warrant publishing to crates.io, but I'll definitely be ~~stealing~~ borrowing this for my own code.
You may want to look into if nginx is caching the file bytes or using mmap or some such optimization when rereading the same file many times.
As a side note, the information on macros in the current book is horrendously slim. The information for the various macro types and how to implement them needs a lot of work. 
I mean, use regex to compare it vs the other substring search crates.
One possibility /u/dtolnay and I have discussed is allowing: #[proc_macro] pub fn foo(input: proc_macro2::TokenStream) -&gt; proc_macro2::TokenStream { // ... } and the compiler will accept that so long as the argument is `Into`/`From` `proc_macro::TokenStream`, and that way you wouldn't have to use the `proc_macro` crate at all! In general though we haven't put a lot of thought and effort into this, it's just known as something we'd definitely like to fix!
Yes. The reason that I haven't invested a ton of time in this is that I want to teach `macro` macros, not `macro_rules` macros. But they're not done yet. We decided to make this tradeoff since macros are mostly used, not written, and you can learn about them elsewhere.
I think /u/acrichto is the author of the post
If that's the case, I don't see an issue. `libc` just needs to be updated with the appropriate link names.
Ah ok. Not me, then. ;)
That's understandable. I know macros quite well but as a new user I would be completely lost right now though. Both for macros by example and for implementing custom derives and other macros. It's not actually so easy to find something with Google either. Just linking to some related resources would help a lot, like https://danielkeep.github.io/tlborm/book/index.html.
I don’t think so? I follow Andy on Twitter and he hasn’t tweeted about any changes.
Linking means that they don’t work offline, and generally aren’t controlled by the project, so it’s harder to make changes as necessary.
This is why I limit the number of threads, to keep the workload CPU bound :-)
Currently attribute macros can't be applied to `mod` items. Will this restrictions be lifted in the future?
You could scan the entire file system or do web reqwests. Anything goes. Now if that's smart is another issue. ;)
Check /u/ct0r reply about with_max_len() , it seems likely that the par_iter() chunks the elements together. This becomes quite noticeable when each work item takes several minutes to process. 
Yes. You do. Otherwise it will try to drop the uninitialized value prior to writing to it.
I'd have to see the code to really know, but it's starting to sound like Rayon might not be the best fit for this problem. Spawning subprocesses is pretty expensive in general, all the more so if each one is talking a long time to finish. Compared to that, spawning a new thread per subprocesses is relatively cheap, and that might make it easier to coordinate them.
As someone who recently had to write a fairly big procedural macro for the first time, I would appreciate more detailed information on the matter. Sure, I can look around at how crates have implemented theirs, look at the docs of syn and proc_macro, but in the end of the day, an exhaustive documentation in the book will save a lot of time!
You probably want to return an iterator there, I think.
All the work happens in gpg, I am just trying to use rayons work stealing to balance the work to a limited number of threads. This way I also keeps the IO load in check and avoid that the performance of the storage system degrades. The program will be run repeatedly in smaller batches as the data becomes ready. My problem was at the end of each batch the remaining work would be limited to fewer and fewer threads, slowing the batch processing down. This should be fixed by not having the work items clump together in very uneven sizes.
In `rustc` the fix is easy and is already implemented under a feature gate (`--extern without path`). The issue is how to present this on the Cargo side.
Hopefully this will be resolved once [const generics](https://github.com/rust-lang/rust/issues/44580) are merged. Even if it takes a while to be stabilized, \`std\` can take advantage of it immediately.
Yes, the tracking issue is https://github.com/rust-lang/rust/issues/54727. Outer attributes on inline modules (`#[attr] mod { /*body*/ }`) could be stabilized right now given enough push, but other cases have some unresolved questions.
I'm porting this to Redox!
Honestly, doing it in really low-level Rust shouldn't be much different from doing it in C. You just read bytes in, parse them, write bytes out. You probably don't need to implement the entirety of HTTP, either.
Because `Vec` doesn't implement `Iterator`? It implements `IntoIterator` instead.
That's right. But with blanket impls the amount of methods type has is infinite.
So today is my first day trying to learn about rust. I am trying to use IntellliJ Rust to develop for the language. Admittedly I am only 19 and don't know a whole lot about installing stuff like this. 
I also feel like it shouldn't be too hard, but I'm not doing very well implementing this though, because this is my first project after going through The Book, as a newbie to Rust. One thing I've been having trouble finding is the socket system call that C has. Should I be using std::net's SocketAdrr or something else? &amp;#x200B;
So you have binary as a string, and you want to get hexadecimal as a string? If so, you can just do it bit by bit - use `rchunks` to convert every group of 4 bits into a hexadecimal digit, then build a string out of the characters you get.
 I also feel like it shouldn't be too hard, but I'm not doing very well implementing this though, because this is my first project after going through The Book, as a newbie to Rust. One thing I've been having trouble finding is the socket system call that C has. Should I be using std::net's SocketAdrr or something else?
You probably want a [`TcpListener`](https://doc.rust-lang.org/std/net/struct.TcpListener.html).
That’s correct. I submitted because I just generally do it, and if someone beats me, the duplicate filter catches it! Looks like it didn’t this time. The mods should remove mine IMHO.
Are you using Cargo, or not?
I believe so. &amp;#x200B;
Excellent - First writeup I've come across
Ok I'm using that and now I can listen for new connections, this is going well. Now for the hard parts...
I recently posted [this question](https://www.reddit.com/r/rust/comments/a58rt7/most_appropriate_way_to_use_macros_to_generate/) because I was having troubles finding useful real-world examples of proc-macros. This is for a work project but hopefully once it's to a good point I'll be able to open-source it as another example.
No! In fact I was wondering if your issue was that you *weren’t* using Cargo. Hmmm
I would use an Elgot coalgebra and proof-level recursoin schemes.
Dynamic dispatch is not an evil thing and the perfomance is indeed slightly worse than static dispatch, but it is not horribly slow. As long as you don’t use dynamic dispatch in a hot loop (eg, millions of times per second), the performance difference is not noticable. Your use-case is a prime example for dynamic dispatch using `Box&lt;dyn Middleware&gt;` assuming that the `Middleware` objects do some actual work.
I'm really looking forward to all of the uses that proc macros give us. The `syn` and `quote` crate are so insanely useful when it comes to proc macros. They make creating a proc macro almost trivially. Comments throughout the crate are fine, it's just a question of combining everything. It took some trial and error, but it's really easy once you get the hang of it. Case in point: https://github.com/Jezza/def_mod I implemented a mod declarations that include implementation routing and export verification within a day or two. I only spent the first couple of hours going through how `syn` used the parser macros, and once I got that from there it was pretty straightforward. 
yeah, would you like a screenshot of the error message as well or was the description I provided good enough?
I'm not really sure what it is we're talking about any more. The original context of this discussion was just about `collect`. Putting allocation semantics into an API doesn't necessarily have to complicate things. In my experience, you can typically have convenience APIs _and_ lower level APIs that permit reusing allocation when necessary. I would not use `SmallVec` everywhere instead of this, because it's less flexible. For example, ripgrep reuses its internal buffers for reading files and they can be quite big---much bigger than what a `SmallVec` might put on the stack. And yet, it's still beneficial to do this from a performance perspective. `SmallVec` might work in some cases, sure.
I'm really looking forward to all of the things people are going to implement with proc macros. Side-note: The `syn` and `quote` crate are so insanely useful when it comes to proc macros. They make creating a one almost trivially. The documentation is pretty good as well. My only compliant would be how it's all combined. It's not perfect clear how you might want to do something. I will say the examples go a long way, but honestly, most of the time, I just ended up searching through the `syn` crate, working out how they did certain language things. Once I got the general gist, which only took the first couple of hours, the rest was really easy. The proc-macro I implemented: https://github.com/Jezza/def_mod It basically expands upon `mod` declarations with implementation routing and statically verified module exports It might also act as a useful example for some people, because it's not massive. A large chunk of the code comes from trying to transform some of the AST nodes into other nodes, but that's to be expected. 
Got it! Thank you a lot! And last but not least question: why or for which cases we should trade off so much of CPU cycles (in most cases 2-3x times) to get up to 1/2 ULP precision using f32 parser instead of up to 1 ULP using f64 parser with rounding to f32?
I would think so, yes.
This sounds like complete gibberish and something Haskel-ly at the same time
If you went through the book, the last chapter was building a web server! That should have given you the basic framework.
This is where I admit that I'm being impatient and that I'm only halfway through the book XD
Ha! Well, chapter 20 should help you out significantly ;)
I guess it's better to actually go through the entire book. I'm going to do that and then I'll do the web server project
There's not really a use-case, to be honest. The cases where you want high-precision with no error require the use of double-precision floats, but I think it's mostly a user-consistency thing: you want someone to call (in Python, but it's true for every language) `float(str(x))` and get the same result every time. Imagine if you had a long-running simulation that fails because `float(str(x)) != x` , when it should be? Yes, should you ever compare floating-points for equality? Generally not.
You can always skip to there too, and just go back if something is confusing.
I'm actually doing much better, maybe because this time I bothered to read the help.
My guess would be some sort of auto-vectorization is being applied that wasn't the case in earlier versions. Perhaps an LLVM upgrade or more clever intermediate compilation might be happening. I don't know the details, but feel it's something along those lines.
Thanks! Me too but I'm *so bad* at them :-).
I'm back at a computer. You can play the wasm version here: https://tryjumping.com/dose-response-roguelike/play/ I wrote about porting it to WebAssembly a year ago: https://tryjumping.com/blog/dose-response-ported-to-webassembly/
Nightly switches from jemalloc to the system allocator. That may be related.
Wasn't that put into stable in the 1.31 release?
I don‘t think so, that switch happened fairly recently and that would‘ve been mentioned. Also I think I heard like a week ago or so that it‘s currently in Beta.
What Steve says is correct -- roguelikes are generally hard and unforgiving. But: I did balance the game some since that post. For example, the is beginning less immediately dangerous. The stronger doses and harder monsters don't spawn right next to you so you've got some breathing space (but not a lot or the beginnings would be boring). I'm pretty terrible at roguelikes myself and I've managed to beat this game. I have not managed to beat almost any roguelike I've played. But yes, unless you want to figure it out on your own, read the help (which actually **didn't exist** back then). All that said, I've tried to make this game pretty accessible to non-roguelike players. Within the "traditional roguelike" genre constraints -- I can't do much about the graphics or the unfamiliarity of the 8-directional movement. But the game is pretty simple to learn and understand fully. And it's pretty deterministic. But don't beat yourself up -- I still lose a lot and I wrote the thing :D. If you do find the experience compelling, or want to try other games that are pretty accessible, check out [868-HACK (PC, iOS)](https://store.steampowered.com/app/274700/868HACK/), [Hoplite (Android, iOS)](https://play.google.com/store/apps/details?id=com.magmafortress.hoplite&amp;hl=en), [Hero Trap (web)](http://noisyowl.com/herotrap/) or [Brogue (desktop)](https://sites.google.com/site/broguegame/home). They're all much easier to learn and play than most of the traditional roguelikes and they're all really good.
Haha, that was fully intended, thanks!
Sorry about that! I just pushed version 0.1.1, which works on stable.
Yeah, you're right. It seems to be scheduled for release in 1.32, it's currently in beta. That might be it. I thought it already was changed in last stable so I didn't consider it, thanks.
Awesome, fingers crossed on your journey! Feel free to ask around in /r/roguelikedev (everyone's really nice there), here, /r/rust_gamedev or PM me any time! If you're okay with a Rust game that looks like mine above, you can also try this tutorial (which I wrote -- well ported from Python to Rust): https://tomassedovic.github.io/roguelike-tutorial/ If you want graphics, you should try [ggez](http://ggez.rs/) instead, though.
I coudn't replicate this on Linux, FWW ➜ speedtest git:(master) ✗ rustup default stable info: using existing install for 'stable-x86_64-unknown-linux-gnu' info: default toolchain set to 'stable-x86_64-unknown-linux-gnu' stable-x86_64-unknown-linux-gnu unchanged - rustc 1.31.1 (b6c32da9b 2018-12-18) ➜ speedtest git:(master) ✗ rustc --version &amp;&amp; cargo build --release &amp;&amp; time ./target/release/speedtest rustc 1.31.1 (b6c32da9b 2018-12-18) Finished release [optimized] target(s) in 0.10s 9999010000 ./target/release/speedtest 0.46s user 0.69s system 99% cpu 1.151 total ➜ speedtest git:(master) ✗ rustup default nightly info: using existing install for 'nightly-x86_64-unknown-linux-gnu' info: default toolchain set to 'nightly-x86_64-unknown-linux-gnu' nightly-x86_64-unknown-linux-gnu unchanged - rustc 1.33.0-nightly (09d6ab90e 2018-12-20) ➜ speedtest git:(master) ✗ rustc --version &amp;&amp; cargo build --release &amp;&amp; time ./target/release/speedtest rustc 1.33.0-nightly (09d6ab90e 2018-12-20) Finished release [optimized] target(s) in 0.10s 9999010000 ./target/release/speedtest 0.46s user 0.74s system 99% cpu 1.199 total 
OMG really? Wow! Thanks! Anything I can do, let me know. I'd definitely love to upstream any changes you end up making.
Yes, it might be. It's either not allocating anything before collecting the sum or allocating a lot faster. So it could be either. One way to find out is to test the same on Windows (where jemalloc is not used in neither stable nor nightly) and see if there's the same difference. If it's not any big difference it's probably that nightly uses os-allocator on linux/osx while stable uses jemalloc as @CryZe92 pointed out. If the same difference is in windows it's probably something along the lines of what you suggest.
That's strange. I'm on OSX.
What's the status with `proc_macro_diagbostics`? Without that implementing any sort of decent error reporting is extremely painful. 
I don't have any comment on your code, but in case yo've missed it, there is a quite tiny but powerful http server library called [warp](https://github.com/seanmonstar/warp) by the same person who built Hyper, which perhaps fits your use-case better.
Maybe try https://capnproto.org/. It's inspired from protobufs. 
There is a [hole chapter about this in "the book"](https://doc.rust-lang.org/book/ch20-00-final-project-a-web-server.html) I hope this is the kind "from scratch" you're searching for. 
I don't think you want to use laminar. Laminar is intended for multiplayer games, where dozens of information packets come in every second, so it doesn't matter if you drop a few. I would suggest you use [ØMQ](https://zeromq.org) (ZeroMQ). It's a library made exactly for that purpose. There are [rust bindings](https://github.com/erickt/rust-zmq) as well.
Yes, definitely needs spreading more widely.
It's possible it's due to how broad the capabilities of procedural macros are, but I'm having trouble envisioning the uses outside of utilizing `serde`, `rocket`, and similar crates. Is there some repo or page with some aggregated examples or perhaps someone is willing to explain in a bit more depth?
Yeah, I've never thought of it like that, but yeah. It can only store `f64`, `bool` or `str` though, but you can also use nested links in the syntax that gets flattened to a single link. It makes it easier to refactor code.
Closing as dupe in favor of https://www.reddit.com/r/rust/comments/a8chzq/procedural_macros_in_rust_2018/ , which has more comments and votes.
As a minor thought on the btrfs question. I do support outputting a json document of the duplicate files. You might have to condition that to be usable with whatever btrfs wants as input, but it might do the trick. 
There's a part two: [https://myrrlyn.net/blog/misc/rust-flow-part-two](https://myrrlyn.net/blog/misc/rust-flow-part-two)
&gt; When crates are added, are dependencies for unrelated crates updated in Cargo.lock? No, but the definition of "unrelated" is pritly loose. This is mostly accomplished by a series of heuristics.
&gt; We're *executing arbitrary code* at compile time, which means we can do just about anything! Does this mean I can offer macros-as-a-service, where I ship a crate with a thin proc-macro shim that submits a token stream to my server and just returns the response?
I used your suggestion and it worked! Thank you very much
Thank you for the hint. I'm using CLion and wanted to try out Fira Code some time ago, but it didn't work (and I didn't care too much), so I forgot about it again. I totally overlooked this checkbox.
Congrats on the game. It really plays well when I played the webGL version. [My little crack fiend didn't last long....](https://i.imgur.com/ugJPP2F.png) I really like the theme and graphics of the game and it reminds me of the old MUDs. Definitely wouldn't be out of place as an old telnet game.
it literally does: try "cargo install hyperfine"
When can we expect `Span` methods to stabilize? I ask because I wrote a macro that needs external files relative to the file in which the macro is eventually used. Since the location of the `Span` is currently unstable, I was forced to depend on the unfounded assumption that `rustc`'s current working directory is always the root of the current crate being compiled and specify a relative path manually. If `rustc` ever changes it's default behavior, the brittle code written by myself and others in a similar position will break, perhaps even silently. 
I can't replicate this on Windows either: testerino / ❯ rustup default stable &amp;&amp; rustc --version &amp;&amp; cargo clean &amp;&amp; cargo build --release &amp;&amp; hyperfine -i ./target/release/testerino.exe No info: using existing install for 'stable-x86_64-pc-windows-msvc' info: default toolchain set to 'stable-x86_64-pc-windows-msvc' stable-x86_64-pc-windows-msvc unchanged - rustc 1.31.1 (b6c32da9b 2018-12-18) rustc 1.31.1 (b6c32da9b 2018-12-18) Compiling testerino v0.1.0 (C:\Users\___\Desktop\Stack\Develop\testerino) Finished release [optimized] target(s) in 0.65s Benchmark #1: ./target/release/testerino.exe Time (mean ± σ): 0.3 ms ± 0.3 ms [User: 1.0 ms, System: 2.0 ms] Range (min … max): 0.0 ms … 2.5 ms 675 runs Warning: Command took less than 5 ms to complete. Results might be inaccurate. Warning: Ignoring non-zero exit code. testerino / 5015ms ❯ rustup default nightly &amp;&amp; rustc --version &amp;&amp; cargo clean &amp;&amp; cargo build --release &amp;&amp; hyperfine -i ./target/release/testerino.exe No info: using existing install for 'nightly-x86_64-pc-windows-msvc' info: default toolchain set to 'nightly-x86_64-pc-windows-msvc' nightly-x86_64-pc-windows-msvc unchanged - rustc 1.33.0-nightly (e40548bc4 2018-12-21) rustc 1.33.0-nightly (e40548bc4 2018-12-21) Compiling testerino v0.1.0 (C:\Users\___\Desktop\Stack\Develop\testerino) Finished release [optimized] target(s) in 0.49s Benchmark #1: ./target/release/testerino.exe Time (mean ± σ): 0.3 ms ± 0.2 ms [User: 0.9 ms, System: 2.1 ms] Range (min … max): 0.0 ms … 0.9 ms 695 runs Warning: Command took less than 5 ms to complete. Results might be inaccurate. Warning: Ignoring non-zero exit code. &amp;#x200B;
Thanks for checking. On OSX I get the same result after rechecking several times trying to eliminate sources that might influence the results. Nightly and Beta channel is the same, but stable is slow. Would be interesting if someone on OSX would check and see if they get the same result.
Can anyone make them? How,?
Yeah I can confirm this on OSX. ``` ryan@lancer:~/tmp/speedtest (master) $ rustc --version &amp;&amp; cargo build --release &amp;&amp; time ./target/release/speedtest rustc 1.31.1 (b6c32da9b 2018-12-18) Compiling speedtest v0.1.0 (/Users/ryan/tmp/speedtest) Finished release [optimized] target(s) in 1.00s 9999010000 real 0m2.007s user 0m1.040s sys 0m0.947s ryan@lancer:~/tmp/speedtest (master) $ rustup default nightly info: using existing install for 'nightly-x86_64-apple-darwin' info: default toolchain set to 'nightly-x86_64-apple-darwin' nightly-x86_64-apple-darwin unchanged - rustc 1.33.0-nightly (e40548bc4 2018-12-21) ryan@lancer:~/tmp/speedtest (master) $ rustc --version &amp;&amp; cargo build --release &amp;&amp; time ./target/release/speedtest rustc 1.33.0-nightly (e40548bc4 2018-12-21) r Compiling speedtest v0.1.0 (/Users/ryan/tmp/speedtest) Finished release [optimized] target(s) in 1.93s 9999010000 real 0m0.493s user 0m0.469s sys 0m0.012s ryan@lancer:~/tmp/speedtest (master) $ ```
By default, Cargo looks for either lib.rs for a library or main.rs for an executable (in the src folder). I'd suggest renaming your file to main.rs for the easiest solution for now. I believe you can also just move it to src/bin, but that is a less common way to do it. [Here's a post with some more details.](https://stackoverflow.com/q/37491436/5641333)
Yep. Publish a bin to crates.io. 
https://github.com/rust-lang/cargo/blob/master/src/bin/cargo/commands/install.rs#L40 You just publish a crate that has a binary.
Thanks for checking (and an small sigh of relief that I wasn't the only one). My guess would be that it has something to do with jemalloc and OSX, but somehow that does not seem to be an issue with Linux. Would be interesting if someone with better knowledge of the memory allocator could confirm if this might be the case or not. If so I guess this is probably fixed in 1.32?
Thanks.
Ahhhh OK. I think I understood it. I'll just have to play around with it more to be 100% sure, but so far so good. Thanks for taking the time and explaining it!!
NOW it does make sense! :-) Thanks!
Leaving another comment so you see this. Do you have any recommendations for good books or other resources to learn design practices that work well with Rust? Seems like OOP approaches can be very problematic in Rust
Using declarative macros, it's [https://github.com/rust-lang/rust/issues/35853](not possible) to make macros that both return other macros, and iterate through parameters; you can do one or the other. Is there a workaround using proc macros?
It's finite given a set of crates, which is what rustdoc operates on.
You’re looking for r/PlayRust
You're looking for /r/playrust. Though, if you stick around here, maybe you won't find yourself getting banned from video games so often :).
&gt; ... rust is basically all the awesomeness of C++ smashed together with all the awesomeness and dependency management of JS. While I enjoy the memory guarantees, this plus a defacto cross platform build system are what really make me care about Rust.
Yes. And I'm both horrified and intrigued of what you described.
Rustup and Cargo are the land of milk and honey compared to the garbage fire that is NPM and the JS tooling ecosystem in general. 
I feel like rust is really good for CLI tools as well and would personally prefer it over Java or C#. Structopt and some other libraries are already pretty nice and there's a working group to improve the CLI experience. Also it doesn't have the startup overhead of other languages with a VM.
It would have been nice if there could be a difficulty choice of some sort. "Normal" and "Lots of food", for example. I keep dying quickly without even meeting another \`@\` (I've only had 2 meets in 30+ games I played, and I died soon after). If you're "terrible" and managed to beat the game, I don't know of a word to describe me.
I also submitted a halite bot, I started with c++, but have an almost comparable bot in rust. It has been weird as when I go back to c++ I want to do things like in rust, but other times I get stuck in rust where I get hung up on some triviality. eg. we have a simple definition =&gt; pub struct PlayerId(pub usize); and I wanted to use this to index the player array in the game structure, I spent almost 6 hours with hundreds variations from trying to define a trait, to player\_id as usize and everything in between trying to do this, the internet was no help going on about traits that seemed to be getting ignored so obviously I was doing them wrong, I could create a player id from a usize but I could not convert it back even though I was trying to explicitly cast it, and it should fit as that is in the definition, putting it into a string and back worked but is so horrible I couldn't accept it. What eventually worked was player\_id.0 I mean is this the most random bullshit out there. Rust who tells me that everything will be explicit, and I have to know that that my structure has some property where I can address it with a "0" like an index, but I can't explicitly cast it to use the fundamental and original type. &amp;#x200B; I expected to have problems with the borrow checker from everything I read, but in reality that has seldom been an issue, occasionally I forget the borrow on the calling side as in C this is hidden, and sometimes I borrow twice, usually as a convenience thing for less typing so this is easy to solve. I have been annoyed by the maths sometimes, but have learnt about the partial equality trait, I can understand some of this, ie. integer division is a surprise in c the first couple of times, but wonder if they have gone too far in rust, (I read a blog recently about fast c and that person claimed that the fastest way to do floating point maths was to let the processor catch the errors, as it will do fine if you let it, so if you end up with an infinity or NAN the processor can deal with this and when you can get your answer it might be NAN or infinity. So perhaps you could ease up a little, build like a result enum to find these errors when we want). &amp;#x200B; I was also a little disappointed with the halite rust starter bot, it seemed like a direct transpose of the c++ starter bot, I would love to see a rust starter bot written in idiomatic rust, by someone who knows rust, I'm sure it would be a different experience. I have already made a lot of changes to the basic bot so it can be cloned and other stuff, but I still suffer from my c++ background so I know it is not particularly good from a rust perspective. &amp;#x200B; On the whole It has been a good experience and I have learnt a lot about rust, there is obviously a lot more to learn, and will get there slowly.
Same here, dependency management and build system initially sucked me in and is still my favorite part. Wish we had this way earlier for low level languages.
And then if you start comparing against C++ dependency management...
Oh my, yes ‘&lt;/farnsworth&gt;’
Here is a link to the ASM and a diff checker. The one on the left is stable (stable-x86\_64-apple-darwin) and the one right is nightly (nightly-x86\_64-apple-darwin) [https://www.diffchecker.com/DQRohruR](https://www.diffchecker.com/DQRohruR) If anyone else wants more information on this let me know. 
Small typo: ``` meaning no more they can be imported ```
lol
Idk what that mean but ok
Similar difference on FreeBSD 12, which uses jemalloc as the system allocator. Difference remains with [jemallocator](https://crates.io/crates/jemallocator). Syscall trace does show some more mmap/munmap activity on stable: munmap: 0.132s/ 408 calls = 0.000s per call. Max=0.000s Min=0.0000s mmap: 0.010s/ 434 calls = 0.000s per call. Max=0.000s Min=0.0000s vs nightly: mmap: 0.001s/ 44 calls = 0.000s per call. Max=0.000s Min=0.0000s munmap: 0.000s/ 8 calls = 0.000s per call. Max=0.000s Min=0.0000s I don't see much difference in the assembly dump to account for this - maybe a difference in stdlib behaviour?
Not exactly a variable, but just another value passed to stack. Why can't closures just be pointers to anonymous functions which also take in extra parameters (the captured environment) behind the scenes? Thanks a lot for your help!
Not an issue, I can tell you right now! It is in fact not smart, since these other resources won't be dependency-tracked by cargo :)
eddyb discussed running proc-macros in miri someday a-la const fn so that they wouldn't have to be in separate crates, but obviously that's a very different system than the one we have now, and something of a stretch goal ;)
People excited about “avoiding the garbage collector” don’t know what they’re talking about. JS Garbage collection is a rounding error in the list of things chewing up ram and cpu cycles in a browser. 
theory is easy to test by looking at the assembler. (if you have osx!) 
In terms of the proportion of resources consumed you are correct. But if the GC takes longer than 16ms (it often takes upwards of 50-100ms) then frames are dropped which causes animations to visibly stutter. It's a very noticeable degradation of user experience.
You can still use [compile_error!](https://doc.rust-lang.org/std/macro.compile_error.html) macro. For instance: ```rust let span = variant.ast().ident.span(); let err = quote_spanned! { span =&gt; compile_error!("variant does not have an `outcome` nor `no_outcome` attribute"); }; return err.into(); ``` Or you can use [`syn::parse::Error`](https://docs.rs/syn/0.15.23/syn/struct.Error.html): ```rust use syn::parse::Error; if pattern_idx.is_some() { return Error::new(arg.ident.span(), "two patterns are not allowed!") .to_compile_error() .into(); } ```
https://i.imgflip.com/2pkhb6.jpg
I will rape your anus until it turns from innie to outtie
WTF
Sorry, just thinking out loud. Just that there are many ways allocations can be a bottleneck, and I think `collect_into_vec` or similar is relevant even if it sits somewhere in the middle ground between convenience and control, because it would help you build those kinds of split APIs you described with very little difference between the two kinds, or make it easy to wrap the reuse-style with a more convenient one.
&gt; To have the safety and comfort of a modern language like C# without needing a garbage collector. I'm not sure about this response. It was originally built to be used in a platform (Firefox) that already had a garbage collector (JS). Whatever gains were made by managing its own memory are being offset by the hundreds of thousands of lines of javascript code it's running when loading web pages. So ultimately all that work to manage its own memory, at least with respect to improving performance, is pointless. &gt; In theory, Rust actually has the potential to be faster than C and C++ because the language semantics make guarantees that would enable more powerful optimizations A false claim made many times over with respect to other languages. You can't get faster than bare metal, so sorry but no. &gt; Also, it's comparable to C and C++, speed-wise, which no other significant language can claim. Multiple languages claim this. I can easily find benchmarks elsewhere on the internet that counter the results in benchmarks you refer to. &gt; To get Python into Java and C#, they had to write whole new Python implementations, Jython and IronPython, which run inside the JVM and the CLR, respectively. There's nothing prevent someone from using JNI to call CPython. It just doesn't make sense because writing a parser for the language is simple enough. &gt; Have you seen the list of companies using Rust in production? For a language that's 8 years old this is actually pretty weak list. 5 years after C++ was introduced it was being used virtually everywhere. Same goes with Java, C#, etc. Contrast that with Rust, where the vast majority of developers still haven't even heard of the language, much less see it as an reasonable option to replace C/C++. And this [survey](http://www.computerworld.in/news/rust-language-too-hard-learn-and-use-says-user-survey) certainly isn't helping.
&gt; Rust's design allows for things like Rayon - you can basically take any collection that gives you an iterator and run functional operations like map/reduce/fold/whatever on it in parallel *yawn* Other languages have been doing this for over 10 years. 
Make it `&amp;[i32]` instead. Arrays must be passed by reference because the function needs to know how much stack space it has to allocate for the arguments. `[i32]` requires a size to be given, so you can't use that to denote something like an "unknown length".
You’re not configuring yourself as the system dns server. When would you use it then if not for your entire system?
You can look in the docs for the library you were trying to fix, find the function, struct, trait, whichever... and click on [src] to the right.
He asked for faith, so i let him choose. 
Wrong sub. This subreddit is for the programming language.
Well, the DOM itself is garbage collected, so this will never be preventable. The host bindings proposal will only allow you to call DOM methods directly from wasm, without going through a Javascript bridge.
I'm curious. Can you post an example? 
 here's [one](https://docs.oracle.com/javase/tutorial/collections/streams/parallelism.html), and I'm pretty sure it wasn't the first.
FYI, this is known as a [slice](https://doc.rust-lang.org/book/ch04-03-slices.html#other-slices). 
Well, Java has the JVM, so it's not really in the same class of languages. I'm not saying the feature itself is unique, what is unique is doing it without a runtime and with static safety guarantees. 
Rust Community is Awesome!
&gt; Perhaps an LLVM upgrade [This Week in Rust 263](https://this-week-in-rust.org/blog/2018/12/04/this-week-in-rust-263/) mentions an LLVM upgrade. The current stable would have been branched a while before then. So there definitely has been an LLVM upgrade, and I would also guess it's responsible in some way.
Not clear what you mean by "same class of languages." There are compilers to machine code for Java -- not hugely popular, but they exist. Same with Rust, there are virtual machines you can run it in (LLVM, WebAssembly). &gt; doing it without a runtime and with static safety guarantees Can't see how this matters. If they're both doing it within the same performance window, who cares? 
it does't work for me. ubuntu 18 06:37:23 \[INFO\] Dose Response version: 1.0.0 06:37:23 \[INFO\] By: Tomas Sedovic &lt;[tomas@sedovic.cz](mailto:tomas@sedovic.cz)\&gt; 06:37:23 \[INFO\] [https://tryjumping.com/dose-response-roguelike/](https://tryjumping.com/dose-response-roguelike/) 06:37:23 \[INFO\] Target triple: x86\_64-unknown-linux-gnu 06:37:23 \[INFO\] Using the sdl backend dbus\[13506\]: arguments to dbus\_type\_is\_basic() were incorrect, assertion "dbus\_type\_is\_valid (typecode) || typecode == DBUS\_TYPE\_INVALID" failed in file ../../../dbus/dbus-signature.c line 323. This is normally a bug in some application using the D-Bus library. &amp;#x200B; D-Bus not built with -rdynamic so unable to print a backtrace Aborted (core dumped) &amp;#x200B;
LLVM isn't a real virtual machine, and I really doubt that compilers for Java actually give you static safety guarantees. Also, while the JVM is really fast, it still adds overhead, and it's only really fast on a few targets. You can't run a JVM on embedded devices, at least not the fast OpenJDK variant of it. 
Sounds awesome. I am total J's noob. How can I use it to build my react web app without ejecting, .. So what can I write in my package file to do this.
Also, there’s no such thing as int in rust; you’d just use ‘x: i32’.
I've been looking the ØMQ direction and found https://github.com/thehydroimpulse/nanomsg.rs which says its "the successor to ZeroMQ". It also has Python+Java bindings, but since its newer I'm not sure how stable/tested it is as they already work on a rewrite https://github.com/nanomsg/nng ?
There's no documentation on how to configure and use it so not really sure how to test it out
Cargo src is a useful tool for this. It lets you browse your code in a manner similar to rustdoc.
The web browser is not, and will never be, an optimal platform for games when better alternatives exist (Android or iOS apps). There are too many unknowns involved in the browser. Multiple tabs, tons of javascript, image loading, unreliable network connections, internal frames, cpu intensive ads, security. And if anything, with AMP Google is trying to move the browser to improving load time and having thinner pages, not fatter. I could be wrong but I don't see wasm ever really taking off. While FF is going all-in, it doesn't seem particularly high on the chrome priority list. The initial load time is too slow, and as with all compiled languages that import external libraries for this or that function, the binary footprint will begin to get huge over time. Also most professional games have tons of images. Switching to a web page and having to download 50 megs each time on mobile is probably not what game developers have in mind as the ideal deployment solution.
Hm. I'd test it, but one of its dependencies, `rustdoc-highlight`, seems currently broken. Thanks for the recommendation! I'll keep it in mind for next time.
&gt; You can't run a JVM on embedded devices, at least not the fast OpenJDK variant of it. That's strange, because the original motivation in creating Java was for running on small devices. See J2ME and the kvm. It's teeny tiny. &gt; I really doubt that compilers for Java actually give you static safety guarantees Huh? Why wouldn't they?
So, how many small devices is Java _fast_ on? They don't even have a JIT on MIPS, and the one they have on ARM is way, way slower than x86. Also, a modern build of OpenJDK is like 150MB, far from "teeny tiny" J2ME.
FYI: you can use different toolchains without changing defaults: `cargo +nightly build`, `cargo +stable build`, etc. Also toolchain names can be partial.
This looks nice, but I'm wondering why you didn't use the `trust-dns` client.
Not sure why you keep referring to OpenJDK. It's not even the premier optimized version of Java. Agree that it's not going to be super-fast on the embedded device, but who says all embedded devices have to be super-fast? They were using it on feature phones and running games on it 20+ years ago.
oh my apologies
Awesome! Gonna try it soon! In the mean time I've opened a small PR that fixes confusion when using npm
Don't forget error handling! Rust's most underrated feature, IMO.
We do that quite a bit in F# with its (admittedly more limited) type providers. Things like generating ORM types from a dbml file (database schema XML generated by SQL Server) or even directly retrieving the schema from the database, although that has the obvious downside the you need to have the db available during compilation. Or this one I implemented recently, [generating code from HTML templates](https://github.com/fsbolero/Bolero/wiki/Templating).
I've added index.d.ts file and types field. If you are using vscode or typescript, it should be now autocompleted.
Does just that require an RFC ? Looks like it wouldn’t be too hard implementation wise to allow “generic” proc macros constrained on T: Into&lt;TokenStream&gt;
Maybe you need this https://github.com/WAVM/WAVM
&gt; I'm not sure about this response. It was originally built to be used in a platform (Firefox) that already had a garbage collector (JS). Whatever gains were made by managing its own memory are being offset by the hundreds of thousands of lines of GC'ing javascript code it's running when loading web pages. Rust was not built for use in Firefox. That came later and it went through quite a lot of flux before version 1.0 and the decision to use it in Firefox came about later. (eg. One of several reasons a lot of people compared it to Go is that it used to be more Go-like, with a comparable green threading runtime.) Rust in its modern form has been designed to be ABI-compatible with C to the point where you can incrementally migrate a codebase function-by-function. librsvg is doing exactly that and it has no GC. Also, you can't write a JavaScript runtime in JavaScript, but you can write one in Rust. &gt; A false claim made many times over with respect to other languages. You can't get faster than bare metal, so sorry but no. C and C++ aren't bare metal. That's obvious by the fact that passing `-O` to a compiler can speed them up. Think about dynamically typed languages for a moment. The most visible reason they're slower than bare metal languages is that the compiler doesn't know how it's going to have to treat any given variable until runtime. By comparison, statically typed languages give the compiler more information to work with when doing optimizations. The same "more information" difference exists between C or C++ and Rust. Rust provides a stricter guarantee that mutable aliasing will not occur, which leaves open more opportunities for the optimizer to restructure the program without altering its semantics. Take a look at https://compileroptimizations.com/ for example of what compilers (C or otherwise) do to your code under the hood when you ask them to perform optimizations. These are why, while nothing can be faster than hand-coded assembly language in theory, C and C++ are usually faster in practice. (Assembly does what you tell it to, even if that means wasting a lot of time. An optimizing compiler is smart enough to recognize things like "This can be computed at compile time" or "If I reorder these, I'll get the same result, but it'll play more nicely with the CPU's cache and branch predictor.) &gt; Multiple languages claim this. I can easily find benchmarks elsewhere on the internet that counter the results in benchmarks you refer to. Exactly why I chose The Benchmarks Game. It's easy to get benchmarking wrong and The Benchmarks Game does the best job of trying to provide an objective assessment of what someone skilled in using each language well would achieve. (Refusing test programs which rely on nightly-only Rust features is an artifact of the policies behind that.) &gt; There's nothing prevent someone from using JNI to call CPython. It just doesn't make sense because writing a parser for the language is simple enough. No, but it's a major pain to write a program of significant size which juggles data back and forth between two different GCs. That's why you don't hear of people doing it in practice. &gt; For a language that's 8 years old this is actually pretty weak list. 5 years after C++ was introduced it was being used virtually everywhere. Same goes with Java, C#, etc. Contrast that with Rust, where the vast majority of developers still haven't even heard of the language, much less see it as an reasonable option to replace C/C++. And this survey certainly isn't helping. Calling Rust "8 years old" is deceptive. The language was going through radical change until the 1.0 release, which only happened 3 years ago. Compare apples to apples. C++ was conceived in 1979 and it took until 1985 for an initial release to be made. Already, that's a year longer than Rust for a comparable phase of its lifecycle. Much of what we recognize as being standard bits of C++ didn't come about until the second release in 1989 and Object Pascal was still hugely popular during that period. Also: 1. You're not controlling for the cultural and systemic differences between 1993. "The vast majority of developers" had a very different meaning 22 years ago, before most people had even heard of something called The Internet and the world of developers skewed significantly more anglocentric. 2. C++ quite literally began as "C with Classes" and, when it came out, it was like porting a codebase from JavaScript to TypeScript. (Switch your existing codebase to a new compiler, then start adding the new constructs incrementally as you feel like it. The first C++ compiler was actually just a preprocessor which spat out C code.) 3. Classes are basically just language sugar for structs containing vtables. They don't require any drastic new knowledge of how to work with the language like compiler-enforced ownership checking does. 4. C++'s popularity was helped significantly when Microsoft threw their weight behind it as the more modern alternative to C for programming Windows applications. This is widely believed to have been what spelled the end for the popularity Object Pascal enjoyed during the DOS era.
Every time someone tells "it's backed by Google" https://gcemetery.co
I think the problem is more that there is no high-livel setup documentation. (See [the one from Babel](https://babeljs.io/docs/en/usage).). &amp;#x200B; I think it's also unclear if it is a drop-in replacement for Babel (and if it is, you could just point to their docs for now).
Yes. Rust does not have aliasing so I expected rustc to have such an optimization. Last time I tested, it does not happen and the generated code keep the match within the loop.
hmm I was expecting more of a getting started, given you say it's v1 (alpha) and say it's alternative to babel I was expecting it to be a bit more complete, ie. an actual alternative to babel. e.g. convert babelrc to swcrc and have it transpile my project. I see there's an empty webpack-loader repo, but if you really want it to be an alternative to babel I would want it to also work standalone, ie. have a cli, config etc. &amp;#x200B; Anyhow I installed it with yarn and tried an example: \`\`\` const swc = require('swc') const code = \`const hello = () =&gt; 'hello world' console.log(hello())\` let app2015 = swc.transform(code) console.log(app2015.code) \`\`\` and it throws \`\`\` internal/modules/cjs/loader.js:752 return process.dlopen(module, path.toNamespacedPath(filename)); \^ &amp;#x200B; Error: [libstd-488b57210df96656.so](https://libstd-488b57210df96656.so): cannot open shared object file: No such file or directory at Object.Module.\_extensions..node (internal/modules/cjs/loader.js:752:18) at Module.load (internal/modules/cjs/loader.js:620:32) at tryModuleLoad (internal/modules/cjs/loader.js:560:12) at Function.Module.\_load (internal/modules/cjs/loader.js:552:3) at Module.require (internal/modules/cjs/loader.js:659:17) at require (internal/modules/cjs/helpers.js:22:18) at Object.&lt;anonymous&gt; (/home/brendan/development/scratch/swctest/node\_modules/swc/lib/index.js:1:80) at Module.\_compile (internal/modules/cjs/loader.js:723:30) at Object.Module.\_extensions..js (internal/modules/cjs/loader.js:734:10) at Module.load (internal/modules/cjs/loader.js:620:32) \`\`\`
is the bin section supposed to be inside or outside of my main method? &amp;#x200B;
I am often checking out new linux based tooling, and as soon as I see `npm` listed as an action in the install section of a README, I close the window tab and move on.
I'm currently working to make prebuilt binary just like node-sass. After it, I will write documentations. &amp;#x200B; (Currently installing swc takes toooooo long I think)
&gt; I'm not sure about this response. It was originally built to be used in a platform (Firefox) that already had a garbage collector (JS). Whatever gains were made by managing its own memory are being offset by the hundreds of thousands of lines of GC'ing javascript code it's running when loading web pages. How is that relevant to the claim? &gt; A false claim made many times over with respect to other languages. You can't get faster than bare metal, so sorry but no. That would be a good argument if C were "bare metal". It isn't.
I got it. I will write docs after making installation fast and easy.
Rust make you better programmer, IMO.
Awesome! Thank you. By the way, any reason why you specify &lt;= dependency versions? Just something I noticed when looking closer at the crate.
/r/playlust
I beat the game! Of course, I had to change a couple lines of code, to get rid of depression and double the probability of anxiety :) Gotta say, it's pretty cool that this is open source. I already had sdl2 installed, apparently, so all I had to do was clone the repo and `cargo run`.
&gt;Switching to a web page and having to download 50 megs each time on mobile is probably not what game developers have in mind as the ideal deployment solution. That's what the whole PWA (Progressive Web Apps) stuff is about, right.
This isn't just a Rust problem. It's a problem across all languages. Reading other people's code is hard. Reading disorganized code makes your job even more challenging. Here are a few techniques I use: Using VSCode, if you set up a Rust project according to whatever the gods demand for the RLS and editor to recognize the project correctly, you can jump to definitions. As you find pieces of a puzzle, right click on your editor window and split the view. Keep all relevant parts of code on the screen at once if you need to. Take notes. Document control flow. Submit the notes as a PR when finished. I realize this won't solve your problem but these techniques may help. 
Thanks! I'll consider it. And of course, being good or bad at games is relative. In the current iteration, I've only beaten it once and I know how it all works. Usually because I'm careless and make a stupid decision rather than because of anything the game throws at me. FWIW, meeting another NPC is kind of a mid-game thing. But I don't object to tweaking the difficulty here, this ain't Dark Souls. I'll try it out and if it still works, I'll add the option.
They essentially are? That generated struct is definitely being stored on the stack, and has no overhead compared to stack variables besides being passed around with the closure. The "self" parameters closures implicitly take in is also just another parameter. The memory overhead is entirely in memory on the stack, and a struct containing some variables will take the exact same amount of space on the stack as the variables it contains would need to - even with alignment, stack variables need to be assigned properly too.
&lt;3
Ah sweet! That's the spirit of things! If you build the game yourself, you can also enter a cheat mode which will let you win more easily. How to do that is left as an exercise for the reader :D. You can also do a pure Rust build with winit and glium if SDL's causing trouble: https://github.com/tryjumping/dose-response/blob/master/DEVELOPER-NOTES.md#pure-rust
Wasm is single threaded as well so will not really help to solve this problem. It might make processing of the data faster. But you also need to serialise things to and from wasm, which will slow things down and it is a trade off between the performance gained by wasm Vs the performance lost through getting things into and out of wasm. They are looking to add wasm threads to the spec and [chrome 70](https://developers.google.com/web/updates/2018/10/wasm-threads) does now support it but I would not yet rely on this feature. Depending on how you design your application you might be able to benefit from wasm, but it would also be quite a bit of effort to do as you will likely need to pull a larger amount of the application into wasm before you see a benefit. [Web workers](https://mobile.htmlgoodies.com/html5/tutorials/introducing-html-5-web-workers-bringing-multi-threading-to-javascript.html) might be one possible way to do what you want. They allow you to process things in a separate thread to the main UI thread but they do also come with some limitations. You could also change the message format to a binary protocol and not use JSON, like with [msgpack](https://msgpack.org/index.html) which will lower the amount of work you need to do to deserialise the data.
I [blogged](https://llogiq.github.io/2018/04/03/corners.html) about this (and other things) earlier this year: Rust is an experiment to see how productive we can become without cutting corners.
Ouch, I'm sorry about that! Thanks for the report. If you don't mind, could you try passing `--glium` to the game? That will use a different backend -- I want to see if it's something to do with the SDL we ship in the game or something else. Also, could you please type these two commands to the terminal and send me their output so I know a bit more about the system? $ uname -a $ lsb_release -a I'm on a 64bit Fedora 28 myself. I'll try to install Ubuntu and reproduce this -- knowing the exact version would help a lot.
&gt; rust is basically all the awesomeness of C++ smashed together with all the awesomeness and dependency management of JS And, very importantly, an ML style (well, without global type inference, but algebraic) type system is also smashed in there. With full language level support for pattern matching (hello [std::variant](https://en.cppreference.com/w/cpp/utility/variant))
Thanks for your feedback. I did not know that there was already a crate for the doh.
I do not believe you can override that with the non-ejected create-react-app (CRA). Babel is pretty core to CRA, and it is even used to compile typescript in the typescript variant of CRA which this compiler doesn't suport. Also, this compiler does not seem to support JSX syntax, which is not part of any ECMA javascript spec, and it would be very uncomfortable using React without JSX support.
agreed, often when people say "The garbage collector is my problem" they mean "the garbage is my problem". The garbage collector is only taking so a lot of time when your garbage is hard to clean.
I'm currently learning Rust and I agree. It's the most pleasurable language I've ever laid my hands on. In fact, I'm enjoying it so much that I'm seriously considering to become a Rust developer once I have my completed my PhD.
It sounds like you're dealing with some quite large JSON objects. I'd recommend splitting them into several smaller objects (although you could still send them in one request) and parsing each object individually. You can use a `setTimeout` between each parse to ensure the browser can do other stuff during your parsing.
I am wondering about the interaction of *arbitrary output* and *incremental compilation*. That is, from an incremental compilation point of view it seems that the output of a proc-macro can never be cached, as there's no guarantee that the output is not random, or time-dependent, seeing as arbitrary code is executed. I imagine incremental compilation can still cache the result of converting the *output* of the proc-macro into actual binary code, but is there anything planned to bypass the proc-macro itself when compiling incrementally?
I'm typing on a phone and I was to laconic asking (hence the downvotes, I assume). `trust-dns` seems to be lacking some examples, and a resolver like this sounds like the perfect example for it. You don't have, by any means, to switch to it -- maybe I should try implementing one. I've been thinking about setting up a DoH or DoT server and client.
I love type providers! One of the many great ideas F# had that make it really unique. It's a shame that MS really doesn't care that much about F#, it's such a neat language with a lot of potential :/ It's funny, I loved the "functional, but practical" approach of F# so much that I started to look for other languages that took it...and I mean, I wouldn't really call Rust a *functional* language (well, not a solely functional one), but it still kind of scratches the same itch. I think Rust could still learn some stuff from F#. For example, Active Patterns are awesome! In Rust, one would probably "just" use macros for this stuff, but still, I miss the baked-in support this feature has in F#.
Cargo takes a lot of inspiration from npm - and they share some of the same problems, i.e. security vulnerabilities. I think we can work together to come up with solutions to these issues (audit, web of trust, ...)
The same issue for me on Archlinux. With the \`--glium\` flag the game works OK. Linux pavluv 4.19.8-arch1-1-ARCH #1 SMP PREEMPT Sat Dec 8 13:49:11 UTC 2018 x86\_64 GNU/Linux
My own [flamer](https://github.com/llogiq/flamer) crate is also a very simple proc_macro. The [overflower](https://github.com/llogiq/overflower) crate is a bit more involved. I'm not finished with mutagen yet.
This isn't exactly what you asked, but this is a good video on how the JS event loop (and threading) works. Might help. &amp;#x200B; [https://www.youtube.com/watch?v=cCOL7MC4Pl0](https://www.youtube.com/watch?v=cCOL7MC4Pl0)
Nice. [Submitted to FreeBSD Ports](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=234269) :) Some things I noticed: - SDL `bundled` feature is enabled by default (had to patch that out — OS packages should use shared libraries as much as possible) - the `glium` backend adds a lot of duplicate old dependencies… `glium` doesn't seem to be maintained really, maybe drop it? - no keyboard settings (custom keys)? (I type on [Colemak](https://colemak.com/) and don't have a numpad…) - no HiDPI support (SDL+Wayland) — looks like the font is statically rendered into a texture at build time and only at 1x resolution
You can run any Rust code on the `HOST`, so you can spawn threads, read / write files, do Network I/O, and pretty much anything you want. 
&gt; My only compliant would be how it's all combined. Yes, the docs of `syn` and `quote` are very minimal and of the form "this tool does this", but there aren't any docs about how you combine the proc macro stack to do useful things.
Yup.
Some people have suggested allowing proc macros to be `const fn`, but `const fn` is not powerful enough for those proc macros to be useful yet. 
&gt; Is it expected that proc macros will work without being in a subcrate soon? This is not expected any time soon.
Oh awesome, thanks! I'm impressed how small the patch actually is. Does it really work? It's been a while since I've used FreeBSD... I know the bundled feature's bad for packagers, but it makes development easier -- especially for Rustaceans who might not necessarily know much about lower-level stuff and want to just `git clone &amp;&amp; cargo build`. I should probably only enable it for the `dev` feature tho. If I may make one suggestion on the port, instead of the default `dev` feature, build it with the `prod` one (equivalent of `cargo build --release --no-default features --features prod`). That's what my releases do and it's the intended way to distribute the game. The default features generate massive files full of replay logs and and the world state at every frame. And you're right that glium is not very well maintained now. I would like to drop it, but it's the only way to build a pure Rust backend right now. Some people are having trouble with SDL, but glium works for them. I do have plans to drop glium in favour of glutin + "raw" opengl, but that has not happened yet. Yea I want to do custom keys, but in the meantime arrows + ctrl/shift modifiers for diagonal movement should work. And right again about the font. Variable font size is very high on my list (but I can't promise when I'll get to it).
I think people are kind of missing the point of your question, which may be this: From a high-level language perspective, why use Rust instead of Go? Two reasons: 1. On average, Rust will produce faster code with smaller memory usage. 2. Once you have around 1000 or more lines of code, Rust is much more readable/maintainable than Go. 
r/playrust
Seconded. It's the first time a language makes me file that error handling is done the way it should be (and while I'm not a programmer, I've dabbled a lot in other PLs and always found error handling to be a very weak spot). One thing that turned out super useful for me in practice is the related fact that `Option` is uses appropriately. The fact that `Iterator::next` does not return the items, but `Option&lt;Item&gt;`s, and thereby forces me to handle the `None` case has saved me from so many logic errors it's not funny (actually, it is, YMMV).
&gt; Rust was not built for use in Firefox. To [quote](http://lambda-the-ultimate.org/node/4009) one of the language's authors, circa 2010: "We aren't going to ship Rust code in Firefox N for any N I can yet foresee, but we are not just hacking on Rust for its own sake." He further states, referring to servo: "But Mozilla cannot bank on anything of the kind at this juncture. We are taking enough risk building a parallel browser engine that won't speed up without more cores than currently ship on common machines." The implication is clear that the language was created to be used within Firefox, as they are doing now. &gt; Compare apples to apples. C++ was conceived in 1979 and it took until 1985 for an initial release to be made. Already, that's a year longer than Rust for a comparable phase of its lifecycle, and C++ didn't change as radically as Rust did. Let's not play with the dates. Going by the Wikipedia's "First appeared," C++ was 1985, Rust was 2010. Similar to your C++ 1979 argument, I could easily argue based on the above referenced thread that Rust was conceived much earlier, say 2007/2008, given how far along the language had progressed by 2010. But 2010 is when they presented for it to be used in the same way that 1985 was when they presented C++. &gt; You're not controlling for the cultural and systemic differences between 1993. "The vast majority of developers" had a very different meaning 22 years ago, before most people had even heard of something called The Internet and the world of developers skewed significantly more anglocentric. But information traveled much slower overall than it does today. I could apply a logarithmic treatment to the speed of the language's growth, which would only mean that Rust is even further behind than it should be. &gt; C++ quite literally began as "C with Classes" and, when it came out, it was like porting a codebase from JavaScript to TypeScript. (Switch your existing codebase to a new compiler, then start adding the new constructs incrementally as you feel like it. The first C++ compiler was actually just a preprocessor which spat out C code.) Java shared no codebase with C yet its growth was even faster than C++. If a language has value and makes users more productive then developers will eagerly make the switch, because the ease of writing and maintaining the new code will quickly outweigh the maintenance costs of supporting the old. They will learn the language at home and bring those new and fresh ideas to work, and do their best to convince their companies to switch. No such grand movement is taking place with respect to Rust. &gt; C++'s popularity was helped significantly when Microsoft threw their weight behind it as the more modern alternative to C for programming Windows applications. This is widely believed to have been what spelled the end for the popularity Object Pascal enjoyed during the DOS era. Saying that "C++ overran Pascal because Microsoft chose it" is a fairly weak excuse. C/C++ was a clearly superior to Pascal. Microsoft had their own version of Pascal too, but it was abandoned because Pascal was simply not as productive and malleable a language as C/C++. Same applies to Rust. Even though the language has been around for some time it still only has Mozilla as the major backer. [Samsung](http://www.eweek.com/development/mozilla-samsung-building-rust-programming-language-servo-browser) was supposed to be a part of it but now they're nowhere to be found. Some of the large internet players are dabbling in small ways, but none seem willing to commit to it anywhere near the way that Mozilla has. You can't blame circumstance on why the language isn't picking up when the [evidence](http://www.computerworld.in/news/rust-language-too-hard-learn-and-use-says-user-survey) is clearly staring at you right in the face.
Woah. This is cool. Are gbl files specific to Scilabs chips or is it a standard thing?
It won’t let me post in there it says post failed always 
GC is the reason for why Java is notoriously laggy and heavy on CPU. You're saying JavaScript is magically different?
Linux anon-P5Q-VM 4.15.0-42-generic #45-Ubuntu SMP Thu Nov 15 19:32:57 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 18.04.1 LTS Release: 18.04 Codename: bionic 
--glium started Thank you
&gt; &gt; I'm not sure about this response. It was originally built to be used in a platform (Firefox) that already had a garbage collector (JS). Whatever gains were made by managing its own memory are being offset by the hundreds of thousands of lines of GC'ing javascript code it's running when loading web pages. &gt; &gt; How is that relevant to the claim? He said Rust's goal was safe memory management with no garbage collection. I stated that Rust was written with the intention of rewriting Firefox in it. But Firefox does tons of garbage collection because it runs hundreds of thousands of lines of javascript in a single session which generates lots of garbage that needs collecting. My point being that to create a new language (Rust) for the sole purpose of avoiding garbage collection, to run in an environment where lots of garbage collection is unavoidable seems completely fruitless, because no matter how well the language is designed or coded in, it will never be able to avoid whatever disadvantages that come with being a part of a system heavily dependent upon garbage collection. 
Well, this subreddit is about the Rust programming language. What do you expect to happen if you post Rust game content here?
This is the future of rust. Making things faster. People want fast stuff and now that mores law is dead then we need efficient languages to speed up mission critical apps.
This is technically accurate (for now), but fixing your JavaScript (or moving it off the UI thread) is always going to be a better investment than chucking it out and re-writing your app in Rust. Tangential to that is the constant improvements by the VM and browser teams to scheduling the collections in ways that are less disruptive to animations. 
This sub is for the programming language rust.
Thanks! I don't think any other vendor uses them, unfortunately. Would make this crate a lot more useful.
And if you are too lazy to lookup crate versions, you can `cargo install cargo-edit` - then `cargo add bignum` will automatically lookup and add the latest version of `bignum` to `Cargo.toml`.
Baby steps, but we need to press forward in search of better more reliable systems. 
Java hasn’t been “laggy and slow” for 15 years at least. The only thing holding the JVM back these days is Oracle. 
Though, the default behaviour of "*" seems to have a rationale behind. Are there situations in which it is a good idea to use it?
And once you have installed `cargo-edit` you can also run `cargo upgrade` to automatically update all dependencies in your `Cargo.toml` to their latest versions!
Note as well that if you have a dependency with a version of `"*"`, you won't be allowed to upload to crates.io (which conversely also means that anything you get from crates.io won't exhibit this behavior).
&gt; To quote one of the language's authors, circa 2010: "We aren't going to ship Rust code in Firefox N for any N I can yet foresee, but we are not just hacking on Rust for its own sake." He further states, referring to servo: "But Mozilla cannot bank on anything of the kind at this juncture. We are taking enough risk building a parallel browser engine that won't speed up without more cores than currently ship on common machines." Rust was certainly heavily influenced by its utility for Firefox, but that's different from it being built specifically for it. If you look at the History section of the Rust Wikipedia page, you'll notice that it was started in 2006 and Mozilla didn't get into things until 2009. &gt; Let's not play with the dates. Going by the Wikipedia's "First appeared," C++ was 1985, Rust was 2010. Similar to your C++ 1979 argument, I could easily argue based on the above referenced thread that Rust was conceived much earlier, say 2007/2008, given how far along the language had progressed by 2010. But 2010 is when they presented for it to be used in the same way that 1985 was when they presented C++. Funny that you should turn to Wikipedia. That's the source I was using. If you scroll down to the History section, you'll see these lines: &gt; In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on "C with Classes", the predecessor to C++. &gt; In 1983, "C with Classes" was renamed to "C++" I've followed along with Rust's development ever since posts about it started popping up on Planet Mozilla and, as far as I've been able to gather, Rust changed at *least* as much between 2010 and 2015 as "C with Classes" changed between 1979 and 1985. The only difference is that Rust didn't change its name along the way. What was released in 2010 was so fundamentally in flux that it's not even fair to call it a "public alpha". It was not "when they presented for it to be used in the same way that 1985 was when they presented C++." Instead, it was the modern equivalent of Stroustrup feeling confident enough in his ideas for C with Classes to share it with his academic peers and solicit feedback. &gt; But information traveled much slower overall than it does today. I could apply a logarithmic treatment to the speed of the language's growth, which would only mean that Rust is even further behind than it should be. But, conversely, there was much more concentration, both of vendors and of buyers. It didn't have to travel as quickly as long as you got it to the right people in the right conferences and they threw their hat in for it. Beyond that, we have a *much* bigger ecosystem of existing resources for other languages dragging down the rate of change and our standards for things like support documentation have risen. (If StackOverflow and the like had been around back when C++ was getting started, it'd have taken longer too. Because everyone was used to just buying a good all-in-one book or set of manuals in the early 90s, there was a clear and obvious path to intentionally establish the requisite minimum documentation threshold. With stuff like SO, you can't anticipate every phrasing of every query someone's going to try. It just has to grow organically.) &gt; Java shared no codebase with C yet its growth was even faster than C++. Again, there are extenuating circumstances. While Java was in a similar position to Rust in being the first viable alternative to C++ in a target niche (performant development of end-user-visible applications with stronger safety guarantees that isn't tied to expensive licenses from a single vendor, like Delphi), there's one big difference: The amount of money and effort which Sun Microsystems was pouring into marketing Java (especially to the managers who held the purse strings) was insane. You really had to have been there. &gt; No such grand movement is taking place with respect to Rust. I remain unconvinced. For example, Rust has been listed as the most loved language on StackOverflow's developer survey for three years running (2016, 2017, and 2018). You'll need more data to back up that argument. &gt; Saying that "C++ overran Pascal because Microsoft chose it" is a fairly weak argument. C/C++ was clearly superior to Pascal. Microsoft had their own version of Pascal too, but it was abandoned because Pascal was simply not as productive and malleable a language as C/C++. That's not *quite* what I said. I said that C++ received a huge boost from being chosen by Microsoft, and that boost is considered to have been the killing blow for a competing language which had been very popular in the DOS era. I was just giving an example of one of the ways in which it being chosen had significant effects. &gt; Samsung was supposed to be a part of it but now they're nowhere to be found. Samsung is a poor example to point to. 1. They have a history of puttering around with oddball stuff, so it's not indicative of whether Rust is a good language or not. 2. They've been doing quite a bit of restructuring lately and Rust isn't the only thing that was hit by it. &gt; Some of the large internet players are dabbling in small ways, but none seem willing to commit to it anywhere near the way that Mozilla has. That depends on how you define "large" and "willing to commit to it". Does Dropbox count? &gt; You can't blame circumstance on why the language isn't picking up when the evidence is clear. First, I'd say that the language *is* picking up... it's just being held back by the fact that, outside of mobile development (which is much more tied to specific Vendor-provided languages), web development is the biggest market right now and Rust's async/await is still pending. Likewise, I'd say that survey *is* helping, because it's informing decisions on where to allocate effort to improve language ergonomics.
Does it implement the advanced optimizations of closure compiler? I use closure compiler for minimization and it's quite slow so a speed up would be very welcome.
https://www.reddit.com/r/rust/comments/9t95fd/howto_setting_up_webassembly_on_stable_rust/ I wrote that tutorial not long ago (for rust 1.30). It goes over all the steps you need to follow to get a wasm module working in a browser using only the normal rust tools. * No node. * No crates. * stable rust. I'd be glad to answer any questions you might have.
Won't that stop at the first error instead of reporting as many as it can tho?
when you are trying to reduce binary size it is beneficial to avoid having different versions of the same crate in your dependency graph
The bin section is configuration for `Cargo.toml`, not your code.
&gt; What else do you propose to get a static lifetime? Just `SmallType` &gt; `expect` should already have a good message. Remember Clean Code, if you write a comment then you failed to express your intention via the code, except for docstrings, which is completely different thing. Good point. What I meant was mainly explaining why that one should never execute. Having it in `expect` may not make sense sometimes. Regarding types, one should use `.into()` if it's not implemented, then obviously, an overflow check is needed or a comment explaining why it never overflows. Stringly typed interfaces mean that you accept strings even if something else would be more appropriate. e.g. `fn foo(number: &amp;str)` `number` obviously shouldn't be a string. `Display`/`Debug` shouldn't fail because o bad `self`, only in case of IO error in the writer. Of course, the whole thread is about "red flags" not "100% wrong code" Red flag just means that I'd review the code. If your type has to be called `PositivePrimeNumberSmallerThan10000`, that might be a good reason to skip that. Further, there's another reason than failing early: if two functions should process the value and they have same requirements, then you save one validity checking. That reason may or may not apply to a particular piece of code. The most obvious alternative to indexing is iterators. Whenever I see C-like for loop with indexing, I always try to turn it into iterator. If that's not possible, that's OK. In the example you provided, you could use `SmallVec` if it wasn't reverse. I wonder if there's a crate for reverse buffer already. Also, `SmallVec` and friends are currently limited by the lack of const generics. &gt; Again too restrictive. I'm pretty sure I can find lots of standard functions that take two slices, two i32, two strings etc... The amount of time in my life I lost to swapped arguments justifies using newtypes or a struct to pass the arguments. The argument naming system is something I like about Swift a lot. However, sometimes the order is obvious enough. I was talking more about things like `fn handle_event(event_id: u32, window_id: u32)` - such thing is present in wxWidgets and It's hard to remember the order.
Yes, in some cases, the benefit is too small. I called it a red flag because I'd stop and think whether that is the case.
I'm surprised that this code doesn't compile ([Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=41435990a5931cad0413918da06a0065)): let mut heap = BinaryHeap::from(vec![1, 3, 2]); let mut peek_mut = heap.peek_mut().unwrap(); *peek_mut = 4; println!("{:?}", heap); The error message states that I can't borrow `heap` as immutable because it's also borrowed as mutable, and it compiles if I wrap the second and third line in a block. Wasn't the whole purpose of NLL to allow code like this?
Yes and no. It's not specific to dscfg, but anything that uses framed codec from tokio. In my previous work, we actually used the same protocol in a non-Rust product.
Yeah -- that way, it's easier to guarantee that the crate will be fully compatible with dependencies. If someone makes a breaking change to an API that renders my crate unusable, it will look like my fault from and end-user perspective. &amp;#x200B; Granted, this does mean it takes a bit of extra effort to keep the crate up to date, which is why I think it's not as common of a practice. But I think `enum_dispatch` has few enough dependencies that it's not a big problem for me to maintain.
intellij-rust autocompletes the latest version for you, it's great. 
Given the restriction on uploading this syntax to crates.io, I only use this syntax when I'm making some toy example to test something, and it saves me the two seconds it would take to look up the latest-ish version. (I have a little shell alias that I use constantly, which creates a new Cargo project in a temp dir and then opens main.rs in my editor. It's almost as convenient as Python for trying out little one liners. I couldn't live without it at this point.)
Wouldn't you put the common field in the outer struct instead of the inner enum? Or does the field occur in nearly all variants except some? In that case can't you simply match on the variants that don't have the field and use `_` to match on the variants that do?
Probably stupid q but what is Babel?
I don’t suppose there is a way to tell wasm-bindgen to use messagepack instead of serde_json for all its implicit serialisation?
See? You made a mistake already. :) It can be safe if you write `T` back - you dropped it in your code, which would cause double free. But you also have to handle panic and that is what people often forget. [Correct version](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3333630690b904a12b8aeaa5d31a1e79) if `f` panics, `mem::forget(panic_abort)` won't run causing `panic_abort` to drop, causing abort, because that's the only sensible thing to do.
That would just cause `drop` to run on invalid value. See my other response.
Wrong sub dude. r/playrust 
The purpose is usually to implement state machine without intermediate state - you can't rewrite the original while you have borrowed inside of an enum. So the solution is to move out `T`, transform it and write back. The problem is, most people trying to do it forget to handle panics.
Oh, I forgot to publish `dscfg`! :D
What command are you using to run this? 
This really ought to be built in to cargo. I mean, it's how almost everyone else does it...
Install \`cargo-edit\`. &amp;#x200B; Then you can do \`cargo add bignum\` and it will look up the latest version automatically.
Are you using web-sys? It has a [fetch](https://rustwasm.github.io/wasm-bindgen/examples/fetch.html) API.
Afaik it also means it won't update the crate, it'll prefer the version you've already downloaded and built.
We had this at some point, I believe (“Wrong subreddit”). But I can no longer see that reason, at least not in Alien Blue. I generally use “Spam” nowadays.
Yes! This too! 
The reason NLL isn't helping here, is that `peek_mut` is returning a [`PeekMut`](https://doc.rust-lang.org/std/collections/binary_heap/struct.PeekMut.html) struct rather than a reference, and that struct has a destructor. NLL isn't allowed to change when destructors run, so it can't shorten the lifetime of that borrow. That said, you can fix this by explicitly calling `drop(peek_mut)` before the last line, and I don't think that worked before NLL. I think the reason for this `PeekMut` struct is that the heap needs to check whether you used the `&amp;mut T` it gave you to swap in a non-max element. If you did, then the heap will need to move that element down, to restore the heap invariant. That work gets done in the destructor of `PeekMut`.
The 2018 edition actually explains why this is still a borrow error: &gt; mutable borrow might be used here, when `peek_mut` is dropped and runs the `Drop` code for type `std::collections::binary_heap::PeekMut` Drops are still performed at the end of lexical scope, which is important here because the updated value you wrote is only sifted into the heap at that point; even if it doesn't have to move the code to check that still needs to be run. If you add an explicit early drop the code compiles successfully: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9a8fdd7cfe18a8424fdaabc314acfb80
You might want to run `cargo new` in a separate folder on the command line, and take a look at the default project it creates. There will be a `Cargo.toml` and a `main.rs` file. It might be that you've gotten some of the contents mixed in between those two things in your first project, and seeing a correct example will help you spot that.
Can you point out where SIMD gets used in the project?
I meant to use msgpack on the js side without using wasm which will introduce a bunch of overhead and defeat the point. As far as I know wasm-bindgen does not implicitly use JSON as a transport format for base types. The only types that wasm uses are i32 i64 f32 and f64 and thus everything that is sent between wasm and js must use one of these. With wasm-bindgen numbers are easy to convert and just stored in their closest matching type and strings/arrays will be done as an array of integer numbers likely with the length added to the start. For more complex types I believe you have to [opt into the serde-serialize feature](https://rustwasm.github.io/wasm-bindgen/reference/arbitrary-data-with-serde.html) and explicitly use the JsValue type. I do not think there is any way around this atm and doing this will be very heavy and not worth it for a lot of situations. To make the most of wasms performance you should keep the state and processing on the wasm side and only send small messages back to js rather than whole objects. You cannot simply replace some function with wasm to expect to see a performance boost, you must design the interface to minimize communication between js and wasm enough that its overhead is smaller than the performance gain you get from doing processing in wasm. 
Never had any issues with Cargo building a project so far, NPM though...
Good call!
You're right, I should have read that error message more carefully :) Since I'm coming from Swift, I never really expect the error messages to be useful... Rust's error message are awesome, thanks!
Of course it works! &gt; The default features generate massive files full of replay logs and and the world state at every frame oh. I thought `replay` would allow replays to be recorded with a CLI flag or something. And what about `stats` and `verifications`?
thanks, I tried using XMLHttpRequest. But that seemed really awkward to use
The reason why this isn’t included in default distribution it’s because no one came up with the solution for adding entries to the `Cargo.toml` that will preserve comments and formatting. 
Ah. I think I'm starting to understand but I'm still confused. Rust prevents you from mutating a location from within a `match` statement which reads that location. This prevents an unclear coding practice which is closely related to ones which (in C) even confuse the compiler. (Iterator invalidation.) The straightforward solution would be to write code that doesn't step on itself. Something like: let next_state: State; let r: WhateverType; match state { // where state: &amp;mut State ... } *state = next_state; When written like that the logical write to the location `*state` happens at a clear point in the control flow. Anything above that point sees the old state, anything below sees the new state. The part I don't see is why it's particularly appealing to factor the `next_state` variable into a separate function. 
&gt;NLL isn't allowed to change when destructors run I wasn't aware of this, but it makes a lot of sense! &gt;That work gets done in the destructor of `PeekMut`. Oh man, this is exactly what I was going to ask next – in fact, when trying to figure out how that works, I stumbled upon the compiler error in my question. I fully expected the sifting to happen from within `deref`, but this is so much better. Thanks for pointing that out to me!
That's ok. Just be sure to read the subreddit info when you post. We've been having a lot of people posting about the game and hopefully the mods of this sub can do something to differentiate it a little better. &amp;#x200B; Feel free to stick around if you want to look into programming with the Rust language.
Yeah, I also miss that option now that they removed it. I usually report stuff as "off-topic", but I preferred how it was before.
To expand on this, in the future this problem would be resolved once a feature called `const generics` gets added to the language.
It certainly warrants more discussion than it's already had (which is almost none!) and I suspect it'd require an RFC yeah because there's certainly possible alternative solutions as well.
There's a [tracking issue](https://github.com/rust-lang/rust/issues/54140) as it's still unstable (no activity to stabilize yet), but you can [create arbitrary errors with `syn::Error`](https://docs.rs/syn/0.15.23/syn/struct.Error.html) on stable today which uses `compile_error!` behind the scenes.
As other posts have mentioned, the DOM is garbage-collected. I don't think you would want it any other way even if you were theoretically redesigning the Web from scratch. There are UI systems that aren't GC'd (reference counting being a form of GC) like Win32 and Xt, but they're all a big pain to work with. We've all basically collectively decided that retained mode UI needs garbage collection of some kind…
Okay, I think I understand the problem with my original code. Question though, if T was capable of inheriting `Copy`, would that also imply that my original code is safe (or, at least as safe as yours)? My current understanding is that as long as something is just on the stack (ie, not referencing anything on the heap), then drop doesn't actually do anything. And if T can inherit `Copy`, then it must not reference the heap. Going in a different direction, what if F decided it didn't like our T, stopped using it and created it's own T which it then returned? Would that not cause the same problem? (The thing I'm getting at is that I don't think it is possible to implement this 100% safely without knowing what F does, or more about T) Regardless, it seems like the correct way to do this would be simply `where F: FnOnce(&amp;mut T) -&gt; R`, or at least `where T: Copy`/`where T: Clone` rather than trying to use unsafe calls to implement a function signature that doesn't make sense in terms of ownership. Lastly, &gt; if f panics, mem::forget(panic_abort) won't run causing panic_abort to drop, causing abort, because that's the only sensible thing to do. Would this not cause memory leaks if there are other things that need to be dropped still on the stack (ie, say a `Box` that was created right before the call to `with`)? Or does a `panic!` in a `panic!` only affect the current stack frame?
The serde-serialise feature is what I meant, yeah. I guess messagepack-lite and a few wrapper functions would work fine. 
It doesn’t break code at the start of the function, it just tries to do something that’s impossible based on what the code is at the beginning... so it doesn’t compile.
&gt;Web workers might be one possible way to do what you want. They allow you to process things in a separate thread to the main UI thread but they do also come with some limitations. Webworkers can parse json. But there's no way to pass this json as js object to Webgl. We have to serialize, deserialize to pass it to UI thread which defeats the purpose of using Web Workers. 
[https://github.com/WebAssembly/host-bindings/issues/18#issuecomment-430605795](https://github.com/WebAssembly/host-bindings/issues/18#issuecomment-430605795) As per this comment wasm-bindgen supports complex js object sharing.
Wow! Thanks for letting me know this! That is news to me.
It changes the types based on whether you're creating an Any or not. This could very well be used to silently call a different trait impl in a way that won't be caught in a standard code review.
I always though active patterns were neat, but they were really hard to understand. 
I would be much more convinced if it actually compiled. Right now, it doesn't *do* anything.
I think this is why Scott Meyers calls destructors the [single most important feature of C++](https://youtu.be/ltCgzYcpFUI?t=952). You can do so much with them, and they compose well even when they're doing interesting things. Another good example of the "protect your data structure's invariants" use case is [`std::vec::Drain`](https://doc.rust-lang.org/std/vec/struct.Drain.html).
So what is the best way to make sure you get the latest version?
Does this mean that, if you were to specify rand 0.6.1, that bignum would still work and also download rand 0.5.0? Does rust support multiple conflicting versions of packages?
idk how to exploit it myself. but this is very surprising to me. how is it not surprising to you? with the Any commented, the code compiles and works! wanna know why it works? because it makes sense. idk why the Any breaks it, but it really shouldn't.
&gt; What was released in 2010 was still so much in flux that it's not even fair to call it a "public alpha". It was not "when they presented for it to be used in the same way that 1985 was when they presented C++." It appears at one point Rust even had garbage collection. So fine, the clock starts at 2015. Let's see where it is in five years time. &gt; Rust has been listed as the most loved language on StackOverflow's developer survey for three years running (2016, 2017, and 2018). This has more to do with the fact that so few Rust developers are using it at work. If the only developers using it are those who have chosen to use it for their own pet projects, then clearly that class of developers is going to have the greatest love for the language. &gt; it's just being held back by the fact that Aside from the "obscure syntax," as a person on the outside looking in, I think it's more held back by the "solution looking for a problem" conundrum. It's low-level, but for most applications C/C++ is stable and good enough to the extent that they won't feel the need to migrate. Also C/C++ developers tend to be a bit older, more set in their ways, have complex codebases to support, and have little desire (and energy) to throw another language's ecosystem into the mix. While it's high-level enough to run in the browser, converting a Javascript developer (usually not the brightest of the bunch) into a Rust developer is no small task. The language is significantly more complicated than Javascript, and outside of the novelty of being a new language and technology, it's not even clear to a Javascript developer why they should even code in it. The existing Javascript toolset is massive, relatively easy to read and copy/paste, and fits in well with concept of scripts and documents. On the other hand debugging wasm is not nearly as straightforward as debugging JS, the Rust/wasm development/deployment cycle isn't conducive to rapid development, it doesn't integrate well with existing frameworks (React, Angular, etc), and wasm is honestly kind of ugly to look at. These are all points I'm sure you've heard before, but it'll be interesting where Rust goes. Unfortunately Firefox doesn't have the browser market share it had 8 years ago when they started investing in Rust (from 30% to 5%), so things are going to be significantly more challenging for them.
Yes, exactly, but you must explicitly note the version, otherwise cargo will take the easy way out.
Is there a way to show the current redundancies? And the whole tree?
As other people have written here, run: `cargo install cargo-edit` Then run: `cargo upgrade` That will upgrade all of the crates in cargo.toml to the latest version. 
This is the same in npm. There, you can use the 'latest' tag if you want to be lazy 
Ah ty, so no version number trickery in the toml?
This. Exactly this. WASM is really meant to give _consistency_ to CPU-bound operations, which may or may not be an actual speedup. If the problem is you're sending too large of batches, the solution is to send smaller batches. How you do that is up to you, but I'd recommend sending them more frequently.
Why would the value be invalid? It's identical, but moved in memory - which happens all the time in Rust.
That's nine As. Crazy¡
No, it's a programming language :P &gt; r/rust &gt; For everything related to the Rust programming language—an open-source systems language that emphasizes safety, performance, and concurrency.
Would be nice, but to be honest any reporting reason does so don't overthink it and just pick any. "Spam" seems to be the most popular.
Fair enough :)
Although in most cases it won't make sense to a) only accept input whose size is known at compile time, b) pass (possibly large) arrays by value, or c) cause instantiation of several separate functions all doing the same thing except with different array sizes.
In the toml you specify the minimum acceptable version. But you can't just say "latest". For all "tricks" available, [see here](https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html).
It is close to 100kb every minute. Do you think its too much?
Hmm, I always thought `cargo upgrade` only upgraded to the latest *compatible* version, e.g., if your `Cargo.toml` said `crate = "1"`, it would update to the newest 1.x version, but would not upgrade to 2.x. Is this still the case?
I think what you describe is `cargo update` (**update** not **upgrade**), which is a Cargo builtin command IIRC?
That has been fixed in one of the newer `cargo-edit` versions. Since that has been resolved, I think it is planned to merge it into cargo (the cargo repo has an issue for it, but I'm on mobile right now).
I like how rust is being used to replace more and more bits of JavaScript and python where they need speeding up. This is an example of a ground up rewrite, but I'm wondering whether it would be possible to rewrite a big JavaScript project from the inside out in rust (e.g. write Babel plugins in rust using their js AST as the interface format, and then switch to using a rust native format for the interface one enough modules have been written in rust). This would allow us to keep the existing test fixtures and configuration for projects, which would help adoption. If I attempt such a project, would you be interested/would it be useful for me to use your code for inspiration?
Ahhh, yes, that's what I was thinking of. Thanks!
There's another tool for that (`cargo install cargo-tree`): `cargo tree --duplicate`
You may want to consider using a static parser library like nom or combine to make a bitstream parser.
https://babeljs.io/: A transpiler for JavaScript. You write JavaScript using all the latest features. Babel will then compile it to JavaScript that's compatible with all browser versions you are targeting.
The API looks pretty intuitive and the documentation is awesome. Nice job. 
I mean if the `JSON.parse` call takes one second, yes. The trick to solving JavaScript hanging the browser is to split the long operation into several smaller operations, where each operation start the next with a `setTimeout(next_operation);` call. The reason is that once the current operation finishes, the browser will run the rendering thread before calling stuff queued by `setTimeout`. The problem is that you can't split a single `JSON.parse` call, so you'll have to make the json chunks smaller, and parse each chunk separately, so the browser can render during parsing. Although be sure to check that the slow part actually is the `JSON.parse` call, and not, for example, some loop that does stuff with the resulting object.
It's actually the way that the rustc codebase is structured. Rust has a bootstrapped compiler, meaning that an older version of rustc compiles the current version of rustc, and so it has its own specialized build script (x.py) which knows how to deal with that. RLS is designed for normal Rust projects so it can't handle rustc. 
what r/Hobofan94 said :) that's exactly the gap `cargo upgrade` fills!
Oh, that makes sense. Thanks!
Note that the problem isn't necessarily the *amount* of data, but how much data you're trying to process at once. For example let's say the data currently looks like this: [{"price": 4},{"price": 5},{"price": 10},{"price": 8}] Imagine this list contains 100kb of data. Now if this takes too long to parse, something you could do instead would be to simply send the following four separate pieces of data: {"price": 4} {"price": 5} {"price": 10} {"price": 8} Parsing each piece independently would be quick, and you could write a loop that parses 500 of them and then calls itself with `setTimeout`, and continues once the timeout finishes. I'm not quite sure of the details with WebSockets, but if you are receiving them with ajax, you shouldn't make an ajax call for every small piece as that would have a large performance impact, but maybe you should send a string that looks something like this: 12|{"price": 4}12|{"price": 5}13|{"price": 10}12|{"price": 8} Here the loop could do something like: let i = 0; while (string is not empty) { let index_of_bar = string.indexOf('|'); let len = parseInt(string.substring(0, index_of_bar)); let json = string.substring(index_of_bar+1, index_of_bar+1+len); string = string.substring(index_of_bar+1+len); handleObject(JSON.parse(json)); if (i++ == 1000) { setTimeout(function() { handleString(string); }); return; } } Even if you receive large amounts of data, handling them like this wouldn't hang the browser.
No problem! If that sentence is confusing it should probably be clarified somewhere in the guide (maybe even a footnote). Opening an issue to ask someone to fix it or creating a pull request to fix it yourself could be a great way to contribute :)
Done. Edited the parent post.
Note that annotating `bar` like let bar: &amp;MyFn&lt;std::borrow::Cow&lt;'static, str&gt;&gt; = foo; also causes the same compilation error. `let () = bar;` yields expected type `dyn for&lt;'a&gt; std::ops::Fn(&amp;'a mut std::borrow::Cow&lt;'_, str&gt;)` Note the lack of `'static`. I'm not sure the `'static` in the type definitions are doing anything. (This, if anything, may be the bug; maybe a rustc expert could look into that?) But forcing `bar: 'static`, by casting to `Any` or (say) by having it match a generic type `&lt;T: 'static&gt;` [like this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=74ee0d8de0157f20c98bb755e8c9c20c) gives the same compilation error. This makes sense. `bar`'s type is not abstracted over the lifetime argument to `Cow`, so if you imply `bar: 'static` then its type will end up using `Cow&lt;'static, str&gt;`. Why does this not happen for `foo`? Because, despite elision, `foo`'s type *does* abstract over over the lifetime argument to `Cow`. `let () = foo;` yields expected type `dyn for&lt;'a, 'b&gt; std::ops::Fn(&amp;'a mut std::borrow::Cow&lt;'b, str&gt;)` Abstracting `bar` over the lifetime argument would require HKT, because `MyFn`'s `T` would have to be of kind `lifetime -&gt; *`.
I think that could work, [given its advantages over ZeroMQ](https://nanomsg.org/documentation-zeromq.html). Seemingly nng is the successor, and there are [rust bindings](https://gitlab.com/neachdainn/nng-rs).
Done. Edited the parent post.
OK, I'll open a PR.
Without allocating: ``` for obj in vec.iter_mut() { *obj = *obj as Box&lt;dyn Trait&gt;; } ```
Does this mean you can't use a function in `rand` 0.6.1 that is not in `rand` 0.5.0?
Some time ago I wrote a webassembly module "from scratch", without using the bindgen library. I wrote up [a draft explaining the process](https://github.com/starwed/Crafty/wiki/POC-for-wasm-map-implementation). I keep meaning to polish it up and post it somewhere more visible, and get some feedback on whether there are better/alternate approaches. But maybe it's helpful?
Seconded, the point of the reporting reasons is to make it easy for us moderators to instantly determine why something was reported, but with /r/playrust posts that's already effortless for us. In this particular case, don't feel the need to overthink it. :)
I have no skin in the game, but I'd love a [no_std](https://rust-embedded.github.io/book/intro/no-std.html) version of these datastructures. Or an integration example showing that it works in a WASM environment.
Cargo will load all of the functions from `rand` 0.6.1 only if you specify that you will be using `rand` 0.6.1 Otherwise you will be limited by the version used by any other crate. So in this example, you will be limited to the functions in `rand` 0.5.0 unless you change cargo.toml.
It has. Most people simply don't notice because their machines are beasts compared to what they had years ago.
I feel like we should use a CVE in Rust (the game) to inject Ferris into the game where he would provide comfort and shelter to the naked and scared masses.
Is there a [upgrade-interactive](https://yarnpkg.com/lang/en/docs/cli/upgrade-interactive/) that has modes for both latest versions and latest compatible versions where you can select what you want to upgrade?
It'll support multiple versions that are incompatible. This happened a lot with me if I had dependencies that depended on different versions of `hyper`. This is how you get errors like "expected type StatusCode, found type StatusCode instead"
What part of this is relevant to rust?
Dude what even get off this sub with your gamer crap 😂😂
Asking as a major C++ novice... what does dependency management actually look like?
A solid error handling method, although this might demand changes to the compiler instead.
Yes, it’s very much desired, but there’s one more chunk of work that needs to be done.
I'm interested in writing cross platform gui based apps. I've looked at a few libraries for interfaces and I've noticed a trend for web-based front ends with rust. How performant is this compared to a library that uses something native like iui supports or QT and GTK?
The comments here should be compiled into documentation and stuck in the release somewhere. This is really good information.
I'm trying to break up my code for the first time and I'm having trouble. I've created a file `config.rs` in `src/` and I took my previously working `struct Config` and `impl Config` and put it in there with `mod config` around it. When I leave my external crates in `main.rs` I get errors about not being able to find `Serialize` and `Deserialize` macros but when I move `#[macro_use] extern crate serde_derive;` into `config.rs` I get `an 'extern crate' loading macros must be at the crate root`. What do I need to do? I tried adding a bang (`!`) between `#` and `[macro_use]` but that didn't do anything.
Here you go: https://github.com/veniamin-ilmer/crate-race/tree/master/benches/bigint_arithmetic
Here you go: https://github.com/veniamin-ilmer/crate-race/tree/master/benches/bigint_arithmetic
I always submit a PR if I can get things to a clean state ;-)
Does caro edit install multiple binaries ? cargo-add, cargo-upgrade, etc.?
You don't need `mod config { ... }` in `config.rs`. Your files should look something like this: * src/config.rs pub struct Config { } impl Config {} * src/main.rs mod config; use config::Config; fn main() { // now you can use Config here just as if it was defined in this file } The way rust's module system works is that you have to explicitly specify the module structure in the source, but you are allowed to move module's *contents* out into separate files. So suppose that you had your initial code in `src/main.rs`: struct Config { } fn main() { // do stuff with Config } Now you want to move `Config` into its own module. So you wrap it in `mod config { ... }`, make the appropriate structs, fields, and functions public, and import stuff we use in main (but everything is still inside `main.rs` - we don't need to create any additional source files yet): mod config { pub struct Config { } } fn main() { // do stuff with Config } And now we want to move the module into its own file, so we move the *contents* of the module into a file named `config.rs` (or `config/mod.rs` - either works, but the name must match module's name), and *keep the module declaration* in `main.rs`: * src/config.rs pub struct Config { } * src/main.rs mod config; use config::Config; fn main() { // do stuff with Config } Now when the compiler sees `mod config` in your main file, and the module doesn't actually have a body, then it tries to find one of those two files, and uses their contents as the body of the module, so it behaves exactly the same as if you wrote all the modules in a single file, like in the previous example.
Thirded. Seeing it reported at all is usually enough for us to remove it.
&gt; Cargo takes a lot of inspiration from npm Does it? It looks more like it’s primary inspiration was rubygems and bundler, to me. Which isn’t surprising considering Yehuda Katz and Carl Lerche, the authors of Bundler, created Cargo. 
It's a fork of [`create-react-app`](https://github.com/facebook/create-react-app) with [some added Rust WASM functionality](https://github.com/facebook/create-react-app/compare/master...thomashorrobin:rust-next) Perhaps not the greatest fan of saying "I made this" for &lt;200 LOC added to a massive existing project, but it could be useful for some people nonetheless so I won't judge.
&gt;I'm currently working on a fairly simple javascript project using WebGL, and while generally fast enough, the garbage collector is proving to be a real nuisance. It seems that even a fairly lightweight render loop is enough to xinvoke the GC and drop occasional frames. I help maintain a js game engine, and the issues you describe are real, but avoidable with enough work. You can use browser debugging tools to help you see where the allocations are coming from. Reusing objects and arrays is often necessary to remove them, which can make the code a bit gnarly. (i.e. if you want a function to return an array, you'll often need to pass in an array for it to use.) One unfortunate thing I've seen is that a lot of more modern ES6 features will silently allocate iterators behind the scenes, so you have to avoid them. The big boon in something like rust+wasm will be that allocation free code becomes easy and idiomatic! (Not rust related, but if it's helpful for your project, you could look at [Crafty's webgl render loop](https://github.com/craftyjs/Crafty/blob/develop/src/graphics/webgl-layer.js#L389), which does avoid allocations. All the code is MIT licensed if you have the urge to borrow.) 
Nice idea. I'll see how well this works out. I mean, in theory this is very much possible, but I depend on some other crates for some bit fiddling. fixedbitset for example doesn't work without std, but I'm sure that can be changed or a replacement could be found. 
Because you don't know what `f` will do with the value. It can simply drop it and reate a new one. Consider this scenario (`T` is `String`): `with(&amp;mut s, |string| { println!("{}", string); "A new string".to_string() })` In this case new string is forgotten and old is dropped twice.
A mix of dumpster fire and train wreck.
It would be cool if it also parse rust and transpile it to js.
it adds the `add`, `upgrade` and `rm` subcommands for cargo, see here: https://github.com/killercup/cargo-edit
Thanks! That worked. I thought that I had tried that configuration originally but I must have missed something somewhere. Now I get the following error: ``` error[E0658]: imports can only refer to extern crate names passed with `--extern` on stable channel (see issue #53130) --&gt; src/main.rs:28:5 | 27 | mod config; | ----------- not an extern crate passed with `--extern` 28 | use config::Config; | ^^^^^^ | = help: add #![feature(uniform_paths)] to the crate attributes to enable note: this import refers to the module defined here --&gt; src/main.rs:27:1 | 27 | mod config; | ^^^^^^^^^^^ ``` Following the instructions and adding `#![feature(uniform_paths)]` at the top of main.rs does get it to compile but I don't really understand the problem here. Could you explain?
ok, I see.
First, `Copy` isn't a guarantee that there isn't a reference to heap data. For instance: // 42 is on heap after this line let foo = Box::new(42); // We create a shared reference to the heap and all shared references are Copy let bar = &amp;*foo; If `T` is `Copy`, you have to write `with&lt;T: Copy, ...&gt;(...)` in order to keep the API safe. But at that point `with()` is pointless function because you can copy from `&amp;mut T` where `T: Copy` already, without using `unsafe`. So yes, it `T: Copy`, your function is safe, but also pointless. &gt; Going in a different direction, what if F decided it didn't like our T, stopped using it and created it's own T which it then returned? Heh, looks like I wasn't entirely clear. What you describe is the exact reason why `with()` function is actually useful. It's pointless otherwise, since you could just take `&amp;mut T`. My code actually handles this case. It makes sure to write the valid value back and makes sure panic stops the program instead of exposing invalid state. &gt; Regardless, it seems like the correct way to do this would be simply where `F: FnOnce(&amp;mut T) -&gt; R` At which point you don't need `with()` in the first place. But you are correct that usually, using reference is the correct way. Sometimes you cant though. Example: enum Fsm { A(String), B(uszie), } fn step(x: &amp;mut Fsm, c: char) { match *x { A(ref mut s) =&gt; { s.push_char(c); if c == 'M' { // Borrowck rejects this because x is already borrowed *x = Fsm::B(s.len()); } }, B(val) =&gt; println!("{}", val), } &gt; Would this not cause memory leaks `forget` doesn't mean "cause a memory leak" but "don't run destructor for value passed to it". It prevents `PanicAbort` from running destructor. That destructor doesn't do any memory cleanup (no heap, it's even zero-sized!), so no memory leak will happen. All other destructors run normally.
Alternatively, use [`requestIdleCallback`](https://developer.mozilla.org/en-US/docs/Web/API/Window/requestIdleCallback). It's designed for this use case.
Any work that needs to be done to get that stabilized? I'd like to use it for a crate that would become stable soon enough, like 6 months to stable is fine by me, longer would be a problem. 
Yes, creating a new state and then replacing it is good approach if it's not expensive. For example, if the state contains heap-allocated data, you'd have to allocate and copy new data and destroy the old every time. If you use `with`, you don't have to. There's also another safe workaround using `Option&lt;State&gt;` or an empty variant of `State` enum and `std::mem::replace()` function. This also prevents abort in case of panics. On the other hand, it's a bit less performant, since you have to check possibility of empty state every time you mutate the state, even if it can only happen when panic occurs.
Just keep a Vec or Map of the different Senders as appropriate (a Vec if you send the same command to all of them, a Map if you're targeting one by some key)
In 2018 edition (which is the default since `1.31.0`), import paths must either start with `crate`, `self`, `super`, or a name of an external crate. So in this case a proper way to import config would be `use crate::config::Config`. In crate root `self` and `crate` are equivalent. I corrected the examples in my previous comment).
Java benchmarks well against any language in terms of speed. Your statement is not true. 
C'mon man we're more civil then that at least. He was just suggesting we put Rust's mascot into the game to calm the RTG players.
Spam also forwards to the admins, if I'm not mistaken. It may be better to use a subreddit-specific reason than a site-wide one.
Isn’t this bad practice anyways? Unpinned versions like this are what caused the whole npm event-stream security issue
Pin to major or expect your project to decay into a non-compiling cesspool in no time at all. Oh, and pin all the way to the patch for pre-1.0 dependencies or risk the same thing.
To add: racer works just fine with rustc https://i.imgur.com/1phG3go.png Although it won't work if you use it with `#![feature(rustc_private)]`.
The median speed maybe...
Omg thanks! I’m using that for sure. 
&gt;Copy isn't a guarantee that there isn't a reference to heap data Hmm.. for some reason I thought a reference could not derive `Copy`, so I was wrong about that. &gt;so no memory leak will happen [I think I disagree with this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2166481c6dcd541b075a16396087ed27). I'm not taking about leaking the T that was passed in, but some other object. See my example above that I think shows a potential memory leak. Notice `Dropping 8` never happens. If you comment out the `panic!` in F, then 8 gets dropped correctly. If you instead comment out the `panic!` in `AbortPanic`, then 5 gets dropped twice (as you've shown it would). If `y` (in `main`) were a `Box`, I assert that its inner contents would not be freed. In the example you posted (`match *x { ... }`), would a better solution be to use an `Option&lt;Fsm&gt;` to store the new x, then after the match block, `if new_x.is_some() { *x = new_x.unwrap(); }` (granted, this violates one of the other red flags you mentioned, and one that someone else mentioned, but it seems reasonable to me in this case).
So chungus is an enemy ai, it will attack anybody if seen, but chungus hates roof campers especially, chungus can scale walls and not only kill roof campers, but steals their loot off their dead bodies. Chungus only steels roofcampers loot.
It kind of seemss like we *should* be able to say "latest".
I would say the advice given in the comments to your older linked post is quite accurate. The syllabus listed there should be doable in Rust, but using Rust will incur significant friction compared to a language such as Julia which is purpose built for numerical analysis. I also think that trying to get to grips with numerical methods such as those mentioned in your previous post is already a challenging task in itself, so adding more complexity is not desirable.
It's only changing the lifetimes in that type, so it would only be able to call a different trait impl if we had lifetime specialization.
I'm not too familiar with numerical analysis, but from what you describe, it should be fine in Rust. I wouldn't say you'll get that familiar with it unless you're really trying to get the ultimate performance. The reason I say that is you'll primarily be taking things as values or const references and returning new data -- pure functions, essentially. This will be relatively easy in any language. If you were to try to get performance from the GPU (for example for matrix algebra), you'll have to dig pretty deep into not just Rust but kernel stuff as well, I believe. A nice middle ground might be trying to use something like `rayon` for parallelization. I would agree with the part about the things that make Rust great being easier to appreciate in a systems programming setting, though I think this would apply to any long-running program more appropriately rather than necessarily systems programming. The things that make Rust amazing start to really shine in multi-threaded environments and asynchronous tasks. You'll encounter neither of those things writing pure functions, but I wouldn't say Rust is *bad* for that. It's actually really nice, especially when compared to C++.
I'll be a bit contrarian and advise you to use Rust for this, or, if you want to learn a new language that's not widely used, either Nim or Chapel would work. Of those three, Rust appears to be the most popular, and seems to me to have crossed that threshold into "languages which you may one day be able to get a job using". Chapel targets high performance scientific computing, so that may put it a bit above Nim for this role, but look for yourself and see which you like best. &amp;#x200B; From reading your other post, you want to learn by implementing, but you also want a language that's good for high performance computing. Julia is out because in your opinion much of the implementation is done for the basic numerics, otherwise Julia would be my first suggestion. &amp;#x200B;
It does install three different binaries 
Minor note: It means any version that's not a prerelease versions. Those are never matched. (Like for example "0.1.0-alpha.1"
You can't (as I understand it) actually upload a crate to crates.io that doesn't have pinned versions of its dependencies. So this is only for your own project. It essentially just allows you to have faster compile times and smaller binaries because you wouldn't be compiling multiple versions of the same crate.
The ~~median~~ speed
But then the crate breaks when any dependency goes through a breaking change. `*` is a nice hack for testing things, but even then, if something lasts long enough for anyone to read it in the future having a real version number in `Cargo.toml` will be greatly helpful. I've had to go through dependencies version by version until I found the one that compiles when trying to work on examples people have made in the past with `*` dependencies. It's not pleasant.
Thanks. I'm by no means an expert in assembly, but I can't see any difference in the output that should cause such a difference?
In my opinion this is a reasonable thing to do. However it will have its advantages and disadvantages. The biggest one is that Rust numerical community is still young. If you chewies python then the representation of a matrix you want to use is numpy and anything you try and do with it someone has succeeded at befor. If you chews Rust, the community is still developing too much to really know what is idiomatic. On the plus side you will be helping to figure that out, and may be "the best implementation in Rust" just because there are not others. (That is how I ended up making [quadrature](https://crates.io/crates/quadrature)). On the other, we have some libraries like [num](https://crates.io/crates/num) and [ndarray](https://crates.io/crates/ndarray) but it is still an open question how far they can take us. From your other thread you are considering using c. Rust will make you learn and struggle with the same sorts of thinking of "methods the way a computer does" as c will, but where c will teach you by crashing (or generally doing undefined things) Rust will teach you by refusing to compile.
Are there some advantages of ddh over [duperemove](https://github.com/markfasheh/duperemove), which I assume has "Yes" for both points?
Yes, the GC takes away absolutely nothing from the speed. It's completely free /s It's not like Java is notorious for its sawtooth memory usage, and the accompanied CPU spikes right before the usage drops.
Interesting! Thanks, that helps 😄
Why do you think this? In my experience, Rustup is virtually identical to nvm and Cargo is virtually identical to npm. If anything, npm is nicer because npmjs.com has scoped and private packages, and npm has a better story for specifying packages from multiple locations. It's a genuine question, but so far I haven't seen any reason Rustup and Cargo are superior to npm. The tooling for JS is a little harder but that's only because JS existed before JS modules did and the language has been evolving for 20 years, whereas Rust is much newer.
Modern Java does not have those characteristics. Check it out yourself. Way smaller GC pauses
Probably not if you're using btrfs. ddh is designed as a file system agnostic tool. ddh might be a bit faster than dupermove given that it's doing less work. Dupermove looks super cool though.
&gt; but is there any reasons that it would not have made it to stable yet? Worth noting that every optimization added in nightly, unless it depends on the user doing something specific with unstable features or is reverted because of side affects, will land in stable in 6-12 weeks. Beta is cut from nightly every 6 weeks, then it becomes stable 6 weeks later when the next beta is cut.
Doesn't the cargo.lock file give you working dependency versions?
My advice: do the homework in whatever language your professor knows best for help, and get the concepts down first. You need to develop muscle memory for some of these algorithms before trying to dive into Rust. Going back after the class to reimplement things will get you a unique appreciation for how Rust differs from other languages. 
One last thing. Would you say the explicit purpose of this `with` is to, effectively, trick the compiler into allowing you to modify a `&amp;mut T` in-place, in code where it would normally fail to compile?
There's always work needed to get something stabilized! I'm not sure what the timeline would look like.
No. What you specify is not the lifetime of `x` and `y`, but relations between lifetimes. You make a statement about them, the compiler checks your function if your statement is correct, and then the compiler uses that statement to check if the usage of your function at the call site is correct. E.g. when writing a function `f(x: &amp;'a i32, y: &amp;'b i32) -&gt; &amp;'a i32` your statement is `the returned reference lives at least as long as the reference given in the first argument`. You can see how you don't really have to know the lifetime of `x` when defining this function, but it's relation to the output. Note that often you don't have to write down lifetime, since lifetime elision kicks in and the compiler figures it out for you.
&gt; Note that often you don't have to write down lifetime, since lifetime elision kicks in and the compiler figures it out for you. I've been learning Rust for the last 3 weeks and, thus far, I needed to declare lifetimes exactly twice. So... the compiler really does most of the legwork in simple programs.
Will it allow "&gt;0" though? That's effectively the same semantic as "*". Even "0" will be a problem for the given rand example.
My application loop only takes ~2ms on my laptop and the core is only 10-20% utilized. Moving it to a worker is pointless and would only add overhead. The problem is when the GC does a major collection it will sometimes block the UI thread.
Wouldn't that function say "the returned reference cannot outlive the reference given in the first argument? The function would create the return value from `x` so `x` must be dropped after the return value
Only if it's checked into version control- I guess for application crates where that's the case, a "latest version" specifier might be useful.
What OSes is this that it "just works" for?
&gt; dependency management dependency management?? the dependency management for `cargo` may be better than C but it's not good at all.
Thank you, this was the information I was looking for. I was kind of hoping that the browser had it's own internal DOM representation which was freed upon update or removal of elements, but I suppose that would require a lot of copying to and from js-land which most of the time would just be unnecessary overhead. It is kind of a bummer though, since it would allow for a truly GC-free path which would definitely benefit certain applications.
Thanks for the advice. Unfortunately, my project is a little different than the typical WebGL use case. It's actually a mix of traditional DOM with WebGL rendered canvas elements mixed in. The user still navigates between views like a traditional webapp so it's not as easy to control allocations as it is with a conventional game loop. I had been considering some of the Rust app frameworks like Yew or draco but, as someone else here has pointed out, the DOM itself is garbage collected so I think suffering the occasional GC pause is going to be a necessary evil for projects like this. I appreciate the link though, I'm sure I could learn a few tricks from that. I'll be going through it later in more detail. Thanks!
This isn't always fair, since as developers we are often forced into certain practices by the APIs and runtime available to us. For my project, there is simply no other way of achieving the basic functionality without a modicum of heap allocation. The problem isn't that my code wasteful per se, it's that the requirements of my project are an awkward fit for web technologies as they currently exist.
I didn't used explicit SIMD. I just compiled with -C target-cpu=native, and transcompiler (swc\_ecma\_transforms) got faster.
Not yet, but it will. Currently just peephole optimization is implemented.
Can you post a link
&gt; This could very well be used to silently call a different trait impl No, because impl selection is independent of lifetimes.
How to get the code highlighter working?
I do a fair amount of dependency wrangling at work, I’m curious what you think cargo either lacks or got wrong.
no globally cached dependencies, linking against two different versions of the same library...
Every dependency you add in C or C++ is essentially a binary with some header files. Not hard. Getting those can be hard if you compile from source. Keeping those in sync and invariant is impossible - there’s no single source of truth, the version of Boost you have may differ from your coworker’s. You end up rolling your own dependency management system, or (more commonly) restricting to a single platform so it’s easier to reason about what’s there (eg. we only run on Linux because we know that libboost-dev is the dependency, or we only support Windows because all you do is use this installer…) It’s a huge distraction from what you actually want to be doing (writing software). The Node/Ruby concept of a file describing your dependencies, a lock file describing unique releases of those dependencies, and a single source of truth for fetching them, is simply incredible by comparison. I think it’ll be interesting to see if the highly standardized world of Cargo is sufficient, or maybe we’ll see some systems projects revert back to make and rustc for some reason of feature which cargo doesn’t support. Time will tell. As much crap as I give the C and C++ way of doing things, it works. It’s built some of the most incredible software the world has ever seen. It’s obviously productive. Can Cargo be more productive, without hamstringing developers. I think yes, but again, time will tell.
You don't have to rewrite the whole program using `rust-cpython`. You can just write a bindings crate which depends on your crate and `rust-cpython`. What `rust-cpython` does is compile-time generation of the glue between the Rust and Python type systems, so you can write Rust code which returns Python types (or Rust types that have had the relevant conversion traits implemented) and then compile it into a Python module without any need to use `unsafe` or manually audit it for correctness. Here's an example of such a wrapper function from my code. fn py_normalize_whitespace(_: Python, in_str: &amp;str) -&gt; PyResult&lt;String&gt; { Ok(normalize_whitespace(in_str).into_owned()) } (The `Python` object serves as a handle for Python's GIL (Global Interpreter Lock) and `PyResult` represents the fact that any value in Python can be `None`.) I then add it to the tree of modules the `.so` file will export using this code: let py_naming = PyModule::new(py, "naming")?; py_naming.add(py, "normalize_whitespace", py_fn!(py, py_normalize_whitespace(in_str: &amp;str)))?; (The PyO3 fork of rust-cpython has a nicer API, but it currently requires nightly Rust.)
thanks! just update the post.
can you talk a little bit about what this is doing or enables people to do? To be honest i have no clue what this is for or what this is doing. i was looking for it on the npm package site and your github repo and the only thing rust related – as far a i can tell – is curl https://sh.rustup.rs -sSf | sh source $HOME/.cargo/env so to give you an honest feedback, i need to know what i am looking at – i don't have the time to look through all that javascript code to know what you did there.
The global caching is frustrated by each dependency really being the product of its code compiled against the code of its dependencies within the current closure. It would be very difficult to have an effective global cache, unless we had a kind of global closure to inherit dependencies from. The two versions of the same library problem is difficult to solve, you’d want some way to resolve a dependency conflict, and I use a system which supports this at work, but it really encourages abandoned software to never get a new maintainer to own validating that the dependency still works.
&gt; It would be very difficult to have an effective global cache cabal does it. &gt; The two versions of the same library problem is difficult to solve Cabal isn't perfect but it has a better solution, namely: don't depend on two different versions of the same library.